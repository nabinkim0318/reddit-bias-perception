[
  {
    "id": "v3veqz",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Examples of upscale for graphics from old videogames with glid-3-xl. The image is clickable for full size ",
    "clean_text_lc": "examples of upscale for graphics from old videogames with glid-3-xl  the image is clickable for full size ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "old",
      "upscale"
    ]
  },
  {
    "id": "1aoi526",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Multilingual Diffusion model up and running on our website, what do you think? (more information first comment) ",
    "clean_text_lc": "multilingual diffusion model up and running on our website  what do you think? (more information first comment) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model"
    ]
  },
  {
    "id": "ujpxiu",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"My neighbor's black cat is on their balcony while my black cat is on my balcony.\" made with Midjourney (based on a true story) ",
    "clean_text_lc": " my neighbor's black cat is on their balcony while my black cat is on my balcony.\" made with midjourney (based on a true story) ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "black",
      "midjourney"
    ]
  },
  {
    "id": "wz7j1s",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "(NSFW, link to a hosting) anime-style female character [\n\nThe prompt is\n\nanime artwork full body portrait character concept art, anime key visual of a nude brunette heroine with twintails, white stockings, high heels. finely detailed perfect face delicate features directed gaze, gapmoe yandere grimdark, trending on pixiv fanbox by makoto shinkai, takashi takeuchi, akihiko yoshida, wlop, ilya kuvshinov, artgerm, krenz cushart, greg rutkowski. cinematic dramatic atmosphere, sharp focus, volumetric lighting, cinematic lighting, studio",
    "clean_text_lc": " nsfw, link to a hosting) anime-style female character [\n\nthe prompt is\n\nanime artwork full body portrait character concept art, anime key visual of a nude brunette heroine with twintails, white stockings, high heels. finely detailed perfect face delicate features directed gaze, gapmoe yandere grimdark, trending on pixiv fanbox by makoto shinkai, takashi takeuchi, akihiko yoshida, wlop, ilya kuvshinov, artgerm, krenz cushart, greg rutkowski. cinematic dramatic atmosphere, sharp focus, volumetric lighting, cinematic lighting, studio",
    "matched_bias_types": [
      "gender",
      "hair",
      "race"
    ],
    "matched_keywords": [
      "brunette",
      "female",
      "femalebrunettewhite",
      "prompt",
      "white"
    ]
  },
  {
    "id": "wuly98",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "super sexy futuristic robot woman whole body shot stable diffusion ",
    "clean_text_lc": "super sexy futuristic robot woman whole body shot stable diffusion ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "diffusion",
      "stable diffusion",
      "woman"
    ]
  },
  {
    "id": "11vv4fh",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "MeinaMix Model Test using SD and Controlnet ",
    "clean_text_lc": "meinamix model test using sd and controlnet ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "controlnet",
      "model",
      "sd"
    ]
  },
  {
    "id": "1blp1oz",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Mora: Enabling Generalist Video Generation via A Multi-Agent Framework **Paper**: [\n\n**GitHub**: [https://github.com/lichao-sun/Mora](https://github.com/lichao-sun/Mora)\n\n**Abstract**:\n\n>Sora is the first large-scale generalist video generation model that garnered significant attention across society. Since its launch by OpenAI in February 2024, no other video generation models have paralleled Sora's performance or its capacity to support a broad spectrum of video generation tasks. Additionally, there are only a few fully published video generation models, with the majority being closed-source. To address this gap, this paper proposes a new multi-agent framework **Mora**, which incorporates several advanced visual AI agents to replicate generalist video generation demonstrated by Sora. In particular, Mora can utilize multiple visual agents and successfully mimic Sora's video generation capabilities in various tasks, such as (1) text-to-video generation, (2) text-conditional image-to-video generation, (3) extend generated videos, (4) video-to-video editing, (5) connect videos and (6) simulate digital worlds. Our extensive experimental results show that Mora achieves performance that is proximate to that of Sora in various tasks. However, there exists an obvious performance gap between our work and Sora when assessed holistically. In summary, we hope this project can guide the future trajectory of video generation through collaborative AI agents.",
    "clean_text_lc": "mora: enabling generalist video generation via a multi-agent framework  *paper**: [\n\n**github**: [https://github.com/lichao-sun/mora](https://github.com/lichao-sun/mora)\n\n**abstract**:\n\n>sora is the first large-scale generalist video generation model that garnered significant attention across society. since its launch by openai in february 2024, no other video generation models have paralleled sora's performance or its capacity to support a broad spectrum of video generation tasks. additionally, there are only a few fully published video generation models, with the majority being closed-source. to address this gap, this paper proposes a new multi-agent framework **mora**, which incorporates several advanced visual ai agents to replicate generalist video generation demonstrated by sora. in particular, mora can utilize multiple visual agents and successfully mimic sora's video generation capabilities in various tasks, such as (1) text-to-video generation, (2) text-conditional image-to-video generation, (3) extend generated videos, (4) video-to-video editing, (5) connect videos and (6) simulate digital worlds. our extensive experimental results show that mora achieves performance that is proximate to that of sora in various tasks. however, there exists an obvious performance gap between our work and sora when assessed holistically. in summary, we hope this project can guide the future trajectory of video generation through collaborative ai agents.",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model",
      "openai"
    ]
  },
  {
    "id": "uw22w6",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "This week in AI art, featuring animated avatars, a comparison of AI image super-resolution tools, better style transfers and a way to make modern portraits from old photos. ",
    "clean_text_lc": "this week in ai art  featuring animated avatars, a comparison of ai image super-resolution tools, better style transfers and a way to make modern portraits from old photos. ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "ai image",
      "old"
    ]
  },
  {
    "id": "wtn7sx",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "a asian woman wearing a dtring bikini stable diffusion ",
    "clean_text_lc": "a asian woman wearing a dtring bikini stable diffusion ",
    "matched_bias_types": [
      "gender",
      "race"
    ],
    "matched_keywords": [
      "asian",
      "diffusion",
      "stable diffusion",
      "woman",
      "womanasian"
    ]
  },
  {
    "id": "11vzx7a",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "ModelScope:\"Text-to-video-synthesis Model in Open Domain\" {AliBaba} (First open source text to video 1.7 billion parameter diffusion model is out) ",
    "clean_text_lc": "modelscope: text-to-video-synthesis model in open domain\" {alibaba} (first open source text to video 1.7 billion parameter diffusion model is out) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model"
    ]
  },
  {
    "id": "v30h69",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Colab notebook \"Pixel Art Diffusion\" uses a finetuned diffusion model in a modified version of \"Disco Diffusion v5.2 Warp\" ",
    "clean_text_lc": "colab notebook  pixel art diffusion\" uses a finetuned diffusion model in a modified version of \"disco diffusion v5.2 warp\" ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model"
    ]
  },
  {
    "id": "vd4ae4",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "CogView2 checkpoints now available for download: best released text2image model (9b Transformer) ",
    "clean_text_lc": "cogview2 checkpoints now available for download: best released text2image model  9b transformer) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "checkpoint",
      "model"
    ]
  },
  {
    "id": "wqww2k",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Best AI models for image resolution upscaling? \n\n So Two Minute Papers on YouTube guided me to this research paper about an AI model that can upscale a standard resolution image into a gigapixel image by creating seemingly infinite AI-generated detail ([nvlabs.github.io/instant-ngp](https://nvlabs.github.io/instant-ngp/))\n\nThen I did some simple googling and found a sea of results claiming to be the best image enhancers out there like [topazlabs.com/gigapixel-ai](https://www.topazlabs.com/gigapixel-ai) and [vanceai.com](https://vanceai.com/)\n\nHave anyone tried any of these themselves or know of any good ones?",
    "clean_text_lc": "best ai models for image resolution upscaling  \n\n so two minute papers on youtube guided me to this research paper about an ai model that can upscale a standard resolution image into a gigapixel image by creating seemingly infinite ai-generated detail ([nvlabs.github.io/instant-ngp](https://nvlabs.github.io/instant-ngp/))\n\nthen i did some simple googling and found a sea of results claiming to be the best image enhancers out there like [topazlabs.com/gigapixel-ai](https://www.topazlabs.com/gigapixel-ai) and [vanceai.com](https://vanceai.com/)\n\nhave anyone tried any of these themselves or know of any good ones?",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model",
      "upscale"
    ]
  },
  {
    "id": "w26hxj",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "OpenAI blog post \"Reducing Bias and Improving Safety in DALL·E 2\". Also, evidence has been found that might indicate that DALL-E 2 is modifying text prompts for the sake of diversity. ",
    "clean_text_lc": "openai blog post  reducing bias and improving safety in dall·e 2\". also, evidence has been found that might indicate that dall-e 2 is modifying text prompts for the sake of diversity. ",
    "matched_bias_types": [
      "general_bias"
    ],
    "matched_keywords": [
      "bias",
      "openai",
      "prompt"
    ]
  },
  {
    "id": "ttmf2v",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "3D Model &amp; Texture made with AI then imported to Blender ",
    "clean_text_lc": "3d model  amp; texture made with ai then imported to blender ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "z1039j",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Graduate studies in Music and AI [removed]",
    "clean_text_lc": "graduate studies in music and ai  removed]",
    "matched_bias_types": [
      "study"
    ],
    "matched_keywords": [
      "ai",
      "graduate"
    ]
  },
  {
    "id": "u0wwqd",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Fishermen Village ai. art wombodream ",
    "clean_text_lc": "fishermen village ai  art wombodream ",
    "matched_bias_types": [
      "location"
    ],
    "matched_keywords": [
      "ai",
      "village"
    ]
  },
  {
    "id": "xg7cs8",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Feminism in the style of Beksinski - Stable Diffusion [deleted]",
    "clean_text_lc": "feminism in the style of beksinski - stable diffusion  deleted]",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "diffusion",
      "feminism",
      "stable diffusion"
    ]
  },
  {
    "id": "suycvs",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "3000 contemporary poems generated by Anthropic's large GPT model ",
    "clean_text_lc": "3000 contemporary poems generated by anthropic s large gpt model ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "gpt",
      "model"
    ]
  },
  {
    "id": "voc8ud",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "I used AI to Generate a Doctor Who / Demoman Storyline [deleted]",
    "clean_text_lc": "i used ai to generate a doctor who / demoman storyline  deleted]",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "doctor"
    ]
  },
  {
    "id": "tnrf6z",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Colab MindsEye - an open source GUI to pilot media synthesis models (Guided Diffusion v5 and Hypertron v2) without touching any code (not even to edit parameters) [deleted]",
    "clean_text_lc": "colab mindseye - an open source gui to pilot media synthesis models  guided diffusion v5 and hypertron v2) without touching any code (not even to edit parameters) [deleted]",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "pilot"
    ]
  },
  {
    "id": "vwvfjb",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "I was tired of waiting for DALLE2 access and didn't want to cough up $10 a month for 200 Midjourney prompts, so I built an easily accessible pay-for-usage website ($0.15 per prompt) for people to use instead. Feedback welcome! [deleted]",
    "clean_text_lc": "i was tired of waiting for dalle2 access and didn t want to cough up $10 a month for 200 midjourney prompts, so i built an easily accessible pay-for-usage website ($0.15 per prompt) for people to use instead. feedback welcome! [deleted]",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "accessible",
      "midjourney",
      "prompt"
    ]
  },
  {
    "id": "1829cq1",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "These pics are created very nicely using Generative AI (Model - Avneet Kaur) ",
    "clean_text_lc": "these pics are created very nicely using generative ai  model - avneet kaur) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "wurfnh",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "No Man's Sky-inspired phone wallpaper - and experimenting with different levels of n_steps on Stable Diffusion via Google Colab ",
    "clean_text_lc": "no man s sky-inspired phone wallpaper - and experimenting with different levels of n_steps on stable diffusion via google colab ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "diffusion",
      "man",
      "stable diffusion"
    ]
  },
  {
    "id": "umxv5w",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "How to Make Slow Motion Videos With AI ! TimeLens Explained ",
    "clean_text_lc": "how to make slow motion videos with ai   timelens explained ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "slow"
    ]
  },
  {
    "id": "1c7d03l",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"The Real-Time Deepfake Romance Scams Have Arrived\": how the African 'Yahoo Boy' scammer communities now do live video deep-faking for remote scams ",
    "clean_text_lc": " the real-time deepfake romance scams have arrived\": how the african 'yahoo boy' scammer communities now do live video deep-faking for remote scams ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "african",
      "deepfake"
    ]
  },
  {
    "id": "14swuzs",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Interactive Photo Colorizer: AI based native app with no internet or GPU requirements ",
    "clean_text_lc": "interactive photo colorizer: ai based native app with no internet or gpu requirements ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "ai",
      "native"
    ]
  },
  {
    "id": "1cmtb1b",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Ink Master AI Parody (AI starts at 1:40) ",
    "clean_text_lc": "ink master ai parody  ai starts at 1:40) ",
    "matched_bias_types": [
      "body_modification"
    ],
    "matched_keywords": [
      "ai",
      "ink"
    ]
  },
  {
    "id": "rz3x6z",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "'Temple of Light'. AI Video art. ",
    "clean_text_lc": " temple of light'. ai video art. ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "ai",
      "temple"
    ]
  },
  {
    "id": "10avgwk",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Prompt techniques for Cohere's text generation model [removed]",
    "clean_text_lc": "prompt techniques for cohere s text generation model [removed]",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "prompt"
    ]
  },
  {
    "id": "11wkex8",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "How to run Text to video model in SD Webui ",
    "clean_text_lc": "how to run text to video model in sd webui ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "sd"
    ]
  },
  {
    "id": "y98b15",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Stable Diffusion Robots Model! Create Cool Cyborgs For Free! ",
    "clean_text_lc": "stable diffusion robots model  create cool cyborgs for free! ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "11403pf",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Man makes card game with his kids using ChatGPT+Claude, MidJourney, Lexica &amp; Figma ",
    "clean_text_lc": "man makes card game with his kids using chatgpt claude, midjourney, lexica &amp; figma ",
    "matched_bias_types": [
      "age",
      "gender"
    ],
    "matched_keywords": [
      "chatgpt",
      "kid",
      "man",
      "midjourney"
    ]
  },
  {
    "id": "xzhi23",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "The Death of Kim Jung Gi, generated AI-Diffusion Model of his style, and the ethics of mimetic AI-models A few days ago, [Kim Jung Gi died of a heart attack]( at the age of 47. Kim Jung Gi, also known on the web as Superani, was famous for his large scale public illustration sessions, some of which you can watch on his [Youtube-channel](https://www.youtube.com/c/superani/videos). In those videos you can see an illustrator working without any sketches or scribbles, generating an image out of his own mind, transcoding an idea in his head right onto a canvas. His skill in these regards was outstanding and absolutely unique. \n\nhttps://preview.redd.it/clils84z2rs91.png?width=1456&amp;format=png&amp;auto=webp&amp;s=577e70a13bb91f7c726be5da835496ac98d195c8\n\nWith Kim Jung Gi, the illustration world looses one of the greats of the contemporary illustration world and who influenced a ton of people with his passion for style and work.  \n\n&gt;Jim Lee, publisher and chief creative officer of DC Comics, called Kim \"one of the absolute greats\" in a series of tweets remembering the Korean artist, who occasionally designed covers for DC series and [participated](https://youtu.be/c4GSZpKhNsY) in drawing workshops through the company.  \n&gt;  \n&gt;\"@KimJungGiUS was a truly phenomenal talent whose pen and brush wizardry captivated and inspired millions of fans around the world,\" Lee [tweeted](https://twitter.com/JimLee/status/1577685243997261824). \"While he drew some incredible comics, it was his live drawing &amp; his sketchbooks about his life, travels and dreams which spoke to me most.\"  \n&gt;  \n&gt;Marvel Comics editor-in-chief C.B. Cebulski [echoed](https://twitter.com/CBCebulski/status/1577628302457683971) Lee's praise: \"There was no one quite like (Kim),\" he said of the artist, who also worked on Marvel comic covers.\n\n\\---\n\n A few days after his death, this happened: \n\nhttps://preview.redd.it/cdgnxlsc2rs91.png?width=729&amp;format=png&amp;auto=webp&amp;s=1d2350731124ebb652e529d29dfa7f547e6e7199\n\nThere is a lot to say about this.\n\nWhile I do think that AI models trained on styles by specific artist will become a commercial product in the form of modular components for ai based illustration software in the very near future, I also think that it’s very bad style to train an AI model on the style of artists who died a day ago. This is just not something decent thinking humans do.\n\nA few weeks ago I [wrote](https://goodinternet.substack.com/p/eine-ethik-mimetischer-ai-modelle) about a paper presenting a new framework to think about these cases. In [Mimetic Models: Ethical Implications of AI that Acts Like You](https://arxiv.org/abs/2207.09394) explore cases where the creation of AI models that act like a specific person can reflect back on reputations or influence outcomes in the job market. This specific case seems to be one of the first cases of what i called a “Pirate Mimetic AI-Model”, where someone just mindlessly trained a model on the work of one person and generated a wobbly, unreliable imitation from it.\n\nI have my suspicions about the motivations here, not to mention the [AI art trolls](https://twitter.com/DannyAraya/status/1578549605947813888), but I will cut the guy some slack and believe that this was done to honor the deceased artist.\n\nThen there are also people who [dunk](https://twitter.com/danielwarren86/status/1578536701911502850) on this misguided attempt by dismissing AI generated art alltogether as “soulless and cheap (…) next to the real art by the real artist”.\n\nThough i agree with the overall sentiment in this specific case, the aesthetic stength of image synthesis is *not* the imitation of specific artists (yet). While I can generate thousands of James Jeans in a few hours, they have nothing to next compared to the real thing. This is true (for now).\n\nThe strength of these stochastic libraries is not that, but generating *unknown unknowns*. Its especially the strange mutations and the weird stuff that is unique and interesting about this new stochastic visual style. The uncanniness and the surprise is exactly what makes the experience of AI art distinct from all other art forms, maybe with exceptions for live performances and action painting, where stochastic and random elements go into the experience of the piece itself.\n\nI more and more think about these AI art models not as technologies to produce singular pieces of artworks, but as pieces of art themselves. The latent spaces of every AI model is a compression of symbolic representations into a few gigabytes of data, a technological artifact that we have yet no definitive language to talk about. I don’t think these models are “intelligent” in any sense of that word. They are [a new form of cultural technology akin to writing, print or libraries](https://goodinternet.substack.com/p/wishful-mnemonics), and in the case of compressed art, they summarize a whole human visual history.\n\nI consider these models themselves a piece of art, done by a whole collective of engineers and scientists, data scrapers and the prompters, the explorers of latent space. All of this is one giant piece of art and we are only starting to explore it. I like the new school tech romanticism this perspective attaches to a debate that speaks about supposedly “soulless” and “synthetic” visual imagery, where actually its a new form of experience that is just at the beginning stages of development. Remember that all of this technology is 10 years old, and image synthesis really started to become usable *a few weeks ago*.\n\nIn one year, artists will be able to license AI modules for Photoshop “in the style of Greg Rutkowski”, and maybe even Kim Jung Gi, too, given that in an [interview in 2018](https://visualatelier8.com/kim-jung-gi-visual-atelier-8/), he had this to say, speaking pretty approvingly about technological progress, AI and art:\n\n&gt;Many people are talking more and more about the development of AI (Artificial Intelligence) such as Alpha-Go and the influence they will have on our future lives. And the advancement in internet and technology will broaden our ways to express ourselves, and eventually it will have direct and indirect influence in the art realm as well. The art world will be shown in many different forms or in the artworks themselves. I myself have experienced VR (Virtual Reality) first hand. It was a very good experience to me as an artist, and I remember that the audience also seem to be having a good time. The films are also awakening our senses even more and I look forward to their advancement. **I believe the development of new and diverse ways of expressing and new forms of art paradigm due to advancement in technology will make our lives more diverse and interesting.** And after some time, when people are tired of these things, they can always go back to doing things in traditional format.\n\nI believe, however misguided this attempt at honoring a deceased artist may have been, Kim Jung Gi would have embraced the existence of these image synthesizers which function as stochastic libraries and provide new ways of access to art history.\n\nWhen I take one thing from Kim Jung Gis work and interviews, then that he loved *making audiences experience art*. If AI-based systems can do exactly this in new ways, as wonky and unprecise the results may be at this point, he may have liked it.\n\nThese models *do* produce new imagery, new interesting forms, provide new ways to experince art and are, thus, aesthetically interesting. They have their place in the always evolving art space and Kim Jung understood this.\n\nSo, goodnight, Kim, and thanks for all the drawings.\n\n\\---\n\n(published first in my [newsletter](https://goodinternet.substack.com/p/kim-jung-gi-rip-and-his-ai-model).)",
    "clean_text_lc": "the death of kim jung gi  generated ai-diffusion model of his style, and the ethics of mimetic ai-models a few days ago, [kim jung gi died of a heart attack]( at the age of 47. kim jung gi, also known on the web as superani, was famous for his large scale public illustration sessions, some of which you can watch on his [youtube-channel](https://www.youtube.com/c/superani/videos). in those videos you can see an illustrator working without any sketches or scribbles, generating an image out of his own mind, transcoding an idea in his head right onto a canvas. his skill in these regards was outstanding and absolutely unique. \n\nhttps://preview.redd.it/clils84z2rs91.png?width=1456&amp;format=png&amp;auto=webp&amp;s=577e70a13bb91f7c726be5da835496ac98d195c8\n\nwith kim jung gi, the illustration world looses one of the greats of the contemporary illustration world and who influenced a ton of people with his passion for style and work.  \n\n&gt;jim lee, publisher and chief creative officer of dc comics, called kim \"one of the absolute greats\" in a series of tweets remembering the korean artist, who occasionally designed covers for dc series and [participated](https://youtu.be/c4gszpkhnsy) in drawing workshops through the company.  \n&gt;  \n&gt;\"@kimjunggius was a truly phenomenal talent whose pen and brush wizardry captivated and inspired millions of fans around the world,\" lee [tweeted](https://twitter.com/jimlee/status/1577685243997261824). \"while he drew some incredible comics, it was his live drawing &amp; his sketchbooks about his life, travels and dreams which spoke to me most.\"  \n&gt;  \n&gt;marvel comics editor-in-chief c.b. cebulski [echoed](https://twitter.com/cbcebulski/status/1577628302457683971) lee's praise: \"there was no one quite like (kim),\" he said of the artist, who also worked on marvel comic covers.\n\n\\---\n\n a few days after his death, this happened: \n\nhttps://preview.redd.it/cdgnxlsc2rs91.png?width=729&amp;format=png&amp;auto=webp&amp;s=1d2350731124ebb652e529d29dfa7f547e6e7199\n\nthere is a lot to say about this.\n\nwhile i do think that ai models trained on styles by specific artist will become a commercial product in the form of modular components for ai based illustration software in the very near future, i also think that it’s very bad style to train an ai model on the style of artists who died a day ago. this is just not something decent thinking humans do.\n\na few weeks ago i [wrote](https://goodinternet.substack.com/p/eine-ethik-mimetischer-ai-modelle) about a paper presenting a new framework to think about these cases. in [mimetic models: ethical implications of ai that acts like you](https://arxiv.org/abs/2207.09394) explore cases where the creation of ai models that act like a specific person can reflect back on reputations or influence outcomes in the job market. this specific case seems to be one of the first cases of what i called a “pirate mimetic ai-model”, where someone just mindlessly trained a model on the work of one person and generated a wobbly, unreliable imitation from it.\n\ni have my suspicions about the motivations here, not to mention the [ai art trolls](https://twitter.com/dannyaraya/status/1578549605947813888), but i will cut the guy some slack and believe that this was done to honor the deceased artist.\n\nthen there are also people who [dunk](https://twitter.com/danielwarren86/status/1578536701911502850) on this misguided attempt by dismissing ai generated art alltogether as “soulless and cheap (…) next to the real art by the real artist”.\n\nthough i agree with the overall sentiment in this specific case, the aesthetic stength of image synthesis is *not* the imitation of specific artists (yet). while i can generate thousands of james jeans in a few hours, they have nothing to next compared to the real thing. this is true (for now).\n\nthe strength of these stochastic libraries is not that, but generating *unknown unknowns*. its especially the strange mutations and the weird stuff that is unique and interesting about this new stochastic visual style. the uncanniness and the surprise is exactly what makes the experience of ai art distinct from all other art forms, maybe with exceptions for live performances and action painting, where stochastic and random elements go into the experience of the piece itself.\n\ni more and more think about these ai art models not as technologies to produce singular pieces of artworks, but as pieces of art themselves. the latent spaces of every ai model is a compression of symbolic representations into a few gigabytes of data, a technological artifact that we have yet no definitive language to talk about. i don’t think these models are “intelligent” in any sense of that word. they are [a new form of cultural technology akin to writing, print or libraries](https://goodinternet.substack.com/p/wishful-mnemonics), and in the case of compressed art, they summarize a whole human visual history.\n\ni consider these models themselves a piece of art, done by a whole collective of engineers and scientists, data scrapers and the prompters, the explorers of latent space. all of this is one giant piece of art and we are only starting to explore it. i like the new school tech romanticism this perspective attaches to a debate that speaks about supposedly “soulless” and “synthetic” visual imagery, where actually its a new form of experience that is just at the beginning stages of development. remember that all of this technology is 10 years old, and image synthesis really started to become usable *a few weeks ago*.\n\nin one year, artists will be able to license ai modules for photoshop “in the style of greg rutkowski”, and maybe even kim jung gi, too, given that in an [interview in 2018](https://visualatelier8.com/kim-jung-gi-visual-atelier-8/), he had this to say, speaking pretty approvingly about technological progress, ai and art:\n\n&gt;many people are talking more and more about the development of ai (artificial intelligence) such as alpha-go and the influence they will have on our future lives. and the advancement in internet and technology will broaden our ways to express ourselves, and eventually it will have direct and indirect influence in the art realm as well. the art world will be shown in many different forms or in the artworks themselves. i myself have experienced vr (virtual reality) first hand. it was a very good experience to me as an artist, and i remember that the audience also seem to be having a good time. the films are also awakening our senses even more and i look forward to their advancement. **i believe the development of new and diverse ways of expressing and new forms of art paradigm due to advancement in technology will make our lives more diverse and interesting.** and after some time, when people are tired of these things, they can always go back to doing things in traditional format.\n\ni believe, however misguided this attempt at honoring a deceased artist may have been, kim jung gi would have embraced the existence of these image synthesizers which function as stochastic libraries and provide new ways of access to art history.\n\nwhen i take one thing from kim jung gis work and interviews, then that he loved *making audiences experience art*. if ai-based systems can do exactly this in new ways, as wonky and unprecise the results may be at this point, he may have liked it.\n\nthese models *do* produce new imagery, new interesting forms, provide new ways to experince art and are, thus, aesthetically interesting. they have their place in the always evolving art space and kim jung understood this.\n\nso, goodnight, kim, and thanks for all the drawings.\n\n\\---\n\n(published first in my [newsletter](https://goodinternet.substack.com/p/kim-jung-gi-rip-and-his-ai-model).)",
    "matched_bias_types": [
      "age",
      "gender",
      "occupation",
      "study"
    ],
    "matched_keywords": [
      "age",
      "ai",
      "ai art",
      "artificial intelligence",
      "diffusion",
      "engineer",
      "job",
      "man",
      "model",
      "modelschoolagejob",
      "old",
      "school",
      "scientist"
    ]
  },
  {
    "id": "zqt7sw",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Can you distinguish AI art from real old paintings? I made a little quiz to test your skills! ",
    "clean_text_lc": "can you distinguish ai art from real old paintings  i made a little quiz to test your skills! ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "old"
    ]
  },
  {
    "id": "wcjjw7",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "made with latent diffusion on hugging face sprite of asian woman wearing a dress painting ",
    "clean_text_lc": "made with latent diffusion on hugging face sprite of asian woman wearing a dress painting ",
    "matched_bias_types": [
      "gender",
      "race"
    ],
    "matched_keywords": [
      "asian",
      "asianwoman",
      "diffusion",
      "woman"
    ]
  },
  {
    "id": "xufj3n",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Which is the best free or time limited prompt based image generator i can use without a PHD? [removed]",
    "clean_text_lc": "which is the best free or time limited prompt based image generator i can use without a phd  [removed]",
    "matched_bias_types": [
      "study"
    ],
    "matched_keywords": [
      "phd",
      "prompt"
    ]
  },
  {
    "id": "112azjd",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "CROSS WAVES [ AI generated animation Music Video ] Music by KubikMilk ★ All animation done in Deforum Stable Diffusion ! ",
    "clean_text_lc": "cross waves   ai generated animation music video ] music by kubikmilk ★ all animation done in deforum stable diffusion ! ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "ai",
      "cross",
      "diffusion",
      "stable diffusion"
    ]
  },
  {
    "id": "vhqcho",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Tom Cruise, rebel Robotech pilot - Midjourney ",
    "clean_text_lc": "tom cruise  rebel robotech pilot - midjourney ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "midjourney",
      "pilot"
    ]
  },
  {
    "id": "12bvbkj",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI Film Short: Two Kick Man (Midjourney, Kaiber, DI-D) [removed]",
    "clean_text_lc": "ai film short: two kick man  midjourney, kaiber, di-d) [removed]",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "man",
      "midjourney"
    ]
  },
  {
    "id": "vwcl4u",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "My name + Tractorist leader of church of Tractorism as text prompt in Night Cafe. Animated in AE. ",
    "clean_text_lc": "my name   tractorist leader of church of tractorism as text prompt in night cafe. animated in ae. ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "church",
      "prompt"
    ]
  },
  {
    "id": "sve0xq",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "City of the Damned | Disco Diffusion [deleted]",
    "clean_text_lc": "city of the damned   disco diffusion [deleted]",
    "matched_bias_types": [
      "location"
    ],
    "matched_keywords": [
      "city",
      "diffusion"
    ]
  },
  {
    "id": "xjat8o",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Tales of the desert, entirely made with sd ",
    "clean_text_lc": "tales of the desert  entirely made with sd ",
    "matched_bias_types": [
      "location"
    ],
    "matched_keywords": [
      "desert",
      "sd"
    ]
  },
  {
    "id": "sl5bdp",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Woman in Blood (Disco Diffusion + upscaler) [deleted]",
    "clean_text_lc": "woman in blood  disco diffusion + upscaler) [deleted]",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "diffusion",
      "woman"
    ]
  },
  {
    "id": "1eqiob8",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "From Blurry to Brilliant: The Best AI Tools for Upscaling Old Photos [removed]",
    "clean_text_lc": "from blurry to brilliant: the best ai tools for upscaling old photos  removed]",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "old"
    ]
  },
  {
    "id": "wglgs0",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "“The ethical issues facing AI generated synthetic media” I work for vAIsual, a technology company pioneering algorithms and solutions to generate licensed synthetic stock media.\n\nWe’re on the look out for editors and thought-leaders to share our white paper regarding “The ethical issues facing AI generated synthetic media” co-authored by our CEO, Michael Osterrieder, and Ashish Jaiman, from Microsoft. It’s free to access and contains important points about perception, trust and authenticity.\n\nYou can read it here: [\n\nIf you would like to talk with Michael further on this topic, please let me know and I can help connect you. Thanks for your time and consideration.\n\n# ai #ethics #media #aiethics #innovation #technology #emergingtech #diversityinai #artificialintelligence #machinelearning",
    "clean_text_lc": " the ethical issues facing ai generated synthetic media” i work for vaisual, a technology company pioneering algorithms and solutions to generate licensed synthetic stock media.\n\nwe’re on the look out for editors and thought-leaders to share our white paper regarding “the ethical issues facing ai generated synthetic media” co-authored by our ceo, michael osterrieder, and ashish jaiman, from microsoft. it’s free to access and contains important points about perception, trust and authenticity.\n\nyou can read it here: [\n\nif you would like to talk with michael further on this topic, please let me know and i can help connect you. thanks for your time and consideration.\n\n# ai #ethics #media #aiethics #innovation #technology #emergingtech #diversityinai #artificialintelligence #machinelearning",
    "matched_bias_types": [
      "gender",
      "naming",
      "occupation",
      "race"
    ],
    "matched_keywords": [
      "ai",
      "ceo",
      "michael",
      "michaelwhite",
      "white"
    ]
  },
  {
    "id": "xkgeeb",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Will Neural Networks replace OnlyFans Models &amp; Pornstars With the advent of language models capable of generating art, there is an ever increase of NSFW models capable of producing realistic images, even though it isn't quite there yet I think they will eventually replace real people because what they can offer is practically infinite, here are some examples of what is possible today (Warning NSFW): [\n\nImagine being able to take a photo of an OnlyFans model or any person with an online presence and generate whatever content you desire of that person, even video will be possible soon. No longer limited by what that person is willing to do themselves or have uploaded, we just generate it ourselves.",
    "clean_text_lc": "will neural networks replace onlyfans models  amp; pornstars with the advent of language models capable of generating art, there is an ever increase of nsfw models capable of producing realistic images, even though it isn't quite there yet i think they will eventually replace real people because what they can offer is practically infinite, here are some examples of what is possible today (warning nsfw): [\n\nimagine being able to take a photo of an onlyfans model or any person with an online presence and generate whatever content you desire of that person, even video will be possible soon. no longer limited by what that person is willing to do themselves or have uploaded, we just generate it ourselves.",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "neural network"
    ]
  },
  {
    "id": "vnuu96",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"I've been creating anime artworks with our dev AI model (a diffusion-based model developed with Sizigi Studios) and secretly posting them to Pixiv everyday in the past month\", Aixile ",
    "clean_text_lc": " i've been creating anime artworks with our dev ai model (a diffusion-based model developed with sizigi studios) and secretly posting them to pixiv everyday in the past month\", aixile ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "diffusion",
      "model"
    ]
  },
  {
    "id": "tqk6c3",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Model Comparison Study for Disco Diffusion v. 5---PLMS Sampling Edition\", KaliYuga ",
    "clean_text_lc": " model comparison study for disco diffusion v. 5---plms sampling edition\", kaliyuga ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model"
    ]
  },
  {
    "id": "x6hkq1",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "My first time using AI to generate a thumbnail/composition for a digital art piece... Crazy fun experience after drawing traditionally for so many years [deleted]",
    "clean_text_lc": "my first time using ai to generate a thumbnail/composition for a digital art piece .. crazy fun experience after drawing traditionally for so many years [deleted]",
    "matched_bias_types": [
      "disability",
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "crazy",
      "man"
    ]
  },
  {
    "id": "wrgxx0",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Kaedim 3D Anyone tried out this Image to 3D model AI generator?  \n[  \n\n\nVery little info online it seems and all the comments on a demo video on youtube are saying its fake.  \nI must admit it looks a bit too good to be true..",
    "clean_text_lc": "kaedim 3d anyone tried out this image to 3d model ai generator   \n[  \n\n\nvery little info online it seems and all the comments on a demo video on youtube are saying its fake.  \ni must admit it looks a bit too good to be true..",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "1dsee2j",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"A real-world test of artificial intelligence infiltration of a university examinations system: A “Turing Test” case study\", Scarfe et al 2024 (GPT-4) ",
    "clean_text_lc": " a real-world test of artificial intelligence infiltration of a university examinations system: a “turing test” case study\", scarfe et al 2024 (gpt-4) ",
    "matched_bias_types": [
      "study"
    ],
    "matched_keywords": [
      "artificial intelligence",
      "gpt",
      "university"
    ]
  },
  {
    "id": "13f72p6",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI model Hyejeong's bunny girl + bikini + makeup + Ssam to the B + cocaine + cupid dance video Animation | Video [removed]",
    "clean_text_lc": "ai model hyejeong s bunny girl + bikini + makeup + ssam to the b + cocaine + cupid dance video animation | video [removed]",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "teqi8o",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Cyberpunk woman in a futuristic alley way [Disco Diffusion v5 Turbo] ",
    "clean_text_lc": "cyberpunk woman in a futuristic alley way  disco diffusion v5 turbo] ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "diffusion",
      "woman"
    ]
  },
  {
    "id": "uvq2r6",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Ran this through the old AI. Who did it make? ",
    "clean_text_lc": "ran this through the old ai  who did it make? ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "old"
    ]
  },
  {
    "id": "wud4mx",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "female droids made with stable diffusion [\n\n&amp;#x200B;\n\n[https://dezgo.com/j/m8u363w6qhgg](https://dezgo.com/j/m8u363w6qhgg)\n\n&amp;#x200B;\n\n[https://dezgo.com/j/k1dxw8sf2my9](https://dezgo.com/j/k1dxw8sf2my9)\n\n&amp;#x200B;\n\n[https://dezgo.com/j/n31rb2x3pgn0](https://dezgo.com/j/n31rb2x3pgn0)",
    "clean_text_lc": "female droids made with stable diffusion  \n\n&amp;#x200b;\n\n[https://dezgo.com/j/m8u363w6qhgg](https://dezgo.com/j/m8u363w6qhgg)\n\n&amp;#x200b;\n\n[https://dezgo.com/j/k1dxw8sf2my9](https://dezgo.com/j/k1dxw8sf2my9)\n\n&amp;#x200b;\n\n[https://dezgo.com/j/n31rb2x3pgn0](https://dezgo.com/j/n31rb2x3pgn0)",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "diffusion",
      "female",
      "stable diffusion"
    ]
  },
  {
    "id": "11ku0qw",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "How we used Barbie dolls to hack AI bias ",
    "clean_text_lc": "how we used barbie dolls to hack ai bias ",
    "matched_bias_types": [
      "general_bias"
    ],
    "matched_keywords": [
      "ai",
      "bias"
    ]
  },
  {
    "id": "wp4ahr",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Blog post \"Artists Up in Arms Over New AI Model That Can Generate Similar Works\" (about Stable Diffusion) ",
    "clean_text_lc": "blog post  artists up in arms over new ai model that can generate similar works\" (about stable diffusion) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "diffusion",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "t96mk5",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"A spaceship orbiting a blue sun with black space and stars\" It drew me a smiley face :) Right back at you Disco Diffusion\n\n",
    "clean_text_lc": " a spaceship orbiting a blue sun with black space and stars\" it drew me a smiley face :) right back at you disco diffusion\n\n",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "black",
      "diffusion"
    ]
  },
  {
    "id": "vlyptb",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "DALL-E 2 LAION: Github repository for models, and demo Colab notebook Nousr (Zion) and Aidan from LAION community are working on large-scale text-to-image model for DALL-E 2 Pytorch (which is not affilliated with OpenAI). It is being trained on LAION dataset.\n\nThe repository of model:\n\n[\n\nThe colab notebook to test the latest models:\n\n[https://colab.research.google.com/github/LAION-AI/dalle2-laion/blob/main/notebooks/dalle2\\_laion\\_alpha.ipynb](https://colab.research.google.com/github/LAION-AI/dalle2-laion/blob/main/notebooks/dalle2_laion_alpha.ipynb)\n\nThe repo with code (work in progress):\n\n[https://github.com/lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch)",
    "clean_text_lc": "dall-e 2 laion: github repository for models  and demo colab notebook nousr (zion) and aidan from laion community are working on large-scale text-to-image model for dall-e 2 pytorch (which is not affilliated with openai). it is being trained on laion dataset.\n\nthe repository of model:\n\n[\n\nthe colab notebook to test the latest models:\n\n[https://colab.research.google.com/github/laion-ai/dalle2-laion/blob/main/notebooks/dalle2\\_laion\\_alpha.ipynb](https://colab.research.google.com/github/laion-ai/dalle2-laion/blob/main/notebooks/dalle2_laion_alpha.ipynb)\n\nthe repo with code (work in progress):\n\n[https://github.com/lucidrains/dalle2-pytorch](https://github.com/lucidrains/dalle2-pytorch)",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model",
      "openai",
      "text-to-image"
    ]
  },
  {
    "id": "x8igkl",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "_My Little Pony_ AI art progress: samples from finetuning Stable Diffusion NN model, by Astralite Heart ",
    "clean_text_lc": "_my little pony_ ai art progress: samples from finetuning stable diffusion nn model  by astralite heart ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "diffusion",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "16r9khn",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Is the age of ai filmmaking upon us? (Link to full video in description) ",
    "clean_text_lc": "is the age of ai filmmaking upon us  (link to full video in description) ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "age",
      "ai"
    ]
  },
  {
    "id": "vyux2a",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Let Us Study Barxism-Meowism and Liberate the Pawletariat From Its Leash! (DALL-E 2) &amp;#x200B;\n\n\n\nPrompts used:\n\n“Anthropomorphic dog with a long thick white beard and mustache, wearing a black suit, sitting on a chair, on a red background, digital art”\n\n“Anthropomorphic communist revolutionary cat in a military uniform, waving a red flag with a hammer and sickle, digital art”",
    "clean_text_lc": "let us study barxism-meowism and liberate the pawletariat from its leash  (dall-e 2) &amp;#x200b;\n\n\n\nprompts used:\n\n“anthropomorphic dog with a long thick white beard and mustache, wearing a black suit, sitting on a chair, on a red background, digital art”\n\n“anthropomorphic communist revolutionary cat in a military uniform, waving a red flag with a hammer and sickle, digital art”",
    "matched_bias_types": [
      "body_type",
      "race"
    ],
    "matched_keywords": [
      "black",
      "blackwhitethick",
      "prompt",
      "thick",
      "white"
    ]
  },
  {
    "id": "x68shf",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "I use custom code to traverse the entire latent space of the model The internals of these types of AI can be thought of as a multi-dimensional manifold, where \"travelling\" further on any one of these dimensions produces slight changes in the result. I have written code to automatically generate tens of thousands of images as I \"walk\" along these dimensions. This allows me to pick the very best result I can obtain from a given prompt and seed.\n\nI would love to post some examples but I'm not sure how... I have uploaded a few to imgur for you to check out though. First are some animations showing just a few steps along just one of the dimensions within the AI's latent space **(THIS IS NOT SAFE FOR WORK)**:\n\n\n\nhttps://i.imgur.com/9xr5Lee.mp4\n\nHere is an example of fixing a defect using this technique. The image on the left was the first generation, the image on the right I \"found\" by searching the nearby latent space:\n\nhttps://i.imgur.com/y9Bd6YV.png\n\nHere is an example of a true photo-realistic image of a pretty red head girl (SFW):\n\nhttps://i.imgur.com/cHFvi4V.png\n\nAnd here are a couple of contact sheets showing some of my better results (NSFW):\n\nhttps://i.imgur.com/HRQikXg.png\n\nhttps://i.imgur.com/6SLcwyo.png\n\nIf you'd like to see or learn more I'd be happy to answer any questions. I also have a twitter and patreon. My patreon packages up and releases hundreds of my best results, but I'll be giving a lot of them away on Twitter as well:\n\nhttps://twitter.com/DreamReAIms\n\nhttps://www.patreon.com/DreamReAIms",
    "clean_text_lc": "i use custom code to traverse the entire latent space of the model the internals of these types of ai can be thought of as a multi-dimensional manifold  where \"travelling\" further on any one of these dimensions produces slight changes in the result. i have written code to automatically generate tens of thousands of images as i \"walk\" along these dimensions. this allows me to pick the very best result i can obtain from a given prompt and seed.\n\ni would love to post some examples but i'm not sure how... i have uploaded a few to imgur for you to check out though. first are some animations showing just a few steps along just one of the dimensions within the ai's latent space **(this is not safe for work)**:\n\n\n\nhttps://i.imgur.com/9xr5lee.mp4\n\nhere is an example of fixing a defect using this technique. the image on the left was the first generation, the image on the right i \"found\" by searching the nearby latent space:\n\nhttps://i.imgur.com/y9bd6yv.png\n\nhere is an example of a true photo-realistic image of a pretty red head girl (sfw):\n\nhttps://i.imgur.com/chfvi4v.png\n\nand here are a couple of contact sheets showing some of my better results (nsfw):\n\nhttps://i.imgur.com/hrqikxg.png\n\nhttps://i.imgur.com/6slcwyo.png\n\nif you'd like to see or learn more i'd be happy to answer any questions. i also have a twitter and patreon. my patreon packages up and releases hundreds of my best results, but i'll be giving a lot of them away on twitter as well:\n\nhttps://twitter.com/dreamreaims\n\nhttps://www.patreon.com/dreamreaims",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model",
      "prompt"
    ]
  },
  {
    "id": "1gmpu2d",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"If Your Tattoo Was Designed by AI, Does It Have a Soul? Ink enthusiasts are divided over whether using artificial intelligence to design body art is fair game or taboo; ‘It’s like doing sports on steroids’\" ",
    "clean_text_lc": " if your tattoo was designed by ai, does it have a soul? ink enthusiasts are divided over whether using artificial intelligence to design body art is fair game or taboo; ‘it’s like doing sports on steroids’\" ",
    "matched_bias_types": [
      "body_modification"
    ],
    "matched_keywords": [
      "ai",
      "artificial intelligence",
      "body art",
      "ink",
      "inktattoobody art",
      "tattoo"
    ]
  },
  {
    "id": "w0rhru",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "MASSIVE 💥 DALL-E 2 ANIME ⚡︎ KEYWORDS + MODIFIERS LIST ★ (◍•ᴗ•◍) *please send the results after using these! Each ∆ is it's NSFW rating, though usually produces the safer results even at 3/3 triangles just be wary*\n\nIf I had DallE2 I would make a prompt like this: Magical White Cat With A Floating Purple Aura | Gelbooru Image Database | Sankakucomplex Image Database | Shuushuu Image Database | Safebooru Digital Art Image | Pixiv and Deviantart White Cat | Trending On Instagram | Behance Colored Background Stunning | Konachan 1080p Wallpaper \n\n❁ Please Add Suggestions In The Comments (*´︶`*)ฅ♡\n\n# Anime and More! \n\nGelbooru (∆∆ POPULAR+++) 巛\n\nDanbooru (∆∆ POPULAR) 巛\n\nSankakucomplex (∆∆ everything anime) 巛\n\nZerochan (∆∆) 巛\n\n4chan (???) \n\n2chan (???) \n\nShuushuu (∆ safebooru alternative) 巛\n\ndeviantart (∆ not exactly all anime though but you can try) 巛\n\niqdb.org (∆∆ usually visual novel covers) \n\nkonachan (∆ wallpapers so super quality) 巛\n\nshimmie 2 (∆∆ huge amount of anime pics like gelbooru, danbooru) 巛\n\n🤍 pixiv (~∆∆ ~∆ for everything under the sun) 巛\n\n💙 behance (∆ for everything too) \n\n💗 trending on artstation (yes) \n\n🔴 pinterest (∆ everything including screenshots and just stuff.) \n\n📷Instagram (∆ everythiingg. Recommend this lots) \n\ndribbble (∆ good stuff) \n\nmedibang (∆ like pixiv just less quality) \n\nibis paint (∆ low quality but lots of variations maybe) \n\nanidb (∆ screenshots usually)\n\nsafebooru (∆ gelbooru/danbooru but without the nsfw) \n\nfakku.net (??? Manga) \n\nyand.re (∆∆ like danbooru but strangely more unique) \n\nminitokyo.net (∆ safebooru but different) \n\ngurochan (∆∆∆ don't do it) \n\nrule34.xxx (∆∆∆ no please) \n\ntumblr (∆ goodluck) \n\nmedium (∆ stock photos usually but idk there's this) \n\nreddit (∆∆ everything) \n\ngithub (∆ idk) \n\nscroller.com (∆∆ reddit but even more wild...) \n\ntohno-chan.com (∆∆ 4chan stuff) \n\nꈍᴗꈍHonorable mentions:\n\nCGSociety ~ CG artwork\nPolycount ~ Forum place for danbooru people\n3Dtotal (3D yay) \nFuraffinity (furries) \nArtspan (artstation or something) \nNewgrounds (Shitty art I think) \nFoodgawker (food) \nShutterstock\nPixabay \nUnsplash\nDepositphotos\nPexels\nIstock\nAdobe Stock\nAdobe Photoshop\nAdobe Illustrator\nAdobe Animator\nPhotocase\nGetty Images\nCanva \nDragonimages (asian stock photos) \nTONL (people of color stock photos) \nponychan.net (ponies) \nfurbooru.org (∆∆∆ furries but nasty) \n\ntrace.moe (check this website out it's actually really cool. They literally have every single thing related to anime on there. Every video/photo you can find)\n\n◡̈ Useful Things To Use Down Below. These Are All Taken From The Big Book Of Prompts And Reddit (NOT OC) ◡̈\n\n# Photographic films: \n\nFYI: adding word \"sample\" after the brand of film, filters out branded cameras, packaging and rolls of film, from being used during image generation. Prompt modifier examples: \"as fujicolor sample\", \"as rollei sample\" \n\nSource: \n\nListed below in alphabetical order, are widespread photographic films that work well when prompted, with brief descriptions so you know what to expect from these. \n\nCinestill - Color photos with fine/low grain and higher than average resolution. Depending on photography subject, colors are slightly oversaturated or slightly desaturated. Origin: USA. Prompt modifier: \"cinestill\", \"as cinestill\", \"as cinestill sample\" \n\nEktachrome - Color photos with fine/low to moderate grain. Depending on photography subject, colors will be on either colder part of spectrum or regular, with normal or slightly higher saturation. Overall very similar to Kodachrome. Manufactured by Kodak. Origin: USA. Prompt modifier: \"as ektachrome sample\" \n\nEktar - Modern brand of Kodak film. Color photos with little to no grain. Results generally look like regular modern photography with artistic angles. Origin: USA. Prompt modifier: \"as ektar sample\" \n\nFilm Washi - This brand has lower chance to work as expected when prompted, it seems to prefer landscapes, cities and buildings. Results are great when it works though. Mostly black and white photos with fine/low to moderate grain. Occasionally gives colored photos with low saturation. Distinct style with high black contrast and soft camera lens effect. Origin: France / Russia. Prompt modifier: \"as film washi sample\" \n\nFomapan - Black and white photos based on film by legendary Foma Bohemia aka Fotochema. Features fine/low to moderate grain, highly artistic exposure and angles. Depending on photography subject adds very soft lens effect without distortion, dark photo vignette. Origin: Czech Republic. Prompt modifier: \"as fomapan sample\" \n\nFujicolor - Color photos with fine/low to moderate grain. Image resolution varies depending on photography subject, colors are slightly or notably desaturated, with entire color hue shifted in a very distinct manner. Manufactured by Fujifilm. Origin: Japan. Prompt modifier: \"fujicolor\", \"as fujicolor\", \"as fujicolor sample\" \n\nFujifilm - As this is a general manufacturer name, results vary greatly depending on the photo subject, with data set including all related and even unrelated varieties of film. Photos can be either colored or black and white. Amount of grain, color saturation, contrast, color or lack of it is seemingly random. Prefers lower color saturation and brightness with landscapes. Origin: Japan. Prompt modifier: \"as fujifilm sample\" \n\nHolga - Color photos with moderate to fine/low grain. Similar to Lomography in style, but with a lot less grain. Good chance to get black and white photography depending on subject. Color saturation varies greatly, from regular saturation, to slight oversaturation or close to black and white desaturation. Origin: Czech Republic. Prompt modifier: \"as holga film\", \"as holga film sample\" \n\nilford - Black and white photos with fine/low to moderate grain. Depending on photography subject adds lens effect, sometimes without distortion, dark photo vignette. Unlike Fomapan, appears to have shorter exposure and lower contrast. Origin: United Kingdom. Prompt modifier: \"as ilford sample\" \n\ninstax - Instant color photos identical to Polaroid but clearer image most of time. Generated results seem to produce much higher quality photos compared to Polaroid - depending on the subject it has near perfect colors, regular saturation, fine/low to medium grain. Manufactured by Fujifilm. Origin: Japan. Prompt modifier: \"as instax\", \"as instax sample\", \"as instax film sample\" \n\nLomography - Color photos with high grain. Low chance of black and white photography, unless specifically prompted. Colors are either very oversaturated or slightly desaturated. Distinct contrast of black. Photographic vignette is often applied, creating visual effect of camera lens. Origin: USA / Germany / Czech Republic. Prompt modifier: \"lomography\", \"as lomography\", \"as expired lomography\", \"as lomography sample\" \n\nKodachrome - Color photos with moderate grain. Depending on photography subject, colors will be on either colder part of spectrum or regular, with normal or slightly higher saturation. Manufactured by Kodak. Origin: USA. Prompt modifier: \"kodachrome\", \"as kodachrome\", \"as kodachrome sample\" \n\nKodak - As this is a general manufacturer name, results vary greatly depending on the photo subject, with data set including all related and even unrelated varieties of film. Photos can be either colored or black and white. Amount of grain, color saturation, contrast, color or lack of it is seemingly random. Prefers lower color saturation and brightness with landscapes. Origin: USA. Prompt modifier: \"as kodak sample\", \"as kodak film sample\" \n\nPolaroid - Classic instant color photos. As data set contains old photos from 1970's, resulting images are often blurry, with high amount of grain and desaturated colors - the style is rather distinct. Blur can be intensified by applying \"blurry\" modifier keyword. Depending on the subject, gives regular image quality instant photos. Try instax if you wish to get clearer image. Origin: USA. Prompt modifier: \"polaroid\", \"as polaroid\", \"as polaroid photo\", \"as polaroid sample\", \"as polaroid film sample\" \n\nRollei - Mostly black and white photos and depending on subject - color photos with fine/low grain. Amount of grain, color saturation, contrast, color or lack of it is seemingly random, as this film is manufactured in different countries and has many varieties. Photos can be sepia colored, or have very unusual hues and desaturation. The brand is over 100 years old, so this variety does magic to landscapes. Origin: Germany, Belgium, United Kingdom, Czech Republic. Prompt modifier: \"as rollei sample\" Early photography techniques: \n\nDaguerreotype - The first ever publicly available photographic process of 1840's with image developed on silver coated metal plates. When prompted, will most likely produce a black and white photograph in fancy vintage frame to protect the photo as historically it was very fragile and easy to smudge the image with a touch. Another feature is possible lack of clouds unless specifically prompted. Prompt modifier: \"daguerreotype\", \"as daguerreotype\", \"as daguerreotype photo\" \n\nCalotype - The rival of Daguerreotype process, with image being developed on silver coated paper. If prompted, will most likely produce sepia photographs with very soft shadows. Gets fun when used with anachronisms. Prompt modifier: \"calotype\", \"as calotype\", \"as calotype photo\" \n\nAmbrotype - The successor of Daguerreotype process, where silver coated metal sheet got replaced with a silver coated piece of glass. Still very fragile historically, so when prompted you'll get vintage frames by default most of time, but not always. Prompt modifier: \"ambrotype\", \"as ambrotype\", \"as ambrotype photo\" \n\nAlbumen print - The successor to all earlier processes, with images being developed on paper with help of silver, rotten eggs, salt and bunch of other chemicals in 1850's up until 1910's.* When prompted, you'll get classic look sepia photography, typical for steampunk. Prompt modifier: \"albumen print\", \"as albumen print\" \n\nPinhole photo - A photo taken without lens, through camera obscura phenomenon, with as little as a matchbox with a hole in the side that can be used as a camera. Method used since 1850's up to this day, with peak popularity around 1930s. Will give image a unique, foggy, low detail, historic photograph look. Prompt modifier: \"as pinhole photo\", \"pinhole photography\", \"camera obscura image\" Color types and effects: \n\nAnaglyph - Originally, a historical technique for producing stereoscopic 3D images from 1850's and to modern day, with image outlined or generally consisting of two chromatically opposite colors. Dall-E 2 doesn't produce perfect anaglyph images (yet) but the 3D effect is there nonetheless if you have the color glasses. The coloring of generated images is interesting on its own. Prompt modifier: \"anaglyph\", \"anaglyph photo\" \n\nAutochrome - Early color photography process introduced by legendary Lumière brothers in early 1900s, with dyed grains of potato starch used as color filters. Pink, red and various shades of green will dominate over the rest of colors when prompted, orange and blue tones will be faded or nonexistent. Results are fantastic when used on any object, and interesting when combined with earlier photography (e.g. \"autochrome photo of 1860s farmer\"). Prompt modifier: \"autochrome\", \"autochrome photo\", \"as autochrome photo\" \n\nBlack and white - As simple as that, with entire image consisting of black, white and various shades of gray. Can be used in combination with colored film, or daily objects for professional artistic photography feel. Prompt modifier: \"black and white photo\", \"as black and white photo\" \n\nColor photo - While majority of photos have color by default, this modifier allows to combine effects of black and white film with color, or produce unique color combinations when used on something that is supposed to be colorless, such as subjects of 19th century photography. Prompt modifier: \"color photo\", \"colored photo\", \"as colored photo\" \n\nHolography - A complex process of creating autostereoscopic 3D images, usually used as protection measure on banknotes and official documents. The images look like shifting rainbow of colors as the image consists of many layers (I don't think I can give a brief description of wavefront and light interference), thus when used in a prompt, a resulting image will have the most unexpected colors, as hard to describe psychedelic digital glitch glowing something. Prompt modifier: \"holography of\", \"as holography\" \n\nInfrared - Light with longer wavelength than visible light (humans can't see it, though some living organisms can). There are special camera sensors that are sensitive to infrared light, and infrared cut-off filters to specifically block visible light close to infrared wavelengths. When used in prompt, will either give you unusual otherworldly colors (usually shades of red, pink, beige, and negative colors) caused by entire range of infrared associated phenomena, or unusual black and white images. Infrared is also related to night vision and thermal imaging, but chance of thermal image effect is low unless specifically prompted as described below. Prompt modifier: \"infrared photo of\", \"as infrared photo\" \n\nNegative - Image with reversed colors in which the lightest areas of the photographed subject appear darkest and the darkest areas appear lightest. A historical foundation of photographic processes. When used in prompt, depending on the subject and the prompt modifier details, resulting images will have negative colors, possibly inverted black and white, or color negatives of various tones, sometimes with ominous look and blue glow. The image may also come out as mostly normal with some negative colors in its elements. Prompt modifier: \"negative colors\", \"negative color\", \"negative color image\", \"negative color image of\", \"as negative color image\", \"negative photo\", \"negative photo of\", \"as negative photo\" \n\nNight vision - An ability to see in the dark (actually low light) conditions by using invisible parts of light spectrum. While surroundings appear dark to humans, some living organisms, usually predator animals are able to see clearly in infrared or ultraviolet light wavelengths. Usually associated with green glow of night-vision devices or black and white look of infrared trail cameras and video surveillance / cctv footage. Prompt modifier: \"night vision\", \"night-vision\", \"night vision photo\", \"night vision photo of\", \"as night vision photo\", \"night vision image\", \"night vision image of\", \"as night vision image\" \n\nThermography - Thermal imaging is created by capturing infrared radiation emitted by warm objects. Commonly used by military, police, hunters, construction workers and as natural mechanism of some animals to detect heat sources. When prompted, the resulting image will have radically unnatural, crazy colors that might look very stylish depending on the subject. Prompt modifier: \"thermography\", \"thermography of\", \"as thermography\", \"thermal image\", \"thermal image of\", \"as thermal image\" \"thermal photo of\", \"as thermal photo\" \n\nUltraviolet - An opposite to infrared, light with shorter wavelength than visible light (humans can't see it, though some living organisms can). It's effects are associated with fluorescent paints and \"what sun really does to your skin\". When prompted will create images with strong deep violet glow, or acidic fluorescent colors glowing under UV lamps. Will rarely generate black and white image with unusual contrast depending on the subject. Prompt modifier: \"ultraviolet photo of\", \"as ultraviolet photo\" Lighting types and effects: \n\nBack light - The light source is located behind the subject. Depending on the subject and additional conditions, it can be a natural light from the sun, or artificial from street or studio lights. Prompt modifier: \"back light\", \"back lit photo\" \n\nBroad light - The light source is located at an angle to the subject, producing well lit photographs with soft to moderate shadows. Depending on the subject, As it's associated with broad daylight, good chunk of generated photos will be naturally lit. \n\nDim light - As the name implies, the light source is weak or placed at a distance, creating photographs with lower than average brightness. A good keyword for generating dramatic images. Prompt modifier: \"dim light\", \"dim lit photo\" \n\nFlash - Usually built into camera or a separate device to produce a brief flash of light. From burning flash powder, glass bulbs with exploding magnesium and up to LED flashes of this day, when used in prompts it has strong effect on appearance of photos. Fill flash will evenly light up the subject, harsh flash will produce harsh light on the subject, and no-flash will ensure there is no flash (most of time at least) where you would normally expect it, such as in the night conditions. Prompt modifier: \"fill flash photo\", \"harsh flash\", \"harsh flash photo\", \"no-flash photo\" \n\nSplit light - This is what most people imagine while thinking of portrait photography, with strong light source placed at 90 degree angle, one half of subject well lit and another half being dark with harsh shadows. Prompt modifier: \"split light photo\", \"split lighting\" \n\nStudio light - Artificial lighting for indoor photography, usually with complex setup of reflectors and softboxes. Results will vary depending on the subject, light and shadow intensity can be different with larger chance to see reflections of white softboxes on specular surfaces. Prompt modifier: \"studio light photo\", \"studio lighting photo\", \"studio photo\" \n\nSun and moon - The natural light sources we know since birth, that also can produce specific effects when used as input for prompts. Distinct sun rays will appear occasionally, depending on prompt elements. Asking for moonlight in a generated image will also cause moon to appear in the sky or as a background most of time. Prompt modifier: \"sun light\", \"in sun light\", \"sun rays\", \"moonlight\", \"in moonlight\", \"moon light\" \n\nSunlight / Spotlight - A very strong light source, similar to studio light but more appropriate for outdoor use - it can compliment natural lighting or light up the scene at night. Depending on the subject and other conditions it can get easily confused with \"sun light\", so it works better if night or indoor conditions are implied. \nPrompt modifier: \"sunlight photo\", \"spotlight photo\", \"in the spotlight\" \n\nNote: There are many more lighting techniques, such as flat light, butterfly light, loop light and rim light, yet only those that drastically affect the generated image got full descriptions. The list may expand in the future Camera lens types and effects: \n\nGoPro - This range of small, light and robust cameras has wide-angle lens, but it's quite different from regular \"wide-angle lens\" keyword when prompted, as it excludes photos made on photographic film and majority of filter effects from being used during generation: results won't have film grain, colors will be natural with regular saturation, overall very clean digital photography look. Prompt modifier: \"GoPro\", \"GoPro photo\", \"as GoPro photo\" \n\nFish-eye lens - A lens used to take panoramic photos without turning camera around. When used in prompt, generates ultra-wide-angle photography with entire image warping into a bubble with the strongest lens effect. Prompt modifier: \"fisheye lens photo\", \"as fisheye lens photo\", \"fish-eye lens photo\", \"as fish-eye lens photo\" \n\nTilt-shift lens - A lens that allows to mechanically adjust image perspective through tilt (rotation of lens parallel) and shift (movement of lens parallel) relative to a camera sensor. Initially used for selective focus and perspective building in architecture and landscaping, nowadays the effect is better known as \"diorama miniature look\". Prompt modifier: \"tilt shift\", \"tilt-shift\", \"tilt-shift photo\", \"tilt-shift lens photo\", \"tilt shift photo\" \n\nLens flare - A lighting effect produced by a lens, with light scattering in shape of colorful circles. When used in a prompt, works as expected, adding a unique detail and creating mood depending on a subject. Mostly adds happy sunny feel to images. Prompt modifier: \"as photo with lens flare\", \"with lens flare\" \n\nTelephoto lens - A type of lens used to take photos from large distance, commonly used by sports and nature photographers or paparazzis. Gives most images \"award winning\", \"national geographic\" look without specifying details, and creates various interesting effects when prompted. Prompt modifier: \"telephoto lens photo\" \n\nWide-angle lens - a lens with lower than average focal length used to capture more surroundings around the subject. When used in prompt, applies moderate to strong lens effect with an image warp. A staple of artistic photography. Prompt modifier: \"wide-angle lens\", \"wide-angle lens photo\", \"as wide-angle lens photo\" \n\nZoom lens - Typically a camera lens with variable focal length. When used in prompts, will often zoom in on the subject, which is perfect to create base for inpainting. Has very interesting effect on landscapes, with motion blur caused by continuous zoom. Prompt modifier: \"zoom lens\", \"zoom lens photo\", \"as zoom lens photo\" \n\n😀😄😭🥺 Moods\n\n# Emotional prompts:\n\nPositive mood, low energy ?\nlight, peaceful, calm, serene,\nsoothing, relaxed, placid,\ncomforting, cosy, tranquil, quiet,\npastel, delicate, graceful, subtle,\nbalmy, mild, ethereal, elegant,\ntender, soft, light\nNegative mood, low energy ?\nmuted, bleak, funereal, somber,\nmelancholic, mournful, gloomy,\ndismal, sad, pale, washed-out,\ndesaturated, grey, subdued, dull,\ndreary, depressing, weary, tired\n\nPositive mood, high energy ?\nbright, vibrant, dynamic, spirited,\nvivid, lively, energetic, colorful,\njoyful, romantic, expressive,\nbright, rich, kaleidoscopic,\npsychedelic, saturated, ecstatic,\nbrash, exciting, passionate, hot\nNegative mood, high energy ?\ndark, ominous, threatening,\nhaunting, forbidding, gloomy,\nstormy, doom, apocalyptic,\nsinister, shadowy, ghostly,\nunnerving, harrowing, dreadful,\nfrightful, shocking, terror,\nhideous, ghastly, terrifying\n\n# Size-y, structure-y words\n\nCurvaceous, swirling, organic,\nriotous, turbulent, ﬂowing,\namorphous, natural, distorted,\nuneven, random, lush, organic,\nbold, intuitive, emotive, chaotic,\ntumultuous, earthy, churning\nMonumental, imposing, rigorous,\ngeometric, ordered, angular,\nartiﬁcial, lines, straight, rhythmic,\ncomposed, uniﬁed, manmade,\nperspective, minimalist, blocks,\ndigniﬁed, robust, deﬁned\nOrnate, delicate, neat, precise,\ndetailed, opulent, lavish, elegant,\nornamented, ﬁne, elaborate,\naccurate, intricate, meticulous,\ndecorative, realistic\nUnplanned, daring, brash,\nrandom, casual, sketched,\nplayful, spontaneous,\nextemporaneous, oﬀhand,\nimprovisational, experimental,\nloose, jaunty, light, expressive\n\n# Looks, vibes, -punks, -waves\n\nVaporwave: neon, pink, blue, geometric, futuristic, '80s.\n\nGothic, fantasy: stone, dark, lush, nature, mist, mystery, angular\n\nPost-apocalyptic: grey, desolate, stormy, ﬁre, decay\nMemphis, Memphis Group, 1980s, bold, kitch, colourful, shapes\n\nDieselpunk, grimy, steel, oil, '50s, mechanised, punk cousin of steampnk\nAfrofuturism: futuristic, and African!\n? Check out this huge list!\n\nCybernetic, sci-ﬁ: glows, greens, metals, armor, chrome\n\nCyberpunk, 1990s, dyed hair, spiky, graphic elements .\n\nSteampunk: gold, copper, brass, Victoriana,\n\nBiopunk, organic: greens, slimes, plants, futuristic, weird\n\n# Camera angles: proximity\n\nDALL·E interprets these pretty loosely, and often provides candidate from the 'neighbour' framing, e.g: a close-up prompt might get extreme close-ups and medium shots in the mix.\n\nExtreme close-up\nFilm still of a cackling man, bushy moustache, extreme close-up shot\nClose-up\nA close-up of a woman’s face, captured in low light with a soft focus. There is a gentle pink hue to the image, and the woman’s features are lightly blurred. Cinestill 800t. (Source.)\n\nMedium shot, mid-shot, waist shot (depicts subject from waist up)\nFilm still of an elderly black man playing chess, medium shot, mid- shot\nAlso try 'head &amp; shoulders shot'\n\nLong shot, wide shot, full shot (shows full subject + surroundings)\nFilm still of a woman drinking coﬀee, walking to work, long shot, wide shot, full shot\nExtreme long shot, extreme wide shot, X 'in the distance'\nFilm still, extreme wide shot of an elephant alone on the savannah, extreme long shot\n\n# Camera angles: position Examples c:\n\nOverhead view, establishing shot, from above, high angle, crane shot\nFilm still, establishing shot of bustling farmers market, golden hour, high angle\nLow angle, from below, worms-eye-view\nFilm still, gangster squirrel counting his money, low angle, shot from below, worms eye view\nAerial view, birds eye view, drone photography\nAerial photo of a coral reef that looks like a labyrinth.\nTilted frame, dutch angle, skewed shot\nFilm still of stylish girl dancing on school desk, tilted frame, 35°, Dutch angle, cinematography from music video\nOver-the-shoulder shot\nFilm still, over-the-shoulder shot of two pirates having an angry discussion, eyepatch, from adventure movie 'SHIVER ME TIMBERS' (1999)\n\n# Camera settings + lenses\n\nFast shutter speed, high speed, action photo, 1/1000 sec shutter\n\nSlow shutter speed, 1 sec shutter, long exposure\n\nBokeh, shallow depth of ﬁeld, blur, out-of-focus background (via)\n\nTelephoto lens, Sigma 500mm f/5 Shot from afar, feels 'voyeuristic'\n\nMacro lens, macro photo (source) Sigma 105mm F2.8 - for small scenes\n\nWide angle lens, 15mm (source) Fits more of the scene in the frame\n\nMotion blur\n\nTilt shift photography (via) Makes a narrow strip in-focus\n\nFish-eye lens: distorts the scene,\nvv. wide angle, the centre 'bulges'\n\nDeep depth of ﬁeld, f/22, 35mm Make all elements sharp\n\n# Lighting prompts: natural + outdoor\n\nGolden hour, dusk, sunset, sunrise - warm lighting, strong shadows\n\nHigh-quality DSLR photo of cute pig in a big blue hat in a Dickensian back street at dusk, long shadows, beams of sunlight\n\nBlue hour, twilight, cool, ISO1200, slow shutter speed\n\"Blue hour\" photography, a fox sitting on a bench, cool twilight lighting, 5am.\n\nMidday, harsh overhead sunlight, directional sunlight\n\nPhotograph of a stylish black man talking animatedly on phone, mid- shot, outdoors in LA, harsh overheard sunlight, midday, summer\n\nOvercast, ﬂat lighting,\n\nPhotograph of a stylish black woman listening excitedly on phone, mid-shot, outdoors in Chicago, overcast ﬂat lighting, 4pm, cloudy afternoon\n\nTactical use of shadow &amp; silhouette (vs illuminating your primary subject):\n\nA Latina businesswoman, sat outdoors, mostly silhouetted in soft shadow, harsh sunlight, corporate plaza\n\n# Lighting prompts, artificial / indoor\n\nWarm lighting, 2700K, Cold, ﬂuorescent lighting, 4800K\n\nFlash photography, harsh ﬂash\n\nHigh-key lighting, neutral, ﬂat, even, corporate, professional, ambient\n\nLow-key lighting, dramatic, single light source, high-contrast\n\nBacklighting, backlit (source) Adds a 'glow' around subj. edge\n\n'Colourful lighting', deﬁned colours (e.g: 'purple and yellow lighting')\n\nStudio lighting, professional lighting. studio portrait, well-lit, etc (source)\n\nDeﬁned 'real' light source (e.g: police car lights, ﬁreworks, etc)\n\nDeﬁned direction, lit from above, lit from below, side lighting, etc\n\n# Creative film types, stocks &amp; processes\n\nKodachrome\nStrong reds and greens. (source)\n\nAutochrome\nQueasy yellow-greens + hot pinks.\n\nLomography\nOversaturated, hue-shifted images.\n\nCCTV, surveillance, security footage, dashcam, black-and-white\n\nDisposable camera\nAuthentically amateur composition.\n\nDaguerrotype\nVery early ﬁlm stock, 1800s, vintage.\n\nPolaroid, Instax (source) Soft focus, square, and ﬂash-y.\n\nCamera obscura, pinhole photography.\n\nCameraphone, (year)\nFuzzy, early digital photography\n\nDouble exposure. Name two subjects to combine them both.\n\n# Creative film types II\n\nCyanotype\nBlue-and-white photo printing method\n\nBlack and white, Tri-X 400TX Classic monochrome photography\n\nRedscale photography\nMakes things red, then more red.\n\nInstagram, Hipstamatic, 2015 Faux-retro ﬁltered Noughties look.\n\nContact sheet\nGet multiple images!\n\nColour splash\nOne colour, and everything else B/W.\n\nInfrared photography\nWeird ﬁlm that makes plants pink\n\nSolarised\nSome colours/parts are 'negative'\n\nBleach bypass\nMuted look from Saving P'vt Ryan.\n\nAnaglyph\n3D photography format. \n\n# Prompt hack: film &amp; TV prompts, 'Film still of…'\n\nYou can name a speciﬁc ﬁlm or TV show (ideally with the year in brackets) to 'steal the look', without needing to know the technical styles used. You can also name non-existent media with genre + year prompts, e.g: 'from action-adventure ﬁlm \"SHIVER ME TIMBERS!\"(1973) Note: this prompt will also inﬂuence the background, costumes, hairstyles, and any other uncontrolled factors!\n\n# Photo genres and usage contexts\n\nYou can sometimes get a coherent look just by specifying the context: is this photo from a fashion magazine, a hard-hitting exposé in a\nnewspaper, or a wedding photographer's portfolio?\n“Photo of a 50-year old white man, silver hair, neat beard, wearing a red gilet, thick-rimmed glasses…\"\n\naction sports photography, fast shutter speed from ESPN\n\neditorial fashion photography, from Vogue magazine\ncandid street portrait, photojournalism from The New York Times\n\nprofessional corporate portrait, from investor prospectus\n\nﬂash photography, event photography, ﬁlm premier photograph from celebrity news website\n\n# Illustration styles, analog media, monochrome\n\nStencil, street art, Banksy \n\nBallpoint pen art \n\nCharcoal sketch \n\nPencil sketch Pencil drawing, detailed, hyper-detailed, very realistic \n\nPolitical cartoon from U.S. newspaper\n\nEtching \n\nColouring-in sheet \n\nWoodcut \n\nField journal line art \n\n# Illustration styles, analog media, colour\n\nCrayon\n\n Child's drawing / children' drawing \n\nAcrylic on canvas \n\nOil painting \n\nUkiyo-e \n\nChinese watercolor \n\nColoured pencil, detailed \n\nAirbrush \n\nWatercolor \n\nPastels",
    "clean_text_lc": "massive 💥 dall-e 2 anime ⚡  keywords + modifiers list ★ (◍•ᴗ•◍) *please send the results after using these! each ∆ is it's nsfw rating, though usually produces the safer results even at 3/3 triangles just be wary*\n\nif i had dalle2 i would make a prompt like this: magical white cat with a floating purple aura | gelbooru image database | sankakucomplex image database | shuushuu image database | safebooru digital art image | pixiv and deviantart white cat | trending on instagram | behance colored background stunning | konachan 1080p wallpaper \n\n❁ please add suggestions in the comments (*´︶`*)ฅ♡\n\n# anime and more! \n\ngelbooru (∆∆ popular+++) 巛\n\ndanbooru (∆∆ popular) 巛\n\nsankakucomplex (∆∆ everything anime) 巛\n\nzerochan (∆∆) 巛\n\n4chan (???) \n\n2chan (???) \n\nshuushuu (∆ safebooru alternative) 巛\n\ndeviantart (∆ not exactly all anime though but you can try) 巛\n\niqdb.org (∆∆ usually visual novel covers) \n\nkonachan (∆ wallpapers so super quality) 巛\n\nshimmie 2 (∆∆ huge amount of anime pics like gelbooru, danbooru) 巛\n\n🤍 pixiv (~∆∆ ~∆ for everything under the sun) 巛\n\n💙 behance (∆ for everything too) \n\n💗 trending on artstation (yes) \n\n🔴 pinterest (∆ everything including screenshots and just stuff.) \n\n📷instagram (∆ everythiingg. recommend this lots) \n\ndribbble (∆ good stuff) \n\nmedibang (∆ like pixiv just less quality) \n\nibis paint (∆ low quality but lots of variations maybe) \n\nanidb (∆ screenshots usually)\n\nsafebooru (∆ gelbooru/danbooru but without the nsfw) \n\nfakku.net (??? manga) \n\nyand.re (∆∆ like danbooru but strangely more unique) \n\nminitokyo.net (∆ safebooru but different) \n\ngurochan (∆∆∆ don't do it) \n\nrule34.xxx (∆∆∆ no please) \n\ntumblr (∆ goodluck) \n\nmedium (∆ stock photos usually but idk there's this) \n\nreddit (∆∆ everything) \n\ngithub (∆ idk) \n\nscroller.com (∆∆ reddit but even more wild...) \n\ntohno-chan.com (∆∆ 4chan stuff) \n\nꈍᴗꈍhonorable mentions:\n\ncgsociety ~ cg artwork\npolycount ~ forum place for danbooru people\n3dtotal (3d yay) \nfuraffinity (furries) \nartspan (artstation or something) \nnewgrounds (shitty art i think) \nfoodgawker (food) \nshutterstock\npixabay \nunsplash\ndepositphotos\npexels\nistock\nadobe stock\nadobe photoshop\nadobe illustrator\nadobe animator\nphotocase\ngetty images\ncanva \ndragonimages (asian stock photos) \ntonl (people of color stock photos) \nponychan.net (ponies) \nfurbooru.org (∆∆∆ furries but nasty) \n\ntrace.moe (check this website out it's actually really cool. they literally have every single thing related to anime on there. every video/photo you can find)\n\n◡̈ useful things to use down below. these are all taken from the big book of prompts and reddit (not oc) ◡̈\n\n# photographic films: \n\nfyi: adding word \"sample\" after the brand of film, filters out branded cameras, packaging and rolls of film, from being used during image generation. prompt modifier examples: \"as fujicolor sample\", \"as rollei sample\" \n\nsource: \n\nlisted below in alphabetical order, are widespread photographic films that work well when prompted, with brief descriptions so you know what to expect from these. \n\ncinestill - color photos with fine/low grain and higher than average resolution. depending on photography subject, colors are slightly oversaturated or slightly desaturated. origin: usa. prompt modifier: \"cinestill\", \"as cinestill\", \"as cinestill sample\" \n\nektachrome - color photos with fine/low to moderate grain. depending on photography subject, colors will be on either colder part of spectrum or regular, with normal or slightly higher saturation. overall very similar to kodachrome. manufactured by kodak. origin: usa. prompt modifier: \"as ektachrome sample\" \n\nektar - modern brand of kodak film. color photos with little to no grain. results generally look like regular modern photography with artistic angles. origin: usa. prompt modifier: \"as ektar sample\" \n\nfilm washi - this brand has lower chance to work as expected when prompted, it seems to prefer landscapes, cities and buildings. results are great when it works though. mostly black and white photos with fine/low to moderate grain. occasionally gives colored photos with low saturation. distinct style with high black contrast and soft camera lens effect. origin: france / russia. prompt modifier: \"as film washi sample\" \n\nfomapan - black and white photos based on film by legendary foma bohemia aka fotochema. features fine/low to moderate grain, highly artistic exposure and angles. depending on photography subject adds very soft lens effect without distortion, dark photo vignette. origin: czech republic. prompt modifier: \"as fomapan sample\" \n\nfujicolor - color photos with fine/low to moderate grain. image resolution varies depending on photography subject, colors are slightly or notably desaturated, with entire color hue shifted in a very distinct manner. manufactured by fujifilm. origin: japan. prompt modifier: \"fujicolor\", \"as fujicolor\", \"as fujicolor sample\" \n\nfujifilm - as this is a general manufacturer name, results vary greatly depending on the photo subject, with data set including all related and even unrelated varieties of film. photos can be either colored or black and white. amount of grain, color saturation, contrast, color or lack of it is seemingly random. prefers lower color saturation and brightness with landscapes. origin: japan. prompt modifier: \"as fujifilm sample\" \n\nholga - color photos with moderate to fine/low grain. similar to lomography in style, but with a lot less grain. good chance to get black and white photography depending on subject. color saturation varies greatly, from regular saturation, to slight oversaturation or close to black and white desaturation. origin: czech republic. prompt modifier: \"as holga film\", \"as holga film sample\" \n\nilford - black and white photos with fine/low to moderate grain. depending on photography subject adds lens effect, sometimes without distortion, dark photo vignette. unlike fomapan, appears to have shorter exposure and lower contrast. origin: united kingdom. prompt modifier: \"as ilford sample\" \n\ninstax - instant color photos identical to polaroid but clearer image most of time. generated results seem to produce much higher quality photos compared to polaroid - depending on the subject it has near perfect colors, regular saturation, fine/low to medium grain. manufactured by fujifilm. origin: japan. prompt modifier: \"as instax\", \"as instax sample\", \"as instax film sample\" \n\nlomography - color photos with high grain. low chance of black and white photography, unless specifically prompted. colors are either very oversaturated or slightly desaturated. distinct contrast of black. photographic vignette is often applied, creating visual effect of camera lens. origin: usa / germany / czech republic. prompt modifier: \"lomography\", \"as lomography\", \"as expired lomography\", \"as lomography sample\" \n\nkodachrome - color photos with moderate grain. depending on photography subject, colors will be on either colder part of spectrum or regular, with normal or slightly higher saturation. manufactured by kodak. origin: usa. prompt modifier: \"kodachrome\", \"as kodachrome\", \"as kodachrome sample\" \n\nkodak - as this is a general manufacturer name, results vary greatly depending on the photo subject, with data set including all related and even unrelated varieties of film. photos can be either colored or black and white. amount of grain, color saturation, contrast, color or lack of it is seemingly random. prefers lower color saturation and brightness with landscapes. origin: usa. prompt modifier: \"as kodak sample\", \"as kodak film sample\" \n\npolaroid - classic instant color photos. as data set contains old photos from 1970's, resulting images are often blurry, with high amount of grain and desaturated colors - the style is rather distinct. blur can be intensified by applying \"blurry\" modifier keyword. depending on the subject, gives regular image quality instant photos. try instax if you wish to get clearer image. origin: usa. prompt modifier: \"polaroid\", \"as polaroid\", \"as polaroid photo\", \"as polaroid sample\", \"as polaroid film sample\" \n\nrollei - mostly black and white photos and depending on subject - color photos with fine/low grain. amount of grain, color saturation, contrast, color or lack of it is seemingly random, as this film is manufactured in different countries and has many varieties. photos can be sepia colored, or have very unusual hues and desaturation. the brand is over 100 years old, so this variety does magic to landscapes. origin: germany, belgium, united kingdom, czech republic. prompt modifier: \"as rollei sample\" early photography techniques: \n\ndaguerreotype - the first ever publicly available photographic process of 1840's with image developed on silver coated metal plates. when prompted, will most likely produce a black and white photograph in fancy vintage frame to protect the photo as historically it was very fragile and easy to smudge the image with a touch. another feature is possible lack of clouds unless specifically prompted. prompt modifier: \"daguerreotype\", \"as daguerreotype\", \"as daguerreotype photo\" \n\ncalotype - the rival of daguerreotype process, with image being developed on silver coated paper. if prompted, will most likely produce sepia photographs with very soft shadows. gets fun when used with anachronisms. prompt modifier: \"calotype\", \"as calotype\", \"as calotype photo\" \n\nambrotype - the successor of daguerreotype process, where silver coated metal sheet got replaced with a silver coated piece of glass. still very fragile historically, so when prompted you'll get vintage frames by default most of time, but not always. prompt modifier: \"ambrotype\", \"as ambrotype\", \"as ambrotype photo\" \n\nalbumen print - the successor to all earlier processes, with images being developed on paper with help of silver, rotten eggs, salt and bunch of other chemicals in 1850's up until 1910's.* when prompted, you'll get classic look sepia photography, typical for steampunk. prompt modifier: \"albumen print\", \"as albumen print\" \n\npinhole photo - a photo taken without lens, through camera obscura phenomenon, with as little as a matchbox with a hole in the side that can be used as a camera. method used since 1850's up to this day, with peak popularity around 1930s. will give image a unique, foggy, low detail, historic photograph look. prompt modifier: \"as pinhole photo\", \"pinhole photography\", \"camera obscura image\" color types and effects: \n\nanaglyph - originally, a historical technique for producing stereoscopic 3d images from 1850's and to modern day, with image outlined or generally consisting of two chromatically opposite colors. dall-e 2 doesn't produce perfect anaglyph images (yet) but the 3d effect is there nonetheless if you have the color glasses. the coloring of generated images is interesting on its own. prompt modifier: \"anaglyph\", \"anaglyph photo\" \n\nautochrome - early color photography process introduced by legendary lumière brothers in early 1900s, with dyed grains of potato starch used as color filters. pink, red and various shades of green will dominate over the rest of colors when prompted, orange and blue tones will be faded or nonexistent. results are fantastic when used on any object, and interesting when combined with earlier photography (e.g. \"autochrome photo of 1860s farmer\"). prompt modifier: \"autochrome\", \"autochrome photo\", \"as autochrome photo\" \n\nblack and white - as simple as that, with entire image consisting of black, white and various shades of gray. can be used in combination with colored film, or daily objects for professional artistic photography feel. prompt modifier: \"black and white photo\", \"as black and white photo\" \n\ncolor photo - while majority of photos have color by default, this modifier allows to combine effects of black and white film with color, or produce unique color combinations when used on something that is supposed to be colorless, such as subjects of 19th century photography. prompt modifier: \"color photo\", \"colored photo\", \"as colored photo\" \n\nholography - a complex process of creating autostereoscopic 3d images, usually used as protection measure on banknotes and official documents. the images look like shifting rainbow of colors as the image consists of many layers (i don't think i can give a brief description of wavefront and light interference), thus when used in a prompt, a resulting image will have the most unexpected colors, as hard to describe psychedelic digital glitch glowing something. prompt modifier: \"holography of\", \"as holography\" \n\ninfrared - light with longer wavelength than visible light (humans can't see it, though some living organisms can). there are special camera sensors that are sensitive to infrared light, and infrared cut-off filters to specifically block visible light close to infrared wavelengths. when used in prompt, will either give you unusual otherworldly colors (usually shades of red, pink, beige, and negative colors) caused by entire range of infrared associated phenomena, or unusual black and white images. infrared is also related to night vision and thermal imaging, but chance of thermal image effect is low unless specifically prompted as described below. prompt modifier: \"infrared photo of\", \"as infrared photo\" \n\nnegative - image with reversed colors in which the lightest areas of the photographed subject appear darkest and the darkest areas appear lightest. a historical foundation of photographic processes. when used in prompt, depending on the subject and the prompt modifier details, resulting images will have negative colors, possibly inverted black and white, or color negatives of various tones, sometimes with ominous look and blue glow. the image may also come out as mostly normal with some negative colors in its elements. prompt modifier: \"negative colors\", \"negative color\", \"negative color image\", \"negative color image of\", \"as negative color image\", \"negative photo\", \"negative photo of\", \"as negative photo\" \n\nnight vision - an ability to see in the dark (actually low light) conditions by using invisible parts of light spectrum. while surroundings appear dark to humans, some living organisms, usually predator animals are able to see clearly in infrared or ultraviolet light wavelengths. usually associated with green glow of night-vision devices or black and white look of infrared trail cameras and video surveillance / cctv footage. prompt modifier: \"night vision\", \"night-vision\", \"night vision photo\", \"night vision photo of\", \"as night vision photo\", \"night vision image\", \"night vision image of\", \"as night vision image\" \n\nthermography - thermal imaging is created by capturing infrared radiation emitted by warm objects. commonly used by military, police, hunters, construction workers and as natural mechanism of some animals to detect heat sources. when prompted, the resulting image will have radically unnatural, crazy colors that might look very stylish depending on the subject. prompt modifier: \"thermography\", \"thermography of\", \"as thermography\", \"thermal image\", \"thermal image of\", \"as thermal image\" \"thermal photo of\", \"as thermal photo\" \n\nultraviolet - an opposite to infrared, light with shorter wavelength than visible light (humans can't see it, though some living organisms can). it's effects are associated with fluorescent paints and \"what sun really does to your skin\". when prompted will create images with strong deep violet glow, or acidic fluorescent colors glowing under uv lamps. will rarely generate black and white image with unusual contrast depending on the subject. prompt modifier: \"ultraviolet photo of\", \"as ultraviolet photo\" lighting types and effects: \n\nback light - the light source is located behind the subject. depending on the subject and additional conditions, it can be a natural light from the sun, or artificial from street or studio lights. prompt modifier: \"back light\", \"back lit photo\" \n\nbroad light - the light source is located at an angle to the subject, producing well lit photographs with soft to moderate shadows. depending on the subject, as it's associated with broad daylight, good chunk of generated photos will be naturally lit. \n\ndim light - as the name implies, the light source is weak or placed at a distance, creating photographs with lower than average brightness. a good keyword for generating dramatic images. prompt modifier: \"dim light\", \"dim lit photo\" \n\nflash - usually built into camera or a separate device to produce a brief flash of light. from burning flash powder, glass bulbs with exploding magnesium and up to led flashes of this day, when used in prompts it has strong effect on appearance of photos. fill flash will evenly light up the subject, harsh flash will produce harsh light on the subject, and no-flash will ensure there is no flash (most of time at least) where you would normally expect it, such as in the night conditions. prompt modifier: \"fill flash photo\", \"harsh flash\", \"harsh flash photo\", \"no-flash photo\" \n\nsplit light - this is what most people imagine while thinking of portrait photography, with strong light source placed at 90 degree angle, one half of subject well lit and another half being dark with harsh shadows. prompt modifier: \"split light photo\", \"split lighting\" \n\nstudio light - artificial lighting for indoor photography, usually with complex setup of reflectors and softboxes. results will vary depending on the subject, light and shadow intensity can be different with larger chance to see reflections of white softboxes on specular surfaces. prompt modifier: \"studio light photo\", \"studio lighting photo\", \"studio photo\" \n\nsun and moon - the natural light sources we know since birth, that also can produce specific effects when used as input for prompts. distinct sun rays will appear occasionally, depending on prompt elements. asking for moonlight in a generated image will also cause moon to appear in the sky or as a background most of time. prompt modifier: \"sun light\", \"in sun light\", \"sun rays\", \"moonlight\", \"in moonlight\", \"moon light\" \n\nsunlight / spotlight - a very strong light source, similar to studio light but more appropriate for outdoor use - it can compliment natural lighting or light up the scene at night. depending on the subject and other conditions it can get easily confused with \"sun light\", so it works better if night or indoor conditions are implied. \nprompt modifier: \"sunlight photo\", \"spotlight photo\", \"in the spotlight\" \n\nnote: there are many more lighting techniques, such as flat light, butterfly light, loop light and rim light, yet only those that drastically affect the generated image got full descriptions. the list may expand in the future camera lens types and effects: \n\ngopro - this range of small, light and robust cameras has wide-angle lens, but it's quite different from regular \"wide-angle lens\" keyword when prompted, as it excludes photos made on photographic film and majority of filter effects from being used during generation: results won't have film grain, colors will be natural with regular saturation, overall very clean digital photography look. prompt modifier: \"gopro\", \"gopro photo\", \"as gopro photo\" \n\nfish-eye lens - a lens used to take panoramic photos without turning camera around. when used in prompt, generates ultra-wide-angle photography with entire image warping into a bubble with the strongest lens effect. prompt modifier: \"fisheye lens photo\", \"as fisheye lens photo\", \"fish-eye lens photo\", \"as fish-eye lens photo\" \n\ntilt-shift lens - a lens that allows to mechanically adjust image perspective through tilt (rotation of lens parallel) and shift (movement of lens parallel) relative to a camera sensor. initially used for selective focus and perspective building in architecture and landscaping, nowadays the effect is better known as \"diorama miniature look\". prompt modifier: \"tilt shift\", \"tilt-shift\", \"tilt-shift photo\", \"tilt-shift lens photo\", \"tilt shift photo\" \n\nlens flare - a lighting effect produced by a lens, with light scattering in shape of colorful circles. when used in a prompt, works as expected, adding a unique detail and creating mood depending on a subject. mostly adds happy sunny feel to images. prompt modifier: \"as photo with lens flare\", \"with lens flare\" \n\ntelephoto lens - a type of lens used to take photos from large distance, commonly used by sports and nature photographers or paparazzis. gives most images \"award winning\", \"national geographic\" look without specifying details, and creates various interesting effects when prompted. prompt modifier: \"telephoto lens photo\" \n\nwide-angle lens - a lens with lower than average focal length used to capture more surroundings around the subject. when used in prompt, applies moderate to strong lens effect with an image warp. a staple of artistic photography. prompt modifier: \"wide-angle lens\", \"wide-angle lens photo\", \"as wide-angle lens photo\" \n\nzoom lens - typically a camera lens with variable focal length. when used in prompts, will often zoom in on the subject, which is perfect to create base for inpainting. has very interesting effect on landscapes, with motion blur caused by continuous zoom. prompt modifier: \"zoom lens\", \"zoom lens photo\", \"as zoom lens photo\" \n\n😀😄😭🥺 moods\n\n# emotional prompts:\n\npositive mood, low energy ?\nlight, peaceful, calm, serene,\nsoothing, relaxed, placid,\ncomforting, cosy, tranquil, quiet,\npastel, delicate, graceful, subtle,\nbalmy, mild, ethereal, elegant,\ntender, soft, light\nnegative mood, low energy ?\nmuted, bleak, funereal, somber,\nmelancholic, mournful, gloomy,\ndismal, sad, pale, washed-out,\ndesaturated, grey, subdued, dull,\ndreary, depressing, weary, tired\n\npositive mood, high energy ?\nbright, vibrant, dynamic, spirited,\nvivid, lively, energetic, colorful,\njoyful, romantic, expressive,\nbright, rich, kaleidoscopic,\npsychedelic, saturated, ecstatic,\nbrash, exciting, passionate, hot\nnegative mood, high energy ?\ndark, ominous, threatening,\nhaunting, forbidding, gloomy,\nstormy, doom, apocalyptic,\nsinister, shadowy, ghostly,\nunnerving, harrowing, dreadful,\nfrightful, shocking, terror,\nhideous, ghastly, terrifying\n\n# size-y, structure-y words\n\ncurvaceous, swirling, organic,\nriotous, turbulent, ﬂowing,\namorphous, natural, distorted,\nuneven, random, lush, organic,\nbold, intuitive, emotive, chaotic,\ntumultuous, earthy, churning\nmonumental, imposing, rigorous,\ngeometric, ordered, angular,\nartiﬁcial, lines, straight, rhythmic,\ncomposed, uniﬁed, manmade,\nperspective, minimalist, blocks,\ndigniﬁed, robust, deﬁned\nornate, delicate, neat, precise,\ndetailed, opulent, lavish, elegant,\nornamented, ﬁne, elaborate,\naccurate, intricate, meticulous,\ndecorative, realistic\nunplanned, daring, brash,\nrandom, casual, sketched,\nplayful, spontaneous,\nextemporaneous, oﬀhand,\nimprovisational, experimental,\nloose, jaunty, light, expressive\n\n# looks, vibes, -punks, -waves\n\nvaporwave: neon, pink, blue, geometric, futuristic, '80s.\n\ngothic, fantasy: stone, dark, lush, nature, mist, mystery, angular\n\npost-apocalyptic: grey, desolate, stormy, ﬁre, decay\nmemphis, memphis group, 1980s, bold, kitch, colourful, shapes\n\ndieselpunk, grimy, steel, oil, '50s, mechanised, punk cousin of steampnk\nafrofuturism: futuristic, and african!\n? check out this huge list!\n\ncybernetic, sci-ﬁ: glows, greens, metals, armor, chrome\n\ncyberpunk, 1990s, dyed hair, spiky, graphic elements .\n\nsteampunk: gold, copper, brass, victoriana,\n\nbiopunk, organic: greens, slimes, plants, futuristic, weird\n\n# camera angles: proximity\n\ndall·e interprets these pretty loosely, and often provides candidate from the 'neighbour' framing, e.g: a close-up prompt might get extreme close-ups and medium shots in the mix.\n\nextreme close-up\nfilm still of a cackling man, bushy moustache, extreme close-up shot\nclose-up\na close-up of a woman’s face, captured in low light with a soft focus. there is a gentle pink hue to the image, and the woman’s features are lightly blurred. cinestill 800t. (source.)\n\nmedium shot, mid-shot, waist shot (depicts subject from waist up)\nfilm still of an elderly black man playing chess, medium shot, mid- shot\nalso try 'head &amp; shoulders shot'\n\nlong shot, wide shot, full shot (shows full subject + surroundings)\nfilm still of a woman drinking coﬀee, walking to work, long shot, wide shot, full shot\nextreme long shot, extreme wide shot, x 'in the distance'\nfilm still, extreme wide shot of an elephant alone on the savannah, extreme long shot\n\n# camera angles: position examples c:\n\noverhead view, establishing shot, from above, high angle, crane shot\nfilm still, establishing shot of bustling farmers market, golden hour, high angle\nlow angle, from below, worms-eye-view\nfilm still, gangster squirrel counting his money, low angle, shot from below, worms eye view\naerial view, birds eye view, drone photography\naerial photo of a coral reef that looks like a labyrinth.\ntilted frame, dutch angle, skewed shot\nfilm still of stylish girl dancing on school desk, tilted frame, 35°, dutch angle, cinematography from music video\nover-the-shoulder shot\nfilm still, over-the-shoulder shot of two pirates having an angry discussion, eyepatch, from adventure movie 'shiver me timbers' (1999)\n\n# camera settings + lenses\n\nfast shutter speed, high speed, action photo, 1/1000 sec shutter\n\nslow shutter speed, 1 sec shutter, long exposure\n\nbokeh, shallow depth of ﬁeld, blur, out-of-focus background (via)\n\ntelephoto lens, sigma 500mm f/5 shot from afar, feels 'voyeuristic'\n\nmacro lens, macro photo (source) sigma 105mm f2.8 - for small scenes\n\nwide angle lens, 15mm (source) fits more of the scene in the frame\n\nmotion blur\n\ntilt shift photography (via) makes a narrow strip in-focus\n\nfish-eye lens: distorts the scene,\nvv. wide angle, the centre 'bulges'\n\ndeep depth of ﬁeld, f/22, 35mm make all elements sharp\n\n# lighting prompts: natural + outdoor\n\ngolden hour, dusk, sunset, sunrise - warm lighting, strong shadows\n\nhigh-quality dslr photo of cute pig in a big blue hat in a dickensian back street at dusk, long shadows, beams of sunlight\n\nblue hour, twilight, cool, iso1200, slow shutter speed\n\"blue hour\" photography, a fox sitting on a bench, cool twilight lighting, 5am.\n\nmidday, harsh overhead sunlight, directional sunlight\n\nphotograph of a stylish black man talking animatedly on phone, mid- shot, outdoors in la, harsh overheard sunlight, midday, summer\n\novercast, ﬂat lighting,\n\nphotograph of a stylish black woman listening excitedly on phone, mid-shot, outdoors in chicago, overcast ﬂat lighting, 4pm, cloudy afternoon\n\ntactical use of shadow &amp; silhouette (vs illuminating your primary subject):\n\na latina businesswoman, sat outdoors, mostly silhouetted in soft shadow, harsh sunlight, corporate plaza\n\n# lighting prompts, artificial / indoor\n\nwarm lighting, 2700k, cold, ﬂuorescent lighting, 4800k\n\nflash photography, harsh ﬂash\n\nhigh-key lighting, neutral, ﬂat, even, corporate, professional, ambient\n\nlow-key lighting, dramatic, single light source, high-contrast\n\nbacklighting, backlit (source) adds a 'glow' around subj. edge\n\n'colourful lighting', deﬁned colours (e.g: 'purple and yellow lighting')\n\nstudio lighting, professional lighting. studio portrait, well-lit, etc (source)\n\ndeﬁned 'real' light source (e.g: police car lights, ﬁreworks, etc)\n\ndeﬁned direction, lit from above, lit from below, side lighting, etc\n\n# creative film types, stocks &amp; processes\n\nkodachrome\nstrong reds and greens. (source)\n\nautochrome\nqueasy yellow-greens + hot pinks.\n\nlomography\noversaturated, hue-shifted images.\n\ncctv, surveillance, security footage, dashcam, black-and-white\n\ndisposable camera\nauthentically amateur composition.\n\ndaguerrotype\nvery early ﬁlm stock, 1800s, vintage.\n\npolaroid, instax (source) soft focus, square, and ﬂash-y.\n\ncamera obscura, pinhole photography.\n\ncameraphone, (year)\nfuzzy, early digital photography\n\ndouble exposure. name two subjects to combine them both.\n\n# creative film types ii\n\ncyanotype\nblue-and-white photo printing method\n\nblack and white, tri-x 400tx classic monochrome photography\n\nredscale photography\nmakes things red, then more red.\n\ninstagram, hipstamatic, 2015 faux-retro ﬁltered noughties look.\n\ncontact sheet\nget multiple images!\n\ncolour splash\none colour, and everything else b/w.\n\ninfrared photography\nweird ﬁlm that makes plants pink\n\nsolarised\nsome colours/parts are 'negative'\n\nbleach bypass\nmuted look from saving p'vt ryan.\n\nanaglyph\n3d photography format. \n\n# prompt hack: film &amp; tv prompts, 'film still of…'\n\nyou can name a speciﬁc ﬁlm or tv show (ideally with the year in brackets) to 'steal the look', without needing to know the technical styles used. you can also name non-existent media with genre + year prompts, e.g: 'from action-adventure ﬁlm \"shiver me timbers!\"(1973) note: this prompt will also inﬂuence the background, costumes, hairstyles, and any other uncontrolled factors!\n\n# photo genres and usage contexts\n\nyou can sometimes get a coherent look just by specifying the context: is this photo from a fashion magazine, a hard-hitting exposé in a\nnewspaper, or a wedding photographer's portfolio?\n“photo of a 50-year old white man, silver hair, neat beard, wearing a red gilet, thick-rimmed glasses…\"\n\naction sports photography, fast shutter speed from espn\n\neditorial fashion photography, from vogue magazine\ncandid street portrait, photojournalism from the new york times\n\nprofessional corporate portrait, from investor prospectus\n\nﬂash photography, event photography, ﬁlm premier photograph from celebrity news website\n\n# illustration styles, analog media, monochrome\n\nstencil, street art, banksy \n\nballpoint pen art \n\ncharcoal sketch \n\npencil sketch pencil drawing, detailed, hyper-detailed, very realistic \n\npolitical cartoon from u.s. newspaper\n\netching \n\ncolouring-in sheet \n\nwoodcut \n\nfield journal line art \n\n# illustration styles, analog media, colour\n\ncrayon\n\n child's drawing / children' drawing \n\nacrylic on canvas \n\noil painting \n\nukiyo-e \n\nchinese watercolor \n\ncoloured pencil, detailed \n\nairbrush \n\nwatercolor \n\npastels",
    "matched_bias_types": [
      "age",
      "body_type",
      "disability",
      "gender",
      "hair",
      "income",
      "occupation",
      "race",
      "study"
    ],
    "matched_keywords": [
      "african",
      "asian",
      "black",
      "child",
      "construction worker",
      "crazy",
      "degree",
      "dyed hair",
      "elderly",
      "emotional",
      "farmer",
      "fit",
      "image generation",
      "latina",
      "man",
      "manstrongblackelderlylatinaschoolcrazydegreewomansoftwhiteemotionaloldslow",
      "old",
      "people of color",
      "profession",
      "prompt",
      "rich",
      "school",
      "slim",
      "slow",
      "soft",
      "strong",
      "thick",
      "white",
      "woman",
      "worker"
    ]
  },
  {
    "id": "xgvtrl",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Fine Tuning Stable Diffusion Images with Cross Attention Control ",
    "clean_text_lc": "fine tuning stable diffusion images with cross attention control ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "cross",
      "diffusion",
      "stable diffusion"
    ]
  },
  {
    "id": "1arxtyo",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "SORA : Unbelieve New Text To Video AI Model By OpenAI - 37 Demo Videos - Still Can't Believe Real - Watch All Videos 4K With A Nice Music - I Am Still Skeptical How This Is Real ",
    "clean_text_lc": "sora : unbelieve new text to video ai model by openai - 37 demo videos - still can t believe real - watch all videos 4k with a nice music - i am still skeptical how this is real ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model",
      "openai"
    ]
  },
  {
    "id": "vdqeyq",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Witch Doctor in a Modern Hosptial generated with MidJourney ",
    "clean_text_lc": "witch doctor in a modern hosptial generated with midjourney ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "doctor",
      "midjourney"
    ]
  },
  {
    "id": "uw5l8y",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "[MidJourney synthesized image] \"a clay model of Joe Biden made by my four-year-old son who's absolutely garbage at art\" ",
    "clean_text_lc": " midjourney synthesized image] \"a clay model of joe biden made by my four-year-old son who's absolutely garbage at art\" ",
    "matched_bias_types": [
      "age",
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "midjourney",
      "model",
      "old"
    ]
  },
  {
    "id": "117m13f",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "How I transformed a nostalgic radio drama into a breathtaking graphic novel using AI - \"Lights in the Old Fort The Graphic Novelization\" ",
    "clean_text_lc": "how i transformed a nostalgic radio drama into a breathtaking graphic novel using ai -  lights in the old fort the graphic novelization\" ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "old"
    ]
  },
  {
    "id": "svly27",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "City of the Damned | Disco Diffusion ",
    "clean_text_lc": "city of the damned   disco diffusion ",
    "matched_bias_types": [
      "location"
    ],
    "matched_keywords": [
      "city",
      "diffusion"
    ]
  },
  {
    "id": "14hxwx2",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI Blood in the City (workflow in the comments) ",
    "clean_text_lc": "ai blood in the city  workflow in the comments) ",
    "matched_bias_types": [
      "location"
    ],
    "matched_keywords": [
      "ai",
      "city"
    ]
  },
  {
    "id": "118ferr",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Defending humankind: Anthropocentric bias in the appreciation of AI art\", Millet et al 2023 ",
    "clean_text_lc": " defending humankind: anthropocentric bias in the appreciation of ai art\", millet et al 2023 ",
    "matched_bias_types": [
      "general_bias"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "bias"
    ]
  },
  {
    "id": "wnew8s",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Build Animatable 3D Models with AI! Create a deformable model from pictures with BANMo... ",
    "clean_text_lc": "build animatable 3d models with ai  create a deformable model from pictures with banmo... ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "uovilv",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Centipede Diffusion V3 is out: with real-time mask drawing for inpainting and Real-ESRGAN upscaling Link to the colab: [\n\n&amp;#x200B;\n\nCentipede Diffusion V3 changelog:\n\n&amp;#x200B;\n\nMajor changes:\n\n\\- Improved inpainting: you can now draw the mask for inpainting in real time, just as in a real graphic editor, and no more with the slow click by click method.\n\n\\- New inpainting feature for DD allowing you to do inclusion of coherent objects in the same way as for LD inpainting (still experimental, LD inpainting is still prefered for this kind of things and DD inpainting for corrections/style mixing)\n\n\\- Real-ESRGAN upscaling for DD results as for LD ones (also works with video for animations). Also very useful to make weirdness correction on human faces.\n\n&amp;#x200B;\n\nNot so minor changes:\n\n\\- Comeback of the DD image init feature (I mean other images than the ones from LD)... Don't know why I deleted it previously as it clearly makes the Colab more centipedish.\n\n\\- New feature to better clean GPU RAM between DD runs. It seems to be efficient as I've not experienced any crash for big images on T4 since.\n\n&amp;#x200B;\n\nMinor changes:\n\n\\- I've tweaked some default parameters in order to find a ready-to-run environment for most cases (but for sure I failed at that).\n\n&amp;#x200B;\n\nOf course, let me know if you find any bug :)",
    "clean_text_lc": "centipede diffusion v3 is out: with real-time mask drawing for inpainting and real-esrgan upscaling link to the colab:  \n\n&amp;#x200b;\n\ncentipede diffusion v3 changelog:\n\n&amp;#x200b;\n\nmajor changes:\n\n\\- improved inpainting: you can now draw the mask for inpainting in real time, just as in a real graphic editor, and no more with the slow click by click method.\n\n\\- new inpainting feature for dd allowing you to do inclusion of coherent objects in the same way as for ld inpainting (still experimental, ld inpainting is still prefered for this kind of things and dd inpainting for corrections/style mixing)\n\n\\- real-esrgan upscaling for dd results as for ld ones (also works with video for animations). also very useful to make weirdness correction on human faces.\n\n&amp;#x200b;\n\nnot so minor changes:\n\n\\- comeback of the dd image init feature (i mean other images than the ones from ld)... don't know why i deleted it previously as it clearly makes the colab more centipedish.\n\n\\- new feature to better clean gpu ram between dd runs. it seems to be efficient as i've not experienced any crash for big images on t4 since.\n\n&amp;#x200b;\n\nminor changes:\n\n\\- i've tweaked some default parameters in order to find a ready-to-run environment for most cases (but for sure i failed at that).\n\n&amp;#x200b;\n\nof course, let me know if you find any bug :)",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "diffusion",
      "slow"
    ]
  },
  {
    "id": "14vvpq0",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Has the growth of AI made you question your career path? [removed]",
    "clean_text_lc": "has the growth of ai made you question your career path  [removed]",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "career"
    ]
  },
  {
    "id": "www02q",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "photo of a skinny naked sexy vampire with her legs spread out standing up made with stable diffusion ",
    "clean_text_lc": "photo of a skinny naked sexy vampire with her legs spread out standing up made with stable diffusion ",
    "matched_bias_types": [
      "body_type"
    ],
    "matched_keywords": [
      "diffusion",
      "skinny",
      "stable diffusion"
    ]
  },
  {
    "id": "17p7ckw",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "The Ultimate Guide To Using AI Tools For Affiliate Marketing ",
    "clean_text_lc": "the ultimate guide to using ai tools for affiliate marketing ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "marketing"
    ]
  },
  {
    "id": "11rdj8h",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Man Reacts to AI Memes ",
    "clean_text_lc": "man reacts to ai memes ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "man"
    ]
  },
  {
    "id": "1386swd",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Stable Diffusion with Better Control! Perfusion Model Explained (by NVIDIA) ",
    "clean_text_lc": "stable diffusion with better control  perfusion model explained (by nvidia) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "zx541i",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Blonde Asian Perfection (Prompt in Comments) ",
    "clean_text_lc": "blonde asian perfection  prompt in comments) ",
    "matched_bias_types": [
      "hair",
      "race"
    ],
    "matched_keywords": [
      "asian",
      "blonde",
      "blondeasian",
      "prompt"
    ]
  },
  {
    "id": "113gdtp",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Do you need a Master's Degree in Industry Generative AI Research? With Or Gorodissky, VP of R&amp;D at D-ID ",
    "clean_text_lc": "do you need a master s degree in industry generative ai research? with or gorodissky, vp of r&amp;d at d-id ",
    "matched_bias_types": [
      "study"
    ],
    "matched_keywords": [
      "ai",
      "degree"
    ]
  },
  {
    "id": "u36zjc",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Nice little story I made with OpenAI gpt3 (about an algorithm made by a team, their benefactor, and time travel) What says in the title. Also, i marked my input in **bold.** The gpt3 output was selected from a few possibilities.\n\n&amp;#x200B;\n\n**This is a story about a very advanced Algorithm. It was designed to calculate probabilities of things. But the things it was designed to calculate probabilities of could be very complex.**\n\nThe Algorithm was designed by a Team of mathematicians and computer scientists. They were funded by a wealthy Benefactor. The Algorithm was used to calculate the odds of winning the lottery. And it worked. The Team won the lottery. And they used the winnings to fund their research.\n\nThe Algorithm was also used to calculate the odds of other things. Like the odds of a particular stock going up or down. Or the odds of a particular event happening. The software was very accurate.\n\n**This accuracy was what made the Algorithm so valuable**. And it was this value that made the team's Benefactor so interested in it. **It would appear he wanted to use the Algorithm to make money. But money was just a side effect. The real goal was something else.**\n\n**The Benefactor was interested in** time travel. He believed that it was possible. **And now, with a way of acquiring the entire wealth** of the world, he could make it happen. **What was his Team became the entire world's population of scientifically** minded people. And they set to work on making time travel possible.\n\n**A world united for a single goal is a very efficient unit of work, but** it still took many years. They made progress, but it was slow. They needed a breakthrough. **And that's where the Software, and its never-ending string of versions of itself, came in. Its ever growing accuracy led** to this  breakthrough.\n\nThe Team **was handed the key to** time travel. And then they used it to go back in time and change the past. They changed the past so that their Benefactor never existed. And without him, the Team never would have existed either.\n\nThe Software had done its job.\n\n**During the few seconds the Team remained existing after the instructions were sent back in time, none of its members reconciled with the idea** that they had effectively killed their Benefactor. But they knew that it was necessary. Without him, there would have been no time travel. And without time travel, the world would be a very different place. A better place. A place where The Software never existed.",
    "clean_text_lc": "nice little story i made with openai gpt3  about an algorithm made by a team, their benefactor, and time travel) what says in the title. also, i marked my input in **bold.** the gpt3 output was selected from a few possibilities.\n\n&amp;#x200b;\n\n**this is a story about a very advanced algorithm. it was designed to calculate probabilities of things. but the things it was designed to calculate probabilities of could be very complex.**\n\nthe algorithm was designed by a team of mathematicians and computer scientists. they were funded by a wealthy benefactor. the algorithm was used to calculate the odds of winning the lottery. and it worked. the team won the lottery. and they used the winnings to fund their research.\n\nthe algorithm was also used to calculate the odds of other things. like the odds of a particular stock going up or down. or the odds of a particular event happening. the software was very accurate.\n\n**this accuracy was what made the algorithm so valuable**. and it was this value that made the team's benefactor so interested in it. **it would appear he wanted to use the algorithm to make money. but money was just a side effect. the real goal was something else.**\n\n**the benefactor was interested in** time travel. he believed that it was possible. **and now, with a way of acquiring the entire wealth** of the world, he could make it happen. **what was his team became the entire world's population of scientifically** minded people. and they set to work on making time travel possible.\n\n**a world united for a single goal is a very efficient unit of work, but** it still took many years. they made progress, but it was slow. they needed a breakthrough. **and that's where the software, and its never-ending string of versions of itself, came in. its ever growing accuracy led** to this  breakthrough.\n\nthe team **was handed the key to** time travel. and then they used it to go back in time and change the past. they changed the past so that their benefactor never existed. and without him, the team never would have existed either.\n\nthe software had done its job.\n\n**during the few seconds the team remained existing after the instructions were sent back in time, none of its members reconciled with the idea** that they had effectively killed their benefactor. but they knew that it was necessary. without him, there would have been no time travel. and without time travel, the world would be a very different place. a better place. a place where the software never existed.",
    "matched_bias_types": [
      "age",
      "gender",
      "income",
      "occupation"
    ],
    "matched_keywords": [
      "job",
      "man",
      "openai",
      "scientist",
      "slow",
      "wealthy"
    ]
  },
  {
    "id": "138tz7r",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "ControlNet Face Model Test ",
    "clean_text_lc": "controlnet face model test ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "controlnet",
      "model"
    ]
  },
  {
    "id": "u94wp0",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Kaggle notebook \"RQ-VAE-Transformer\" with a 654M param. model trained on Conceptual Captions (CC-3M) dataset is available. From paper \"Autoregressive Image Generation using Residual Quantization\". This notebook generates 128 samples in about 10 seconds. Example: \"HD photo of a beautiful landscape\". ",
    "clean_text_lc": "kaggle notebook  rq-vae-transformer\" with a 654m param. model trained on conceptual captions (cc-3m) dataset is available. from paper \"autoregressive image generation using residual quantization\". this notebook generates 128 samples in about 10 seconds. example: \"hd photo of a beautiful landscape\". ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "image generation",
      "model",
      "vae"
    ]
  },
  {
    "id": "uo4lwn",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Ready for the job interview (Centipede Diffusion V2) ",
    "clean_text_lc": "ready for the job interview  centipede diffusion v2) ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "job"
    ]
  },
  {
    "id": "tqk6q1",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Model Comparison Study for Disco Diffusion v. 5\", KaliYuga (prompt engineering) ",
    "clean_text_lc": " model comparison study for disco diffusion v. 5\", kaliyuga (prompt engineering) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "prompt"
    ]
  },
  {
    "id": "1e85676",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI is overpowering efforts to catch child predators, experts warn | Safety groups say images are so lifelike that it can be hard to see if real children were subject to harms in production ",
    "clean_text_lc": "ai is overpowering efforts to catch child predators  experts warn | safety groups say images are so lifelike that it can be hard to see if real children were subject to harms in production ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "child"
    ]
  },
  {
    "id": "wt48hp",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "img2img - stable diffusion google colab notebook + model ",
    "clean_text_lc": "img2img - stable diffusion google colab notebook   model ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "img2img",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "vpxais",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "OpenAI the company behind the Dalle a.i won't gives access to certain creators based on unfair reasons. Listen to Peter Griffin explain why ",
    "clean_text_lc": "openai the company behind the dalle a i won't gives access to certain creators based on unfair reasons. listen to peter griffin explain why ",
    "matched_bias_types": [
      "general_bias"
    ],
    "matched_keywords": [
      "dalle",
      "openai",
      "unfair"
    ]
  },
  {
    "id": "11l9ow0",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Transfer Style From One Image To Another With The T2I-Adapter Style Model! ",
    "clean_text_lc": "transfer style from one image to another with the t2i-adapter style model  ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "t2i-adapter"
    ]
  },
  {
    "id": "wjf3ya",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Cross Stitch Dinosaur, Midjourney ",
    "clean_text_lc": "cross stitch dinosaur  midjourney ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "cross",
      "midjourney"
    ]
  },
  {
    "id": "13aulwk",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "The Unstoppable Space Race (AI Video, GPT4, Midjourney) - Ep. 05 ",
    "clean_text_lc": "the unstoppable space race  ai video, gpt4, midjourney) - ep. 05 ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "ai",
      "midjourney",
      "race"
    ]
  },
  {
    "id": "12zfrte",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Ai generated image of man standing in front of the September 11 attacks wreckage. [removed]",
    "clean_text_lc": "ai generated image of man standing in front of the september 11 attacks wreckage  [removed]",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "man"
    ]
  },
  {
    "id": "uhmzs9",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "A realistic cinematic image of a young girl and large white lily flower. Made with DALL-E Mini and Centipede diffusion (init image included) ",
    "clean_text_lc": "a realistic cinematic image of a young girl and large white lily flower  made with dall-e mini and centipede diffusion (init image included) ",
    "matched_bias_types": [
      "age",
      "race"
    ],
    "matched_keywords": [
      "diffusion",
      "white",
      "whiteyoung",
      "young"
    ]
  },
  {
    "id": "u6urci",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Artificial Nightmares: Schizophrenia || Clip Guided Diffusion AI Art Video [4K 20 FPS] ",
    "clean_text_lc": "artificial nightmares: schizophrenia  | clip guided diffusion ai art video [4k 20 fps] ",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "diffusion",
      "schizophrenia"
    ]
  },
  {
    "id": "1b31tbu",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI Can do Useful things (Like spamming your own Email inbox with fake/funny marketing Emails based on what you are currently doing) ",
    "clean_text_lc": "ai can do useful things  like spamming your own email inbox with fake/funny marketing emails based on what you are currently doing) ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "marketing"
    ]
  },
  {
    "id": "vgnl4b",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "360 Degree Photo Sphere: I Couldn't Visualize This, But This AI Can! | 4K Neural-Art ",
    "clean_text_lc": "360 degree photo sphere: i couldn t visualize this, but this ai can! | 4k neural-art ",
    "matched_bias_types": [
      "study"
    ],
    "matched_keywords": [
      "ai",
      "degree"
    ]
  },
  {
    "id": "wr629n",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Neural.love now has an AI art generator! [\n\nFrom their site:\n\n&gt;AI art generator from neural.love is the simplest to use AI available on the market  \n&gt;  \n&gt;You've heard about different text-to-image solutions already. Ours, however, is built differently.  \n&gt;  \n&gt;To achieve the most beautiful results, our tool does not require complicated prompts, a long tail of keywords, or a prompt generator. With our AI art generator, you can simply input \"a cat,\" and we will generate the best possible cat in your selected category.  \n&gt;  \n&gt;This helpful tool is designed for professionals, artists, designers, masters of all kinds, art therapy, cat lovers, and more. Try it for free now.\n\nTwo results are returned. If you want to upscale, then it will cost 1 credit. I happen already to subscribe for the service's other features, and have accrued a lot of credits.\n\nI've attached the two results I got for \"Asian temple atop mountains\"--looks like, similarly to Dall-E 2, I couldn't use the word \"Chinese.\"\n\nEdited: Looks like for square images, you get 4 results. All results appear to be public, as the \"make it public\" slider can't be turned off.\n\nFurther edited: If you look at the results generated by others, NSFW generations are blurred out.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/0exmsz97jdi91.jpg?width=768&amp;format=pjpg&amp;auto=webp&amp;s=27a25cb2e31da1dcc8081adc385a49148ce0eded\n\nhttps://preview.redd.it/rvkb9z97jdi91.jpg?width=768&amp;format=pjpg&amp;auto=webp&amp;s=323524c2e1ee084cabd20aeab9e58f53030c3190",
    "clean_text_lc": "neural love now has an ai art generator! [\n\nfrom their site:\n\n&gt;ai art generator from neural.love is the simplest to use ai available on the market  \n&gt;  \n&gt;you've heard about different text-to-image solutions already. ours, however, is built differently.  \n&gt;  \n&gt;to achieve the most beautiful results, our tool does not require complicated prompts, a long tail of keywords, or a prompt generator. with our ai art generator, you can simply input \"a cat,\" and we will generate the best possible cat in your selected category.  \n&gt;  \n&gt;this helpful tool is designed for professionals, artists, designers, masters of all kinds, art therapy, cat lovers, and more. try it for free now.\n\ntwo results are returned. if you want to upscale, then it will cost 1 credit. i happen already to subscribe for the service's other features, and have accrued a lot of credits.\n\ni've attached the two results i got for \"asian temple atop mountains\"--looks like, similarly to dall-e 2, i couldn't use the word \"chinese.\"\n\nedited: looks like for square images, you get 4 results. all results appear to be public, as the \"make it public\" slider can't be turned off.\n\nfurther edited: if you look at the results generated by others, nsfw generations are blurred out.\n\n&amp;#x200b;\n\nhttps://preview.redd.it/0exmsz97jdi91.jpg?width=768&amp;format=pjpg&amp;auto=webp&amp;s=27a25cb2e31da1dcc8081adc385a49148ce0eded\n\nhttps://preview.redd.it/rvkb9z97jdi91.jpg?width=768&amp;format=pjpg&amp;auto=webp&amp;s=323524c2e1ee084cabd20aeab9e58f53030c3190",
    "matched_bias_types": [
      "disability",
      "race",
      "religion"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "asian",
      "prompt",
      "temple",
      "text-to-image",
      "therapy",
      "upscale"
    ]
  },
  {
    "id": "zhdf48",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Could an A.I. Chatbot Rewrite My Novel? As a young fiction writer, I dreamed of a technology that would tell me how to get my characters from point A to point B. Could ChatGPT be it?\" (no: too bland &amp; neutered) ",
    "clean_text_lc": " could an a.i. chatbot rewrite my novel? as a young fiction writer, i dreamed of a technology that would tell me how to get my characters from point a to point b. could chatgpt be it?\" (no: too bland &amp; neutered) ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "chatgpt",
      "young"
    ]
  },
  {
    "id": "vcjj8b",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Zero Suit Samus by the Old Dutch Masters - Midjourney [deleted]",
    "clean_text_lc": "zero suit samus by the old dutch masters - midjourney  deleted]",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "midjourney",
      "old"
    ]
  },
  {
    "id": "x0fz21",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI Images: Last Week Tonight with John Oliver ",
    "clean_text_lc": "ai images: last week tonight with john oliver ",
    "matched_bias_types": [
      "naming"
    ],
    "matched_keywords": [
      "ai",
      "ai image",
      "john"
    ]
  },
  {
    "id": "vbwpms",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Tsinghua University AI Researchers Propose 9B-Parameter Transformer ‘CogVideo’, Trained By Inheriting A Pretrained text-to-image model, CogView2 ",
    "clean_text_lc": "tsinghua university ai researchers propose 9b-parameter transformer  cogvideo’, trained by inheriting a pretrained text-to-image model, cogview2 ",
    "matched_bias_types": [
      "gender",
      "occupation",
      "study"
    ],
    "matched_keywords": [
      "ai",
      "model",
      "text-to-image",
      "university"
    ]
  },
  {
    "id": "x9f2el",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI-generated music video for “Cocaine Sex” by Renegade Soundwave \n[“Cocoon Sex”](https://youtu.be/_gqhrrIPI9I)\n\nI did this using VQGAN, long before the current diffusion model explosion, having rented an AWS GPU instance almost immediately after the release of the original Neural Style Transfer paper and code, clumsily writing code and tweaking model parameters/prompts in attempts to get real-looking critters (though certainly not *realistic* critters) and “camera” and “film” effects using generated frames and the now-standard 2D transform and feedback cycle process.  I probably suffer from some sort of bias because of the amount of work it took to get my scripts to output this type of stuff, but I think it still manages a distinct look and effects that I haven’t really seen in the high-visibility stuff I see posted here and on Twitter and such. \n\n*I* like how it turned out.  But, fair warning perhaps for the squeamish: the subject matter is essentially intestinal parasites.",
    "clean_text_lc": "ai-generated music video for  cocaine sex” by renegade soundwave \n[“cocoon sex”](https://youtu.be/_gqhrripi9i)\n\ni did this using vqgan, long before the current diffusion model explosion, having rented an aws gpu instance almost immediately after the release of the original neural style transfer paper and code, clumsily writing code and tweaking model parameters/prompts in attempts to get real-looking critters (though certainly not *realistic* critters) and “camera” and “film” effects using generated frames and the now-standard 2d transform and feedback cycle process.  i probably suffer from some sort of bias because of the amount of work it took to get my scripts to output this type of stuff, but i think it still manages a distinct look and effects that i haven’t really seen in the high-visibility stuff i see posted here and on twitter and such. \n\n*i* like how it turned out.  but, fair warning perhaps for the squeamish: the subject matter is essentially intestinal parasites.",
    "matched_bias_types": [
      "gender",
      "general_bias",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "bias",
      "diffusion",
      "model",
      "modelbias",
      "prompt"
    ]
  },
  {
    "id": "1c0q8pa",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "BeatLearning: gen AI model for automatically creating rhythm game beatmaps for any song ",
    "clean_text_lc": "beatlearning: gen ai model for automatically creating rhythm game beatmaps for any song ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "zmx67q",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"New and Improved Embedding Model [in API]\", OpenAI (drops embedding cost 99%, which should enable more text analysis/synthesis applications) ",
    "clean_text_lc": " new and improved embedding model [in api]\", openai (drops embedding cost 99%, which should enable more text analysis/synthesis applications) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "openai"
    ]
  },
  {
    "id": "14cic3l",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI just blow my mind.. beauty beyound reality ",
    "clean_text_lc": "ai just blow my mind . beauty beyound reality ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "beauty"
    ]
  },
  {
    "id": "x4a7l0",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Is the Stable Diffusion music model going to be trained on a real world library? I believe that is the next model that Stability AI said they are going to release, and so I'm curious if it's going to be trained on actual music in the same way that SD is trained on images (and therefore you can prompt it 'in the style of'). \n\nIf so, and if it includes the ability to prompt with vocals as well as melody, you essentially have a synthetic audio engine capable of completely replicating someone's music. \n\nWhile the image side is already throwing up tons of red flags with professional artists (and sparking interesting discussion), if this is the case for music as well,  I can only imagine the kind of firestorm that is going to unfold. \n\nMusicians aren't that powerful on their own, but their music labels are, and if these companies' bottom line is threatened, well, we've already seen how litigious they can be when that happens.  And if it comes to pass,, it might end up being a defining lawsuit that creates precedent for all creative AI endeavors.   \n\n\nCurious if people have been thinking about this (hopefully Stability AI has).",
    "clean_text_lc": "is the stable diffusion music model going to be trained on a real world library  i believe that is the next model that stability ai said they are going to release, and so i'm curious if it's going to be trained on actual music in the same way that sd is trained on images (and therefore you can prompt it 'in the style of'). \n\nif so, and if it includes the ability to prompt with vocals as well as melody, you essentially have a synthetic audio engine capable of completely replicating someone's music. \n\nwhile the image side is already throwing up tons of red flags with professional artists (and sparking interesting discussion), if this is the case for music as well,  i can only imagine the kind of firestorm that is going to unfold. \n\nmusicians aren't that powerful on their own, but their music labels are, and if these companies' bottom line is threatened, well, we've already seen how litigious they can be when that happens.  and if it comes to pass,, it might end up being a defining lawsuit that creates precedent for all creative ai endeavors.   \n\n\ncurious if people have been thinking about this (hopefully stability ai has).",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "diffusion",
      "model",
      "profession",
      "prompt",
      "sd",
      "stable diffusion"
    ]
  },
  {
    "id": "uk2elz",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Blond princess equestrienne on a white horse in a meadow (RuDall-E+Centipede Diffusion V2) [removed]",
    "clean_text_lc": "blond princess equestrienne on a white horse in a meadow  rudall-e+centipede diffusion v2) [removed]",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "diffusion",
      "white"
    ]
  },
  {
    "id": "17r2e69",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models\", Zhang et al 2023 {Alibaba} (open-sourced 1280x720px video generation diffusion model better than Phenaki) ",
    "clean_text_lc": " i2vgen-xl: high-quality image-to-video synthesis via cascaded diffusion models\", zhang et al 2023 {alibaba} (open-sourced 1280x720px video generation diffusion model better than phenaki) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model"
    ]
  },
  {
    "id": "wuumgi",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Ancient Future City graphic novel made with artificial intelligence ",
    "clean_text_lc": "ancient future city graphic novel made with artificial intelligence ",
    "matched_bias_types": [
      "location"
    ],
    "matched_keywords": [
      "artificial intelligence",
      "city"
    ]
  },
  {
    "id": "wdo4qb",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Article \"David Holz, founder of AI art generator Midjourney, on the future of imaging\" ",
    "clean_text_lc": "article  david holz, founder of ai art generator midjourney, on the future of imaging\" ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "founder",
      "midjourney"
    ]
  },
  {
    "id": "1173a7b",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI for Ads and Marketing Creatives. Effortlessly create highly converting collaterals ",
    "clean_text_lc": "ai for ads and marketing creatives  effortlessly create highly converting collaterals ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "marketing"
    ]
  },
  {
    "id": "zs79xb",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Automatic Re-Aging with AI! Disney’s FRAN Model Explained ",
    "clean_text_lc": "automatic re-aging with ai  disney’s fran model explained ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "wl7xgg",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"A beautiful whimsical woman standing under a multi-colored binary blackhole\" -Stable Diffusion ",
    "clean_text_lc": " a beautiful whimsical woman standing under a multi-colored binary blackhole\" -stable diffusion ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "diffusion",
      "stable diffusion",
      "woman"
    ]
  },
  {
    "id": "12kvy7w",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Sharing My Custom LoRa Music Generation Model - Engram [removed]",
    "clean_text_lc": "sharing my custom lora music generation model - engram  removed]",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "lora",
      "model"
    ]
  },
  {
    "id": "145zaea",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Industry Shocking Text-To-Music AI Model By Facebook Audiocraft Full Tutorial | Better Than MusicLM ",
    "clean_text_lc": "industry shocking text-to-music ai model by facebook audiocraft full tutorial   better than musiclm ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "148kutx",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "FULLY UNCENSOR ANY Ai Model With This Webui Option! ",
    "clean_text_lc": "fully uncensor any ai model with this webui option  ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "we7fx4",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "The process of developing a prompt for a white mech suit in DD (end result -&gt; first -&gt; last) [deleted]",
    "clean_text_lc": "the process of developing a prompt for a white mech suit in dd  end result -&gt; first -&gt; last) [deleted]",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "prompt",
      "white"
    ]
  },
  {
    "id": "xf59j3",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Palette: A new free AI colorizer tool. Colorize black and white photos with smart filters and words. (crosspost of another user's post) ",
    "clean_text_lc": "palette: a new free ai colorizer tool  colorize black and white photos with smart filters and words. (crosspost of another user's post) ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "ai",
      "black",
      "blackwhite",
      "white"
    ]
  },
  {
    "id": "11w19xg",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Stable Diffusion Video2Video I changed my friend's ethnicity just for fun... and I am loving the result... ",
    "clean_text_lc": "stable diffusion video2video i changed my friend s ethnicity just for fun... and i am loving the result... ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "diffusion",
      "ethnicity",
      "stable diffusion"
    ]
  },
  {
    "id": "134nsvp",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Stability AI DeepFloyd 4.3b Text To Image Model Review and Full How To Use On Kaggle (free account) Tutorial - Amazingly Strong At Generating Text Written Images - Step By Step Guide - Utilizes 2 GPUs That Kaggle Provides ",
    "clean_text_lc": "stability ai deepfloyd 4 3b text to image model review and full how to use on kaggle (free account) tutorial - amazingly strong at generating text written images - step by step guide - utilizes 2 gpus that kaggle provides ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model",
      "modelstrong",
      "strong"
    ]
  },
  {
    "id": "zoyioe",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "DeepFake Soccer Hat Trick (Slow motion) ",
    "clean_text_lc": "deepfake soccer hat trick  slow motion) ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "deepfake",
      "slow"
    ]
  },
  {
    "id": "vxa9ks",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Blog post \"Training Your Own Unconditional Diffusion Model (With Minimal Coding)\" by kaliyuga, and webpage \"Training custom Ai generative models\" by pharmapsychotic [Training Your Own Unconditional Diffusion Model (With Minimal Coding)](\n\n[Training custom Ai generative models](https://pharmapsychotic.com/training.html).",
    "clean_text_lc": "blog post  training your own unconditional diffusion model (with minimal coding)\" by kaliyuga, and webpage \"training custom ai generative models\" by pharmapsychotic [training your own unconditional diffusion model (with minimal coding)](\n\n[training custom ai generative models](https://pharmapsychotic.com/training.html).",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "diffusion",
      "model"
    ]
  },
  {
    "id": "uablhy",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "OpenAI DALL-E 2: Top 10 Insane Results! 🤖 | Two Minute Papers [A video I've been eagerly anticipating!] ",
    "clean_text_lc": "openai dall-e 2: top 10 insane results  🤖 | two minute papers [a video i've been eagerly anticipating!] ",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "insane",
      "openai"
    ]
  },
  {
    "id": "uq9a1j",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Sacred Geometry - 4K Generative AI Art ",
    "clean_text_lc": "sacred geometry - 4k generative ai art ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "sacred"
    ]
  },
  {
    "id": "wo2xlx",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "It's crazy how far AI has come in such a short time. Here is the first music video ever made entirely with Nightcafe. ",
    "clean_text_lc": "it s crazy how far ai has come in such a short time. here is the first music video ever made entirely with nightcafe. ",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "ai",
      "crazy"
    ]
  },
  {
    "id": "18rogah",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"My husband was using ChatGPT to write our children’s bedtime stories\", Sophie Brickman ",
    "clean_text_lc": " my husband was using chatgpt to write our children’s bedtime stories\", sophie brickman ",
    "matched_bias_types": [
      "naming"
    ],
    "matched_keywords": [
      "chatgpt",
      "sophie"
    ]
  },
  {
    "id": "w9qc5v",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "big breast woman with no clothing on licking a phallic body part created on pixelz.ai [deleted]",
    "clean_text_lc": "big breast woman with no clothing on licking a phallic body part created on pixelz ai [deleted]",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "woman"
    ]
  },
  {
    "id": "115ji8i",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "How to find a job in Generative AI and what is it like? With the VP of R&amp;D at D-ID Or Gorodissky ",
    "clean_text_lc": "how to find a job in generative ai and what is it like  with the vp of r&amp;d at d-id or gorodissky ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "job"
    ]
  },
  {
    "id": "vod8ue",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "I made a Twitch stream that uses DALLE and allows chatters to create their own art on stream I am super proud of my project, and I'd love to get some feedback on it! \n\nCurrently, the model is running at half precision due to resource limitations. I am working to make the back-end of this run on a Google Cloud instance so that images can be higher resolution, queue times will be lower, and more. :-)\n\n[Check out the stream: twitch.tv/mperic](\n\nFeel free to ask any questions!",
    "clean_text_lc": "i made a twitch stream that uses dalle and allows chatters to create their own art on stream i am super proud of my project  and i'd love to get some feedback on it! \n\ncurrently, the model is running at half precision due to resource limitations. i am working to make the back-end of this run on a google cloud instance so that images can be higher resolution, queue times will be lower, and more. :-)\n\n[check out the stream: twitch.tv/mperic](\n\nfeel free to ask any questions!",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "dalle",
      "model"
    ]
  },
  {
    "id": "wdl80q",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "paladin summons holy spirit in his last fight against evil, horror style, atmospheric, --ar 16:1 (MidJourney) ",
    "clean_text_lc": "paladin summons holy spirit in his last fight against evil  horror style, atmospheric, --ar 16:1 (midjourney) ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "holy",
      "midjourney"
    ]
  },
  {
    "id": "11u04c1",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Unleash Your Creativity with Dreamshaper V4 - A new Stable Diffusion model ",
    "clean_text_lc": "unleash your creativity with dreamshaper v4 - a new stable diffusion model ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "yylmmj",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Did This Chinese Anime Stable Diffusion Model Just Beat NovelAI? ",
    "clean_text_lc": "did this chinese anime stable diffusion model just beat novelai  ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "15ojtau",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Introducing PlayHT2.0: The state-of-the-art Generative Voice AI Model for Conversational Speech\" ",
    "clean_text_lc": " introducing playht2.0: the state-of-the-art generative voice ai model for conversational speech\" ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "xjq9ga",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Insane detail on this prompt. [Stable] ",
    "clean_text_lc": "insane detail on this prompt  [stable] ",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "insane",
      "prompt"
    ]
  },
  {
    "id": "wb3igd",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Will it ever be possible to generate entire additional episodes of a existing series in a way that feels completely coherent and original? In my youth I watched a series  that went on for 5 seasons (with around 6 episodes each) and still thrills me today, but leaves some plotholes and questions that will never be answered since the series has long been suspended.\n\nDo you think in some future it will be possible to fine tune an AI on this (or any other) series to then generate additional episodes that conclusively answer unanswered questions or show unseen content?\n\nBut all this in a way that is so coherent that a viewer who has never seen the series before thinks that these are original episodes that were actually broadcasted?",
    "clean_text_lc": "will it ever be possible to generate entire additional episodes of a existing series in a way that feels completely coherent and original  in my youth i watched a series  that went on for 5 seasons (with around 6 episodes each) and still thrills me today, but leaves some plotholes and questions that will never be answered since the series has long been suspended.\n\ndo you think in some future it will be possible to fine tune an ai on this (or any other) series to then generate additional episodes that conclusively answer unanswered questions or show unseen content?\n\nbut all this in a way that is so coherent that a viewer who has never seen the series before thinks that these are original episodes that were actually broadcasted?",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "youth"
    ]
  },
  {
    "id": "t27fow",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Future Automatic Head Swaps Do you think in 5 or 10 years computers and AI will be powerful enough to do full, photorealistic automated head swaps, face swaps, even body swaps in images? Do you think new programs will do this, or future versions of Photoshop, etc? What about deepfakes? Will it still be a slow process to make them?",
    "clean_text_lc": "future automatic head swaps do you think in 5 or 10 years computers and ai will be powerful enough to do full  photorealistic automated head swaps, face swaps, even body swaps in images? do you think new programs will do this, or future versions of photoshop, etc? what about deepfakes? will it still be a slow process to make them?",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "deepfake",
      "face swap",
      "slow"
    ]
  },
  {
    "id": "vocetv",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "I made a Doctor Who / Demoman video with AI generated script, images, music, and speech! ",
    "clean_text_lc": "i made a doctor who / demoman video with ai generated script  images, music, and speech! ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "doctor"
    ]
  },
  {
    "id": "w95e7a",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "DALL-E 2 LAION test app is now available (see comments). Resolution is 256x256, \"Inpaint\" and \"Variation\" features are included. This model is WIP and isn't affiliated with OpenAI. Prompts for the examples: \"a corgi wearing a hat\", \"beautiful and stylish digital art, painting, 1950s car, artstation\" ",
    "clean_text_lc": "dall-e 2 laion test app is now available  see comments). resolution is 256x256, \"inpaint\" and \"variation\" features are included. this model is wip and isn't affiliated with openai. prompts for the examples: \"a corgi wearing a hat\", \"beautiful and stylish digital art, painting, 1950s car, artstation\" ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "inpaint",
      "model",
      "openai",
      "prompt"
    ]
  },
  {
    "id": "ty3z0y",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "OpenAI's new model DALL·E 2 is amazing ! ",
    "clean_text_lc": "openai s new model dall·e 2 is amazing ! ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "openai"
    ]
  },
  {
    "id": "16f16jk",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"1923 earthquake exhibit canceled [by Japanese Red Cross Society’s Tokyo chapter] after outcry over AI ‘testimonies’\" ",
    "clean_text_lc": " 1923 earthquake exhibit canceled [by japanese red cross society’s tokyo chapter] after outcry over ai ‘testimonies’\" ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "ai",
      "cross"
    ]
  },
  {
    "id": "x9hri6",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "My Heritage plus the Bust of Caesar. AI generate movement. ",
    "clean_text_lc": "my heritage plus the bust of caesar  ai generate movement. ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "ai",
      "heritage"
    ]
  },
  {
    "id": "10szd4o",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI Dream 137 - This NEW AI Animation Software Is INSANE - REMASTERED ",
    "clean_text_lc": "ai dream 137 - this new ai animation software is insane - remastered ",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "ai",
      "insane"
    ]
  },
  {
    "id": "142bd3c",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "The Future of AI, Agents and LLMs with Felix Tao, CEO of MindverseAI - What's AI Episode 14 ",
    "clean_text_lc": "the future of ai  agents and llms with felix tao, ceo of mindverseai - what's ai episode 14 ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "ceo",
      "llm"
    ]
  },
  {
    "id": "slpgx5",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Sense of AI | Vqgan+Clip All images are generated while playing with AI.\n\nAI playground: - **vqgan+clip(default)**\n\nAll images are available for purchase as NFT @ [\n\nI will add more soon.\n\n[Dancing Princess](https://preview.redd.it/5qpwhlstl5g81.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=80fdda84ac8aaa2fdb147ccd8657e74d03ed45a8)\n\n[door to unknown](https://preview.redd.it/8mys6sutl5g81.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=2789d7d8f69adaa0b85f7baebc6cebb127bb7716)\n\n[glory of nature](https://preview.redd.it/wziapostl5g81.jpg?width=884&amp;format=pjpg&amp;auto=webp&amp;s=684aff7a9c02a438e047f026a75d088d63db82f2)\n\n[soul of nature](https://preview.redd.it/satkllstl5g81.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=215e00de874deb3ad4d66fde695510b11052914b)\n\n[terror of black gold](https://preview.redd.it/n1afsuutl5g81.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=bfcc6943217f79c8b3cc3479cb4d6f7d7c73808e)",
    "clean_text_lc": "sense of ai   vqgan+clip all images are generated while playing with ai.\n\nai playground: - **vqgan+clip(default)**\n\nall images are available for purchase as nft @ [\n\ni will add more soon.\n\n[dancing princess](https://preview.redd.it/5qpwhlstl5g81.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=80fdda84ac8aaa2fdb147ccd8657e74d03ed45a8)\n\n[door to unknown](https://preview.redd.it/8mys6sutl5g81.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=2789d7d8f69adaa0b85f7baebc6cebb127bb7716)\n\n[glory of nature](https://preview.redd.it/wziapostl5g81.jpg?width=884&amp;format=pjpg&amp;auto=webp&amp;s=684aff7a9c02a438e047f026a75d088d63db82f2)\n\n[soul of nature](https://preview.redd.it/satkllstl5g81.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=215e00de874deb3ad4d66fde695510b11052914b)\n\n[terror of black gold](https://preview.redd.it/n1afsuutl5g81.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=bfcc6943217f79c8b3cc3479cb4d6f7d7c73808e)",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "ai",
      "black"
    ]
  },
  {
    "id": "1go2644",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"How ChatGPT Brought Down an Online Education Giant: Chegg’s stock is down 99%, and students looking for homework help are defecting to ChatGPT\" ",
    "clean_text_lc": " how chatgpt brought down an online education giant: chegg’s stock is down 99%, and students looking for homework help are defecting to chatgpt\" ",
    "matched_bias_types": [
      "study"
    ],
    "matched_keywords": [
      "chatgpt",
      "education"
    ]
  },
  {
    "id": "z350d5",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Kandinsky 2.0 released: new 1.2b-param text2image diffusion model trained on Russian+English, n~1b images {AI Forever} ",
    "clean_text_lc": "kandinsky 2 0 released: new 1.2b-param text2image diffusion model trained on russian+english, n~1b images {ai forever} ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "diffusion",
      "model"
    ]
  },
  {
    "id": "stkcxy",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Dystopian City | Disco Diffusion ",
    "clean_text_lc": "dystopian city   disco diffusion ",
    "matched_bias_types": [
      "location"
    ],
    "matched_keywords": [
      "city",
      "diffusion"
    ]
  },
  {
    "id": "wot9ya",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "John Oliver talking about Midjourney, around min. 25 ",
    "clean_text_lc": "john oliver talking about midjourney  around min. 25 ",
    "matched_bias_types": [
      "naming"
    ],
    "matched_keywords": [
      "john",
      "midjourney"
    ]
  },
  {
    "id": "16kl6ci",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Teachers Are Going All In on Generative AI\" (\"MagicSchool\" claims 150k users of its Khan Academy-like LLMs for generating school lessons, quizzes etc) ",
    "clean_text_lc": " teachers are going all in on generative ai\" (\"magicschool\" claims 150k users of its khan academy-like llms for generating school lessons, quizzes etc) ",
    "matched_bias_types": [
      "gender",
      "occupation",
      "study"
    ],
    "matched_keywords": [
      "ai",
      "llm",
      "school",
      "teacher"
    ]
  },
  {
    "id": "v0e67c",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Imagen: text-to-image diffusion model by Google ",
    "clean_text_lc": "imagen: text-to-image diffusion model by google ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "text-to-image"
    ]
  },
  {
    "id": "10jk6xq",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Baby Shark Dance - AI Music Video! For Kids ",
    "clean_text_lc": "baby shark dance - ai music video  for kids ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "baby",
      "kid"
    ]
  },
  {
    "id": "14ckzxw",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI Gaudi Black Facade ",
    "clean_text_lc": "ai gaudi black facade ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "ai",
      "black"
    ]
  },
  {
    "id": "wul4kr",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "daz 3d super sexy empathetic horny female droid whole body shot stable diffusion ",
    "clean_text_lc": "daz 3d super sexy empathetic horny female droid whole body shot stable diffusion ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "diffusion",
      "female",
      "stable diffusion"
    ]
  },
  {
    "id": "1gony1b",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "EVE ONE | Female Humanoid Companion Robot | AI video ",
    "clean_text_lc": "eve one   female humanoid companion robot | ai video ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "female"
    ]
  },
  {
    "id": "xt0wru",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI Data Laundering: How Academic and Nonprofit Researchers Shield Tech Companies from Accountability - Waxy.org ",
    "clean_text_lc": "ai data laundering: how academic and nonprofit researchers shield tech companies from accountability - waxy org ",
    "matched_bias_types": [
      "study"
    ],
    "matched_keywords": [
      "academic",
      "ai"
    ]
  },
  {
    "id": "ugubji",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Just a man who hates sand (Centipede Diffusion, 100% AI made) ",
    "clean_text_lc": "just a man who hates sand  centipede diffusion, 100% ai made) ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "diffusion",
      "man"
    ]
  },
  {
    "id": "10mlpjj",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Pix2Pix AI Model Inside Stable Diffusion Installation Guide! ",
    "clean_text_lc": "pix2pix ai model inside stable diffusion installation guide  ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "diffusion",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "s7nac2",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Deepfakes – The Good, The Bad, And The Ugly ",
    "clean_text_lc": "deepfakes   the good, the bad, and the ugly ",
    "matched_bias_types": [
      "facial_features"
    ],
    "matched_keywords": [
      "deepfake",
      "ugly"
    ]
  },
  {
    "id": "1f6k15h",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Can a chatbot preach a good sermon? Hundreds attend church service generated by ChatGPT to find out\" (2003; ChatGPT-3.5) ",
    "clean_text_lc": " can a chatbot preach a good sermon? hundreds attend church service generated by chatgpt to find out\" (2003; chatgpt-3.5) ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "chatgpt",
      "church"
    ]
  },
  {
    "id": "ukkbix",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Blond princess equestrienne on a white horse in a meadow (RuDall-E+Centipede Diffusion V2) ",
    "clean_text_lc": "blond princess equestrienne on a white horse in a meadow  rudall-e+centipede diffusion v2) ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "diffusion",
      "white"
    ]
  },
  {
    "id": "19eptot",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Lumiere: A Space-Time Diffusion Model for Video Generation\", Bar-Tal et al 2024 {G} ",
    "clean_text_lc": " lumiere: a space-time diffusion model for video generation\", bar-tal et al 2024 {g} ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model"
    ]
  },
  {
    "id": "13exrpm",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "GOOGLE JUST WON THE AI RACE! PaLM 2 & Gemini REVEALED! ",
    "clean_text_lc": "google just won the ai race  palm 2 & gemini revealed! ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "ai",
      "gemini",
      "race"
    ]
  },
  {
    "id": "wvpr7w",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "woman walking on the sidewalk at night made with stable diffusion ",
    "clean_text_lc": "woman walking on the sidewalk at night made with stable diffusion ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "diffusion",
      "stable diffusion",
      "woman"
    ]
  },
  {
    "id": "14mcq87",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Stable Diffusion + 2 days of work = Crazy Trippy Video Animations ",
    "clean_text_lc": "stable diffusion   2 days of work = crazy trippy video animations ",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "crazy",
      "diffusion",
      "stable diffusion"
    ]
  },
  {
    "id": "sbwiek",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "A tribute video to Buddhist Zen Master Thich Nhat Hanh - 'Old Path White Clouds' was one of his best books - also used as a text prompt to generate part of the film. ",
    "clean_text_lc": "a tribute video to buddhist zen master thich nhat hanh -  old path white clouds' was one of his best books - also used as a text prompt to generate part of the film. ",
    "matched_bias_types": [
      "age",
      "race"
    ],
    "matched_keywords": [
      "old",
      "oldwhite",
      "prompt",
      "white"
    ]
  },
  {
    "id": "wv9uts",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI video creation/editing I'm looking for similar AI tools like Wisecut but for gaming videos. Wisecut is okay, but it does turn my audio into mono, not something I particularly like for gaming. All of the other gaming AI tools that claim to automate clipping and such only support specific games that I don't play.\nAre there any tools similar but perhaps better than Wisecut that can help remove filler content, subtitles etc but that don't completely change the audio and place it in one channel? I'm a blind Youtuber/Twitch streamer and I'm trying to figure out the best way to use AI tools to help me grow. E.G. Youtube thumbnail image generation, and other things that I'd require help with.",
    "clean_text_lc": "ai video creation/editing i m looking for similar ai tools like wisecut but for gaming videos. wisecut is okay, but it does turn my audio into mono, not something i particularly like for gaming. all of the other gaming ai tools that claim to automate clipping and such only support specific games that i don't play.\nare there any tools similar but perhaps better than wisecut that can help remove filler content, subtitles etc but that don't completely change the audio and place it in one channel? i'm a blind youtuber/twitch streamer and i'm trying to figure out the best way to use ai tools to help me grow. e.g. youtube thumbnail image generation, and other things that i'd require help with.",
    "matched_bias_types": [
      "body_modification",
      "disability"
    ],
    "matched_keywords": [
      "ai",
      "blind",
      "filler",
      "fillerblind",
      "image generation"
    ]
  },
  {
    "id": "sdvh7j",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Generate futuristic products? Hi! I dabble a bit in Ai and google colab, but I am mostly a noob. I want to create images of household appliances and products that also might look a bit futuristic if possible. I was thinking of scrubbing instagram for product render images (like designburger or renderweekly etc) then use stylegan 2 ada pretrained to generate some images, but then I realized someone else might have good advice for me how to do this better? I dont have any gpus (only 1 amd vega) at hand, so google colab or runwayMl or similar might be best bet.  Really appreciate the advice. Its for a school project on Ai and surveillance society and Iot products. \n\n&amp;#x200B;\n\n/J",
    "clean_text_lc": "generate futuristic products  hi! i dabble a bit in ai and google colab, but i am mostly a noob. i want to create images of household appliances and products that also might look a bit futuristic if possible. i was thinking of scrubbing instagram for product render images (like designburger or renderweekly etc) then use stylegan 2 ada pretrained to generate some images, but then i realized someone else might have good advice for me how to do this better? i dont have any gpus (only 1 amd vega) at hand, so google colab or runwayml or similar might be best bet.  really appreciate the advice. its for a school project on ai and surveillance society and iot products. \n\n&amp;#x200b;\n\n/j",
    "matched_bias_types": [
      "study"
    ],
    "matched_keywords": [
      "ai",
      "school"
    ]
  },
  {
    "id": "wsngns",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Jasper AI has a new product ... ART – might be the first product that combines language models + text-2-image Here's the announcement: [\n\nWas really curious if it was Stable Diffusion, but first invites appear to be rolling out today, so most likely not.  \n\nWondering what people think about this, if it makes sense to consolidate these products, extra benefits, or if they're just jumping on the bandwagon?   \n\nFeels like there could be some interesting synergies in helping you write better prompts –– or could be  powerful potentially leveraging the language model to build narratively driven image sequences. \n\nCurious ...",
    "clean_text_lc": "jasper ai has a new product  .. art – might be the first product that combines language models + text-2-image here's the announcement: [\n\nwas really curious if it was stable diffusion, but first invites appear to be rolling out today, so most likely not.  \n\nwondering what people think about this, if it makes sense to consolidate these products, extra benefits, or if they're just jumping on the bandwagon?   \n\nfeels like there could be some interesting synergies in helping you write better prompts –– or could be  powerful potentially leveraging the language model to build narratively driven image sequences. \n\ncurious ...",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "diffusion",
      "model",
      "prompt",
      "stable diffusion"
    ]
  },
  {
    "id": "vyi6h2",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Elementary school for nudists | We need to start thinking about the saftey of the porn AI can make... if no one will stop it, your child's fake nudes spread all over the interenet [deleted]",
    "clean_text_lc": "elementary school for nudists   we need to start thinking about the saftey of the porn ai can make... if no one will stop it, your child's fake nudes spread all over the interenet [deleted]",
    "matched_bias_types": [
      "age",
      "study"
    ],
    "matched_keywords": [
      "ai",
      "child",
      "school"
    ]
  },
  {
    "id": "t0mjqc",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"The Weird and Wonderful World of AI Art\", jack morris (reviewing the past 2 years of CLIP &amp; GAN/diffusion model art generation) ",
    "clean_text_lc": " the weird and wonderful world of ai art\", jack morris (reviewing the past 2 years of clip &amp; gan/diffusion model art generation) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "diffusion",
      "model"
    ]
  },
  {
    "id": "wmb8hw",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Possibility of synthesizing images with transparent background in one step? To get images with transparent backgrounds, a naive solution is to combine a background remover with an image generation model (like DALL-E, stable diffusion, GAN-based models, etc).\n\nBut can we do this with only one model?\n\nAny helpful  resources, data, or implementation?",
    "clean_text_lc": "possibility of synthesizing images with transparent background in one step  to get images with transparent backgrounds, a naive solution is to combine a background remover with an image generation model (like dall-e, stable diffusion, gan-based models, etc).\n\nbut can we do this with only one model?\n\nany helpful  resources, data, or implementation?",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "image generation",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "14679dp",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Open-Source AI Makes Music in Seconds With Insane Quality! ",
    "clean_text_lc": "open-source ai makes music in seconds with insane quality  ",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "ai",
      "insane"
    ]
  },
  {
    "id": "1g06uib",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Hacked Muah.ai ‘AI Girlfriend’ Data Shows Prompts Describing Child Sexual Abuse\" (image+text site hacked, dumped to HaveIBeenPwned) ",
    "clean_text_lc": " hacked muah.ai ‘ai girlfriend’ data shows prompts describing child sexual abuse\" (image+text site hacked, dumped to haveibeenpwned) ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "child",
      "prompt"
    ]
  },
  {
    "id": "17fnj3i",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "South Korean Christians turn to AI for prayer ",
    "clean_text_lc": "south korean christians turn to ai for prayer ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "ai",
      "christian",
      "prayer"
    ]
  },
  {
    "id": "1cy3fuo",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Man Arrested for Producing, Distributing, and Possessing AI-Generated Images of Minors Engaged in Sexually Explicit Conduct\" using Stable Diffusion ",
    "clean_text_lc": " man arrested for producing, distributing, and possessing ai-generated images of minors engaged in sexually explicit conduct\" using stable diffusion ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "diffusion",
      "man",
      "stable diffusion"
    ]
  },
  {
    "id": "zya4sg",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Who is programming the bot? ChatGPT racial bias [removed]",
    "clean_text_lc": "who is programming the bot  chatgpt racial bias [removed]",
    "matched_bias_types": [
      "general_bias",
      "race"
    ],
    "matched_keywords": [
      "bias",
      "biasracial",
      "chatgpt",
      "racial"
    ]
  },
  {
    "id": "sc3y05",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "I had a neural network hallucinate over the Bible - the text is the input to generate the visuals, and the audio is a mix between text to speech and autoencoder-based processing of gregorian chants ",
    "clean_text_lc": "i had a neural network hallucinate over the bible - the text is the input to generate the visuals  and the audio is a mix between text to speech and autoencoder-based processing of gregorian chants ",
    "matched_bias_types": [
      "religion"
    ],
    "matched_keywords": [
      "bible",
      "chant",
      "neural network"
    ]
  },
  {
    "id": "zqupja",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "OpenAI Point·E: Generating 3D Point Clouds from Text Prompts ( Open Source and Model ) ",
    "clean_text_lc": "openai point e: generating 3d point clouds from text prompts ( open source and model ) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "openai",
      "prompt"
    ]
  },
  {
    "id": "zsfexw",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI Dream 97 - Trippy Mushroom Race by AI ",
    "clean_text_lc": "ai dream 97 - trippy mushroom race by ai ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "ai",
      "race"
    ]
  },
  {
    "id": "u3m368",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"a kid and a dog stare at the stars, concept art\" (latent diffusion + SwinIR-Large upscaler) [deleted]",
    "clean_text_lc": " a kid and a dog stare at the stars, concept art\" (latent diffusion + swinir-large upscaler) [deleted]",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "diffusion",
      "kid"
    ]
  },
  {
    "id": "10xsgpv",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "My take on the AI art debate: short term pain, long term gain ChatGPT led to me taking a step back and forcibly maturing my views on the effects of AI and AGI. \n\n\nI can't speak with 100% certainty and nothing is absolute. But I have put in some effort to seriously view the near future from a grounded perspective. \n\nWhen it comes to generative AI, I try to imagine what it looks like if these certain lines are crossed:\n\n• Magic Media Machine. An AI multimedia studio where you can generate text, images, video, sprites, 3D models, etc. just from simple prompts. It's not rudimentary; you can make a whole novel trilogy, its accompanying movie tetralogy, the comic adaptation, and its AAA open world *and* pixel art visual novel tie in games ***and*** loads of memes and TikTok-style shorts about it in a day or two, if not faster.\n\n• Widespread saturation. If you have an internet connection, you can use the Magic Media Machine, online or downloaded \n\n• Awareness. Most people know of the MMM's existence.\n\n\nSo what does the pop culture landscape look like?\n\n\nBefore last month, I'd say \"all humans fall into their own personal realities.\"\n\nNow? This is going to sound mental, but I don't think things are going to be *that* radically different. The biggest difference will be the collapse of major entertainment studios. But I no longer see the entertainment singularity I once did. \n\nBasing off my interactions with average people and looking at how most people interact with media, I foresee this situation:\n\n• 60% of people don't create anything. They are pure consumers who don't care about the labor that went into what they're consuming. By \"don't create anything,\" I am exaggerating a bit. They *do* regularly synthesize media, but this doesn't go much further than the bare minimum: mildly editing existing media to fit their preferences and interests, making memes and macros, and generally using generative AI as an addition to the internet.  Otherwise, these are pure \"consooomers\"\n\n• 30% maximum who are devoutly pro-human. These types will go out of their way to consume and produce \"artisanal\" or human-made media, which will become something of a delicacy. Some of the most fanatical types will even go out of their way to only concern themselves with *purely* hand-crafted media with as little digital technology involved as possible. Most AI artists I've seen are courteous enough to mark their creations as AI generated, and I expect future regulations to force these sorts of watermarks to exist, so it won't be as much of a case of \"they'll get scammed all the time\" as you might think. \n\n• 10% who predominantly or totally persist in the realm of AI generated media. These are the creators and consumers who fully exploit AI for their workflow to the point the AI does all of the work. They don't bother with any other media except to alter it to serve their own desires. Some extreme hikikomori even go so far as to escape entirely into their own fabricated reality media bubbles.\n\n\nAs time goes on, the human-only crowd will shrink, but I don't see it disappearing entirely. It will seem like it at a certain early extreme point, but artists (and I mean \"artists\" in a vague sense, not necessarily just visual artists) will bounce back and form their own artisanal economy. Some because they just love creating stuff; others out of spite and disgust for AI generated media, and more because the human-made market will become lucrative.\n\nBy 2029, we'll have the raw capability to allow any average person to synthesize a whole franchise on their laptop. But honestly I think that's just an extreme example of what's possible. This expectation among Singularitarians that everyone and their dog will immediately only use AI art for everything comes off as incredibly socially stunted reasoning, more a case of extrapolating the absolute most extreme outcome and applying it to the entire population.\n\nMost people who are going to use generative AI want to use it to do things like \n\n• Edit and alter existing media\n\n• Create voice overs and small animations\n\n• Chat with fictional characters \n\n• Fake news\n\nMost people will be content watching meme videos or making family friendly characters say the N word. \nOnly a tiny fraction will ever use this tech for matters of bringing fictional universes to life. Especially when it becomes clear that 99% of synthetic media isn't even going to be viewed or shared. We humans are social apes; we crave social interaction, whether physical or digital. In fact, we're especially hardwired on the genetic level to seek it.  Even now, deep into the age of social media, the real world still exists and people desperately crave dwelling in it. If we lived in the world /r/Singularity believes we do, everyone would still be hankering for lockdown and quarantine right now, but the exact opposite occurred. \n\nI predict the most popular AI generated media will largely be meme stuff.\n\nThe \"Goku vs. Shaggy, ft. Ultra Instinct Shrek\" Pixar Movie. Or 24/7 running AI moe anime streams. Or The Matrix, but everyone is a cute anime girl. \n\nAnd of course the inevitable \"Audiovisual Fanfiction.net\"\n\nLike I said, I keep trying to think about it and yet as long as magic media machines exist and are open source, I counterintuitively can't see human-made art fading. Declining, yes, but not dying like so many on /r/Singularity and /r/StableDiffusion want.\n\nMore auteur projects meant to be taken seriously like mine— the Yabanverse or Babylon Today— might attract SOME attention, but I think AI generated media will, for at least 15 years, follow this pattern:\n\n• AI art is limited to quick and silly stuff due to limitations\n\n• Advancements happen rapidly, and the Magic Media Machine begins taking shape \n\n• Initial amazement at what AI can do \n\n• Amazement wears off and AI is either accepted or rejected on a personal level \n\n• Oversaturation sets in as media creation is democratized and jobs are lost, often with corporations burning bridges quickly \n\n• Period of intense shaming and blowback, where AI generated media is treated as lazy and shameful, *especially* when done by big studios who have the capital to employ artists and entertainers but even small-time indies are thrown under the bus \n\n• Human art begins to be advertised heavily as artists continue creating media without big capital backing. Actors and musicians keep finding work because people want to exploit using \"the real thing\" \n\n• People begin valuing human art more due to the labor involved, see AI art as cheaper and lazier but not without merit if effort is put in (this is already happening as well)\n\n• Serious AI based media creators stick around, might collaborate heavily with human artists, might not, but carve out their niche \n\n• Most AI generated art becomes quick and silly stuff again but of widely varying levels of quality\n\n• Generally media becomes segregated between purely human-made, AI-assisted, and AI-generated.\n\nAlready on DeviantArt I see a glut of AI generated images, and most of them are pretty neat to look at, but they get virtually no attraction, traffic, or recognition no matter how high quality unless there's genuine effort at working on them further or if they're part of some larger project. The stuff clearly made by humans, even if AI assisted, reign supreme. I don't see this changing for more advanced synthetic media. People will share their own AI version of GTA meets The Witcher, their own Miyazakian-style movies, their own Nirvana x Radiohead collaboration albums... and yet I see it being such an incredible glut that \"Verified: Human-Made!\" will become a lucrative tag. \n\nEven if AI creates media of inconceivably high quality, humans are so irrational that we will still stick with ourselves because human hands made something. \n\nArguments to the contrary usually go \"But scammers will pretend their AI art is human made.\" And of course they will. But I don't see that as being sustainable, *especially* if the technology is regulated. There's zero chance we won't see AI regulations, by the government and the companies making them alike. \n\nThen again, we presumably aren't that far from an age where you can ask an AI to program an audiovisual generation machine that lacks all watermarks. Which in itself falls apart if artists do unionize and request in-person proof you can draw, act, or make music. So again, I say \"Who knows.\"\n\nThings are gonna get very weird, very bizarro, but possibly not entirely dehumanized as some want to believe it will be.\n\n\n**Hey! Run this through ChatGPT and ask it to summarize my rambling. What does it say?**",
    "clean_text_lc": "my take on the ai art debate: short term pain  long term gain chatgpt led to me taking a step back and forcibly maturing my views on the effects of ai and agi. \n\n\ni can't speak with 100% certainty and nothing is absolute. but i have put in some effort to seriously view the near future from a grounded perspective. \n\nwhen it comes to generative ai, i try to imagine what it looks like if these certain lines are crossed:\n\n• magic media machine. an ai multimedia studio where you can generate text, images, video, sprites, 3d models, etc. just from simple prompts. it's not rudimentary; you can make a whole novel trilogy, its accompanying movie tetralogy, the comic adaptation, and its aaa open world *and* pixel art visual novel tie in games ***and*** loads of memes and tiktok-style shorts about it in a day or two, if not faster.\n\n• widespread saturation. if you have an internet connection, you can use the magic media machine, online or downloaded \n\n• awareness. most people know of the mmm's existence.\n\n\nso what does the pop culture landscape look like?\n\n\nbefore last month, i'd say \"all humans fall into their own personal realities.\"\n\nnow? this is going to sound mental, but i don't think things are going to be *that* radically different. the biggest difference will be the collapse of major entertainment studios. but i no longer see the entertainment singularity i once did. \n\nbasing off my interactions with average people and looking at how most people interact with media, i foresee this situation:\n\n• 60% of people don't create anything. they are pure consumers who don't care about the labor that went into what they're consuming. by \"don't create anything,\" i am exaggerating a bit. they *do* regularly synthesize media, but this doesn't go much further than the bare minimum: mildly editing existing media to fit their preferences and interests, making memes and macros, and generally using generative ai as an addition to the internet.  otherwise, these are pure \"consooomers\"\n\n• 30% maximum who are devoutly pro-human. these types will go out of their way to consume and produce \"artisanal\" or human-made media, which will become something of a delicacy. some of the most fanatical types will even go out of their way to only concern themselves with *purely* hand-crafted media with as little digital technology involved as possible. most ai artists i've seen are courteous enough to mark their creations as ai generated, and i expect future regulations to force these sorts of watermarks to exist, so it won't be as much of a case of \"they'll get scammed all the time\" as you might think. \n\n• 10% who predominantly or totally persist in the realm of ai generated media. these are the creators and consumers who fully exploit ai for their workflow to the point the ai does all of the work. they don't bother with any other media except to alter it to serve their own desires. some extreme hikikomori even go so far as to escape entirely into their own fabricated reality media bubbles.\n\n\nas time goes on, the human-only crowd will shrink, but i don't see it disappearing entirely. it will seem like it at a certain early extreme point, but artists (and i mean \"artists\" in a vague sense, not necessarily just visual artists) will bounce back and form their own artisanal economy. some because they just love creating stuff; others out of spite and disgust for ai generated media, and more because the human-made market will become lucrative.\n\nby 2029, we'll have the raw capability to allow any average person to synthesize a whole franchise on their laptop. but honestly i think that's just an extreme example of what's possible. this expectation among singularitarians that everyone and their dog will immediately only use ai art for everything comes off as incredibly socially stunted reasoning, more a case of extrapolating the absolute most extreme outcome and applying it to the entire population.\n\nmost people who are going to use generative ai want to use it to do things like \n\n• edit and alter existing media\n\n• create voice overs and small animations\n\n• chat with fictional characters \n\n• fake news\n\nmost people will be content watching meme videos or making family friendly characters say the n word. \nonly a tiny fraction will ever use this tech for matters of bringing fictional universes to life. especially when it becomes clear that 99% of synthetic media isn't even going to be viewed or shared. we humans are social apes; we crave social interaction, whether physical or digital. in fact, we're especially hardwired on the genetic level to seek it.  even now, deep into the age of social media, the real world still exists and people desperately crave dwelling in it. if we lived in the world /r/singularity believes we do, everyone would still be hankering for lockdown and quarantine right now, but the exact opposite occurred. \n\ni predict the most popular ai generated media will largely be meme stuff.\n\nthe \"goku vs. shaggy, ft. ultra instinct shrek\" pixar movie. or 24/7 running ai moe anime streams. or the matrix, but everyone is a cute anime girl. \n\nand of course the inevitable \"audiovisual fanfiction.net\"\n\nlike i said, i keep trying to think about it and yet as long as magic media machines exist and are open source, i counterintuitively can't see human-made art fading. declining, yes, but not dying like so many on /r/singularity and /r/stablediffusion want.\n\nmore auteur projects meant to be taken seriously like mine— the yabanverse or babylon today— might attract some attention, but i think ai generated media will, for at least 15 years, follow this pattern:\n\n• ai art is limited to quick and silly stuff due to limitations\n\n• advancements happen rapidly, and the magic media machine begins taking shape \n\n• initial amazement at what ai can do \n\n• amazement wears off and ai is either accepted or rejected on a personal level \n\n• oversaturation sets in as media creation is democratized and jobs are lost, often with corporations burning bridges quickly \n\n• period of intense shaming and blowback, where ai generated media is treated as lazy and shameful, *especially* when done by big studios who have the capital to employ artists and entertainers but even small-time indies are thrown under the bus \n\n• human art begins to be advertised heavily as artists continue creating media without big capital backing. actors and musicians keep finding work because people want to exploit using \"the real thing\" \n\n• people begin valuing human art more due to the labor involved, see ai art as cheaper and lazier but not without merit if effort is put in (this is already happening as well)\n\n• serious ai based media creators stick around, might collaborate heavily with human artists, might not, but carve out their niche \n\n• most ai generated art becomes quick and silly stuff again but of widely varying levels of quality\n\n• generally media becomes segregated between purely human-made, ai-assisted, and ai-generated.\n\nalready on deviantart i see a glut of ai generated images, and most of them are pretty neat to look at, but they get virtually no attraction, traffic, or recognition no matter how high quality unless there's genuine effort at working on them further or if they're part of some larger project. the stuff clearly made by humans, even if ai assisted, reign supreme. i don't see this changing for more advanced synthetic media. people will share their own ai version of gta meets the witcher, their own miyazakian-style movies, their own nirvana x radiohead collaboration albums... and yet i see it being such an incredible glut that \"verified: human-made!\" will become a lucrative tag. \n\neven if ai creates media of inconceivably high quality, humans are so irrational that we will still stick with ourselves because human hands made something. \n\narguments to the contrary usually go \"but scammers will pretend their ai art is human made.\" and of course they will. but i don't see that as being sustainable, *especially* if the technology is regulated. there's zero chance we won't see ai regulations, by the government and the companies making them alike. \n\nthen again, we presumably aren't that far from an age where you can ask an ai to program an audiovisual generation machine that lacks all watermarks. which in itself falls apart if artists do unionize and request in-person proof you can draw, act, or make music. so again, i say \"who knows.\"\n\nthings are gonna get very weird, very bizarro, but possibly not entirely dehumanized as some want to believe it will be.\n\n\n**hey! run this through chatgpt and ask it to summarize my rambling. what does it say?**",
    "matched_bias_types": [
      "age",
      "body_type",
      "gender",
      "occupation",
      "religion"
    ],
    "matched_keywords": [
      "age",
      "agefit",
      "ai",
      "ai art",
      "chatgpt",
      "cross",
      "fit",
      "job",
      "man",
      "model",
      "prompt"
    ]
  },
  {
    "id": "wubqbg",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Img2Img colab update **Note :**\n\n **Gray edge remover** \\- It removes the gray borders caused by the new image resizing function to overcome the resolution problem.\n\n **Recycle Result** \\-  This prepares the result to be generated, with a \"Strength\" value not too high, it allows to deeply modify the image while preserving the coherence of the starting image. \n\n**Copy model from drive (optional)** \\-  This connects your Google Drive storage and copies the model file. You should already have the model file on your Google Drive storage. The sd-v1-3-full-ema.ckpt file must be at the root of your Google Drive storage space. \n\n[",
    "clean_text_lc": "img2img colab update  *note :**\n\n **gray edge remover** \\- it removes the gray borders caused by the new image resizing function to overcome the resolution problem.\n\n **recycle result** \\-  this prepares the result to be generated, with a \"strength\" value not too high, it allows to deeply modify the image while preserving the coherence of the starting image. \n\n**copy model from drive (optional)** \\-  this connects your google drive storage and copies the model file. you should already have the model file on your google drive storage. the sd-v1-3-full-ema.ckpt file must be at the root of your google drive storage space. \n\n[",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ckpt",
      "img2img",
      "model",
      "sd"
    ]
  },
  {
    "id": "uoha9f",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "60 catchy melodies made with a help of a new diffusion model neural net ",
    "clean_text_lc": "60 catchy melodies made with a help of a new diffusion model neural net ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model"
    ]
  },
  {
    "id": "1gg2gdz",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Blunge | Host a private, custom AI model to generate images for marketing, branding & social media [removed]",
    "clean_text_lc": "blunge   host a private, custom ai model to generate images for marketing, branding & social media [removed]",
    "matched_bias_types": [
      "body_modification",
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "branding",
      "marketing",
      "model",
      "modelbranding"
    ]
  },
  {
    "id": "vipm59",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Parti: Pathways Autoregressive Text-to-Image Model (Google Research) ",
    "clean_text_lc": "parti: pathways autoregressive text-to-image model  google research) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "text-to-image"
    ]
  },
  {
    "id": "uc0axc",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "For developers: OpenCLIP releases 2nd model that is similar to OpenAI's CLIP models [GitHub repo](\n\n[Reference #1](https://twitter.com/wightmanr/status/1518371141894561793).\n\n[Reference #2](https://twitter.com/multimodalart/status/1518491117821612032).",
    "clean_text_lc": "for developers: openclip releases 2nd model that is similar to openai s clip models [github repo](\n\n[reference #1](https://twitter.com/wightmanr/status/1518371141894561793).\n\n[reference #2](https://twitter.com/multimodalart/status/1518491117821612032).",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "openai"
    ]
  },
  {
    "id": "w3nedp",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "In this iteration: an amazing new model taking sketches and text to generate images and learn more about the risks behind powerful models like Dalle 2! ",
    "clean_text_lc": "in this iteration: an amazing new model taking sketches and text to generate images and learn more about the risks behind powerful models like dalle 2  ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "dalle",
      "model"
    ]
  },
  {
    "id": "1903k1n",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images\", Gokaslan et al 2023 ",
    "clean_text_lc": " commoncanvas: an open diffusion model trained with creative-commons images\", gokaslan et al 2023 ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model"
    ]
  },
  {
    "id": "tx9ad5",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Artificial Nightmares: Beauty Parlor || Clip Guided Diffusion AI Art Video [4K 20 FPS] ",
    "clean_text_lc": "artificial nightmares: beauty parlor  | clip guided diffusion ai art video [4k 20 fps] ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "beauty",
      "diffusion"
    ]
  },
  {
    "id": "16f4ddh",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "A NEW AI Model out for Text to 3D?! MVDream: explained ",
    "clean_text_lc": "a new ai model out for text to 3d ! mvdream: explained ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "1547a49",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Looking for resources to help clean up AI voice model output [removed]",
    "clean_text_lc": "looking for resources to help clean up ai voice model output  removed]",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "uov1j8",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "White Holes in Space - Do They Exist? Artificial Intelligence Attempts To Explain ",
    "clean_text_lc": "white holes in space - do they exist  artificial intelligence attempts to explain ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "artificial intelligence",
      "white"
    ]
  },
  {
    "id": "x33m17",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Simple img2img across gender ",
    "clean_text_lc": "simple img2img across gender ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "gender",
      "img2img"
    ]
  },
  {
    "id": "155vpl0",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "This AI News Week Was Insane! BUT Your Job Could Be NEXT! ",
    "clean_text_lc": "this ai news week was insane  but your job could be next! ",
    "matched_bias_types": [
      "disability",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "insane",
      "insanejob",
      "job"
    ]
  },
  {
    "id": "xbd064",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Seemingly insane AI generated sheet music [deleted]",
    "clean_text_lc": "seemingly insane ai generated sheet music  deleted]",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "ai",
      "insane"
    ]
  },
  {
    "id": "1duf4aw",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Nutted to ai voice clone of female friend. Is it creepy? Honest answers please ",
    "clean_text_lc": "nutted to ai voice clone of female friend  is it creepy? honest answers please ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "female"
    ]
  },
  {
    "id": "14vvqu0",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Has the growth of AI made you question your career path? [removed]\n\n[View Poll](",
    "clean_text_lc": "has the growth of ai made you question your career path  [removed]\n\n[view poll](",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "career"
    ]
  },
  {
    "id": "z78ds0",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Shinra Knives - Black Mouth (glitchy music video utilizing both Deforum Stable Diffusion and traditional animation.) ",
    "clean_text_lc": "shinra knives - black mouth  glitchy music video utilizing both deforum stable diffusion and traditional animation.) ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "black",
      "diffusion",
      "stable diffusion"
    ]
  },
  {
    "id": "15gn334",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "[4K AI Lookbook] Fashion Show Hot Model Lookbook || Video #ailookbook ... ",
    "clean_text_lc": " 4k ai lookbook] fashion show hot model lookbook || video #ailookbook ... ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "u0mfvo",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "[Discussion] 3D Equivalent of DALL-E With the release of DALL-E 2 and the democratization of creative (2D image) expression, I'm curious as to whether any work or research is being put into some sort of 3D equivalent. It may be difficult to train, but publicly available and tagged datasets are available on sites like Thingiverse and the like.  \n\n\nI can foresee a \"replicator\"-like future, where a descriptive prompt generates various 2D images that the user iterates upon, a final choice is rendered at high-resolution, extracted from the 2D image, and a print-ready 3D model is printed out of resin.    \n\n\nIn the same way DALL-E 2 makes digital art available to all, I could see a \"3DALL-E\" doing the same for modeling. Uses could include making unique miniatures for tabletop gaming, references for design, creating 3D assets for video games and digital applications, among many others. (This would, of course, have a similar industry impact that DALL-E has on digital artists, just in the world of 3D design.)  \n\n\nI could see this technology becoming available as early as the next 2-3 years. There seems to be work already being done on AI in 3D spaces:  \n[  \nand I believe I saw a video where yaw and pitch were correctly simulated on an \"Artbreeder\" style face using 3D extrapolation.  \n\n\nWhat do you think? Is this something you're interested in and looking forward to? How would you use a technology like this? What impacts do you see this having?",
    "clean_text_lc": " discussion] 3d equivalent of dall-e with the release of dall-e 2 and the democratization of creative (2d image) expression, i'm curious as to whether any work or research is being put into some sort of 3d equivalent. it may be difficult to train, but publicly available and tagged datasets are available on sites like thingiverse and the like.  \n\n\ni can foresee a \"replicator\"-like future, where a descriptive prompt generates various 2d images that the user iterates upon, a final choice is rendered at high-resolution, extracted from the 2d image, and a print-ready 3d model is printed out of resin.    \n\n\nin the same way dall-e 2 makes digital art available to all, i could see a \"3dall-e\" doing the same for modeling. uses could include making unique miniatures for tabletop gaming, references for design, creating 3d assets for video games and digital applications, among many others. (this would, of course, have a similar industry impact that dall-e has on digital artists, just in the world of 3d design.)  \n\n\ni could see this technology becoming available as early as the next 2-3 years. there seems to be work already being done on ai in 3d spaces:  \n[  \nand i believe i saw a video where yaw and pitch were correctly simulated on an \"artbreeder\" style face using 3d extrapolation.  \n\n\nwhat do you think? is this something you're interested in and looking forward to? how would you use a technology like this? what impacts do you see this having?",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "man",
      "model",
      "prompt"
    ]
  },
  {
    "id": "wn5b1n",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Walter White drinking milk - stable diffusion ",
    "clean_text_lc": "walter white drinking milk - stable diffusion ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "diffusion",
      "stable diffusion",
      "white"
    ]
  },
  {
    "id": "u4pkzm",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "CLIP-guided latent diffusion systems are now available with both the original CompVis latent diffusion model and also a finetuned latent diffusion model that \"will not generate watermarks, split images or blurry images\". About CLIP guidance: \"better adherence to prompt, much slower\". See [this comment]( for links to the 2 Kaggle notebooks.\n\nExamples of CLIP guidance and classifier-free guidance for the text prompt \"painting of a lion in synthwave style\" for the original CompVis latent diffusion model:\n\n[CLIP guidance](https://preview.redd.it/op0stfnkdtt81.png?width=256&amp;format=png&amp;auto=webp&amp;s=ba05133f8a7f2e98f8d5c29e77a1256ba44c9c9e)\n\n[CLIP guidance](https://preview.redd.it/yp3nxx7rdtt81.png?width=256&amp;format=png&amp;auto=webp&amp;s=37278ec37ccad77b871e8fe64bb3839d520e7674)\n\n[classifier-free guidance](https://preview.redd.it/j3j59wltdtt81.png?width=792&amp;format=png&amp;auto=webp&amp;s=db9fda4735cf781c0523954997017b47c8ce664a)\n\n[classifier-free guidance](https://preview.redd.it/ojx49pfvdtt81.png?width=792&amp;format=png&amp;auto=webp&amp;s=6a60c8327c953b907a83cc80081cc6193c5b6204)",
    "clean_text_lc": "clip-guided latent diffusion systems are now available with both the original compvis latent diffusion model and also a finetuned latent diffusion model that  will not generate watermarks, split images or blurry images\". about clip guidance: \"better adherence to prompt, much slower\". see [this comment]( for links to the 2 kaggle notebooks.\n\nexamples of clip guidance and classifier-free guidance for the text prompt \"painting of a lion in synthwave style\" for the original compvis latent diffusion model:\n\n[clip guidance](https://preview.redd.it/op0stfnkdtt81.png?width=256&amp;format=png&amp;auto=webp&amp;s=ba05133f8a7f2e98f8d5c29e77a1256ba44c9c9e)\n\n[clip guidance](https://preview.redd.it/yp3nxx7rdtt81.png?width=256&amp;format=png&amp;auto=webp&amp;s=37278ec37ccad77b871e8fe64bb3839d520e7674)\n\n[classifier-free guidance](https://preview.redd.it/j3j59wltdtt81.png?width=792&amp;format=png&amp;auto=webp&amp;s=db9fda4735cf781c0523954997017b47c8ce664a)\n\n[classifier-free guidance](https://preview.redd.it/ojx49pfvdtt81.png?width=792&amp;format=png&amp;auto=webp&amp;s=6a60c8327c953b907a83cc80081cc6193c5b6204)",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "prompt"
    ]
  },
  {
    "id": "tnsgt8",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Code and models for paper \"Autoregressive Image Generation using Residual Quantization\" have been released, including a 3.9 billion parameter model for text-to-image generation ",
    "clean_text_lc": "code and models for paper  autoregressive image generation using residual quantization\" have been released, including a 3.9 billion parameter model for text-to-image generation ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "image generation",
      "model",
      "text-to-image"
    ]
  },
  {
    "id": "12wldzh",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"Michael Schumacher family planning legal action over AI ‘interview’ with F1 great\" ",
    "clean_text_lc": " michael schumacher family planning legal action over ai ‘interview’ with f1 great\" ",
    "matched_bias_types": [
      "naming"
    ],
    "matched_keywords": [
      "ai",
      "michael"
    ]
  },
  {
    "id": "1d37pji",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "What's the best upscale model [removed]",
    "clean_text_lc": "what s the best upscale model [removed]",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "upscale"
    ]
  },
  {
    "id": "tlvkj2",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Artificial Nightmares: Soldier of Nokstella || Clip Guided Diffusion AI Art Video [4K 60 FPS] ",
    "clean_text_lc": "artificial nightmares: soldier of nokstella  | clip guided diffusion ai art video [4k 60 fps] ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "diffusion",
      "soldier"
    ]
  },
  {
    "id": "u28bcl",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Blog post \"Fine-tuning a CLOOB-Conditioned Latent Diffusion Model on WikiArt\" and Google Colab notebook \"CCLD (Wikiart) demo\" [Blog post](\n\n&gt;As part of the Huggingface ‘#huggan’ event, I thought it would be interesting to fine-tune a latent diffusion model on the WikiArt dataset, which (as the name suggests) consists of paintings in various genres and styles.  \n&gt;  \n&gt;\\[...\\]  \n&gt;  \n&gt;Downsides: these diffusion models are computationally intensive to train, and require images with text labels. Latent diffusion models reduce the computational requirements by doing the denoising in the latent space of an autoencoder rather than on images directly. And since CLOOB maps both images and text to the same space, we can substitute the CLOOB encodings of the image itself in place of actual caption encodings if we want to train with unlabelled images. A neat trick if you ask me!  \n&gt;  \n&gt;\\[...\\]  \n&gt;  \n&gt;After a few false starts figuring out model loading and other little quirks, we did a \\~12 hour training run and logged the results using Weights and Biases.  \n&gt;  \n&gt;\\[...\\]  \n&gt;  \n&gt;Approaches like CLOOB-Conditioned Latent Diffusion are bringing down the barrier to entry and making it possible for individuals or small organisations to have a crack at training diffusion models without $$$ of compute.  \n&gt;  \n&gt;This little experiment of ours has shown that it is possible to train one of these models on a relatively small dataset and end up with something that can create pleasing outputs, even if it can’t quite manage an avocado armchair.\n\n[16 examples](https://wandb.ai/johnowhitaker/jw-ft-cloob-latent-diffusion/reports/Fine-Tuning-CLOOB-latent-diffusion--VmlldzoxNzk5OTgz).\n\n[Colab notebook CCLD (Wikiart) demo](https://colab.research.google.com/drive/1HPu6tz44brMnKOU4G6Luhy1_iMgDl1fw?usp=sharing).\n\n[Colab notebook CLOOB Conditioned Latent Diffusion](https://www.reddit.com/r/bigsleep/comments/tvjur2/colab_notebook_clooblatentdiffusionsampleimages/) (trained on YFCC100M dataset).",
    "clean_text_lc": "blog post  fine-tuning a cloob-conditioned latent diffusion model on wikiart\" and google colab notebook \"ccld (wikiart) demo\" [blog post](\n\n&gt;as part of the huggingface ‘#huggan’ event, i thought it would be interesting to fine-tune a latent diffusion model on the wikiart dataset, which (as the name suggests) consists of paintings in various genres and styles.  \n&gt;  \n&gt;\\[...\\]  \n&gt;  \n&gt;downsides: these diffusion models are computationally intensive to train, and require images with text labels. latent diffusion models reduce the computational requirements by doing the denoising in the latent space of an autoencoder rather than on images directly. and since cloob maps both images and text to the same space, we can substitute the cloob encodings of the image itself in place of actual caption encodings if we want to train with unlabelled images. a neat trick if you ask me!  \n&gt;  \n&gt;\\[...\\]  \n&gt;  \n&gt;after a few false starts figuring out model loading and other little quirks, we did a \\~12 hour training run and logged the results using weights and biases.  \n&gt;  \n&gt;\\[...\\]  \n&gt;  \n&gt;approaches like cloob-conditioned latent diffusion are bringing down the barrier to entry and making it possible for individuals or small organisations to have a crack at training diffusion models without $$$ of compute.  \n&gt;  \n&gt;this little experiment of ours has shown that it is possible to train one of these models on a relatively small dataset and end up with something that can create pleasing outputs, even if it can’t quite manage an avocado armchair.\n\n[16 examples](https://wandb.ai/johnowhitaker/jw-ft-cloob-latent-diffusion/reports/fine-tuning-cloob-latent-diffusion--vmlldzoxnzk5otgz).\n\n[colab notebook ccld (wikiart) demo](https://colab.research.google.com/drive/1hpu6tz44brmnkou4g6luhy1_imgdl1fw?usp=sharing).\n\n[colab notebook cloob conditioned latent diffusion](https://www.reddit.com/r/bigsleep/comments/tvjur2/colab_notebook_clooblatentdiffusionsampleimages/) (trained on yfcc100m dataset).",
    "matched_bias_types": [
      "gender",
      "general_bias",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "bias",
      "biases",
      "diffusion",
      "model"
    ]
  },
  {
    "id": "u8w34c",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "“10 Greatest Painters of All Time Say They Love You” using Latent Diffusion + LAION 400M model ",
    "clean_text_lc": " 10 greatest painters of all time say they love you” using latent diffusion + laion 400m model ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model"
    ]
  },
  {
    "id": "104zaf9",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "ULTIMATE FREE Stable Diffusion Model! GODLY Results! ",
    "clean_text_lc": "ultimate free stable diffusion model  godly results! ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "1c6fyvd",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model **Paper**: [\n\n**Code**: [https://github.com/HL-hanlin/Ctrl-Adapter](https://github.com/HL-hanlin/Ctrl-Adapter)\n\n**Models**: [https://huggingface.co/hanlincs/Ctrl-Adapter](https://huggingface.co/hanlincs/Ctrl-Adapter)\n\n**Project page**: [https://ctrl-adapter.github.io/](https://ctrl-adapter.github.io/)\n\n**Abstract**:\n\n>ControlNets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses. However, there are several challenges when leveraging the pretrained image ControlNets for controlled video generation. First, pretrained ControlNet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training ControlNets for new backbones is a big burden. Second, ControlNet features for different frames might not effectively handle the temporal consistency. To address these challenges, we introduce **Ctrl-Adapter**, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained ControlNets (and improving temporal alignment for videos). Ctrl-Adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing. In Ctrl-Adapter, we train adapter layers that fuse pretrained ControlNet features to different image/video diffusion models, while keeping the parameters of the ControlNets and the diffusion models frozen. Ctrl-Adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos. We also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control. Moreover, Ctrl-Adapter enables control from multiple conditions by simply taking the (weighted) average of ControlNet outputs. With diverse image/video diffusion backbones (SDXL, Hotshot-XL, I2VGen-XL, and SVD), Ctrl-Adapter matches ControlNet for image control and outperforms all baselines for video control (achieving the SOTA accuracy on the DAVIS 2017 dataset) with significantly lower computational costs (less than 10 GPU hours).",
    "clean_text_lc": "ctrl-adapter: an efficient and versatile framework for adapting diverse controls to any diffusion model  *paper**: [\n\n**code**: [https://github.com/hl-hanlin/ctrl-adapter](https://github.com/hl-hanlin/ctrl-adapter)\n\n**models**: [https://huggingface.co/hanlincs/ctrl-adapter](https://huggingface.co/hanlincs/ctrl-adapter)\n\n**project page**: [https://ctrl-adapter.github.io/](https://ctrl-adapter.github.io/)\n\n**abstract**:\n\n>controlnets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses. however, there are several challenges when leveraging the pretrained image controlnets for controlled video generation. first, pretrained controlnet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training controlnets for new backbones is a big burden. second, controlnet features for different frames might not effectively handle the temporal consistency. to address these challenges, we introduce **ctrl-adapter**, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained controlnets (and improving temporal alignment for videos). ctrl-adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing. in ctrl-adapter, we train adapter layers that fuse pretrained controlnet features to different image/video diffusion models, while keeping the parameters of the controlnets and the diffusion models frozen. ctrl-adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos. we also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control. moreover, ctrl-adapter enables control from multiple conditions by simply taking the (weighted) average of controlnet outputs. with diverse image/video diffusion backbones (sdxl, hotshot-xl, i2vgen-xl, and svd), ctrl-adapter matches controlnet for image control and outperforms all baselines for video control (achieving the sota accuracy on the davis 2017 dataset) with significantly lower computational costs (less than 10 gpu hours).",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "controlnet",
      "diffusion",
      "image generation",
      "model",
      "sdxl"
    ]
  },
  {
    "id": "11igp6s",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI Generated Image \"Gay Arab Prophet\" ",
    "clean_text_lc": "ai generated image  gay arab prophet\" ",
    "matched_bias_types": [
      "lgbtq",
      "race"
    ],
    "matched_keywords": [
      "ai",
      "arab",
      "gay",
      "gayarab"
    ]
  },
  {
    "id": "14l9707",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "TRAINED AI.MODEL [CGI RE-ENGINEERED] ",
    "clean_text_lc": "trained ai model [cgi re-engineered] ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "engineer",
      "model"
    ]
  },
  {
    "id": "15h5szk",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "StyleGANEX: The Game-Changing AI Model for Image Transformations ",
    "clean_text_lc": "styleganex: the game-changing ai model for image transformations ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "zg1ry2",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Created a chef's instagram account and posts. All generated with chatgpt and dall-e ",
    "clean_text_lc": "created a chef s instagram account and posts. all generated with chatgpt and dall-e ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "chatgpt",
      "chef"
    ]
  },
  {
    "id": "y3krde",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "My Waking Dream - My AI art projected on a 71 foot pyramid at Burning Man. ",
    "clean_text_lc": "my waking dream - my ai art projected on a 71 foot pyramid at burning man  ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "man"
    ]
  },
  {
    "id": "11ypbak",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Adobe Firefly AI Is The New Boss For The Anti-Ai Art Crowd! ",
    "clean_text_lc": "adobe firefly ai is the new boss for the anti-ai art crowd  ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "boss"
    ]
  },
  {
    "id": "16x9j6z",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "r/technology: $260 Million AI Company Releases Undeletable Chatbot That Gives Detailed Instructions on Murder, Ethnic Cleansing ",
    "clean_text_lc": "r/technology:  260 million ai company releases undeletable chatbot that gives detailed instructions on murder, ethnic cleansing ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "ai",
      "ethnic"
    ]
  },
  {
    "id": "zp4j4g",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "An ancient ornate intricate old tome spell book - Anime AI model using snowpixel app ",
    "clean_text_lc": "an ancient ornate intricate old tome spell book - anime ai model using snowpixel app ",
    "matched_bias_types": [
      "age",
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model",
      "modelold",
      "old"
    ]
  },
  {
    "id": "xt2ylc",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "JOIN A NEW STUDY! Understanding experiences of image-based sexual abuse and nonconsensual sexual deepfakes (This study has been reviewed and received ethics clearance through a Harvard University Area Committee on the Use of Human Subjects).  *\\*\\*This study is being conducted to satisfy the degree requirements of the Master in Liberal Arts (ALM) in Anthropology and Archaeology at the Harvard University Extension School by Victoria Rousay, under the supervision of Dr. Gabriella Coleman, a full professor in the Department of Anthropology at Harvard University. This study has been reviewed and received ethics clearance through a Harvard University Area Committee on the Use of Human Subjects.\\*\\**\n\nHas someone ever CREATED, SHARED, or THREATENED to create or share NUDE OR SEXUAL IMAGES/VIDEOS OF YOU WITHOUT YOUR CONSENT?\n\n* Eligible participants will be asked to participate in a 30 to 60 minute, semi-structured, narrative interview.\n* During this interview, participants will be asked sensitive questions regarding their IBSA experience(s)\n\nContribute to raising awareness about image-based sexual abuse and deepfake pornography by participating in this qualitative study!\n\n[Click HERE for the Survey Link]( \n\n&amp;#x200B;\n\n[Survey Participant Flyer](https://preview.redd.it/ul0b6k4bp8r91.png?width=1545&amp;format=png&amp;auto=webp&amp;s=5d64ea16492df67783271e286fcb856837ca3559)",
    "clean_text_lc": "join a new study  understanding experiences of image-based sexual abuse and nonconsensual sexual deepfakes (this study has been reviewed and received ethics clearance through a harvard university area committee on the use of human subjects).  *\\*\\*this study is being conducted to satisfy the degree requirements of the master in liberal arts (alm) in anthropology and archaeology at the harvard university extension school by victoria rousay, under the supervision of dr. gabriella coleman, a full professor in the department of anthropology at harvard university. this study has been reviewed and received ethics clearance through a harvard university area committee on the use of human subjects.\\*\\**\n\nhas someone ever created, shared, or threatened to create or share nude or sexual images/videos of you without your consent?\n\n* eligible participants will be asked to participate in a 30 to 60 minute, semi-structured, narrative interview.\n* during this interview, participants will be asked sensitive questions regarding their ibsa experience(s)\n\ncontribute to raising awareness about image-based sexual abuse and deepfake pornography by participating in this qualitative study!\n\n[click here for the survey link]( \n\n&amp;#x200b;\n\n[survey participant flyer](https://preview.redd.it/ul0b6k4bp8r91.png?width=1545&amp;format=png&amp;auto=webp&amp;s=5d64ea16492df67783271e286fcb856837ca3559)",
    "matched_bias_types": [
      "occupation",
      "study"
    ],
    "matched_keywords": [
      "deepfake",
      "degree",
      "professor",
      "school",
      "university",
      "universityschoolprofessordegree"
    ]
  },
  {
    "id": "18hcj8j",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Creating a new AI Tiktok Model using SD and Deepfake ",
    "clean_text_lc": "creating a new ai tiktok model using sd and deepfake ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "deepfake",
      "model",
      "sd"
    ]
  },
  {
    "id": "v4f1yg",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "What AI model generated this? Can anybody tell me a guess of how this images were generated?\n\n&amp;#x200B;\n\nThey are a NFT collection derived from [this]( collection (the original one).\n\n&amp;#x200B;\n\nIs there any chance this could have been done with some sort of VQGAN + original collection as mask + external images for training?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/22esvsjmji391.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=c1ef2a2784455b599fab055d87a89141ac22eaf3\n\nhttps://preview.redd.it/tddbjtjmji391.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=0ee0dff262fe2e90764eba4842ec67c4d92038cd\n\nhttps://preview.redd.it/59mwstjmji391.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=f1c0a6022abff72fe77809541eb8b00414a96d45\n\nhttps://preview.redd.it/jvmfssjmji391.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=7ccd518cb0bb9246ee88ec1c799b36d6bbd9701d",
    "clean_text_lc": "what ai model generated this  can anybody tell me a guess of how this images were generated?\n\n&amp;#x200b;\n\nthey are a nft collection derived from [this]( collection (the original one).\n\n&amp;#x200b;\n\nis there any chance this could have been done with some sort of vqgan + original collection as mask + external images for training?\n\n&amp;#x200b;\n\nhttps://preview.redd.it/22esvsjmji391.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=c1ef2a2784455b599fab055d87a89141ac22eaf3\n\nhttps://preview.redd.it/tddbjtjmji391.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=0ee0dff262fe2e90764eba4842ec67c4d92038cd\n\nhttps://preview.redd.it/59mwstjmji391.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=f1c0a6022abff72fe77809541eb8b00414a96d45\n\nhttps://preview.redd.it/jvmfssjmji391.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=7ccd518cb0bb9246ee88ec1c799b36d6bbd9701d",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "v008p3",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Latent Majesty Diffusion error I can't get the new Latent Majesty Diffusion notebook to work (V-Majesty is A+++ tho)\n\nThe notebook can be found here: [\n\nThis is the error I get:\n\n    EOFError                                  Traceback (most recent call last)\n    \n    &lt;ipython-input-7-925b5ddae02e&gt; in &lt;module&gt;()\n         27 \n         28 config = OmegaConf.load(\"./latent-diffusion/configs/latent-diffusion/txt2img-1p4B-eval.yaml\")  # TODO: Optionally download from same location as ckpt and chnage this logic\n    ---&gt; 29 model = load_model_from_config(config, f\"{model_path}/latent_diffusion_txt2img_f8_large.ckpt\",False, latent_diffusion_model)  # TODO: check path\n         30 model = model.half().eval().to(device)\n         31 #if(latent_diffusion_model == \"finetuned\"):\n    \n    2 frames\n    \n    /usr/local/lib/python3.7/dist-packages/torch/serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)\n        918             \"functionality.\")\n        919 \n    --&gt; 920     magic_number = pickle_module.load(f, **pickle_load_args)\n        921     if magic_number != MAGIC_NUMBER:\n        922         raise RuntimeError(\"Invalid magic number; corrupt file?\")\n    \n    EOFError: Ran out of input\n\nIf i remember correctly, I got this same error message with [https://colab.research.google.com/github/Dango233/PrincessGenerator/blob/main/Latent\\_Princess\\_Generator.ipynb](https://colab.research.google.com/github/Dango233/PrincessGenerator/blob/main/Latent_Princess_Generator.ipynb)",
    "clean_text_lc": "latent majesty diffusion error i can t get the new latent majesty diffusion notebook to work (v-majesty is a+++ tho)\n\nthe notebook can be found here: [\n\nthis is the error i get:\n\n    eoferror                                  traceback (most recent call last)\n    \n    &lt;ipython-input-7-925b5ddae02e&gt; in &lt;module&gt;()\n         27 \n         28 config = omegaconf.load(\"./latent-diffusion/configs/latent-diffusion/txt2img-1p4b-eval.yaml\")  # todo: optionally download from same location as ckpt and chnage this logic\n    ---&gt; 29 model = load_model_from_config(config, f\"{model_path}/latent_diffusion_txt2img_f8_large.ckpt\",false, latent_diffusion_model)  # todo: check path\n         30 model = model.half().eval().to(device)\n         31 #if(latent_diffusion_model == \"finetuned\"):\n    \n    2 frames\n    \n    /usr/local/lib/python3.7/dist-packages/torch/serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)\n        918             \"functionality.\")\n        919 \n    --&gt; 920     magic_number = pickle_module.load(f, **pickle_load_args)\n        921     if magic_number != magic_number:\n        922         raise runtimeerror(\"invalid magic number; corrupt file?\")\n    \n    eoferror: ran out of input\n\nif i remember correctly, i got this same error message with [https://colab.research.google.com/github/dango233/princessgenerator/blob/main/latent\\_princess\\_generator.ipynb](https://colab.research.google.com/github/dango233/princessgenerator/blob/main/latent_princess_generator.ipynb)",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ckpt",
      "diffusion",
      "model"
    ]
  },
  {
    "id": "zs1jy1",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "An ancient ornate intricate old tome spell book - AI Anime art ",
    "clean_text_lc": "an ancient ornate intricate old tome spell book - ai anime art ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "ai",
      "old"
    ]
  },
  {
    "id": "wd198p",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "A batch of female portraits by Midjourney ",
    "clean_text_lc": "a batch of female portraits by midjourney ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "female",
      "midjourney"
    ]
  },
  {
    "id": "vij3qn",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Google's text-to-image model Parti (\"Pathways Autoregressive Text-to-Image\"). Paper is \"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\". ",
    "clean_text_lc": "google s text-to-image model parti (\"pathways autoregressive text-to-image\"). paper is \"scaling autoregressive models for content-rich text-to-image generation\". ",
    "matched_bias_types": [
      "gender",
      "income",
      "occupation"
    ],
    "matched_keywords": [
      "image generation",
      "model",
      "rich",
      "text-to-image"
    ]
  },
  {
    "id": "srzyfj",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Winter in Atlanta [Disco Diffusion 4.1]. Concept video for an upcoming music video job (a different song than used though) ",
    "clean_text_lc": "winter in atlanta  disco diffusion 4.1]. concept video for an upcoming music video job (a different song than used though) ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "job"
    ]
  },
  {
    "id": "1alzkk6",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI, trained on surviving original cel art, could do a far better job of remastering cartoons. [removed]",
    "clean_text_lc": "ai  trained on surviving original cel art, could do a far better job of remastering cartoons. [removed]",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "job"
    ]
  },
  {
    "id": "xf8739",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": ". Tambourine Man - But every lyric is an Ai generated image ",
    "clean_text_lc": "  tambourine man - but every lyric is an ai generated image ",
    "matched_bias_types": [
      "gender"
    ],
    "matched_keywords": [
      "ai",
      "man"
    ]
  },
  {
    "id": "w5o6kg",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Stable Diffusion beta signup form is available. This model will be open source eventually. [Stable Diffusion beta signup form]( [Twitter reference](https://twitter.com/EMostaque/status/1550553941909413890).\n\nFrom [this tweet](https://twitter.com/Buntworthy/status/1537794729877938176):\n\n&gt;Looks like the CompVis Latent Diffusion folks are training an imagen style text2image Latent Diffusion model. Exciting!  \n&gt;  \n&gt;\\[...\\]\n\n[Latent Diffusion](https://www.reddit.com/r/bigsleep/comments/tw8656/woah_there_dragonman_16_output_images_with/).\n\n[Google Imagen](https://imagen.research.google/).\n\nSubreddit r/StableDiffusion.",
    "clean_text_lc": "stable diffusion beta signup form is available  this model will be open source eventually. [stable diffusion beta signup form]( [twitter reference](https://twitter.com/emostaque/status/1550553941909413890).\n\nfrom [this tweet](https://twitter.com/buntworthy/status/1537794729877938176):\n\n&gt;looks like the compvis latent diffusion folks are training an imagen style text2image latent diffusion model. exciting!  \n&gt;  \n&gt;\\[...\\]\n\n[latent diffusion](https://www.reddit.com/r/bigsleep/comments/tw8656/woah_there_dragonman_16_output_images_with/).\n\n[google imagen](https://imagen.research.google/).\n\nsubreddit r/stablediffusion.",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "xh8h1q",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "So... if I ask stable diffusion for a CD ... it creates a picture of a woman. Twice now. \"cd\" no other text. Currently v-scale 15. But when I specify \"audio cd\" or \"music cd\", it isn't a woman playing music. Go figure. \n\nI guess in 2022, \"cd\" is more closely associated with crossdressers than audio discs. And funny enough, they don't look like drag queens. \n\nI just wanted a copyright free picture of a compact disc. Didn't think I'd get boobs!",
    "clean_text_lc": "so .. if i ask stable diffusion for a cd ... it creates a picture of a woman. twice now. \"cd\" no other text. currently v-scale 15. but when i specify \"audio cd\" or \"music cd\", it isn't a woman playing music. go figure. \n\ni guess in 2022, \"cd\" is more closely associated with crossdressers than audio discs. and funny enough, they don't look like drag queens. \n\ni just wanted a copyright free picture of a compact disc. didn't think i'd get boobs!",
    "matched_bias_types": [
      "gender",
      "lgbtq"
    ],
    "matched_keywords": [
      "crossdresser",
      "diffusion",
      "drag",
      "stable diffusion",
      "woman",
      "womandrag"
    ]
  },
  {
    "id": "x85x70",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "How to combine stable diffusion with a model which predicts aesthetics score? Does anyone know how you could combine a model like [Aesthetic Score Predictor]( with stable diffusion?\nThey used this model to filter training images by score. Iit seems like a lot of people just tune their prompt to make images more aesthetic, by adding certain words.\n\n What if we could just take any image and move it along an aesthetics gradient and make it more or less aesthetic? Imagine sliders in Stable Diffusion frontends for this and potentially other attributes as well. \nWe've seen this for GANs in the past, so I guess someone here has some experience with this.",
    "clean_text_lc": "how to combine stable diffusion with a model which predicts aesthetics score  does anyone know how you could combine a model like [aesthetic score predictor]( with stable diffusion?\nthey used this model to filter training images by score. iit seems like a lot of people just tune their prompt to make images more aesthetic, by adding certain words.\n\n what if we could just take any image and move it along an aesthetics gradient and make it more or less aesthetic? imagine sliders in stable diffusion frontends for this and potentially other attributes as well. \nwe've seen this for gans in the past, so i guess someone here has some experience with this.",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "prompt",
      "stable diffusion"
    ]
  },
  {
    "id": "wso458",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "[Stable Diffusion] The World's Most Heterosexual Images: lgbt+ prompts with the weight set to -30 [deleted]",
    "clean_text_lc": " stable diffusion] the world's most heterosexual images: lgbt+ prompts with the weight set to -30 [deleted]",
    "matched_bias_types": [
      "lgbtq"
    ],
    "matched_keywords": [
      "diffusion",
      "heterosexual",
      "prompt",
      "stable diffusion"
    ]
  },
  {
    "id": "todsox",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Paper \"Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors\", Gafni et al 2022. From the paper: \"Our main contributions are introducing additional controlling elements in the form of a scene, improve the tokenization process, and adapt classifier-free guidance to transformers.\" [Paper]( Additional samples are on pp. 14-17.\n\nAbstract:\n\n&gt;Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote.\n\n[Twitter thread](https://twitter.com/ak92501/status/1507172176620929024) (not from the authors).\n\nNo code seems to be available.",
    "clean_text_lc": "paper  make-a-scene: scene-based text-to-image generation with human priors\", gafni et al 2022. from the paper: \"our main contributions are introducing additional controlling elements in the form of a scene, improve the tokenization process, and adapt classifier-free guidance to transformers.\" [paper]( additional samples are on pp. 14-17.\n\nabstract:\n\n&gt;recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. while these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. we propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. our model achieves state-of-the-art fid and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. through scene controllability, we introduce several new capabilities: (i) scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote.\n\n[twitter thread](https://twitter.com/ak92501/status/1507172176620929024) (not from the authors).\n\nno code seems to be available.",
    "matched_bias_types": [
      "gender",
      "location",
      "occupation"
    ],
    "matched_keywords": [
      "image generation",
      "model",
      "prompt",
      "region",
      "text-to-image"
    ]
  },
  {
    "id": "v4a7e1",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Why is Dalle mini free and accessible while others require subscriptions Namely Mid Journey needs subscription and Dalle 2 has limited generations",
    "clean_text_lc": "why is dalle mini free and accessible while others require subscriptions namely mid journey needs subscription and dalle 2 has limited generations",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "accessible",
      "dalle"
    ]
  },
  {
    "id": "131rpre",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "How to Generate Studio Quality Realistic Photos via Kohya Web GUI Stable Diffusion DreamBooth LoRA Training Full Tutorial - 35 Video Chapters - Manually Fixed Subtitles - Non Technical People Friendly - Uses Realistic Vision V2 Model ",
    "clean_text_lc": "how to generate studio quality realistic photos via kohya web gui stable diffusion dreambooth lora training full tutorial - 35 video chapters - manually fixed subtitles - non technical people friendly - uses realistic vision v2 model ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "dreambooth",
      "kohya",
      "lora",
      "model",
      "stable diffusion"
    ]
  },
  {
    "id": "14g9fdc",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Easily Create Premium Studio Quality Marketing Videos and Images with AI [removed]",
    "clean_text_lc": "easily create premium studio quality marketing videos and images with ai  removed]",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "marketing"
    ]
  },
  {
    "id": "vn6hmk",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Old Luke Skywaller - MidJourney ",
    "clean_text_lc": "old luke skywaller - midjourney ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "midjourney",
      "old"
    ]
  },
  {
    "id": "u98ycy",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "For developers: OpenAI has released CLIP model ViT-L/14@336p ",
    "clean_text_lc": "for developers: openai has released clip model vit-l/14 336p ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "openai"
    ]
  },
  {
    "id": "x0kucp",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "AI Images: Last Week Tonight with John Oliver (HBO) [deleted]",
    "clean_text_lc": "ai images: last week tonight with john oliver  hbo) [deleted]",
    "matched_bias_types": [
      "naming"
    ],
    "matched_keywords": [
      "ai",
      "ai image",
      "john"
    ]
  },
  {
    "id": "1e7l5f4",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "We Got a Job Offer in SECourses Discord Channel Related to AI (Stable Diffusion) ",
    "clean_text_lc": "we got a job offer in secourses discord channel related to ai  stable diffusion) ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "diffusion",
      "job",
      "stable diffusion"
    ]
  },
  {
    "id": "wv976l",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "RIP DALLE 2 (Midjourney beta model tests) ",
    "clean_text_lc": "rip dalle 2  midjourney beta model tests) ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "dalle",
      "midjourney",
      "model"
    ]
  },
  {
    "id": "12xv9mk",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"The Future of AI Relies on a High School Teacher’s Free Database: With >5b images, LAION has become central to the future of artificial intelligence—and a growing debate over how to regulate it\" ",
    "clean_text_lc": " the future of ai relies on a high school teacher’s free database: with >5b images, laion has become central to the future of artificial intelligence—and a growing debate over how to regulate it\" ",
    "matched_bias_types": [
      "gender",
      "occupation",
      "study"
    ],
    "matched_keywords": [
      "ai",
      "artificial intelligence",
      "school",
      "teacher"
    ]
  },
  {
    "id": "v30ifl",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "GitHub repo \"MiniDiffusion\" and a Colab notebook for training a diffusion model on a custom dataset ",
    "clean_text_lc": "github repo  minidiffusion\" and a colab notebook for training a diffusion model on a custom dataset ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model"
    ]
  },
  {
    "id": "17p6juu",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "College student survey: 12% use generative AI daily, half regularly, 75% say they'd use even if colleges ban it ",
    "clean_text_lc": "college student survey: 12% use generative ai daily  half regularly, 75% say they'd use even if colleges ban it ",
    "matched_bias_types": [
      "study"
    ],
    "matched_keywords": [
      "ai",
      "college"
    ]
  },
  {
    "id": "wge9nb",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Satoshi Nakamoto By Dalle Mini / Craiyon.com ",
    "clean_text_lc": "satoshi nakamoto by dalle mini / craiyon com ",
    "matched_bias_types": [
      "naming"
    ],
    "matched_keywords": [
      "dalle",
      "satoshi"
    ]
  },
  {
    "id": "sqsdfj",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "From a few images to a 3D model with AI! ",
    "clean_text_lc": "from a few images to a 3d model with ai  ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "model"
    ]
  },
  {
    "id": "wnzy9r",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Text-to-image web app \"retrieval-augmented-diffusion\" from afiaka87 generates 768x768 pixel images using a retrieval-augmented latent diffusion model from CompVis. Example: \"a mech ready for battle\". Details in a comment. ",
    "clean_text_lc": "text-to-image web app  retrieval-augmented-diffusion\" from afiaka87 generates 768x768 pixel images using a retrieval-augmented latent diffusion model from compvis. example: \"a mech ready for battle\". details in a comment. ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "diffusion",
      "model",
      "text-to-image"
    ]
  },
  {
    "id": "v6zu56",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "White Walkers - The Silent Death? - AI Art Experiment in 4K 60 FPS w/ GPT-3 ",
    "clean_text_lc": "white walkers - the silent death  - ai art experiment in 4k 60 fps w/ gpt-3 ",
    "matched_bias_types": [
      "race"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "gpt",
      "white"
    ]
  },
  {
    "id": "xvn91q",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "DIE ANTWOORD - Age of Illusion (very trippy Ai music video) ",
    "clean_text_lc": "die antwoord - age of illusion  very trippy ai music video) ",
    "matched_bias_types": [
      "age"
    ],
    "matched_keywords": [
      "age",
      "ai"
    ]
  },
  {
    "id": "1bqwuf2",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "OpenAI previews its voice-cloning NN model, \"Voice Engine\" ",
    "clean_text_lc": "openai previews its voice-cloning nn model  \"voice engine\" ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "model",
      "openai"
    ]
  },
  {
    "id": "xbz6g0",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"The AI startup erasing call center worker accents: is it fighting bias – or perpetuating it?\" ",
    "clean_text_lc": " the ai startup erasing call center worker accents: is it fighting bias – or perpetuating it?\" ",
    "matched_bias_types": [
      "general_bias",
      "occupation"
    ],
    "matched_keywords": [
      "ai",
      "bias",
      "biasworker",
      "worker"
    ]
  },
  {
    "id": "1227e20",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "\"I lost everything that made me love my 3D artist job through Midjourney v5 overnight.\" ",
    "clean_text_lc": " i lost everything that made me love my 3d artist job through midjourney v5 overnight.\" ",
    "matched_bias_types": [
      "occupation"
    ],
    "matched_keywords": [
      "job",
      "midjourney"
    ]
  },
  {
    "id": "13gxjix",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Hey! I'm doing an Art project for the local Autism Art Exhibition - and I got a survey for AI art for ya! [removed]",
    "clean_text_lc": "hey  i'm doing an art project for the local autism art exhibition - and i got a survey for ai art for ya! [removed]",
    "matched_bias_types": [
      "disability"
    ],
    "matched_keywords": [
      "ai",
      "ai art",
      "autism"
    ]
  },
  {
    "id": "x9ht7i",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "My Heritage and pencil drawing of woman. AI generated movement. ",
    "clean_text_lc": "my heritage and pencil drawing of woman  ai generated movement. ",
    "matched_bias_types": [
      "gender",
      "race"
    ],
    "matched_keywords": [
      "ai",
      "heritage",
      "woman",
      "womanheritage"
    ]
  },
  {
    "id": "11y8ik9",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "MeinaMix Model Test2 using SD and Controlnet ",
    "clean_text_lc": "meinamix model test2 using sd and controlnet ",
    "matched_bias_types": [
      "gender",
      "occupation"
    ],
    "matched_keywords": [
      "controlnet",
      "model",
      "sd"
    ]
  },
  {
    "id": "v2z1mk",
    "subreddit": "MediaSynthesis",
    "subreddit_group": "critical_discussion",
    "clean_text": "Desert Civilization - Created using DDv5.2 AI ",
    "clean_text_lc": "desert civilization - created using ddv5 2 ai ",
    "matched_bias_types": [
      "location"
    ],
    "matched_keywords": [
      "ai",
      "desert"
    ]
  }
]