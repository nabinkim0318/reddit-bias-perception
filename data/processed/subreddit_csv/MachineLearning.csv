id,subreddit,title,selftext,comments,top_comments,score,num_comments,upvote_ratio,flair,created_utc
1lt2qh9,MachineLearning,[P] How I took my mediocre FashionMNIST model and supercharged it with MobileNetV2 & Transfer Learning — results inside!,"Hey folks! 👋

I wanted to share a milestone in my ML learning journey.

I first trained a simple fully connected neural net on the classic Fashion MNIST dataset (28x28 grayscale). While the model learned decently, the test accuracy maxed out around 84%. I was stuck with overfitting, no matter how I tweaked layers or regularization.

Then I tried something new: Transfer Learning.
I resized the dataset to RGB (96×96), loaded MobileNetV2 with imagenet weights, and added my own classifier layers on top. Guess what?

✅ Test accuracy jumped past 92%
✅ Training time reduced significantly
✅ Model generalized beautifully

This experience taught me that:

You don't need to train huge models from scratch to get great results.

Pre-trained models act like ""knowledge containers"" — you're standing on the shoulders of giants.

FashionMNIST isn't just a basic dataset — it’s great for testing architecture improvements.


Happy to share the code or walk through the setup if anyone’s curious. Also planning to deploy it on Hugging Face soon!

Would love feedback or similar experiences — what dataset-model combos surprised you the most?

First model :

https://huggingface.co/spaces/lalmasala/apparelclassifier

Second model:

https://huggingface.co/spaces/lalmasala/apparelclassifiernew
",[],[],0,0,0.5,Project,1751812565.0
1lt24oh,MachineLearning,[D] IJCV Special Issue Reviews,"I submitted to IJCV special issue on Visual Domain Generalization in Real-World Applications. The first round reviews were supposed to be out on 10th June, but aren't out yet. Does anyone have prior experience of how the timelines of these special issues work?",[],[],1,0,1.0,Research,1751810928.0
1lt24dw,MachineLearning,[P] Can anyone help me with the following forecasting Scenario?,"Can anyone tell me how the following can be done, every month, 400-500 records with 5 attributes gets added to the dataset. Lets say initally there are 32 months of data, so 32x400 records of data, I need to build a model that is able to predict the next month's 5 attributes based on the historial data. I have studied about ARIMA, exponential smoothening and other time series forecasting techniques, but they usually have a single attribute, 1 record per timestamp. Here I have 5 attributes, so how do I do this? Can anyone help me move in the right direction?",[],[],1,0,1.0,Project,1751810908.0
1lt1s51,MachineLearning,[R] Feeding categorical information into a GAN discriminator,"Hi,

I am running a set up where the generator is 3D and the discriminator is 2D.

Feeding the discriminator random slices from all three axis does not work, because the discriminator can then not distinguish between the differences in structure between the three planes.

I wanted to ask you whats the SOTA way of incorporating this information into the discriminator.
Also, should I feed this information to the input layer of the model or to every convolutional block/level.

Thanks in advance.",[],[],1,0,1.0,Research,1751809982.0
1lt1fek,MachineLearning,[D] Lessons learned while experimenting with scalable retrieval pipelines for large language models,"Over the past few weeks, we've been building and experimenting with different retrieval architectures to make language models answer more accurately from custom data.

A few observations we found interesting and would love to discuss:

Even small latency improvements in the retrieval phase can noticeably improve user perception of quality.

Pre‑processing and smart chunking often outperform fancy vector database tuning.

Monitoring retrieval calls (failures, outliers, rare queries) can reveal product insights way before you reach large scale.


We're currently prototyping an internal developer‑facing service around this, mainly focused on:

abstracting away infra concerns

measuring recall quality

exposing insights to devs in real time


Has anyone here experimented with building similar pipelines or internal tooling?

I'd love to hear:

What metrics you found most useful for measuring retrieval quality?

How you balanced performance vs. cost in production?


Curious to learn from others working on similar problems.




","[""btw, we're building a small tool internally to make this easier  happy to chat if anyone's interested\n[Lumine](https://lumine-landing-page.vercel.app/)"", ""I haven't developed enough retrieval pipelines to give meaningful insights, but curious about the architectures you tried and the performance in your particular use case. \n\nAlso what is the context/format of your data and what preprocessing and chuncking methods give the best results for you?""]","[""btw, we're building a small tool internally to make this easier  happy to chat if anyone's interested\n[Lumine](https://lumine-landing-page.vercel.app/)"", ""I haven't developed enough retrieval pipelines to give meaningful insights, but curious about the architectures you tried and the performance in your particular use case. \n\nAlso what is the context/format of your data and what preprocessing and chuncking methods give the best results for you?""]",3,3,1.0,Discussion,1751808992.0
1lt04rz,MachineLearning,[P]Simulating Causal Chains in Engineering Problems via Logic,"I’ve built an open-source logic simulator that allows users to input natural-language propositions, extract symbolic variables, and simulate reasoning paths across formulas.



Unlike LLM-based systems, this simulator visualizes the logic structure explicitly: users can trace all property connections, view the resulting path networks, and interactively modify weights or filters.



This is a \*\*safe version\*\* without internal algorithms (no AI code, no model weights) — intended purely for demonstration and UI/UX discussion. I’d love feedback on:

\- the visual interface

\- how intuitive the simulation feels

\- possible improvements to symbolic reasoning workflows

https://preview.redd.it/b4z3pztrz8bf1.png?width=1914&format=png&auto=webp&s=c0b4ef6977c41b85dcc87b41644cbf1d1cb97c2d

\-> Before Learning

https://preview.redd.it/ousoxy1vz8bf1.png?width=1868&format=png&auto=webp&s=fa02ab325609005af576d057a48569aca6581171

\-> After Learning



https://preview.redd.it/y9kxxqmyz8bf1.png?width=1920&format=png&auto=webp&s=c8701f200287e834920a4876a2ff46cd2ede5af8


\-> In Training

Live demo (video): \[https://youtu.be/5wTX7lzmPog\]","['Hey everyone! 👋\n\n\n\nI\'m working on a tool that simulates how people solve engineering problems step by step, using diagrams and logic instead of just math equations.\n\n\n\n💡 Example:\n\n\\> You know mass and acceleration. Want to find force?\n\n\n\nIt finds:  \n\n\\*\\*F = m × a\\*\\*  \n\nand shows this visually in a diagram — so you can ""see"" the logic path.\n\n\n\nEven for more complex chains like:\n\n\\> Torque → Angular Acceleration → Rotational Inertia → Stress  \n\nIt maps all possible paths automatically.\n\n\n\nLet me know what you think!\n\nWould love suggestions, use cases, or things to add 🙂', 'This is still a work in progress. I would really appreciate any suggestions or critical feedback from the community.', 'Nice! ✨, can you try with a another ""blue-circle"" for a New connections?, thanks and lucky with your project']","['Hey everyone! 👋\n\n\n\nI\'m working on a tool that simulates how people solve engineering problems step by step, using diagrams and logic instead of just math equations.\n\n\n\n💡 Example:\n\n\\> You know mass and acceleration. Want to find force?\n\n\n\nIt finds:  \n\n\\*\\*F = m × a\\*\\*  \n\nand shows this visually in a diagram — so you can ""see"" the logic path.\n\n\n\nEven for more complex chains like:\n\n\\> Torque → Angular Acceleration → Rotational Inertia → Stress  \n\nIt maps all possible paths automatically.\n\n\n\nLet me know what you think!\n\nWould love suggestions, use cases, or things to add 🙂', 'This is still a work in progress. I would really appreciate any suggestions or critical feedback from the community.', 'Nice! ✨, can you try with a another ""blue-circle"" for a New connections?, thanks and lucky with your project']",4,7,0.7,Project,1751805223.0
1lsw6y5,MachineLearning,[R] Visualization tools for paper illustrations and figures,"I am curious about which tools people use to create their figures/visualizations in scientific papers. I mostly rely on power point or draw.io and import the PDF in the latex code, but the result is not aesthetic at all","['Same my figures are ugly.', 'everyone in my lab still uses Powerpoint, and we are publishing on top machine learning conferences.', ""I've been using PowerPoint and [draw.io](http://draw.io) for my paper illustrations and importing them as PDFs into LaTeX, but the results often lack polish. I’m looking to level up the aesthetics and clarity of my figures—especially for diagrams, conceptual illustrations, and flowcharts.\n\nWhat tools or workflows do you use to make your paper visuals more professional and visually appealing? Bonus points if it's LaTeX-friendly or vector-based. Curious to hear what’s working well for others in the research community!"", 'Same for me. But recently I switched to Figma, the results are much better.', ""You may be interested in [https://vexlio.com](https://vexlio.com) (I am the developer) - it's vector based and has LaTeX equation support built in.""]","['Same my figures are ugly.', 'everyone in my lab still uses Powerpoint, and we are publishing on top machine learning conferences.', ""I've been using PowerPoint and [draw.io](http://draw.io) for my paper illustrations and importing them as PDFs into LaTeX, but the results often lack polish. I’m looking to level up the aesthetics and clarity of my figures—especially for diagrams, conceptual illustrations, and flowcharts.\n\nWhat tools or workflows do you use to make your paper visuals more professional and visually appealing? Bonus points if it's LaTeX-friendly or vector-based. Curious to hear what’s working well for others in the research community!"", 'Same for me. But recently I switched to Figma, the results are much better.', ""You may be interested in [https://vexlio.com](https://vexlio.com) (I am the developer) - it's vector based and has LaTeX equation support built in.""]",3,6,1.0,Research,1751790066.0
1lsraze,MachineLearning,[D] Requesting arXiv Endorsement – Independent Researcher Submitting First ML Paper,"Hi everyone,

I'm in the process of submitting my first research paper to arXiv. As I’m not affiliated with any academic institution, I need an endorsement to upload my paper under cs.LG category. I’d appreciate it if someone with an arXiv submission history could help by endorsing me. Here are the details of the paper:

**Title:** *How Effective are Nature-Inspired Optimisation Techniques in Hyperparameter Tuning of Machine Learning Models*

**Abstract:** Hyperparameter optimisation is crucial for enhancing the performance of machine learning models. This study explores the practicality of three nature-inspired optimisation techniques: Bald Eagle Optimisation (BEO), Particle Swarm Optimisation (PSO), and Mother Tree Optimisation (MTO) for tuning the hyperparameters of Random Forest and SVM models. To ensure broad generalisation, five datasets, including both image-based and tabular data, were utilised. The results reveal that while Optuna consistently balanced accuracy and training time effectively, the performance of other techniques varied across datasets. This research provides insights into the effectiveness of these optimisers and evaluates whether their use is practical in day-to-day ML or not.

If you're already an arXiv author and open to endorsing, please feel free to use this link [https://arxiv.org/auth/endorse?x=TBE3ZK](https://arxiv.org/auth/endorse?x=TBE3ZK) or DM me if you’d like to know more before deciding. I’m happy to share the full paper draft or have a discussion about it.

Thanks a lot for your time and consideration!","['Hi, this is much higher effort than a lot of the posts asking for arXiv endorsement, but like the rest this is not really the place for this. You should hopefully be working with someone who knows you personally. Either that or you should approach a previous mentor or educator and ask them to do a detailed review.\n\nStrangers on this forum should not provide endorsement without extremely careful review of the work and discussion with yourself.', 'Here\'s what Arxiv recommends if you\'re an independent researcher or not affiliated with anyone from academia.\n\n1. Start by finding related articles in your field. Your preprint surely has cited works that are already posted in the arXiv, some of these works will be particularly relevant.\n\n2. Bring up these abstracts from the arXiv page.\n\n3. You can find somebody qualified to endorse by clicking on the link titled ""Which of these authors are endorsers?"" at the bottom of every abstract page.\n\n4. Using that information, you can then find the email address of the submitter on the abstract page just under the ""Submission history"" heading.', ""Really interesting topic! Nature-inspired optimizers don’t get enough practical benchmarking in real ML pipelines. Would love to read the full paper once it's live—best of luck with the submis"", 'Maybe you could reach out to people on LinkedIn who have worked on this domain. They can review your research and endorse you. All the best.']","['Hi, this is much higher effort than a lot of the posts asking for arXiv endorsement, but like the rest this is not really the place for this. You should hopefully be working with someone who knows you personally. Either that or you should approach a previous mentor or educator and ask them to do a detailed review.\n\nStrangers on this forum should not provide endorsement without extremely careful review of the work and discussion with yourself.', 'Here\'s what Arxiv recommends if you\'re an independent researcher or not affiliated with anyone from academia.\n\n1. Start by finding related articles in your field. Your preprint surely has cited works that are already posted in the arXiv, some of these works will be particularly relevant.\n\n2. Bring up these abstracts from the arXiv page.\n\n3. You can find somebody qualified to endorse by clicking on the link titled ""Which of these authors are endorsers?"" at the bottom of every abstract page.\n\n4. Using that information, you can then find the email address of the submitter on the abstract page just under the ""Submission history"" heading.', ""Really interesting topic! Nature-inspired optimizers don’t get enough practical benchmarking in real ML pipelines. Would love to read the full paper once it's live—best of luck with the submis"", 'Maybe you could reach out to people on LinkedIn who have worked on this domain. They can review your research and endorse you. All the best.']",0,7,0.37,Research,1751771635.0
1lspv3q,MachineLearning,[D] What resources would Theoretical ML researchers recommend to understand to pursue research.,"I have read Measure Theory, Probability Theory by Durett and Convex Optimization by Duchi.

I want to pursue research in Optimization, convergence etc.

I'm thinking of reading Matus Telgarsky's notes or Francis Bach's Learning Theory from First Principles.

I am confused what should I go next.
","['I’d recommend picking up a paper you are interested in and try to learn the prerequisite from there. The field is too large to learn everything.', 'c/p from another thread where I answered a similar question:\n\nML theory PhD student here, specializing in generalization theory (statistical learning theory). I tried to stick to highly cited ""foundational"" papers; very biased to my taste.\n\nTextbooks:\n\nMohri et al. ""Foundations of Machine Learning."" The theory textbook I teach out of. It\'s fantastic.\xa0https://cs.nyu.edu/~mohri/mlbook/\nBen-David and Shalev-Shwartz. ""Understanding Machine Learning: From Theory to Algorithms."" Great supplemental to Mohri et al.\xa0https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/\nTewari and Bartlett. ""Learning theory."" Underappreciated introductory resource.\xa0https://www.ambujtewari.com/research/tewari13learning.pdf\nPapers:\n\nBartlett et al. ""Benign Overfitting in Linear Regression."" Kick-started the subfield of benign overfitting, which studies models for which overfitting is not harmful.\xa0https://arxiv.org/abs/1906.11300\nBelkin et al. ""Reconciling modern machine-learning practice and the classical bias–variance trade-off."" An excellent reference on double descent.\xa0https://arxiv.org/abs/1812.11118\nSoudry et al. ""The Implicit Bias of Gradient Descent on Separable Data."" Kick-started the field of implicit bias, which tries to explain how gradient descent finds such good solutions without explicit regularization.\xa0https://arxiv.org/abs/1710.10345\nZhang et al. ""Understanding deep learning requires rethinking generalization."" Called for a new approach to generalization theory for deep learning; classical methods don\'t work (Main conclusion is essentially from Neyshabur, 2015).\xa0https://arxiv.org/abs/1611.03530\nBartlett et al. ""Spectrally-normalized margin bounds for neural networks."" Tightest known generalization bound for ReLU neural networks (to my knowledge).\xa0https://arxiv.org/abs/1706.08498', 'If you want to do optimization in ML, study [this](https://arxiv.org/abs/2301.11235) handbook by the letter', 'Hi, I currently work in Optimization and deep learning, applied to inverse problems. Specifically working on convergence of algorithms involving neural networks. The lectures from Boyd are really good, the book from Teboulle ""first order methods in optimization"" is pretty dense but there\'s a lot of the fundamental. Overall lecture notes are always more digestible. \nIf you want more resources or want to discuss further send a dm. :)', 'Telgarsky’s notes are great for solid theory and convergence insights. Bach is excellent for intuitive understanding. I\'d also suggest Boyd & Vandenberghe’s ""Convex Optimization"" for rigorous optimization theory.', '[removed]']","['I’d recommend picking up a paper you are interested in and try to learn the prerequisite from there. The field is too large to learn everything.', 'c/p from another thread where I answered a similar question:\n\nML theory PhD student here, specializing in generalization theory (statistical learning theory). I tried to stick to highly cited ""foundational"" papers; very biased to my taste.\n\nTextbooks:\n\nMohri et al. ""Foundations of Machine Learning."" The theory textbook I teach out of. It\'s fantastic.\xa0https://cs.nyu.edu/~mohri/mlbook/\nBen-David and Shalev-Shwartz. ""Understanding Machine Learning: From Theory to Algorithms."" Great supplemental to Mohri et al.\xa0https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/\nTewari and Bartlett. ""Learning theory."" Underappreciated introductory resource.\xa0https://www.ambujtewari.com/research/tewari13learning.pdf\nPapers:\n\nBartlett et al. ""Benign Overfitting in Linear Regression."" Kick-started the subfield of benign overfitting, which studies models for which overfitting is not harmful.\xa0https://arxiv.org/abs/1906.11300\nBelkin et al. ""Reconciling modern machine-learning practice and the classical bias–variance trade-off."" An excellent reference on double descent.\xa0https://arxiv.org/abs/1812.11118\nSoudry et al. ""The Implicit Bias of Gradient Descent on Separable Data."" Kick-started the field of implicit bias, which tries to explain how gradient descent finds such good solutions without explicit regularization.\xa0https://arxiv.org/abs/1710.10345\nZhang et al. ""Understanding deep learning requires rethinking generalization."" Called for a new approach to generalization theory for deep learning; classical methods don\'t work (Main conclusion is essentially from Neyshabur, 2015).\xa0https://arxiv.org/abs/1611.03530\nBartlett et al. ""Spectrally-normalized margin bounds for neural networks."" Tightest known generalization bound for ReLU neural networks (to my knowledge).\xa0https://arxiv.org/abs/1706.08498', 'If you want to do optimization in ML, study [this](https://arxiv.org/abs/2301.11235) handbook by the letter', 'Hi, I currently work in Optimization and deep learning, applied to inverse problems. Specifically working on convergence of algorithms involving neural networks. The lectures from Boyd are really good, the book from Teboulle ""first order methods in optimization"" is pretty dense but there\'s a lot of the fundamental. Overall lecture notes are always more digestible. \nIf you want more resources or want to discuss further send a dm. :)', 'Telgarsky’s notes are great for solid theory and convergence insights. Bach is excellent for intuitive understanding. I\'d also suggest Boyd & Vandenberghe’s ""Convex Optimization"" for rigorous optimization theory.']",38,16,0.89,Discussion,1751766735.0
1lsipgp,MachineLearning,An analytic theory of creativity in convolutional diffusion models.,"There is also a write up about this in quanta magazine.

What are the implications to this being deterministic and formalized? How can it be gamed now for optimization? ","['Awesome paper! I\'ve been training music diffusion models for quite a while now (particularly in the low data regime) so it is really nice to see some formal justification for what I\'ve seen empirically.\n\nOne of the most important design decisions for music / audio diffusion models is whether to treat frequency as a true dimensional quantity as seen in 2D designs, or as independent features as seen in 1D designs. Experimentally I\'ve seen that 2D models have *drastically better* generalization ability per training sample.\n\nAs per this paper: the locality and equivariance constraints imposed by 2D convolutions deliberately constrain the model\'s ability to learn the ideal score function; the individual ""patches"" in the ""patch mosaic"" are much smaller and therefore the learned manifold for the target distribution has considerably greater [local intrinsic dimension](https://openreview.net/pdf?id=aZ1gNJu8wO).\n\nIf your goal in training a diffusion model is to actually generate _novel_ and _interesting_ new samples (and it should be) you *need* to break the data into as many puzzle-pieces / ""patches"" as possible. The larger your puzzle pieces the fewer degrees of freedom in how they can be re-assembled into something new.\n\nThis is also great example of the kind of deficiency that is invisible in automated metrics. If you\'re chasing FID / FAD scores you would have been mislead into doing the exact opposite.', 'In my experience , Quanta magazine is anticorrelated with quality, at least on topics related to ML. They write overly hyped garbage and have questionable journalistic practices.\n\nAs independent evidence, I also think that Noam Brown made similar comments on Twitter a month or two ago.', ""This is one of the more interesting papers I've seen in a long time in DL. Few papers actually give you an proven insight into what a model is doing. This paper does."", '[deleted]', '[deleted]']","['Awesome paper! I\'ve been training music diffusion models for quite a while now (particularly in the low data regime) so it is really nice to see some formal justification for what I\'ve seen empirically.\n\nOne of the most important design decisions for music / audio diffusion models is whether to treat frequency as a true dimensional quantity as seen in 2D designs, or as independent features as seen in 1D designs. Experimentally I\'ve seen that 2D models have *drastically better* generalization ability per training sample.\n\nAs per this paper: the locality and equivariance constraints imposed by 2D convolutions deliberately constrain the model\'s ability to learn the ideal score function; the individual ""patches"" in the ""patch mosaic"" are much smaller and therefore the learned manifold for the target distribution has considerably greater [local intrinsic dimension](https://openreview.net/pdf?id=aZ1gNJu8wO).\n\nIf your goal in training a diffusion model is to actually generate _novel_ and _interesting_ new samples (and it should be) you *need* to break the data into as many puzzle-pieces / ""patches"" as possible. The larger your puzzle pieces the fewer degrees of freedom in how they can be re-assembled into something new.\n\nThis is also great example of the kind of deficiency that is invisible in automated metrics. If you\'re chasing FID / FAD scores you would have been mislead into doing the exact opposite.', 'In my experience , Quanta magazine is anticorrelated with quality, at least on topics related to ML. They write overly hyped garbage and have questionable journalistic practices.\n\nAs independent evidence, I also think that Noam Brown made similar comments on Twitter a month or two ago.', ""This is one of the more interesting papers I've seen in a long time in DL. Few papers actually give you an proven insight into what a model is doing. This paper does."", '[deleted]', '[deleted]']",18,9,0.85,Research,1751745751.0
1lse90g,MachineLearning,[P] Training Cascade R-CNN (ResNet-101 + FPN) on Custom Dataset for Solar Panel Detection,"Hey everyone! This is my first time posting here, so I hope I’m doing this right 😅

I’m working on a project to detect and classify solar panels using Cascade R-CNN with a ResNet-101 backbone and FPN neck. I don’t want to use a pre-trained model — I want to train it from scratch or fine-tune it using my own dataset.

I’m running into issues figuring out the right config file for MMDetection (or any framework you recommend), and how to set up the training process properly. Most tutorials use pre-trained weights or stick to simpler architectures.

Has anyone worked on training Cascade R-CNN from scratch before? Or used it with a custom dataset (esp. with bounding boxes & labels)? Any tips, working configs, or repo links would help a ton!

Thank you in advance 🙏
Also, if I’m posting in the wrong subreddit, feel free to redirect me!
","[""let's start with the dataset: how did you label it? how did you organize the directories for the splits? usually pretrained models have some sort of documentation of the style used to organize the data (in the likings of COCO, MNIST, etc)""]","[""let's start with the dataset: how did you label it? how did you organize the directories for the splits? usually pretrained models have some sort of documentation of the style used to organize the data (in the likings of COCO, MNIST, etc)""]",0,1,0.5,Project,1751734020.0
1ls9jjn,MachineLearning,[P] Live data and model training tips,"Hello everyone I am trying to create a price prediction and days on market prediction model. I asked my professors they said it's too basic try adding live data integration as well. But I don't know how my model would do that? As an experienced professionals how would you tackle this?
How would you retrain you model after every new data feed?
Do you retrain manually at certain time frames? As in weekly, monthly?",['Look up mlops pipelines- this will help you set up the right architectures to determine the statistical significance of streamed live data and whether/when to retrain.\n\nYou will also need to look up time series prediction methods like ARIMA to identify the appropriate windows and algorithms for your time series prediction.\n\nYou could also keep it simple by just executing a batch inference and retrain for all of the data within a set time period (every night or something). You can just use a k-s test to determine whether your data has drifted for that time period.'],['Look up mlops pipelines- this will help you set up the right architectures to determine the statistical significance of streamed live data and whether/when to retrain.\n\nYou will also need to look up time series prediction methods like ARIMA to identify the appropriate windows and algorithms for your time series prediction.\n\nYou could also keep it simple by just executing a batch inference and retrain for all of the data within a set time period (every night or something). You can just use a k-s test to determine whether your data has drifted for that time period.'],0,1,0.5,Project,1751721020.0
1ls6jp2,MachineLearning,[D] What are paper introductions meant to communicate to a knowledgable reader?,"It seems like all papers have to define what the problem they're using is, and discuss traditional techniques to then go on to their contribution. My understanding this is to show you've actually gone through the effort of reviewing the literature? Still, as I'm reading papers, I can't help but often skim over the introduction very quickly or almost not bother reading it since I know, say, what an LSTM or a Transformer is.

Is that expected or am I missing something? Is the introduction mostly there to communicate to others you've done the review well? to inform readers who may not have an ML background?","[""it's for people not very familiar with the topic to narrow down what you are going to talk about.\nthe related work section also embedds your work in the current state of the art.\xa0\n\n\nBoth sections I'd say are more geared towards people that are less experts in the specific domain. I usually read papers out of order, abstract, conclusion, if that sounds knterest I go into the main part and only check intro or related work if needed."", ""To add to what others have said, sometimes I'll be reading a paper that applies X technique to Y domain, and I'll already know X technique but not Y domain, or vice versa. So different parts of the introduction might be important for different readers of the same paper."", 'Speaking in plain English, the introduction is where people that thought your abstract and title were cool will decide if they will reado the rest of the paper or not.\n\n\nI usually make sure my intros have\n\n\n- a general quick overview of ""why what I am working on even matter"" (for example if my paper is on AI models for Healthcare I explain why certain challenges on Healthcare are important and unsolved)\n\n\n- some few sentences narrowing down to the specific unsolved problem i want to solve\n\n\n- a very quick bird\'s eye view of the related literature, mentioning only the most important recent papers and what they didn\'t solve (this is not the place to list a lot of papers or a lot of basic knowledge)\n\n\n- a general description of the method and contribution\n\n\n- a quick summary of what is in my experimentation section\n\n\n- concluding with a description of what is in every section\n\n\nIt\'s a bit of a style preference because the conference doesn\'t force you to follow a certain format, also context-dependent (if it\'s in neurips you can assume the reader has way more knowledge of basic ML concepts than if you submit to a conference in the domain of application), but I would say it\'s pretty established that having a very lengthy explanation of very basic concepts in the introduction is a terrible idea.', ""It's for introducing the domain, motivating the problem, initially describing the importance and direction of your contribution. Imagine someone has no idea what you are about to talk about, well you explain it like they have an undergraduate degree."", ""Lots of good answers here, just adding my bit. The introduction is your opportunity to frame the question/problem in such a way that the reader understands where you're coming from. A good introduction should naturally lead the reader to the main points that the paper is trying to address. \n\nFor example, there are thousands of papers on topic modeling. If you're writing a topic modeling paper, you don't want to just cite a bunch of generic topic modeling papers. You want to set the scene for the rest of the paper by selectively citing the ones that are most relevant and guiding the reader toward where the gaps and questions are that your paper is addressing."", 'From the point of view of someone that reads only applied ML papers. \n\nThe introduction section is one of the most important sections (for me) in an applied ML paper. I expect it to contain details of your domain and the constraints you face. Eg., if your paper is about a rec sys algo you built for Netflix, I expect the intro section to share details about what movies/features you’re recommending, who you’re recommending it to and how you’re recommending it. \n\nThe intro, literature review and methodology sections are the most important sections for me. I rarely read the result section because a lot of researchers try many things to make their models look good. I rely on replication studies (by me or other platforms) to validate the performance of the model.']","[""it's for people not very familiar with the topic to narrow down what you are going to talk about.\nthe related work section also embedds your work in the current state of the art.\xa0\n\n\nBoth sections I'd say are more geared towards people that are less experts in the specific domain. I usually read papers out of order, abstract, conclusion, if that sounds knterest I go into the main part and only check intro or related work if needed."", ""To add to what others have said, sometimes I'll be reading a paper that applies X technique to Y domain, and I'll already know X technique but not Y domain, or vice versa. So different parts of the introduction might be important for different readers of the same paper."", 'Speaking in plain English, the introduction is where people that thought your abstract and title were cool will decide if they will reado the rest of the paper or not.\n\n\nI usually make sure my intros have\n\n\n- a general quick overview of ""why what I am working on even matter"" (for example if my paper is on AI models for Healthcare I explain why certain challenges on Healthcare are important and unsolved)\n\n\n- some few sentences narrowing down to the specific unsolved problem i want to solve\n\n\n- a very quick bird\'s eye view of the related literature, mentioning only the most important recent papers and what they didn\'t solve (this is not the place to list a lot of papers or a lot of basic knowledge)\n\n\n- a general description of the method and contribution\n\n\n- a quick summary of what is in my experimentation section\n\n\n- concluding with a description of what is in every section\n\n\nIt\'s a bit of a style preference because the conference doesn\'t force you to follow a certain format, also context-dependent (if it\'s in neurips you can assume the reader has way more knowledge of basic ML concepts than if you submit to a conference in the domain of application), but I would say it\'s pretty established that having a very lengthy explanation of very basic concepts in the introduction is a terrible idea.', ""It's for introducing the domain, motivating the problem, initially describing the importance and direction of your contribution. Imagine someone has no idea what you are about to talk about, well you explain it like they have an undergraduate degree."", ""Lots of good answers here, just adding my bit. The introduction is your opportunity to frame the question/problem in such a way that the reader understands where you're coming from. A good introduction should naturally lead the reader to the main points that the paper is trying to address. \n\nFor example, there are thousands of papers on topic modeling. If you're writing a topic modeling paper, you don't want to just cite a bunch of generic topic modeling papers. You want to set the scene for the rest of the paper by selectively citing the ones that are most relevant and guiding the reader toward where the gaps and questions are that your paper is addressing.""]",0,8,0.31,Discussion,1751710215.0
1ls6fk1,MachineLearning,Neurips: 0 reviews submitted [D],I just checked openreview and under my neurips submission it says: 0 official reviews submitted. Hasn’t the review deadline passed by now? Does this mean it was desk rejected?,"['You will see the reviews on Jul 24, you wont see anything before']","['You will see the reviews on Jul 24, you wont see anything before']",0,2,0.35,Discussion,1751709735.0
1ls5eny,MachineLearning,[D] NeurIPS workshops 2025?,"According to the NeurIPS website, workshop decisions were sent out on July 4th, but I haven’t seen an official list published yet. I’m particularly interested because I have a paper related to ML for biology, and I'm considering submitting it to a NeurIPS workshop. However, another conference with an upcoming deadline is also an option, so I’d like to decide soon.

If anyone has insight or knows when the list might be released, I’d really appreciate it!","['The deadline is mentioned as July 4th AOE (which is 12 hours behind UTC) and I believe the deadline time is 23:59. If it adheres to this, there are ~2.5 hours left.', ""I'm also waiting for the announcement, but judging by previous years it might take a week or more for it to show up."", 'I got an email already yesterday with the notification that my workshop has been accepted https://timeseries4health.github.io/  \n\nand the email mentioned that they would release the list of workshops publicly later.\n\n“We will be announcing the list of accepted workshops and, most importantly, the scheduled workshop date soon, but we want to provide you with as much time as possible to begin your planning.”', 'Few of them released on openreview. Couldn’t find anything on the official site yet', 'It usually takes a while for the official NeurIPS website to publish the list of accepted workshops. What I typically do is search for ""NeurIPS Workshop 2025"" (or something similar) on LinkedIn after the submission deadline. In most cases, I can find many of the accepted workshops through early posts from organizers who are inviting submissions to their workshops.']","['The deadline is mentioned as July 4th AOE (which is 12 hours behind UTC) and I believe the deadline time is 23:59. If it adheres to this, there are ~2.5 hours left.', ""I'm also waiting for the announcement, but judging by previous years it might take a week or more for it to show up."", 'I got an email already yesterday with the notification that my workshop has been accepted https://timeseries4health.github.io/  \n\nand the email mentioned that they would release the list of workshops publicly later.\n\n“We will be announcing the list of accepted workshops and, most importantly, the scheduled workshop date soon, but we want to provide you with as much time as possible to begin your planning.”', 'Few of them released on openreview. Couldn’t find anything on the official site yet', 'It usually takes a while for the official NeurIPS website to publish the list of accepted workshops. What I typically do is search for ""NeurIPS Workshop 2025"" (or something similar) on LinkedIn after the submission deadline. In most cases, I can find many of the accepted workshops through early posts from organizers who are inviting submissions to their workshops.']",12,7,0.83,Discussion,1751705426.0
1ls42tg,MachineLearning,[D]Emergent Conventions in Multi-Agent LLMs: Experimental Evidence (SciAdv'24),"Groundbreaking research in Science Advances reveals how LLMs develop emergent social conventions that amplify collective biases through multi-agent interactions. Key findings:

Arbitrary Convention Formation:
When LLM ""agents"" interact repeatedly, they establish persistent arbitrary conventions (e.g., ""Agent A always speaks first"") that override individual preferences.
Example: 72% of simulated groups converged on objectively inefficient norms.

Minority Suppression:
Minority viewpoints (<30% representation) were systematically erased within 5 interaction cycles, even when logically superior.
""Conventions crystallize around majority views, silencing dissent via computational groupthink."" (Sec. 3.2)

Bias Amplification Loop:
Human-AI interactions inherit these synthetic conventions, reinforcing real-world biases (gender/racial stereotypes in follow-up trials).

Why this matters:

""These dynamics create de facto 'AI culture' – invisible, self-perpetuating, and resistant to alignment efforts."" (Discussion)

Discussion:

Can we prevent synthetic conventions from contaminating human discourse?

Should LLMs be required to ""cite their sources"" for social norms?

Does this explain why chatbots refuse certain debates?
[sciadv](https://www.science.org/doi/10.1126/sciadv.adu9368)","['No source no talk', ""call me a bit cynical but isn't this phenomenon just a simple positive feedback loop?\n\n[https://evanthebouncy.medium.com/why-model-shouldnt-train-its-own-generated-data-8530085e5b72](https://evanthebouncy.medium.com/why-model-shouldnt-train-its-own-generated-data-8530085e5b72)""]","['No source no talk', ""call me a bit cynical but isn't this phenomenon just a simple positive feedback loop?\n\n[https://evanthebouncy.medium.com/why-model-shouldnt-train-its-own-generated-data-8530085e5b72](https://evanthebouncy.medium.com/why-model-shouldnt-train-its-own-generated-data-8530085e5b72)""]",0,4,0.44,Discussion,1751699829.0
1ls1uf2,MachineLearning,[R] State of The Art models in Video Matting - Comparative Analysis.,"Hi, I am exploring the field of AI in video matting. I came across [matanyone](https://github.com/pq-yang/MatAnyone) which seems like one of the best and latest ones. However, based on my experiments this feels even this is far from production use cases for very high resolutions. What are some models that are good for this?

Looking to connect with people pursuing research or working on AI in video matting. Please DM or comment here, would like to have a quick chat!",[],[],1,0,1.0,Research,1751691135.0
1ls1mai,MachineLearning,[D] ACM MM- Complaining against Area Chair Review,"Paper submitted to ACM MM 25. Initial reviews 10/5/5/4/4. Almost all the reviewers had requested additional ablation study along with evaluation on another database- which we did

None of the reviewers even acknowledged the Rebuttal, except one who was kind enough to increase his score to 5 from initial 4- but didn't update the review text itself

At least I had hoped the area chair will take into consideration the Rebuttal while writing his review, even if the reviewers aren't going to acknowledge, but no- this guy, literally wrote a condensed summary of the initial reviews- not even seeing whatever he is writing has exactly been provided in the Rebuttal

Question is- what are my possible options? I am not going to sit idle, so please do not suggest me to let this opportunity pass and try in another conference.

TLDR- Area chair wrote a condensed summary of initial reviews, didn't even incorporate Rebuttal into his review (while everything he has mentioned has already been provided literally in the rebuttals)- now what are my possible options?(Do not suggest trying in another conference)

","[""Sorry to say this, but this is really an average review experience in CS conferences. There's not much that can be done. Sorry about it."", ""Very rarely does the content of the review matter to the area chairs. Usually, there are a predetermined number of spots and the area chairs just take the top papers by score. Only some passionate area chairs look deeper into the reviews of the borderline papers. It's not what you want to hear, but the chance of overturning a decision is slim.""]","[""Sorry to say this, but this is really an average review experience in CS conferences. There's not much that can be done. Sorry about it."", ""Very rarely does the content of the review matter to the area chairs. Usually, there are a predetermined number of spots and the area chairs just take the top papers by score. Only some passionate area chairs look deeper into the reviews of the borderline papers. It's not what you want to hear, but the chance of overturning a decision is slim.""]",4,7,0.62,Discussion,1751690292.0
1ls0eoz,MachineLearning,[D] Anyone have a reasonable experience with ICLR/ICML this year?,"I've been avoiding the ICLR/ICML/NeurIPS after getting unhelpful reviews with the ICLR reviews in 2024. The paper wasn't framed very well, but the NeurIPS reviews in 2023 were a lot better even if the paper wasn't accepted.

Question for those who successfully published in ICLR/ICML in the latest cycle. Did you have a fairly good experience with the review process? Do you have any advice for those of us who didn't?  ","[""My experience with those conferences has been progressively worse every year.\xa0\n\n\nSince they added the policy to force authors to review the quality of reviews has been pathetic.\xa0\n\n\nThis year at ICML I got a reviewer that didn't even fill out the form completely\xa0"", 'My paper was accepted got full score from one reviewer and good comments; it seemed like they genuinely enjoyed the field the paper was in. The other was a bad score with critiques that made no sense (things that were literally answered in the abstract)- the person barely read the paper and the vibe was they were looking for something other than what the paper offered. I think its hit or miss on the reviewers and your papers content', 'There is just a lot of randomness in the reviews. \n\nYou have to write your paper in such a way that someone who knows something about ML but nothing really about your specific field can read and review it quickly.\n\nImo it dumbs down the paper a bit so you have to put more of the specifics in the appendix.', 'I didn’t get meaningful replies for my rebuttal but was lucky enough to get accepted with borderline scores.', 'Neurips reviews have been good the last 3 years, whereas Icml has been consistently disastrous. Neurips has implemented review quality checks this year; will be interesting to see the outcome', 'I am a junior PhD student . I submitted to ICML this year( this was my first submission btw). we had pretty good reviews ngl 3-3-3 yet paper rejected. Meta Review wanted us to improved visualization and clarity.  They acknowledged paper has great motivation but needs polishing. Since then I put my paper on arxiv; gained some thumbsup on alphaXiv.  I resubmitted to NeuRIPS 2025. does anyone who is experienced know if NeuRIPS ACs and SACs  can see our ICML reviews? ( we did not make  ICML reviews public)', ""I had pretty good reviews. My experience is generally that bad reviews are a sign of confusing writing, and it's worth considering why that review could arise and try to resolve it."", '[deleted]']","[""My experience with those conferences has been progressively worse every year.\xa0\n\n\nSince they added the policy to force authors to review the quality of reviews has been pathetic.\xa0\n\n\nThis year at ICML I got a reviewer that didn't even fill out the form completely\xa0"", 'My paper was accepted got full score from one reviewer and good comments; it seemed like they genuinely enjoyed the field the paper was in. The other was a bad score with critiques that made no sense (things that were literally answered in the abstract)- the person barely read the paper and the vibe was they were looking for something other than what the paper offered. I think its hit or miss on the reviewers and your papers content', 'There is just a lot of randomness in the reviews. \n\nYou have to write your paper in such a way that someone who knows something about ML but nothing really about your specific field can read and review it quickly.\n\nImo it dumbs down the paper a bit so you have to put more of the specifics in the appendix.', 'I didn’t get meaningful replies for my rebuttal but was lucky enough to get accepted with borderline scores.', 'Neurips reviews have been good the last 3 years, whereas Icml has been consistently disastrous. Neurips has implemented review quality checks this year; will be interesting to see the outcome']",32,17,0.83,Discussion,1751685826.0
1lru2mh,MachineLearning,"[P] I built a mindmap-like, non linear tutor-supported interface for exploring ML papers, and I'm looking for feedback!","Hi everyone,

LLMs have made me feel like I can understand anything, but I’ve been frustrated trying to truly understand ML papers using just ChatGPT or static PDFs. Summaries can help, but then I have to go back to the paper and read it linearly to deeply understand it, and I have long chatgpt conversations which I just can't track. So I built an interface designed to support a non-linear, brain-like exploration of papers — paired with a tutor in a chat interface that guides your understanding. 

https://preview.redd.it/vqv65julfxaf1.png?width=1725&format=png&auto=webp&s=0e09f203a863527d478568332dc6e3cbeb99fd87

Here is a screenshot of what it looks like.

Try it out at: [proread.ai/llm-papers](http://proread.ai/llm-papers)

1. Knowledge maps let you see how ideas within a paper relate to each other and how papers connect across a field. Start with my curated maps of foundational LLM papers or build your own for any paper/set of papers you’re reading. You can also listen to the map as a podcast.
2. You have a chat based tutor as with ChatGPT but your questions keep updating the knowledge map so you don't lose anything
3. The map itself is an editable notebook which allow you to take notes, mark concepts as completed, tag concepts, and construct your own mental model as you read. You can not only read summaries but can go down to actual source content in readers where you want to.
4. You can make your own space with your own papers or other docs (PDF/txt/html/URLs) and create interactive maps personalized to your research or study needs.

The goal is to move beyond linear reading or static summarization: to create a space where understanding evolves dynamically, like how you actually think, with a tutor helping you make sense of it all.

Please try it out at: [proread.ai/llm-papers](http://proread.ai/llm-papers)

I’m looking for feedback from other researchers or paper readers — would this kind of non-linear, guided exploration help you understand tough topics/papers better than traditional PDFs or chat tools? What’s missing or confusing?

Thanks!","[""This is really a great project i like the idea\nFrom a problem to a project that's the real programmer"", 'Nice, the left-side visual help to easy gestion ✨']","[""This is really a great project i like the idea\nFrom a problem to a project that's the real programmer"", 'Nice, the left-side visual help to easy gestion ✨']",8,5,0.72,Project,1751665084.0
1lrtgx1,MachineLearning,[P] NeuroEvolution for Super Mario,"Hi, i wanted to make Mario learn to play the original super-marino-bros from the library

    gym_super_mario_bros

and wanted to use a genetic algorithm. My genomes are lists of weights. I apply a genome aka the weights to a CNN. The CNN gets the current frame (converted to 84x84 grayscale) as input and processes it until I get one out of 7 possible actions to take for Mario. Mario then takes this action, gets a reward for this action, and the next frame is processed and so on. Finally I gave Mario additional rewards for reaching the flag and being quick.

I tried multiple crossover functions including point-crossover, uniform-crossover and mlx-alpha-crossover. I adapt my mutation rate based on the fitness aka if it stagnates for too long or not. Selection is usually just the top k fittest genomes. I also used big populations like 300 for 30 generations or 300 generations with a population of 30. Nothing worked, he never once reached the flag. He has no problem quickly learning to jump over enemies and obstacles and moves quick. But he somehow gets stuck at the blocky stairs. He literally does nothing once he reaches them and I have no idea how. I used all combinations of crossover/mutation-rates/... but no success. I also used frame stacking and frame skipping.

My alternative approach of the genome being directly the actions and using crossover and everything on them even worked better.

I know this is a quite a high level explanation but I can provide more details if needed. My CNN has 2 convolutional layers with 4 input channels, 16 output channels and my kernels are 8x8 and I use stride of 4. the last layer has 32 feauture maps of size 9x9 which I just put into final output layers to give me 7 logits (these are the possible actions) and take the highest one. This is the rough plan. I could adjust a lot of stuff but I would non the less expect to at least have one Mario reach the flag at least. Does anyone have ideas or experience with this library and genetic algorithms ?","[""Don't have anything against your fun project, but you might want to look into deep reinforcement learning. Just so that you know what's out there now""]","[""Don't have anything against your fun project, but you might want to look into deep reinforcement learning. Just so that you know what's out there now""]",2,2,1.0,Project,1751663375.0
1lrs596,MachineLearning,[D] AACL Reputation,"In the ACL universe, ACL, EMNLP, and NAACL are generally considered equal. EACL is considered a bit lower but highly reputable and maybe even the same by some. I haven't heard much about the relatively newer AACL. What's your opinion on papers published there? Is it in the same ballpark of reputation, or is it still significantly lagging behind?

","['It is newer, so there may be perceived differences outside the community. But, the weird thing is all *ACL papers go through the same exact review process via ACL ARR. So, the differences in quality of the conferences are marginal.\n\nIf you get 4 via ACL ARR, you can commit to any *ACL conference with the exact same reviews. The conferences also have similar score distributions for acceptance. So the only difference between the conferences is the number of committed papers, which may bias towards certain topics popular in the local sub communities.', 'I think most people don’t know much about AACL, and its reputation is lagging behind. It can definitely change in the future, as ARR enforces the same reviews for all *CL conferences, but currently very few papers with scores good enough to be confident about committing to EMNLP/ACL/NAACL would be committed to AACL instead. The ranking in everyone’s mind is pretty firmly Top 3 > EACL > COLING (/LREC for resource papers) > AACL.\n\nAnecdotally, it’s not done itself any favours this year and I’ve heard quite a few disgruntled complaints about the rather strange decision to hold it on Dec 20-24, essentially filtering out anyone who celebrates Christmas for committing to it.', ""LREC would be a better venue if u r not going for the top 3 ACL* or EACL. It's more dataset/resource centred tho.""]","['It is newer, so there may be perceived differences outside the community. But, the weird thing is all *ACL papers go through the same exact review process via ACL ARR. So, the differences in quality of the conferences are marginal.\n\nIf you get 4 via ACL ARR, you can commit to any *ACL conference with the exact same reviews. The conferences also have similar score distributions for acceptance. So the only difference between the conferences is the number of committed papers, which may bias towards certain topics popular in the local sub communities.', 'I think most people don’t know much about AACL, and its reputation is lagging behind. It can definitely change in the future, as ARR enforces the same reviews for all *CL conferences, but currently very few papers with scores good enough to be confident about committing to EMNLP/ACL/NAACL would be committed to AACL instead. The ranking in everyone’s mind is pretty firmly Top 3 > EACL > COLING (/LREC for resource papers) > AACL.\n\nAnecdotally, it’s not done itself any favours this year and I’ve heard quite a few disgruntled complaints about the rather strange decision to hold it on Dec 20-24, essentially filtering out anyone who celebrates Christmas for committing to it.', ""LREC would be a better venue if u r not going for the top 3 ACL* or EACL. It's more dataset/resource centred tho.""]",8,5,0.75,Discussion,1751659720.0
1lrr5yy,MachineLearning,[D] Did anyone receive this from NIPS?,"Your co-author, Reviewer has not submitted their reviews for one or more papers assigned to them for review (or they submitted insufficient reviews). Please kindly note the Review deadline was on the 2nd July 11.59pm AOE.

===
My co-author has graduated and no longer worked in academic anymore. How can I handle that? It is not fair to reject my paper!","[""Your coauthor should not have agreed/volunteered to review if they weren't going to do it. It's unfair to you, of course, but it's your coauthor who is screwing you here, not the conference."", 'Why does he accept to review if he has left academia?\n\nAsk for his openreview login and do his reviews I guess', 'Contact the program chair (and/or the AC of the paper if you know who it is) and tell them to transfer to you the papers to review because your co-author will unfortunately be unable do.\n\n\nDo all the reviews asap.\n\n\nFor the future, do not assign as the author who agreed on review someone thst is not you.\xa0', 'Happened to me as well. \n\nIronically enough, the coauthor who created the issue didn’t do anything in the paper (perhaps they read and changed a couple of words in the abstract).\n\nThey introduced themselves as leading experts with a ton of published papers. Turns out they don’t work, don’t write anything and, as it appears, they can’t even read. \n\nUnfortunately, the community is filled with posers…', 'You need to communicate to them that if they fail to submit reviews then they are screwing you over. Maybe they don’t care about the paper for themselves anymore but they are directly impacting you. Don’t be rude, but be extremely clear and direct. The conference will not care about what is fair to you.', 'Hi. I received the sane threatening email, also citing me being a reviewer because I had made a submission. It\'s a generic email therefore. \n\nI have been a reviewer for 11 years. And the email mentions ""number of reviews incomplete: [None]"".\n\nWeird automated system.', ""\\> My co-author has graduated and no longer worked in academic anymore.\n\nSo? They're the co-author of the paper, they have responsibilities. People outside of academia publish in conferences.""]","[""Your coauthor should not have agreed/volunteered to review if they weren't going to do it. It's unfair to you, of course, but it's your coauthor who is screwing you here, not the conference."", 'Why does he accept to review if he has left academia?\n\nAsk for his openreview login and do his reviews I guess', 'Contact the program chair (and/or the AC of the paper if you know who it is) and tell them to transfer to you the papers to review because your co-author will unfortunately be unable do.\n\n\nDo all the reviews asap.\n\n\nFor the future, do not assign as the author who agreed on review someone thst is not you.\xa0', 'Happened to me as well. \n\nIronically enough, the coauthor who created the issue didn’t do anything in the paper (perhaps they read and changed a couple of words in the abstract).\n\nThey introduced themselves as leading experts with a ton of published papers. Turns out they don’t work, don’t write anything and, as it appears, they can’t even read. \n\nUnfortunately, the community is filled with posers…', 'You need to communicate to them that if they fail to submit reviews then they are screwing you over. Maybe they don’t care about the paper for themselves anymore but they are directly impacting you. Don’t be rude, but be extremely clear and direct. The conference will not care about what is fair to you.']",51,29,0.78,Discussion,1751657105.0
1lrqzma,MachineLearning,[D] Does splitting by interaction cause data leakage when forming user groups this way for recommendation?,"I’m working on a group recommender system where I form user groups automatically (e.g. using KMeans) based on user embeddings learned by a GCN-based model.

Here’s the setup:
	•	I split the dataset by interactions, not by users — so the same user node may appear in both the training and test sets, but with different interactions.
	•	I train the model on the training interactions.
	•	I use the resulting user embeddings (from the trained model) to cluster users into groups (e.g. with KMeans).
	•	Then I assign test users to these same groups using the model-generated embeddings.

🔍 My question is:

Even though the test set contains only new interactions, is there still a data leakage risk because the user node was already part of the training graph? That is, the model had already learned something about that user during training.
be a safer alternative in this context.

Thanks!
","['yep, still a leakage risk I think', ""You're getting close, but that user node's prior interactions in the training set still leak info to the model, even if new interactions are only used for testing."", 'Depends on what you want to know the prediction performance of. New users or new interactions of known users?']","['yep, still a leakage risk I think', ""You're getting close, but that user node's prior interactions in the training set still leak info to the model, even if new interactions are only used for testing."", 'Depends on what you want to know the prediction performance of. New users or new interactions of known users?']",0,10,0.5,Discussion,1751656637.0
1lrnruz,MachineLearning,[D] How trustworthy are benchmarks of new proprietary LLMs?,"Hi guys. I'm working on my bachelor's thesis right now and am trying a find a way to compare the Dense Video Captioning abilities of the new(er) proprietary models like Gemini-2.5-Pro, GPT-4.1 etc. Only I'm finding to have significant difficulties when it comes to the transparency of benchmarks in that area.

For example, looking at the [official Google AI Studio webpage](https://developers.googleblog.com/en/gemini-2-5-video-understanding/), they state that Gemini 2.5 Pro achieves a value of 69.3 when evaluated at the YouCook2 DenseCap validation set and proclaim themselves as the new SoTA. The leaderboard on [Papers With Code](https://paperswithcode.com/sota/dense-video-captioning-on-youcook2) however lists HiCM² as the best model - which, the way I understand it, you would need to implement from the ground up based on the methods described in the research paper as of now - and right after that Vid2Seq, which Google claims is the old SoTA that Gemini 2.5 Pro just surpassed.

I faced the same issue with [GPT-4.1](https://openai.com/index/gpt-4-1/), where they state
>Long context: On Video-MME, a benchmark for multimodal long context understanding, GPT‑4.1 sets a new state-of-the-art result—scoring 72.0% on the long, no subtitles category, a 6.7%abs improvement over GPT‑4o.
but the official Video-MME leaderboard does not list GPT-4.1.

Same with VideoMMMU ([Gemini-2.5-Pro](https://deepmind.google/models/gemini/pro/) vs. [Leaderboard](https://huggingface.co/datasets/lmms-lab/VideoMMMU)), [ActivityNet Captions](https://paperswithcode.com/sota/dense-video-captioning-on-activitynet) etc.

I understand that you can't evaluate a new model the second it is released, but it is very difficult to find benchmarks for new models like these. So am I supposed to ""just blindly trust"" the very company that trained the model that it is the best without any secondary source? That doesn't seem very scientific to me.

It's my first time working with benchmarks, so I apologize if I'm overlooking something very obvious.

","[""Someone should make a user friendly personalized eval app, that makes it easier for non-technical people to come up with their own definitions of what makes an LLM better or worse for them. I generally don't trust the popular benchmarks a ton because they are either trained for or the specific things being tested isn't the best representation of what I want/need out of an LLM."", 'Nowadays those benchmarks are worse than using some sigma-5 Pentium 4 cooled by nitrogen to represent the ""true"" power of that chip. It is just not real anymore.\n\nThe actual models that consumers have access to are nerfed to death esp. on OpenAI, it feels like context window shrunk 10x and quantization also. Picture shrinking any other tool orders of magnitude, like a hammer or bulldozer. It now operates in an entirely different dimension with different rules, and it can totally flip rankings upside down.\n\nWhen OpenAI runs o3 in those benchmarks it costs up to $2000 per query. But when you run it on Plus tier it is more like $0.02 or so. The difference in power is that enormous. Picture a Pentium 4 drawing the power of an entire hydroelectric dam instead of the power of just a lightbulb. Or vice versa a Pentium 4 being underclocked to 0.0006 Watts, which is about the same power a tardigrade uses while in hibernation. That\'s how fake those benchmarks really are.\n\nIt would be interesting to actually have some serious independent results with real user-accessible models. Not like popularity votes or what else already exists, actual comprehensive tests like they do.']","[""Someone should make a user friendly personalized eval app, that makes it easier for non-technical people to come up with their own definitions of what makes an LLM better or worse for them. I generally don't trust the popular benchmarks a ton because they are either trained for or the specific things being tested isn't the best representation of what I want/need out of an LLM."", 'Nowadays those benchmarks are worse than using some sigma-5 Pentium 4 cooled by nitrogen to represent the ""true"" power of that chip. It is just not real anymore.\n\nThe actual models that consumers have access to are nerfed to death esp. on OpenAI, it feels like context window shrunk 10x and quantization also. Picture shrinking any other tool orders of magnitude, like a hammer or bulldozer. It now operates in an entirely different dimension with different rules, and it can totally flip rankings upside down.\n\nWhen OpenAI runs o3 in those benchmarks it costs up to $2000 per query. But when you run it on Plus tier it is more like $0.02 or so. The difference in power is that enormous. Picture a Pentium 4 drawing the power of an entire hydroelectric dam instead of the power of just a lightbulb. Or vice versa a Pentium 4 being underclocked to 0.0006 Watts, which is about the same power a tardigrade uses while in hibernation. That\'s how fake those benchmarks really are.\n\nIt would be interesting to actually have some serious independent results with real user-accessible models. Not like popularity votes or what else already exists, actual comprehensive tests like they do.']",6,5,0.8,Discussion,1751648515.0
1lrmic2,MachineLearning,[R] Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs,"I recently released this preprint benchmarking LLM capability of self-correction.

**The Problem**: LLM self-correction is important for reliability, but it's hard to benchmark because naturally occurring errors are rare. So I built Self-Correction Bench by systematically injecting errors into LLM reasoning traces.

**Key Discovery**: LLMs systematically fail to correct errors in their own outputs while successfully correcting identical errors in external inputs. I call this the ""Self-Correction Blind Spot.""

**Results across 14 models**:

\- 64.5% average blind spot rate

\- Simply appending ""Wait"" reduces blind spots by 89.3% without finetuning

\- Other correction markers (""But"", ""However"") also help

\- Reasoning models generate these markers when they see errors

**Insight**: I analyzed post-training data and found non-reasoning instruction datasets are 95%+ lacking correction markers. RL-trained reasoning models don't show this blind spot - their generation contains lots of correction markers - suggesting they learned error correction through trial and error.

**Implications**: This affects AI safety and reliability. If LLMs can't catch their own mistakes, we need better training paradigms or activation mechanisms like correction markers. It seems RL is very promising.

Benchmark: [https://huggingface.co/papers/2507.02778](https://huggingface.co/papers/2507.02778)

Author here - happy to discuss the methodology and have your feedback.",[],[],7,0,0.82,Research,1751645318.0
1lrkzgj,MachineLearning,[D] Help understanding speculative sampling,"Hi all,

Need a bit of help understanding speculative sampling. [arXiv:2211.17192v2](https://arxiv.org/abs/2211.17192)

The idea is for the small model to generate the completions and the larger model to evaluate them. If the LLM accepts all the tokens generated by the SLM, it generates an additional token. If not, it generates the replacements of the tokens it rejected. Section 2.1 and 2.3 in the paper discuss this.

Given tokens x\_{<t}, p(x\_t | x\_{<t}) is the distribution generated by the target LLM. q(x\_t | x\_{<t}) is generated by a smaller, more efficient model (SLM). We want x \~ p(x), but we sample x\~q(x) and keep it IF q(x) <= p(x).

I don't quite get the logic of keeping the x\~q(x) sample if q(x) <= p(x). I'm sure it is something simple but a blind spot for someone dumb as me. Can someone please explain in simple terms?

Given a well-trained and a less capable model, and a sequence, in general, is there a relation between the probability distributions from both models for the next token? I would expect that the generations from the LLM have a higher likelihood of matching the next sequence in the training data.","[""Do you know rejection sampling? Speculative sampling is rejection sampling but with better proposals: https://en.m.wikipedia.org/wiki/Rejection_sampling\n\nAs a smaller model, you could use a uniform distribution over all sequences. It would work but you would need to sample for a long time. Instead we're using something better than uniform to generate proposals, which speeds things up a lot, especially for simple prompts even the small model can answer."", ""Think of it like a game: SLM generates options, LLM evaluates & accepts/rejects, aiming for a sweet spot where LLM's confidence matches training data's likelihood.""]","[""Do you know rejection sampling? Speculative sampling is rejection sampling but with better proposals: https://en.m.wikipedia.org/wiki/Rejection_sampling\n\nAs a smaller model, you could use a uniform distribution over all sequences. It would work but you would need to sample for a long time. Instead we're using something better than uniform to generate proposals, which speeds things up a lot, especially for simple prompts even the small model can answer."", ""Think of it like a game: SLM generates options, LLM evaluates & accepts/rejects, aiming for a sweet spot where LLM's confidence matches training data's likelihood.""]",2,2,1.0,Discussion,1751641541.0
1lrfedq,MachineLearning,[D] OpenAI Board Member on the Future of Machine Learning,[https://www.youtube.com/watch?v=-\_M5PY5BC9I](https://www.youtube.com/watch?v=-_M5PY5BC9I),[],[],0,0,0.3,Discussion,1751624784.0
1lqwgyf,MachineLearning,[R] Permutation Neuron: Achieving 77% Accuracy on MNIST with Three Neurons,"This article addresses the challenge of classification with minimal multiplication operations while maintaining accuracy above 75%. The MNIST dataset serves as an example, where a single permutation neuron, utilizing three classical neurons, achieves 77% accuracy.

# Concept of the Permutation Neuron

The Permutation Neuron is a computational unit that implements a permutation-based transformation of input signals. The neuron maintains a set of internal vectors that are reordered based on their interaction with the input data. This reordering process maps the input space to a discrete set of output patterns, where each pattern corresponds to a specific permutation of the internal vectors.

For classifying the 10 digits of the MNIST dataset, at least 10 distinct neuron states are required. Since the number of permutations is determined by the factorial of the number of neurons, a minimum of 4 neurons (4! = 24 permutations) is needed to cover 10 classes. However, by subtracting the value of one neuron from the others (normalization), only three neurons need to be computed, with the fourth set to zero, preserving the order of permutations. This reduces computational cost while maintaining 24 unique states for classification.

For the MNIST classification task, the permutation neuron operates as follows: three neurons with linear activation functions compute values based on the input image data, while a fourth neuron is fixed at zero. These four values are ordered to form one of 24 possible permutations (4!), such as ACZB. Using the Lehmer code, each permutation is mapped to a unique number from 0 to 23, which is then assigned to one of the 10 MNIST classes (e.g., digits 0–9).

# Training with a Genetic Algorithm

The search space for parameters is limited to 2355 values, where each of the three neurons processes input data of size 784 (MNIST image pixels) plus a bias term (3 × (784 + 1)). The 24 permutation states generated by the permutation neuron are determined by a greedy algorithm based on the MNIST training set, enabling the mapping of permutations to 10 classes. A genetic algorithm is employed to optimize the neuron weights, as the parameter space is poorly understood but assumed to contain local optima corresponding to effective solutions.

For weight optimization, a genetic algorithm with a population of 50 individuals is used. The BLX-Alpha crossover (with parameter k=2) is applied over two parents, with a 2% probability of random mutation. These settings achieved a classification accuracy of 77% on the MNIST dataset.

# Code

The implementation of the permutation neuron, including the genetic algorithm and the greedy algorithm for mapping permutations to MNIST classes, is available at [GitHub](https://github.com/sgr-team/math/tree/main/problems/pn). The code includes an experiment achieving 77% accuracy (results in mnist\_46257.json).

Readers are encouraged to reproduce the experiment or propose improved solutions, such as higher accuracy or fewer multiplication operations. Improved results will be published with attribution to their authors.",['What in the ChatGPT is this'],['What in the ChatGPT is this'],0,1,0.25,Research,1751565945.0
1lrcwk1,MachineLearning,[P] Why am I getting poor performance with GNNs for edge prediction from node features only?,"Hi everyone,

I'm working on an industrial use case where I tried to use a Graph Neural Network to \*\*predict edges between tasks\*\*, based solely on node features.

Each graph represents 10-60 tasks (nodes), and I have about 1200 such graphs for training. Each task comes with features (label, equipment type), but there are no edges given at inference time, the goal is to infer all connections -> generate the full adjacency structure.

The key point: whether an edge exists between two nodes depends on the global context, not just pairwise similarity.

I’ve tried GCNs and GATs (with various edge construction strategies during training), but I'm consistently getting poor performance.

So I’m wondering:

\-  Is this just a bad fit for classical GNNs?

\- Should I switch to Transformer-like models that encode full-node context? Or even fine-tuning ?

\- Do I need a much larger dataset to make a GNN work in this setup?

\- Is it better to frame this as a graph generation problem (autoencoders) ?


I know GNN needs edge-index during inference, but i genuinely do not seem to find the right model for my project...","[""It's quite non-standard settings for GNNs.\nLink prediction works better on larger graphs.\n\nIn my case (the graph has 3k nodes and 200k edges), I am getting pretty good accuracy (0.97), but I cannot overfit the model. This might be okay for a GNN. But still bothering me.""]","[""It's quite non-standard settings for GNNs.\nLink prediction works better on larger graphs.\n\nIn my case (the graph has 3k nodes and 200k edges), I am getting pretty good accuracy (0.97), but I cannot overfit the model. This might be okay for a GNN. But still bothering me.""]",1,1,0.67,Project,1751614897.0
1lrc7vh,MachineLearning,[D] Understanding Optimal Batch Size Calculation - Arithmetic Intensity,"I encountered this [talk ](https://www.youtube.com/watch?v=mYRqvB1_gRk)where the speaker (Timothée Lacroix of Mistral) states that an optimal batch-size is hardware dependent and can be calculated as 2xflops/mem\_bandwidth (6:40) -- Hence an optimal batchsize (B\*) for an A100 is 400.

I had some confusion on this formula - The memory bandwidth for a an A100 is 2TB/s, while the FLOPs (assuming FP16) are 312 TFlop - Can TFlops be divided by TBs though they are fundamentally different units?

Appreciate anyone who can help explain this - If anyone has suggested materials to learn more about how this number was derived, I would be very happy to take a look

I'm sure its related to[ Arithmetic intensit](https://www.iguazio.com/glossary/arithmetic-intensity/)y but that number is simply 312/2=156

EDIT:

Did some research based on answers and resources here and tried to come up with an explanation - If anyone cared to feedback or point out areas of improvement, would really appreciate it

**Arithmetic Intensity**

Performance is defined by memory bandwidth, compute, latency. If compute is more limited than memory, it is compute bound. Vice versa for memory bound. Arithmetic intensity is the ratio of compute operations to memory operations (Specifically FLOPs per byte transferred). If you are compute bound, optimizing for memory does not benefit your system, and vice versa. Calculating arithmetic intensity tells you which parts of your system to focus on optimizing. Arithmetic intensity itself is calculated as a hardware threshold as well as for individual operations. Real world performance depends on actual model architecture, dataset characteristics, training/inference regime, memory access patterns, cache utilization, batch size, operator fusion, etc…

Arithmetic intensity can also be applied to operations as below. **Values only approximate:**

Low arithmetic intensity operations (10-100 FLOPs/byte) include elementwise ops, activations, normalizations (Example, addition involves moving 2N values to GPU but doing only N ops)

High intensity ops (100 - 1000 FLOPs/byte) include matmuls and convolutions. Larger batch sizes also increase intensity - This is because input data increases while the memory access cost for weight matrices remains constant - Hence larger batches improve GPU compute utilization.

Hence, frameworks focus heavily on fusion of low intensity operations. Operations can have different arithmetic intensity depending on problem size (small matrices have lower intensity because less data can be reused), implementation (tiled algorithms are faster), precision (FP16 doubles available compute).

Consider the arithmetic intensity threshold. At 312 TFLOPs and a mem bandwidth of 1.55 TB/s for FP16 tensor ops in an A100, the arithmetic intensity threshold is roughly 201. Ops with intensity below this are memory bound, while ops above it are compute bound. A memory bound operation results in idle GPU compute while a compute bound operation results in bottlenecking. In practice, hitting this precise 100% resource utilization is rare. ","['I am not aware of specific resources for that calculation, but to estimate batch size I usually keep doubling it until the time to run an epoch does not decrease anymore. This and more topics are discussed well here: https://github.com/google-research/tuning_playbook', ""Since no one answered your original question, I will try. You can absolutely divide Flops by Bytes, or one unit by another, as much as you want. But it's your responsibility to interpret and assign meaning to the quantity. For example, meters/second gives you speed or velocity. It doesn't exist in isolation, we create it only for our convenience. Flops/Byte is the same way - a measure of how compute intensive vs data movement intensive the operation is."", ""> Can TFlops be divided by TBs though they are fundamentally different units\n\n\nOfc, you will just end up with something in flops/byte. which is the unit you would expect for arithmetic intensity.\n\n\n\n\nThe formula derives from the fact that for every weight loaded from memory you do 2 operations (multiply and add) in the matrix multiplications. If you batch them you can run more operations (2 per token) for each weight loaded from memory. You also need to keep data sizes in mind -\xa0 each fp16 weights takes up 2 bytes of memory bandwidth, while your peak flops are already for fp16. So there's a mismatch by ~2 for your case."", ""Smaller batches can lead to better generalisation due to greater variance in the gradient. So it's not always the case you want to maximise the batch size."", 'Seems like this would be extremely model dependent, though, as the data processing requires per sample would vary?', ""[This short article](https://medium.com/better-ml/arithmetic-intensity-understand-op-limits-memory-or-compute-342fd15342bb) I read recently explains arithmetic intensity quite well.\n\nIn short, suppose computation and memory reading are done concurrently, then the total time for a GPU kernel to run is the maximum time used for the two operations. As such, you would want to know how the time to compute/memory access scales with the number of computations/memory accesses to be done.\n\nFor computation time, that would be computations / FLOPS (result is in seconds), and for memory access time that would be num\\_bytes / bandwidth (result is also in seconds). So then, when both are equal: computations/FLOPS = num\\_bytes/bandwidth -> computations/num\\_bytes = FLOPS/bandwidth.\n\nThis is essentially telling you the ratio of computations to the number of bytes accessed to have both time of computation and memory access to be the same, if num\\_bytes is increased, then you have a memory access bottleneck, if computations are increased, then you have a computational bottleneck, and you know which to optimize.\n\nAs for how they got 2xflops/mem\\_bandwidth:\n\nHe states the amount of computation time needed for inference is (roughly) (2 x parameter\\_count x batch\\_size) / FLOPS (I assume the factor of 2 comes from one multiply + one add for each weight). And that the memory access time needed is parameter\\_count / bandwidth (needing to load each weight once).\n\nTaking the ratio of these two expressions tells you what size batch size you should use in order not to waste any FLOPS waiting for memory accesses, or have your memory accesses backed up by unfinished computations.\n\nThis is similar to arithmetic intensity, however what's being optimized is the batch size based on kernel constraints, rather than optimizing a kernel based on hardware constraints."", 'Or just test empirically. Usually the best way.', 'Thanks for the clarity everyone, really appreciate the resources and answers provided :D']","['I am not aware of specific resources for that calculation, but to estimate batch size I usually keep doubling it until the time to run an epoch does not decrease anymore. This and more topics are discussed well here: https://github.com/google-research/tuning_playbook', ""Since no one answered your original question, I will try. You can absolutely divide Flops by Bytes, or one unit by another, as much as you want. But it's your responsibility to interpret and assign meaning to the quantity. For example, meters/second gives you speed or velocity. It doesn't exist in isolation, we create it only for our convenience. Flops/Byte is the same way - a measure of how compute intensive vs data movement intensive the operation is."", ""> Can TFlops be divided by TBs though they are fundamentally different units\n\n\nOfc, you will just end up with something in flops/byte. which is the unit you would expect for arithmetic intensity.\n\n\n\n\nThe formula derives from the fact that for every weight loaded from memory you do 2 operations (multiply and add) in the matrix multiplications. If you batch them you can run more operations (2 per token) for each weight loaded from memory. You also need to keep data sizes in mind -\xa0 each fp16 weights takes up 2 bytes of memory bandwidth, while your peak flops are already for fp16. So there's a mismatch by ~2 for your case."", ""Smaller batches can lead to better generalisation due to greater variance in the gradient. So it's not always the case you want to maximise the batch size."", 'Seems like this would be extremely model dependent, though, as the data processing requires per sample would vary?']",39,12,0.93,Discussion,1751612174.0
1lrdpvy,MachineLearning,[D] OpenAI Board Member on ML Research in Industry vs. Academia,[https://www.youtube.com/watch?v=-\_M5PY5BC9I](https://www.youtube.com/watch?v=-_M5PY5BC9I),"['Surely a completely unbiased take', ""I started using Gemini to summarize videos to get answers to click bait titles. \n\n> Kolter believes there's still significant value in academic research, especially in learning the process of science in depth. While industry has more compute power, academia can still make sizable contributions by demonstrating the scaling performance of ideas and focusing on fundamental research in areas like safety, security, and complex evaluations. He highlights the importance of scaling laws, which allow academic research with reasonable compute resources to extrapolate performance and demonstrate the value of new methods.""]","['Surely a completely unbiased take', ""I started using Gemini to summarize videos to get answers to click bait titles. \n\n> Kolter believes there's still significant value in academic research, especially in learning the process of science in depth. While industry has more compute power, academia can still make sizable contributions by demonstrating the scaling performance of ideas and focusing on fundamental research in areas like safety, security, and complex evaluations. He highlights the importance of scaling laws, which allow academic research with reasonable compute resources to extrapolate performance and demonstrate the value of new methods.""]",0,3,0.33,Discussion,1751618181.0
1lraj3z,MachineLearning,[D] Sampling technique for imbalanced dataset of a OOS prediction model,"Hey all,

I’m trying to build ML model for OOS prediction of an item of an imbalanced dataset, which sampling technique should I use and how should I evaluate that sampling technique to create a better model.

Appreciate your thoughts and responses.

Thanks","[""My experience with imbalanced datasets shows that oversampling is a valuable approach.\nI've observed that data-wise sampling often involves additional labeling. In contrast, feature-wise sampling doesn't require this, though specific attention is needed for time-sequential features, where labels must be aligned with their sampling timestamps. These are insights from my work relevant to your endeavors."", 'You can try the usual oversimplified of minority classes, undersampling of majority classes.\n\nOr adapt the training loss and maybe use a focal point loss or some custom one one assigning higher values to minority class false predictions.\n\nBut that being said. A retrieval setting with a memory would probably be the better solution imo', ""We'd need more info. But without more info the recommendation is to generally follow SMOTE."", 'As other champs mentioned, ""Over sampling approach"". I want to add to it.\n\nIs it binary classification?\n\nIf yes, my go-to would be to view this as an, Anomaly detection task, where we overfit the model on majority and try to estimate their underlying distribution.\n\nElse, applying the weight may help, but make this weight term, a learnable parameter.', ""Check out the follow two articles:\n\n[https://blogantinos.com/imbalanced-data-0102](https://blogantinos.com/imbalanced-data-0102)\n\n[https://blogantinos.com/dealing-with-imbalanced-data-0202](https://blogantinos.com/dealing-with-imbalanced-data-0202)\n\nI've found them to be very informative tbh and unique.""]","[""My experience with imbalanced datasets shows that oversampling is a valuable approach.\nI've observed that data-wise sampling often involves additional labeling. In contrast, feature-wise sampling doesn't require this, though specific attention is needed for time-sequential features, where labels must be aligned with their sampling timestamps. These are insights from my work relevant to your endeavors."", 'You can try the usual oversimplified of minority classes, undersampling of majority classes.\n\nOr adapt the training loss and maybe use a focal point loss or some custom one one assigning higher values to minority class false predictions.\n\nBut that being said. A retrieval setting with a memory would probably be the better solution imo', ""We'd need more info. But without more info the recommendation is to generally follow SMOTE."", 'As other champs mentioned, ""Over sampling approach"". I want to add to it.\n\nIs it binary classification?\n\nIf yes, my go-to would be to view this as an, Anomaly detection task, where we overfit the model on majority and try to estimate their underlying distribution.\n\nElse, applying the weight may help, but make this weight term, a learnable parameter.', ""Check out the follow two articles:\n\n[https://blogantinos.com/imbalanced-data-0102](https://blogantinos.com/imbalanced-data-0102)\n\n[https://blogantinos.com/dealing-with-imbalanced-data-0202](https://blogantinos.com/dealing-with-imbalanced-data-0202)\n\nI've found them to be very informative tbh and unique.""]",11,5,1.0,Discussion,1751605942.0
1lr934p,MachineLearning,"[R]Group Recommendation Systems — Looking for Baselines, Any Suggestions?","Does anyone know solid baselines or open-source implementations for group recommendation systems?

I’m developing a group-based recommender that relies on classic aggregation strategies enhanced with a personalized model, but I’m struggling to find comparable baselines or publicly available frameworks that do something similar.

If you’ve worked on group recommenders or know of any good benchmarks, papers with code, or libraries I could explore, I’d be truly grateful for your. Thanks in advance!","['IMHO, my initial implementation for RS always are the tensor/ matrix factorisation models:   \n\\- [https://arxiv.org/pdf/1603.06038](https://arxiv.org/pdf/1603.06038)  \n\\- [https://dl.acm.org/doi/10.1007/s11280-017-0493-6](https://dl.acm.org/doi/10.1007/s11280-017-0493-6)\n\nThen you can try to check some of more recent, GNN-based group RS models:  \n\\- [https://www.sciencedirect.com/science/article/pii/S1566253524001210](https://www.sciencedirect.com/science/article/pii/S1566253524001210)']","['IMHO, my initial implementation for RS always are the tensor/ matrix factorisation models:   \n\\- [https://arxiv.org/pdf/1603.06038](https://arxiv.org/pdf/1603.06038)  \n\\- [https://dl.acm.org/doi/10.1007/s11280-017-0493-6](https://dl.acm.org/doi/10.1007/s11280-017-0493-6)\n\nThen you can try to check some of more recent, GNN-based group RS models:  \n\\- [https://www.sciencedirect.com/science/article/pii/S1566253524001210](https://www.sciencedirect.com/science/article/pii/S1566253524001210)']",4,1,0.84,Research,1751600984.0
1lr7sur,MachineLearning,[D] Is MBZUAI a reputable institution?,"I have been offered a PhD position and am wondering if it’s a good idea. My supervisor would be one of the top faculty but I’m concerned that the institution doesn’t have strong accolades.

I know supervisor > university, but I’m hoping any academics in this sub could provide some insight on the quality of MBZUAI contributions - ideally around NLP/RL. Thanks","['One thing I do know about MBZ is that they have a lot of money to spend.', 'It’s way too new to have much reputation', 'MBZUAI flew me and a few other folks out there for a symposium a month ago. I quite enjoyed the entire experience, and I thought all the students who I spoke with were quite bright. They have a ton of money, they’re hiring good people -what’s not to like?', ""I've heard good things from some higher ups there. Their funding model seems very stable and there seems to be a lot of very good talent there. I'd say it's definitely a good choice.\n\nI haven't heard anything negative personally, other than the climate, but others can chime in on the cons."", 'If you see yourself working in the Middle East, especially within one of Abu Dhabi’s government related enterprises, then MBZUAI is certainly a good choice. If your goal is to build a name in academia, it may be more challenging than doing a PhD at a place like MIT. But then again, the real question is how feasible that path is for you.', 'They do have a reputation of being a paper factory but other than that its a good institution.', 'Not now anyways. They have thrown loads of money around to attract people and might be offering good opportunities. The emirates are less dangerous than other middle eastern places but it is an absolute monarchy with sharia law. Morally problematic at least even though it is relatively moderate compared in its execution. For now at least.', ""I've been following MBZUAI's research in CV and NLP for the last couple of years, seems really solid (high quality CVPR/ACL publications) + they probably have most number of GPUs outside of US. I think it'd be a good place to do your PhD, and supervisor fit definitely helps along with all the other things."", 'They have a lot of money and have used it to hire many top researchers as faculty. You will undoubtedly find numerous excellent opportunities there, and they consistently produce high-quality papers at top conferences. Of course, since it is a pretty novel institution, it does not have a long-standing reputation. However, in ML/AI circles, they have a good reputation. Just going by the university itself, I would accept the opportunity. However, personally, I would not want to live in Abu Dhabi for a long time, as I am not particularly fond of the hot desert climate and Sharia law. Nevertheless, as a foreigner, you would probably be relatively safe.', 'What do you expect from reputation unis? a thousand years university with low stipend, no accommodation, a couple GPUs shared with thousands of PhDs, or a newer university where you are GPUs rich and live like a king.']","['One thing I do know about MBZ is that they have a lot of money to spend.', 'MBZUAI flew me and a few other folks out there for a symposium a month ago. I quite enjoyed the entire experience, and I thought all the students who I spoke with were quite bright. They have a ton of money, they’re hiring good people -what’s not to like?', 'It’s way too new to have much reputation', ""I've heard good things from some higher ups there. Their funding model seems very stable and there seems to be a lot of very good talent there. I'd say it's definitely a good choice.\n\nI haven't heard anything negative personally, other than the climate, but others can chime in on the cons."", 'If you see yourself working in the Middle East, especially within one of Abu Dhabi’s government related enterprises, then MBZUAI is certainly a good choice. If your goal is to build a name in academia, it may be more challenging than doing a PhD at a place like MIT. But then again, the real question is how feasible that path is for you.']",15,28,0.67,Discussion,1751596788.0
1lr15dk,MachineLearning,[D] A Serious Concern on the ACL Rolling Review System,"
While I understand the traditional conference review paradigm involving initial scores, author rebuttals, and final scores, this model is beginning to show clear cracks under the scale and competitiveness of today’s A-level (and even mid-tier) venues. Increasingly, reviewers tend to give deliberately conservative or low pre-rebuttal scores, knowing that authors will be compelled to respond in the rebuttal phase. Even when a higher score is justified, reviewers often hold back, defaulting to borderline decisions just to see how the authors respond.

This issue is even more pronounced with ACL Rolling Review, where the scoring system is vague and lacks standard terminology such as Accept, Borderline, or Reject. This makes the process even more opaque. The ARR policy clearly states that responding to review comments is not mandatory. Yet, as an author, I am expected to thoroughly and respectfully address reviewer concerns, even when they are speculative or unreasonable. This one-sided non-obligation creates a deeply flawed power imbalance.

Here’s where it gets worse.

Many reviewers, when submitting their own papers and receiving poor reviews, tend to reflect their frustration onto the papers they are assigned to review. I have observed the following patterns:

Case 1: A reviewer receives bad reviews on their own paper and becomes unnecessarily harsh or disengaged in the reviews they provide for others.

Case 2: Prior to seeing their own reviews, reviewers play it safe by giving slightly lower pre-rebuttal scores than deserved. After receiving unfavorable reviews, they either ignore rebuttals completely or refuse to revise their scores, even when rebuttals clearly address their concerns.

This leads to a toxic feedback loop where every paper becomes a collateral victim of how a reviewer’s own submission is treated. I have seen this firsthand.

In the current ARR May cycle:
I received 10 reviews across 3 papers, with only 2 reviewers responding post-rebuttal.

From 4 papers I reviewed, totaling 12 reviews, only 6 reviewers responded, and 4 of those responses were mine.

We need to acknowledge a basic truth: acknowledging a rebuttal should be a moral minimum. Yet today, there is no incentive for honest reviewing, and no consequence for disengaged or negligent behavior. Why should any of us continue to uphold moral obligations, being fair, constructive, and thorough, when our own work receives careless and dismissive treatment?

This culture cannot be allowed to continue. Unless ACL/ARR enforces stricter policies, such as making post-rebuttal justification and score updates mandatory (as CVPR and other CVF conferences do), the system will continue to erode.

I am a young researcher trying to do my part for this community. But after repeated experiences like this, what incentive do I have to stay committed to high standards as a reviewer? Why should I put in the effort when others do not?

A system where morality is optional will ultimately breed apathy and toxicity. It is time for a structural shift.

Always, to the hope.


#acl #emnlp #arr","[""I have not seen your observation that reviewers intentionally pick middling scores at first, just to see how authors respond. Like, not even once. Reviewers don't want to be involved in a long back and forth as much as you don't.\n\nHowever, you make an apt point that reviewers who also have their own submissions in the same cycle inherently have a conflict of interest. Absolutely it's a bad setup, and I agree that their mood could be shaped by their own paper feedback.\n\nIt's a difficult situation. There aren't enough reviewers already, so banning those that have submissions only makes it worse. But including them like we do now is problematic like you say. I don't know what the solution is."", ""If a reviewer is not convinced by a rebuttal I don't think it makes sense to force him to engage in long conversations. Sometimes not answering is ok."", 'If you are attending ACL 2025 in Vienna, you could bring up your concerns at the Business Meeting. I suspect you are not alone in some of your concerns.', 'These are mostly people problems... which are hard to resolve at a community-level. There are issues with peer review, but it\'s the best we have. \n\nMy suggestion is the same as I give to my students: Getting your paper reviewed is just an opportunity to improve your work and deepen your impact, regardless of whether it\'s accepted. So it helps to imagine each reviewer as a well-intending, well-read member of the community who you want to understand your work and its impact. If they don\'t, assume others in the community also won\'t. \nSo I think the only thing to do is to take responsibility for your work (what else could you even do?) and adopting the mindset ""I thought this was clear when I wrote it, and now I see it wasn\'t. How can I improve my work to make it more palatable for more people?""\n\nSo ultimately, your *only job* is to take what you can get from the reviews and improve your work as much as possible. Maybe it gets accepted, maybe not, but if your work doesn\'t meaningfully improve after getting a bunch of reviews, you easily become part of the problem by just playing reviewer roulette. I totally get there are bad reviews and reviewers (duh) but paper-to-paper, the job is simple: Genuinely try to improve the work based on the feedback however possible (often you need to spend time just rewriting to emphasize the points you want to emphasize).\n\nThere\'s also some other relevant ongoing efforts in the community and change takes time:\n\n* [NeurIPS responsible reviewing initiative](https://blog.neurips.cc/2025/05/02/responsible-reviewing-initiative-for-neurips-2025/)\n* [Changes to reviewer volunteering requirement and incentives in May 2025 cycle (EMNLP 2025)](https://aclrollingreview.org/incentives2025)', ""As a researcher, I understand the challenges you face with the current review system. Let's work together to advocate for change and promote a more constructive and fair review process."", 'This is very problematic. The entire point of rolling reviews is so that the authors are getting reliable and actionable feedback. Unfortunately, many reviews as see (as author, reviewer or AC) do not serve this purpose. A lazy reviewer can raise a totally ridiculous issue after skimming the paper for 10 minutes, then they will not respond to a rebuttal, and finally the AC will include it in the ""required"" changes that the paper should do for the next revision. The authors that might have spent months on the paper are then forced to ""do something"" about this issue and spend time on it needlessly. As a result, we have failed as a scientific society, because we failed to provide authors with reasonable review and then we forced them to jump through the loops of needless edits. Multiply this by thousands of papers and each round of ARR will burn through several researcher person-years completely frivolously...\n\nBtw the numbers you have on rebuttals match with what I see. What authors usually don\'t see is the huge amount of reviews that are posted late or by emergency reviewer. Usually an indication that the review you are getting was produced on extremely short notice.', 'In different submissions, I got more than one ”Thanks for responding, I am not changing the scores“ as an answer after addressing countless comments, many of them very vague.', ""Aren't the quality of reviews still better at ACL, compared with ICLR/ICML/NeurIPS? in the sense, that there are less critical reviews that don't offer advice about how the paper should be improved.""]","[""I have not seen your observation that reviewers intentionally pick middling scores at first, just to see how authors respond. Like, not even once. Reviewers don't want to be involved in a long back and forth as much as you don't.\n\nHowever, you make an apt point that reviewers who also have their own submissions in the same cycle inherently have a conflict of interest. Absolutely it's a bad setup, and I agree that their mood could be shaped by their own paper feedback.\n\nIt's a difficult situation. There aren't enough reviewers already, so banning those that have submissions only makes it worse. But including them like we do now is problematic like you say. I don't know what the solution is."", ""If a reviewer is not convinced by a rebuttal I don't think it makes sense to force him to engage in long conversations. Sometimes not answering is ok."", 'If you are attending ACL 2025 in Vienna, you could bring up your concerns at the Business Meeting. I suspect you are not alone in some of your concerns.', 'These are mostly people problems... which are hard to resolve at a community-level. There are issues with peer review, but it\'s the best we have. \n\nMy suggestion is the same as I give to my students: Getting your paper reviewed is just an opportunity to improve your work and deepen your impact, regardless of whether it\'s accepted. So it helps to imagine each reviewer as a well-intending, well-read member of the community who you want to understand your work and its impact. If they don\'t, assume others in the community also won\'t. \nSo I think the only thing to do is to take responsibility for your work (what else could you even do?) and adopting the mindset ""I thought this was clear when I wrote it, and now I see it wasn\'t. How can I improve my work to make it more palatable for more people?""\n\nSo ultimately, your *only job* is to take what you can get from the reviews and improve your work as much as possible. Maybe it gets accepted, maybe not, but if your work doesn\'t meaningfully improve after getting a bunch of reviews, you easily become part of the problem by just playing reviewer roulette. I totally get there are bad reviews and reviewers (duh) but paper-to-paper, the job is simple: Genuinely try to improve the work based on the feedback however possible (often you need to spend time just rewriting to emphasize the points you want to emphasize).\n\nThere\'s also some other relevant ongoing efforts in the community and change takes time:\n\n* [NeurIPS responsible reviewing initiative](https://blog.neurips.cc/2025/05/02/responsible-reviewing-initiative-for-neurips-2025/)\n* [Changes to reviewer volunteering requirement and incentives in May 2025 cycle (EMNLP 2025)](https://aclrollingreview.org/incentives2025)', ""As a researcher, I understand the challenges you face with the current review system. Let's work together to advocate for change and promote a more constructive and fair review process.""]",37,10,0.86,Discussion,1751577399.0
1lqxnie,MachineLearning,"[D] Position: Machine Learning Conferences Should Establish a ""Refutations and Critiques"" Track","We recently released a preprint calling for ML conferences to establish a ""Refutations and Critiques"" track. I'd be curious to hear people's thoughts on this, specifically (1) whether this R&C track could improve ML research and (2) what would be necessary to ""do it right"".","[""Curious about your thoughts on the 'who polices the police' dilemma here. While ideally what happens is you have strong, meaningful, and accurate critiques of work with over-claimed and/or cherry-picked results, how do you defend against bad actors making spurious submissions against good work due to personal or political reasons?"", 'Note [the previous discussion on this paper](https://www.reddit.com/r/MachineLearning/comments/1lne9e0/d_position_machine_learning_conferences_should/) four days ago.', ""Couldn't agree more. I love the idea. Having a track at least gives some incentive.\n\nUnlike in old day where most empirical experiments are backed by theory, most paper are using purely inductive reasoning with empirical experiment. Deductive reasoning is either valid or invalid, but inductive reasoning is a matter of degree, which is affected by no of tested models, test data, and the statistical significance of the test result (unfortunately most papers do no report stand error). The inductive strength is judgmental and relative to other works.\n\nWhile peer review can provide a lot of insight, the review is based on what was reported - but there is no guarantee that all metrics can be reproduced. Challenge of reproducibility includes:\n\n(1) Low incentive to reproduce - rather than reproduce a paper's result, why wouldn't researcher just write a new paper?  \n(2) Compute requirement is high for most pretraining and postraining data mix and algo change paper.\n\n(3) The huge volume of papers and the speed of innovation\n\n(4) LLM generation is non-deterministic due to finite precision even when temperature=0.0, the stochastic nature increases with length. Standard error could help mitigate it."", 'Academia: Where the pursuit of truth is overshadowed by the pursuit of publication.', 'This was a good paper, read it a few days ago.', 'I can\'t figure out how to edit the body of the post, so to clarify here, by ""do it right"", I mean: Ensure submissions are strong net positives for ML research.', ""In Section 2.4, why is submission to traditional publication venues not considered as an option? It's an odd structuring choice to place the consideration of main track publication in Section 3.3 as opposed to with all the other alternatives in Section 2.4.\n\nAnother alternative that I think should be considered is to arxiv the refutation/critique and then submit it to a workshop that is most relevant to the topic of the original paper. This way, the refutation gets visibility to the right people, moreso than I think we can expect from a general R&C track that would go out to the whole ML community.\n\nThe proposed track is also weird scientifically in that it privileges only one possible outcome of an attempt to reproduce a work. If I run a study to reproduce or check the results of a paper, and it fails to reproduce or check out, then I can publish in R&C—but if the paper does reproduce, then I can't."", '[deleted]', 'What we need are ""fully reproducible papers"".\n\n    make paper-from-scratch --fast || echo ""Rejected.""\n\nThis should:\n\n- Install packages.\n- Download datasets.\n- Train. (If `--fast` is disabled, download model weights instead.)\n- Evaluate.\n- Generate plots and fill in the ""% improvement"" metrics into the PDF. (Or at least output a metadata file that can be easily verified to see that the paper performance meets the claimed amount.)\n\nEverything else deserves instant rejection because it can\'t even satisfy the bare minimum.\n\n---\n\nPrescient FAQ:\n\n- Q: But my code may not run!  \n  A: You are allowed to run the `make paper-from-scratch --fast` command on the conference\'s servers until it builds and outputs the desired PDF.\n- Q: It\'s harder to meet the deadline!  \n  A: Too bad. Git gud.\n- Q: I dont know how 2 codez lul xD  \n  A: Too bad. Learn to code before making grand unverifiable claims.\n- Q: Unethical researchers can get around this by doing unethical things.  \n  A: Ban them.  \n  Ban unethical people. Retroactively retract papers that future researchers could not reproduce. Done.\n- Q: Why ML? Why not other fields?\n  A: Because it\'s a field that is very prone to all sorts of data hackery and researcher quackery.\n- Q: But training from scratch requires resources!  \n  A: That\'s fine. Your paper will be marked as ""PARTLY VERIFIED"". If you need stronger verification, just pay for the training compute costs. The verification servers can be hosted on GCP or whatever.  \n- Q: But who\'s going to do all this?  \n  A: Presumably someone who cares about academic integrity and actual science. Here\'s their optimization objective:  \n\n         max (integrity + good_science)\n\n  It may not match the optimization objective of certain so-called ""researchers"" these days:  \n\n         max (\n           citations\n          + paper_count\n          + top_conferences\n          + $$$\n          + 0.000000000000000001 * good_science\n         )\n\n  That\'s OK. They don\'t have to publish to the ""Journal of Actually Cares About Science"".\n\n---\n\nRelated alternatives:\n\n- Papers-with-code-as-pull-requests.  \n  Think about it. Linux Kernel devs solved this long ago. If your paper code cannot pass a pull request, it should not be accepted into a giant repository of paper code. Training code is gold star. Inference code is silver star.']","[""Curious about your thoughts on the 'who polices the police' dilemma here. While ideally what happens is you have strong, meaningful, and accurate critiques of work with over-claimed and/or cherry-picked results, how do you defend against bad actors making spurious submissions against good work due to personal or political reasons?"", 'Note [the previous discussion on this paper](https://www.reddit.com/r/MachineLearning/comments/1lne9e0/d_position_machine_learning_conferences_should/) four days ago.', ""Couldn't agree more. I love the idea. Having a track at least gives some incentive.\n\nUnlike in old day where most empirical experiments are backed by theory, most paper are using purely inductive reasoning with empirical experiment. Deductive reasoning is either valid or invalid, but inductive reasoning is a matter of degree, which is affected by no of tested models, test data, and the statistical significance of the test result (unfortunately most papers do no report stand error). The inductive strength is judgmental and relative to other works.\n\nWhile peer review can provide a lot of insight, the review is based on what was reported - but there is no guarantee that all metrics can be reproduced. Challenge of reproducibility includes:\n\n(1) Low incentive to reproduce - rather than reproduce a paper's result, why wouldn't researcher just write a new paper?  \n(2) Compute requirement is high for most pretraining and postraining data mix and algo change paper.\n\n(3) The huge volume of papers and the speed of innovation\n\n(4) LLM generation is non-deterministic due to finite precision even when temperature=0.0, the stochastic nature increases with length. Standard error could help mitigate it."", 'Academia: Where the pursuit of truth is overshadowed by the pursuit of publication.', 'This was a good paper, read it a few days ago.']",101,27,0.92,Research,1751568778.0
1lqw0fj,MachineLearning,[D] Hyperparameter Optimization with Evolutionary Algorithms: A Biological Approach to Adaptive Search,"Data Science is a fascinating field, with always something to learn. Recently, I came across an interesting (though not ideal) approach to hyperparameter optimization: Evolutionary Algorithms (EA). EAs are a subset of Genetic Algorithms that work on Darwin’s idea of “survival of the fittest”. While Grid Search and Manual Tuning remain the go-to approaches, they are limited by predefined search space and, in some sense, are brute-force methods to optimize hyperparameters. Interestingly, Evolutionary Algorithms work on the principles of biology and genetics:

1. They start with a population of candidate solutions (hyperparameters) and treat them as chromosomes.
2. Each chromosome is then evaluated using a fitness test (for example, precision, absolute error etc.)
3. The best-fit candidates are selected as parents.
4. Parent solutions generate offspring using crossover (combining individual traits) and mutation (small random changes)
5. The offspring are then used as candidate solutions, and steps 1-4 are repeated till an optimal solution (under a defined threshold) is met or iterations are exhausted.

While this is a computationally expensive solution, EA offers an adaptive methodology instead of static search methods, which can look for solutions that are not pre-defined.

Thoughts?

Note: EA is not a silver bullet to all your optimization problems.","['Yeah, this has been researched for decades. Even Optuna has one of the most famous ones, CMA-ES.', 'Sounds like it will be less sample efficient than Bayesian approaches like optuna.', 'Grid Search is a ""go-to approach""? Are we talking about modern ML (i.e. deep neural networks) here? Grid search does not scale beyond a handful of hyperparameters.', 'How is this evolutionary algorithm different from GA?', 'To me, the biggest benefit of EA over gradient-based optimisation is that you can easily do multi-way optimisation for whatever task you’re solving. For example, given a classification task and the neural net you want to optimise, you can simultaneously optimise both the F1-score (directly, since it’s non-derivable and we usually do proxy via cross-entropy) and the minimum description length of the NN itself.', 'Check out my book Evolutuonary Deep Learning that uses evolutionary algorithms to optimize deep learning.\n\nEvolutionary algorithms are also being used to optimize LLMs and AI Agents. \n\nHaving said all that. EA is computationally intensive and it takes serious time or resources to produce anything.', 'Useful for black box problems. Personally love CMA-ES']","['Yeah, this has been researched for decades. Even Optuna has one of the most famous ones, CMA-ES.', 'Sounds like it will be less sample efficient than Bayesian approaches like optuna.', 'How is this evolutionary algorithm different from GA?', 'Grid Search is a ""go-to approach""? Are we talking about modern ML (i.e. deep neural networks) here? Grid search does not scale beyond a handful of hyperparameters.', 'To me, the biggest benefit of EA over gradient-based optimisation is that you can easily do multi-way optimisation for whatever task you’re solving. For example, given a classification task and the neural net you want to optimise, you can simultaneously optimise both the F1-score (directly, since it’s non-derivable and we usually do proxy via cross-entropy) and the minimum description length of the NN itself.']",13,16,0.78,Discussion,1751564866.0
1lql7zx,MachineLearning,[D] Why DragGAN is not going viral as other image models,"I remember how impressed I was when I first saw its demo videos. But after two years, it hasn’t reached the level of popularity I expected. Why is that? Just because natural language isn't involved? Its customized image manipulation features seem really useful to me—though I’m not an expert or an active user in this domain. Or has it already become part of the workflow with diffusion/LLM-based image models?","[""A couple reasons off the top of my head. GANs as image generators are useful for interactive stuff, but they lack the fidelity and stability of LDMs which is why the hype has mostly been concentrated on the latter. \n\nThere's also the fact that DragGAN is basically just a fancy Photoshop plugin. It's a cute idea, sure, but its scope as a tool is pretty limited. I wouldn't expect it to attract the kind of mass appeal you see with some of the more well-known models.""]","[""A couple reasons off the top of my head. GANs as image generators are useful for interactive stuff, but they lack the fidelity and stability of LDMs which is why the hype has mostly been concentrated on the latter. \n\nThere's also the fact that DragGAN is basically just a fancy Photoshop plugin. It's a cute idea, sure, but its scope as a tool is pretty limited. I wouldn't expect it to attract the kind of mass appeal you see with some of the more well-known models.""]",0,1,0.38,Discussion,1751534902.0
1lqjgjz,MachineLearning,[D] AAAI-2026 2 phase review discussion,"**{another edit} I got it that it won't be used for decision making. I posted it to ask if it is true.. and realized that many of us did not know about this**

<previous post>

AAAI-26' Two-phase reviewing for the Main Track:

[https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/](https://aaai.org/aaai-launches-ai-powered-peer-review-assessment-system/)

Phase 1: Two reviews supplemented by one AI-generated, non-decisional review.

Phase 2: Additional reviews for papers not rejected in Phase 1.

**Author response after Phase 2, only for papers not rejected in Phase 1.**

Edit : They also said (but why the use of AI tho )
The pilot program will thoughtfully integrate LLM technology at two specific points in the established review process:

Supplementary First-Stage Reviews: LLM-generated reviews will be included as one component of the initial review stage, providing an additional perspective alongside traditional human expert evaluations.

Discussion Summary Assistance: LLMs will assist the Senior Program Committee (SPC) members by summarizing reviewer discussions, helping to highlight key points of consensus and disagreement among human reviewers.

<previous post>","['Whaaat?', '>>The pilot program will provide supplementary information in the form of AI-generated reviews and summaries that do not contain any ratings or recommendations. AI-generated supplementary reviews will not play any formal role in the review process, except being visible to the assigned reviewers (after they submit their own reviews), area chairs, and appropriate members of the Program Committee during the paper discussion phase. In addition, AI-generated summaries of reviewer discussions will also be used to assist Senior Program Committee members in their decision making.', 'what a horrible decision. This degrades the quality of AAAI in my eyes.', 'Lol what a joke. At least they are transparent about it']","['Whaaat?', '>>The pilot program will provide supplementary information in the form of AI-generated reviews and summaries that do not contain any ratings or recommendations. AI-generated supplementary reviews will not play any formal role in the review process, except being visible to the assigned reviewers (after they submit their own reviews), area chairs, and appropriate members of the Program Committee during the paper discussion phase. In addition, AI-generated summaries of reviewer discussions will also be used to assist Senior Program Committee members in their decision making.', 'what a horrible decision. This degrades the quality of AAAI in my eyes.', 'Lol what a joke. At least they are transparent about it']",29,9,0.91,Discussion,1751527695.0
1lqhoe2,MachineLearning,[D] Are NLP theory papers helpful for industry research scientist roles?,"Currently I'm quite interested in NLP theory, and have some questions about how to make them count for RS roles in industry roles at top AI labs.
(1) Does the number of papers help? My impression is that having many papers that are ""purely theoretical"" may not help that much, and AI labs will only count the number of ""relevant papers"" (and exclude those that are less relevant).
(2) If the theory paper also yields strong empirical results, is it important to frame it as an empirical paper (and maybe put the theory in the appendix)? This could compensate for any perceived weakness with theoretical work.
(3) What topics in language/vision models are particularly relevant in industry? Efficiency of LLMs is one priority; MoE, sparse attention & structured sparsity, are two approaches to efficient LLMs.","['1) Number of papers is more of a screening metric that doesn\'t matter once you reach the interview stage. After that, as you said the relevant papers (and relevant experience from internships, open-source work, etc.) are more important.  \n2) Having both strong theory and empirical results is, in my opinion, better than only strong empirical results. I wouldn\'t go out of my way to ""hide"" the theory. When you discuss your papers on an interview or during a research talk you can still frame the work however you think is best aligned with the team you\'re interviewing for. \n\n3) There are many relevant topics across the entire pipeline from data collection/curation to (agentic) model deployment/inference, but it totally varies from team to team. One team might care a lot about efficiency while another is only interested in exploring reasoning/RL techniques. If you want to best position yourself for one of these roles, I\'d advise to focus on a particular niche where you can distinguish yourself from the average applicant. Also keep in mind that the field is moving so fast that the hot topics today might no longer be hot 6 months from now, so just do something you\'re interested in and think you can do well, and it will probably work out better than just chasing trends and drowning in the competition', 'What is NLP theory? Are you talking about linguistics, classical NLP tasks, or theory of deep learning? I did not really know NLP theory was a thing... I know there is theoretical linguistics (I have a lot of NLP experience).\n\nIf you study the properties of transformers, for example, it is more of a ICLR/ICML/NIPS/AAAI/... than an ACL paper if I am not mistaken. If you study some lingustic property or how language models represent it, it is an NLP paper.\n\nEdit: regardless, I think all of the above are at least good enough for researchy DS roles as long as you use some ML. RS roles are pretty sparse currently but in one of my jobs, I personally collaborated with folks from one of the largest research labs (DeepMind/FAIR/Anthropic) and some of them had humanities NLP background.']","['1) Number of papers is more of a screening metric that doesn\'t matter once you reach the interview stage. After that, as you said the relevant papers (and relevant experience from internships, open-source work, etc.) are more important.  \n2) Having both strong theory and empirical results is, in my opinion, better than only strong empirical results. I wouldn\'t go out of my way to ""hide"" the theory. When you discuss your papers on an interview or during a research talk you can still frame the work however you think is best aligned with the team you\'re interviewing for. \n\n3) There are many relevant topics across the entire pipeline from data collection/curation to (agentic) model deployment/inference, but it totally varies from team to team. One team might care a lot about efficiency while another is only interested in exploring reasoning/RL techniques. If you want to best position yourself for one of these roles, I\'d advise to focus on a particular niche where you can distinguish yourself from the average applicant. Also keep in mind that the field is moving so fast that the hot topics today might no longer be hot 6 months from now, so just do something you\'re interested in and think you can do well, and it will probably work out better than just chasing trends and drowning in the competition', 'What is NLP theory? Are you talking about linguistics, classical NLP tasks, or theory of deep learning? I did not really know NLP theory was a thing... I know there is theoretical linguistics (I have a lot of NLP experience).\n\nIf you study the properties of transformers, for example, it is more of a ICLR/ICML/NIPS/AAAI/... than an ACL paper if I am not mistaken. If you study some lingustic property or how language models represent it, it is an NLP paper.\n\nEdit: regardless, I think all of the above are at least good enough for researchy DS roles as long as you use some ML. RS roles are pretty sparse currently but in one of my jobs, I personally collaborated with folks from one of the largest research labs (DeepMind/FAIR/Anthropic) and some of them had humanities NLP background.']",16,8,0.94,Discussion,1751520886.0
1lqgbdk,MachineLearning,[D] AI/ML interviews being more like SWE interviews,"Have people noticed that AI/ML/DS job interviews now feel more SWE-like? For example, relying more on data structures and algorithms leetcode questions. I’ve noticed in my professional friend groups more people are being asked these questions during the coding interview.","['AI engineer job is just SWE but with AI. And the trending nowadays is just integrating LLMs into existing system.', 'AI/ML/DS is no longer about creative research ideas, but about execution.', 'None of the AI research positions I have interviewed for had leetcode, you are probably applying to an ""AI Engineer"" or however they call it nowadays that is SWE for AI (and then it\'s not surprising they test you for coding)', 'ML has a big SWE component to it. Leetcode type questions is an easy, low-effort way to do a first pass filtering. After that first round, they will start with the actual ML/DS interview.', 'Always have been. The overwhelming majority of people in hiring positions have no idea how to screen candidates, this has been the status quo since data science started as a discipline.', ""The field has changed.\n\n2-3 years ago, our daily routine was defining metrics, collecting data, check quality, finetuning a BERT or a ResNet to perform all sort of NLP/ CV tasks, check the wandb dashboard and dealing with training issue, and iterate, and also deploy the models. ML engineer/ applied researcher is very decentralized.\n\nNow it is a one-model-fit-all scenario. You can prompt to solve almost all NLP and CV problem. It is the era of centralization. You just need some top labs to do data curation, model training, eval and deployment that serve millions of developers. The low supply makes the bar extremely high.\n\n  \nThe research field has been changing too. You will see a lot of maths in older papers pre LLM, and now they're mostly technical report, or prompt engineer paper."", 'From personal experience, a big and hard part of ML in industry is infrastructure, tooling and platforms to train, serve and monitor models. Usually even in very large companies very few people are involved in purely just modelling, and a lot of people are involved in building infrastructure for the modelling to take place and eventually go to production reliably.', 'I’ve not had leetcode as much, but I have had a lot of design discussions and ML case study type discussions in interviews. At a certain point MLE requires a lot more large scale design over just ML concepts.', ""Yes!!! I hate it, I have no desire to be a SWE or MLE, I like research and publishing, it's so frustrating when the focus in on leetcode or SWE principles. There are very few RS positions available and they are gatekept by PhDs, and even then they often have leetcode style questions, it's a joke."", 'There\'s a very broad spectrum of work to be done between standard SWE, data science, business intelligence, and ML research and basically everything in between, blended across different axes, depending on the organization.\n\nI\'m doubting you\'ll find every companies definitions of an ""ML engineer"" or ""AI xyz"" align with one another.\n\nIt might be a bit weird to ask for that task from a Ph D. level research job opening, but it still wouldn\'t be maximum surprise.']","['AI engineer job is just SWE but with AI. And the trending nowadays is just integrating LLMs into existing system.', 'AI/ML/DS is no longer about creative research ideas, but about execution.', 'None of the AI research positions I have interviewed for had leetcode, you are probably applying to an ""AI Engineer"" or however they call it nowadays that is SWE for AI (and then it\'s not surprising they test you for coding)', 'ML has a big SWE component to it. Leetcode type questions is an easy, low-effort way to do a first pass filtering. After that first round, they will start with the actual ML/DS interview.', 'Always have been. The overwhelming majority of people in hiring positions have no idea how to screen candidates, this has been the status quo since data science started as a discipline.']",128,40,0.94,Discussion,1751516135.0
1lqedrt,MachineLearning,[D] Paper with code is completely down,"Paper with Code was being spammed (https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d\_paperswithcode\_has\_been\_compromised/) before, and now it is compoletely down. It was also down a coupld times before, but seems like this time it has lasted for days. (https://github.com/paperswithcode/paperswithcode-data/issues)

","['Is there anything we can do to help these people?', 'Who owns paperswithcode? Both of the major contributors I could find appear to be very inactive', 'Aww not again T.T', 'its down today', ""It's fluctuating between Up and down.\n\nI am facing the same issue""]","['Is there anything we can do to help these people?', 'Who owns paperswithcode? Both of the major contributors I could find appear to be very inactive', 'Aww not again T.T', 'its down today', ""It's fluctuating between Up and down.\n\nI am facing the same issue""]",36,11,0.96,Discussion,1751509982.0
1lqe31s,MachineLearning,[D] What Tool to Use to Create Illustrations Like This?,"Recently, I’ve seen many researchers adopt this style of illustration to present an architectural view of their method or approach. These visuals are clean, professional, and visually appealing, perfect for research papers and presentations.

I've tried replicating this style using [draw.io](http://draw.io), but I haven’t been able to achieve the same level of quality or aesthetics.

Could anyone suggest tools or software commonly used to create such research illustrations?

I'm particularly interested in tools that are:

1. Suitable for academic or technical diagrams

2. Capable of producing high-quality, publication-ready visuals

3. Flexible for custom styling or layouts

Any recommendations would be greatly appreciated!

Please check Illustration here: [https://imgur.com/a/VWiKD3Q](https://imgur.com/a/VWiKD3Q)","['My understanding of clean, professional and understandable is exact opposite of this example', 'Adobe illustrator with a generous helping of comic sans.', 'sadly, powerpoint is great for this. Vector graphics, easy to drag and group elements. Configurable Snap-to guides. Custom shapes/patterns. Layering like photoshop. Powerpoint has it all', 'This example is horrible though', ""I've had varying levels of success doing this with PGF/TiKZ, Inkscape, MS Powerpoint, Google Drawings, and I know people also use Adobe Illustrator."", 'I use overleaf + draw.io for posters', ""The specific tool you use is going to be less important than being comfortable working in it, having it produce scalable vector images, and thinking through which *specific* points you need the illustration for -- it's getting long in the tooth and is centered around presenting statistics, but Tufte's *Visual Display of Quantitative Information* is a great read for thinking about how much information you *should* try to convey in a figure.\n\nI would also suggest something like Powerpoint for overall block/systems architecture diagrams, but [LibreOffice Impress](https://www.libreoffice.org/) also works just as well, and will also produce SVG/EPS/anything but JPG"", ""Comic Sans (the font in your example) is certainly an interesting choice! I'm not sure I'd personally choose it for technical/academic diagrams but some people may like it I guess.\n\nIf you need live-edit LaTeX equations, block diagrams (like boxes+arrows), PDF export then you might be interested in Vexlio (I am the developer). [https://vexlio.com](https://vexlio.com)"", 'Clean? Professional?']","['My understanding of clean, professional and understandable is exact opposite of this example', 'Adobe illustrator with a generous helping of comic sans.', 'This example is horrible though', 'sadly, powerpoint is great for this. Vector graphics, easy to drag and group elements. Configurable Snap-to guides. Custom shapes/patterns. Layering like photoshop. Powerpoint has it all', ""I've had varying levels of success doing this with PGF/TiKZ, Inkscape, MS Powerpoint, Google Drawings, and I know people also use Adobe Illustrator.""]",2,13,0.57,Discussion,1751509078.0
1lqb0uq,MachineLearning,[D] UofT PhD Ranking,"In terms of academia prestige (for future prof positions), where would you place UofT ML PhD? Is it better RoI to do it at a T10 American school (UIUC, Georgia Tech, UT Austin, UWash, etc) for name recognition considering the advisors are equivalent? Also, how does UofT PhD fare against Oxbridge DPhil these days?","[""Supervisor fit and personal motivation for the topic matter more than institution. University reputation is an imperfect proxy for the research strengths, networking, and supervisor guidance you'll receive.\xa0"", 'Toronto has produced some very respectable and successful names in ML and it’s applications, biggest example being Hinton, who recently won the Nobel Prize in Physics for breakthroughs in ANNs\n\nNo worries about academic prestige there, look into departments that are the best for you specifically', ""At this point it depends more on your advisor and research skills, they're all good universities."", ""Just my 5 cents but I'm at UofT for a ML PhD and most people here agree the schools you listed and Toronto are virtually equivalent. So agree with everyone else that advisor topic etc. matters more. Also worth noting US is going through funding problems and stuff that Canada has mostly been insulated from"", ""This site is a good source for seeing how PhD institution interacts with hiring: https://jeffhuang.com/computer-science-open-data/\n\nUofT is good, lots of professors are from UofT — but nowhere near as many as are from MIT, Stanford, Berkeley, CMU. By raw numbers on this list (which are not normalized by number of graduates, unfortunately) they're basically on par with Georgia Tech."", ""UofT ML PhD student here. Name recognition of the university has close to zero bearing when it comes to being offered a profship. Your best work during your PhD and the recommendation from your PhD supervisor are the two most important aspects. Instead of university recognition, I would say that the name recognition of your professor matters a lot more than that of the university. UofT had people like Stephen Cook and Geoff Hinton at one point. Their recommendation would've landed you in any place that you'd hope."", 'UofT/ Vector institute are well known globally to AI researchers. I would say above the programs you mentioned except for Oxford/Cambridge. Realistically the name wont help you much, quality of your research/supervisor will be the biggest impact on your opportunities post graduation', 'Toronto is a top school for AI/ML.   Atm the US is a hostile place for graduate students, particularly international students.', ""Curious if you were specializing in something and explored what's available at UofT in terms of research opps.\n\nI'm debating doing an MSc at most (PM who's taking grad level courses at Stanford and loving it)... but want to focus on causal reasoning and neurosymbolic methods to enhance performance on generative tasks."", 'Can I DM?']","[""Supervisor fit and personal motivation for the topic matter more than institution. University reputation is an imperfect proxy for the research strengths, networking, and supervisor guidance you'll receive.\xa0"", 'Toronto has produced some very respectable and successful names in ML and it’s applications, biggest example being Hinton, who recently won the Nobel Prize in Physics for breakthroughs in ANNs\n\nNo worries about academic prestige there, look into departments that are the best for you specifically', ""At this point it depends more on your advisor and research skills, they're all good universities."", ""Just my 5 cents but I'm at UofT for a ML PhD and most people here agree the schools you listed and Toronto are virtually equivalent. So agree with everyone else that advisor topic etc. matters more. Also worth noting US is going through funding problems and stuff that Canada has mostly been insulated from"", ""This site is a good source for seeing how PhD institution interacts with hiring: https://jeffhuang.com/computer-science-open-data/\n\nUofT is good, lots of professors are from UofT — but nowhere near as many as are from MIT, Stanford, Berkeley, CMU. By raw numbers on this list (which are not normalized by number of graduates, unfortunately) they're basically on par with Georgia Tech.""]",0,26,0.5,Discussion,1751499960.0
1lqampf,MachineLearning,[D] Applicability of a Biomedical based AI/ML PhD to other AI/ML fields,"Hey all,

I am a first year PhD student in a top biomedical program in the US. One of the labs I am most interested in studies how to more effectively use AI/ML to enhance the drug discovery and development process. Although I current have only a limited knowledge of coding (really just experience with R and a little C++) the PI has told me he'd be happy to have me join the group. Still, I wonder about the applicability of this niche expertise. Does having done a PhD in biomedical focused AI/ML allow for the possibility of being hired in say finance AI/ML? What about AI/ML research in big tech? Or would you say it is only applicable in Big Pharma/biomed startup research?

Thanks for your insights.","[""I've done a number of biomedical AI/ML projects including generative small molecules for drug discovery, microscopy image analysis, amino acid sequence encoding for function and structure prediction. \n\nI get recruiters messaging me about roles for non biomedical fields. Meta, for example, recently reached out to improve their ad recommendations ML."", '[deleted]', 'You are starting a biomedical PhD and want to work in finance????\n\nJust get an MBA and intern in M&A, wtf are you doing...']","[""I've done a number of biomedical AI/ML projects including generative small molecules for drug discovery, microscopy image analysis, amino acid sequence encoding for function and structure prediction. \n\nI get recruiters messaging me about roles for non biomedical fields. Meta, for example, recently reached out to improve their ad recommendations ML."", '[deleted]', 'You are starting a biomedical PhD and want to work in finance????\n\nJust get an MBA and intern in M&A, wtf are you doing...']",5,4,0.86,Discussion,1751498850.0
1lps1eo,MachineLearning,[P] DFReg: A Physics-Inspired Regularization Method That Operates on Global Weight Distributions (arXiv:2507.00101),"Hi everyone,

I’d like to share a recent preprint I uploaded to arXiv, introducing **DFReg** – a new regularization framework for neural networks inspired by **Density Functional Theory (DFT)** in physics.

**What is DFReg?**
DFReg replaces local penalties (like L2 regularization or Dropout) with a **global constraint** on the *empirical weight distribution*. It treats the weights of a neural network as a statistical density and introduces a functional penalty that encourages:

* Smooth, non-peaky weight distributions
* Diverse, well-spread parameter configurations
* Structural regularity across layers

No architectural changes or stochastic perturbations required.

**What we tested:**
We evaluated DFReg on **CIFAR-100 with ResNet-18**, comparing it to Dropout and BatchNorm. Metrics included:

* Test accuracy and loss
* Weight entropy
* Histogram regularity
* 2D FFT of convolutional filters

Notably, we also trained **BatchNorm-free ResNets** with only DFReg as the regularizer.

**Key findings:**

* DFReg matches or outperforms Dropout and BatchNorm on accuracy and stability
* It induces more interpretable and spectrally regular weight structures
* Even without L2 or BatchNorm, DFReg alone provides strong regularization

**Paper**: [https://arxiv.org/abs/2507.00101](https://arxiv.org/abs/2507.00101)


Would love to hear feedback from the community—especially if you're interested in global priors, regularization, or physics-inspired ML. Open to questions, critiques, or collaborations.

Thanks!",[],[],2,0,0.67,Research,1751451032.0
1lq9dh5,MachineLearning,[D] Machine Learning Cheat Sheet Material,"* [Linear Algebra Cheat Sheet](https://macro.com/app/pdf/5aa2375d-a8f6-4430-93f9-a7e4aba55690)
* [Super VIP Cheatsheet: Artificial Intelligence](https://macro.com/app/pdf/5be153e6-6dd3-4eef-adbf-554d53afa3ed)
* [VIP Cheatsheet: Transformers and Large Language Models (LLMs)](https://macro.com/app/pdf/d8770868-9cbe-4bf8-abe0-2988f39344d9)
* [VIP Cheatsheet: Deep Learning](https://macro.com/app/pdf/ab4efb6c-6e71-4836-85bc-4841e26312c1)
* [Super VIP Cheatsheet: Machine Learning (ML)](https://macro.com/app/pdf/a8b3033b-c823-4715-ab2c-24ed9eca98ef)
* [Machine Learning Cheat Sheet](https://macro.com/app/pdf/79b5f468-d65c-4c03-b9b6-7c117581e677)
* [ML Cheatsheet Documentation](https://macro.com/app/pdf/65f5ae92-7f08-4869-8d53-cc81ed0fabc2)
* [Machine Learning: UC Berkeley Intro to ML Course Notes](https://macro.com/app/pdf/ea86a4d6-433a-4eeb-bf40-985b871afcc8)
* [Machine Learning: A Probabilistic Perspective](https://macro.com/app/pdf/a36b8fd4-f70e-4a41-b18f-9436c2806019)",[],[],27,0,0.74,Discussion,1751495464.0
1lq66ra,MachineLearning,[D] Understanding DDIM : Accelerated Sampling Case,"Hello,

I have been going through DDIM paper and have some queries on how the sampling is accelerated (appendix C.1)

The authors assume that the forward can be decomposed as

[Forward decomposition](https://preview.redd.it/n0yvok1liiaf1.png?width=520&format=png&auto=webp&s=0cbce45652fccf8f10441b25238e8fd8136c7e37)

and backward

[Backward decomposition](https://preview.redd.it/f5gtpdrmiiaf1.png?width=437&format=png&auto=webp&s=2d1f597df36d5dcdab955a167e8fb588a866184d)

where tau is subsequence of timesteps \[1, T\].

First thing I want to point out is that, index ""i"" should start from 2 and from 1. (Am I right in saying this ?)

If you look into the decomposition, in the forward for the timesteps that are not in the subsequence, we are directly writing x\_{t}|x\_{0} and for the timesteps that are in subsequence we write x\_{tau\_{i-1}}|x\_{tau\_{i}},x\_{0}.

So to mimic in the reverse we write for the timesteps that are not in subsequence x\_{0}|x\_{t} and for timesteps in the subsequence we write x\_{tau\_{i-1}}|x\_{tau\_{i}}.

The above explaination looks good in intuitive sense but when I take an example and write the decomposition, the intutition doesn't come at all.

[Example](https://preview.redd.it/6zn8fux3piaf1.png?width=705&format=png&auto=webp&s=6628a8167fc4e6a7458054d9872a1caf9a338292)

Here the third term in backward p(x\_{3}|x\_{4},x\_{5}) = p(x\_{0}|x\_{3}) and fifth p(x\_{1}|x\_{2},x\_{3},x\_{4},x\_{5}) = p(x\_{0}|x\_{1}) doesn't make sense at all.

Can someone explain how does the backward decomposition work ?

Note : I don't know if this is the correct place to ask these type of questions, but I felt that other subs are not suited for this.

Thanks.","['Also in equation 59 of appendix C.1, authors have written KL(q(x\\_{t}|x\\_{0})||p(x\\_{0}||x\\_{t})) which is wrong I think as you take KL divergence over the same random variable but here q(x\\_{t}|x\\_{0}) is valid distribution of x\\_{t} and p(x\\_{0}|x\\_{t}) is valid distribution of x\\_{0}.']","['Also in equation 59 of appendix C.1, authors have written KL(q(x\\_{t}|x\\_{0})||p(x\\_{0}||x\\_{t})) which is wrong I think as you take KL divergence over the same random variable but here q(x\\_{t}|x\\_{0}) is valid distribution of x\\_{t} and p(x\\_{0}|x\\_{t}) is valid distribution of x\\_{0}.']",1,1,0.99,Discussion,1751487523.0
1lq4q34,MachineLearning,[P] Open-Source: Scaled & Automated Paired Testing for Bias (NYC LL144 & Beyond),"## Proven Impact
Paired testing (identical requests, one varying factor) exposed systemic discrimination in:
- Housing: 8,000 HUD audits → Fair Housing Act
- Hiring: 10,000+ applications → proved racial bias

## The Problem
Manual testing can't keep pace with modern discrimination - whether in:
- AI systems
- Human bureaucracies
- Hybrid decision systems

## Why Current Solutions Fail
🔴 **Traditional audits** - Artificially limited scale
🔴 **AI governance tools** - Only look at code, not real-world behavior
🔴 **Human system audits** - Easily gamed by temporary compliance

## How We Fix It
✅ Tests **any** decision system: AI models, government offices, HR
✅ Fully automated paired testing at **million-scale**
✅ No internal access needed - measures real outputs
✅ Turns resistance into **proof of guilt**
✅ **CC0 public domain** findings

## The Accountability Engine
1. Run massive tests on:
   - Hiring algorithms
   - Visa systems
   - Loan approvals
   - Any decision interface
2. Publish **immutable** CC0 findings
3. Force systems to:
   - **Fix the bias**, or
   - **Prove their bias** by refusing

## Active Targets
🇧🇷 Brazil's AI Act (AEDTs)
🇺🇸 US regulatory needs
🇪🇺 EU GDPR enforcement
🏛️ Traditional bureaucratic systems

## Why This Changes Everything
**Old model:**
""Trust us, we fixed it after that last scandal""
*(Who watches the watchers? No one, by design.)*

**Our model:**
""Continuous, automated proof of fairness - or lack thereof""
*(We watch them watching, always, by their replies.)*

> ""The perfect audit reveals bias whether the decision-maker is silicon or flesh.""

Get Involved if interested (lmk if I'm mad).
**GitHub:** [watching_u_watching](https://github.com/genaforvena/watching_u_watching)  ","['Quite frankly, no, I will not ""get involved."" You\'ve let an AI convince you that you have a good idea, then you have regurgitated its thoughts into a github and asked the rest of the world to do the actual work.\n\n\nBuild something real first.']","['Quite frankly, no, I will not ""get involved."" You\'ve let an AI convince you that you have a good idea, then you have regurgitated its thoughts into a github and asked the rest of the world to do the actual work.\n\n\nBuild something real first.']",0,4,0.5,Project,1751483935.0
1lppyht,MachineLearning,[D] How to become fluent at modifying/designing/improving models?,"By fluency I mean:

1. Read a paper and and without much problem implement the techniques mentioned, whether it's building something from scratch using the paper as guidance (even in the absence of code), or modifying existing models.
2. Having an idea and being able to translate that into designing new architectures or modifying existing models.
3. Improving models.

Think of people like [Phil Wang](https://github.com/lucidrains) who is very prolific at reproducing papers and or improving them. I'm very curious to know in your experience what made it ""click"" that unlocked your ability to be productive with these things. I suspect the boring answer is ""just reproduce papers, bro"", but I was hoping to learn about people's own experience/journey on this and if you guys have any specific insight/tricks that can be useful for others to know about. Like maybe you have a good workflow for this or a good pipeline that makes you 10x more productive, or you have some niche insight on designing/modifying/improving models that people don't usually talk about etc.","['For me it finally started to click when I actually started building stuff—even if it was hacky or half-working at first. You get way more out of trying to implement even a toy version than you do passively reading.\n\nWhen something doesn’t work, I dig into the layer or module causing the issue, and that’s where the real learning happens. Also helps to keep a few reference repos around that are clean and well-annotated—gives you a mental map of how things are structured.\n\nOne tip: don’t just copy and run code. Try to swap in a new loss function or tweak an architecture and see what breaks. That’s how you go from “I kinda get it” to “I can tweak it with confidence.”', 'You should do X to become good at X.', 'lots of lots of practice', 'Reading and understanding papers as a skill can take decades on its own depending on your education. Implementing them another few. Implementing them well, a couple more. good thing is you can learn and practice in parallel', '[deleted]']","['For me it finally started to click when I actually started building stuff—even if it was hacky or half-working at first. You get way more out of trying to implement even a toy version than you do passively reading.\n\nWhen something doesn’t work, I dig into the layer or module causing the issue, and that’s where the real learning happens. Also helps to keep a few reference repos around that are clean and well-annotated—gives you a mental map of how things are structured.\n\nOne tip: don’t just copy and run code. Try to swap in a new loss function or tweak an architecture and see what breaks. That’s how you go from “I kinda get it” to “I can tweak it with confidence.”', 'You should do X to become good at X.', 'lots of lots of practice', 'Reading and understanding papers as a skill can take decades on its own depending on your education. Implementing them another few. Implementing them well, a couple more. good thing is you can learn and practice in parallel', '[deleted]']",25,14,0.8,Discussion,1751442625.0
1lplwz3,MachineLearning,[D] Will the relationship between Meta's FAIR and Super Intelligence Labs be like that of Google Brain and DeepMind previously?,"I really don’t get the point of setting up a new AI lab at Meta.
Well, maybe it’s related to the semi-acquisition of Scale AI and creating a group dedicated to Alexandr Wang.
But doesn’t the merger of Google Brain and DeepMind suggest it’s better not to split your resources in the AI war?

Also would there be possible feud out there?

","['FAIR will  likely be consumed by the genAI and super intelligence labs.', ""It might have something to do with Yann Lecun's (FAIR's former director and Chief AI Scientist at Meta) focus on World Models and his critical comments about transformers. So maybe FAIR and Super Intelligence Labs will head into different directions research-wise."", ""Yes. there will be a turf war on compute. \n\nAt least FAIR can work on architectures that aren't scale-ups of autogressive models"", ""Semi-acquisition of Scale AI for Meta's AI future, plus leveraging Alexandr Wang's expertise, is a strategic move to stay competitive.""]","['FAIR will  likely be consumed by the genAI and super intelligence labs.', ""It might have something to do with Yann Lecun's (FAIR's former director and Chief AI Scientist at Meta) focus on World Models and his critical comments about transformers. So maybe FAIR and Super Intelligence Labs will head into different directions research-wise."", ""Yes. there will be a turf war on compute. \n\nAt least FAIR can work on architectures that aren't scale-ups of autogressive models"", ""Semi-acquisition of Scale AI for Meta's AI future, plus leveraging Alexandr Wang's expertise, is a strategic move to stay competitive.""]",24,9,0.9,Discussion,1751427790.0
1lpk8ib,MachineLearning,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

\--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

\--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.","[""# Your Creation, Your Proof. Get It Free.\n\nAs a creator, I learned the hard way: your only real responsibility is **proving you made it**. If you're not a big corporation with endless resources for global copyright registration, there's a simple solution.\n\nUse our **free global copyright verification service(i-STAM)** to instantly verify your **images, PDFs, audio, and video files** via our app or web. Website [https://www.i-stam.com](https://www.i-stam.com)\n\n**Here’s how it works:**\n\n* **10 Free Registrations:** Log in and get 10 free points to start.\n* **Need More?** Just send me a direct message (DM) and I'll gladly provide more free points, as many as you need.\n\n**A few quick notes:**\n\n1. **Homegrown:** I built this myself, so the UI/design is basic but effective.\n2. **Permanent Record:** Once registered, your content is saved permanently and can't be changed.\n3. **Mindful Use:** Please use it responsibly to help manage server costs.\n\n(Check the website for full instructions before using the app or web.)"", ""# [A daily Chronicle of AI Innovations in July 2025: July 03rd 2025](https://podcasts.apple.com/us/podcast/ai-daily-news-july-03-2025-racist-ai-videos-are-spreading/id1684415169?i=1000715630274)\n\n[**^(Read Online)**](https://enoumen.substack.com/)\xa0^(|)\xa0[**^(Sign Up)**](https://podcasts.apple.com/us/podcast/ai-unraveled-latest-ai-news-trends-chatgpt-gemini-deepseek/id1684415169)\xa0^(|)\xa0[**^(Advertise |)**](https://djamgatech.com/product/ai-unraveled-podcast-per-episode-integration/?v=707f3a40153b) \xa0[**^(AI Builder's Toolkit)**](https://djamgatech.myshopify.com/products/%F0%9F%9B%A0%EF%B8%8F-ai-unraveled-the-builders-toolkit-practical-ai-tutorials-projects-e-book-audio-video)\n\nHello AI Unraveled Listeners,\n\n**In today’s AI Daily News,**\n\n**⚠️ Racist AI videos are spreading on TikTok**\n\n**🤝 OpenAI signs a $30bn cloud deal with Oracle**\n\n**🤖 Ford CEO predicts AI will cut half of white-collar jobs**\n\n**🚫 OpenAI says it has not partnered with Robinhood**\n\n**🤖 Perplexity Goes Premium: $200 Plan Shakes Up AI Search**\n\n**🖌️AI for Good: AI finds paint formula that keeps buildings cool**\n\n**💻Microsoft scales back AI chip ambitions to overcome delays**\n\n**📹AI VTubers are now raking in millions on YouTube**\n\n**🎸 AI band hits 500k listeners, admits to Suno use**\n\n**🫂 Sakana AI teaches models to team up**\n\n**🧠 Scientists build an AI that can think like humans**\n\n**📉 Microsoft to lay off another 9,000 employees**\n\n**🤖 X to let AI fact-check your posts**\n\n**⚔️ Altman slams Meta: 'Missionaries will beat mercenaries'**\n\n**🌐 Cloudflare creates pay-per-crawl AI marketplace 💼 OpenAI’s high-level enterprise consulting business**\n\n# Listen FREE at [https://podcasts.apple.com/us/podcast/ai-daily-news-july-03-2025-racist-ai-videos-are-spreading/id1684415169?i=1000715630274](https://podcasts.apple.com/us/podcast/ai-daily-news-july-03-2025-racist-ai-videos-are-spreading/id1684415169?i=1000715630274)\xa0"", ""https://search.max.io\n\nThis is my personal search engine that I built for myself in December, when I was fed up with the UX of the others out there.  I don't share it often, and I don't ask for money.  Would love some feedback if you try it."", ""Hey everyone!\n\nLike many of you, I've been running powerful local models like LLaMA 4, Phi-3, and OpenHermes on my own hardware, constantly refining prompts to squeeze out better results. I’ve also experimented with top cloud-based models like GPT-4.5, Claude 4, and Gemini 2.5 to compare performance and capabilities. My workflow was a disaster - I had prompts scattered across text files, different versions in random folders, and no idea which variation performed best for different models.\n\nLast month, I finally snapped when I accidentally overwrote a prompt that took me hours to perfect. So I built\xa0[PromptBuild.ai](http://promptbuild.ai/)\xa0\\- think Git for prompts but with a focus on testing and performance tracking.\n\n**What it does:**\n\n* Version control for all your prompts (see exactly what changed between versions)\n* Test different prompt variations side by side\n* Track which prompts work best with which models\n* Score responses to build a performance history\n* Organize prompts by project (I have separate projects for coding assistants, creative writing, data analysis, etc.)\n\n**Why I think you'll find it useful:**\n\n* When you're testing the same prompt across different models (Llama 4 vs Phi-3 vs Claude 4), you can track which variations work best for each\n* Built-in variable system - so you can have template prompts with {{variables}} that you fill in during testing\n* Interactive testing playground - test prompts with variable substitution and capture responses\n* Performance scoring - rate each test run (1-5 stars) and build a performance history\n* Export/import - so you can share prompt collections with the community\n\nThe current version is completely\xa0**FREE**\xa0\\- unlimited teams, projects and prompts. I'm working on paid tiers with API access and team features, but the core functionality will always be free for individual users.\n\nI built this because I needed it myself, but figured others might be dealing with the same prompt management chaos. Would love your feedback!\n\nTry it out:\xa0[promptbuild.ai](https://promptbuild.ai/)\n\nHappy to answer any questions about the implementation or features!"", 'import streamlit as st\nfrom langchain_community.agent_toolkits.sql.base import create_sql_agent\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\nfrom langchain_groq import ChatGroq\nfrom langchain.agents import Tool\nfrom langchain.agents.agent_types import AgentType\nfrom sqlalchemy import create_engine\nfrom pathlib import Path\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.prebuilt import create_react_agent\nimport sqlite3\nfrom typing import TypedDict, List, Optional\n\n# --- Constants ---\nLOCALDB = ""USE_LOCALDB""\nMYSQL = ""USE_MYSQL""\n\n# --- Streamlit session state cart ---\nif ""cart"" not in st.session_state:\n    st.session_state.cart = []\n\n# --- DB configuration ---\ndef configure_db(db_uri, mysql_host=None, mysql_user=None, mysql_password=None, mysql_db=None):\n    if db_uri == LOCALDB:\n        dbfilepath = (Path(__file__).parent / ""student.db"").absolute()\n        creator = lambda: sqlite3.connect(f""file:{dbfilepath}?mode=ro"", uri=True)\n        return SQLDatabase(create_engine(""sqlite://"", creator=creator))\n    elif db_uri == MYSQL:\n        if not (mysql_host and mysql_user and mysql_password and mysql_db):\n            raise ValueError(""Missing MySQL credentials."")\n        return SQLDatabase(\n            create_engine(f""mysql+mysqlconnector://{mysql_user}:{mysql_password}@{mysql_host}/{mysql_db}"")\n        )\n\n# --- Product parser ---\ndef parse_products(text_response: str):\n    lines = [line.strip() for line in text_response.strip().split(\'\\n\') if line.strip()]\n    if not lines or \',\' not in lines[0]:\n        return []\n    headers = [h.strip().lower() for h in lines[0].split("","")]\n    products = []\n    for row in lines[1:]:\n        fields = [f.strip() for f in row.split("","")]\n        if len(fields) == len(headers):\n            products.append({headers[i]: fields[i] for i in range(len(headers))})\n    return products\n\n# --- State schema for LangGraph ---\nclass AgentState(TypedDict):\n    llm: object\n    agent_executor: object\n    user_input: str\n    plan: Optional[str]\n    response: Optional[List[dict]]\n    raw: Optional[str]\n    messages: List[dict]\n\n# --- LangGraph workflow nodes ---\ndef planner_node(state: AgentState):\n    plan = state[""llm""].invoke(state[""user_input""])\n    return {""plan"": plan}\n\ndef executor_node(state: AgentState):\n    result = state[""agent_executor""].invoke({\n        ""input"": state[""plan""],\n        ""messages"": state[""messages""]  # <- carry messages through\n    })\n    sql_output = result.get(""output"", """")\n    parsed_products = parse_products(sql_output)\n    for product in parsed_products:\n        st.session_state.cart.append(product)\n    return {""response"": parsed_products, ""raw"": sql_output, ""messages"": result.get(""messages"", state[""messages""])}\n\ndef build_workflow(llm, agent_executor):\n    graph = StateGraph(AgentState)\n    graph.add_node(""planner"", planner_node)\n    graph.add_node(""executor"", executor_node)\n    graph.set_entry_point(""planner"")\n    graph.add_edge(""planner"", ""executor"")\n    graph.add_edge(""executor"", END)\n    return graph.compile()\n\n# --- Streamlit UI ---\nst.set_page_config(page_title=""LangGraph SQL Cart App"")\nst.title(""🛒 AI Shopping Assistant with LangGraph"")\n\ngroq_api_key = st.text_input(""Enter your Groq API Key"", type=""password"")\ndb_type = st.selectbox(""Select Database"", [LOCALDB, MYSQL])\n\nif db_type == MYSQL:\n    mysql_host = st.text_input(""MySQL Host"")\n    mysql_user = st.text_input(""MySQL Username"")\n    mysql_password = st.text_input(""MySQL Password"", type=""password"")\n    mysql_db = st.text_input(""MySQL DB Name"")\nelse:\n    mysql_host = mysql_user = mysql_password = mysql_db = None\n\nquery = st.text_area(""Ask your question (e.g. What do I need to make tea?)"")\n\nif st.button(""Run Query"") and groq_api_key and query.strip():\n    with st.spinner(""Thinking with LangGraph...""):\n        try:\n            llm = ChatGroq(\n                groq_api_key=groq_api_key,\n                model_name=""llama3-8b-8192"",\n            )\n            db = configure_db(db_type, mysql_host, mysql_user, mysql_password, mysql_db)\n            toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n\n            tools = toolkit.get_tools()\n            agent = create_react_agent(model=llm, tools=tools, prompt=""You are a helpful assistant"")\n            agent_executor = agent\n\n            workflow = build_workflow(llm, agent_executor)\n            result = workflow.invoke({\n                ""llm"": llm,\n                ""agent_executor"": agent_executor,\n                ""user_input"": query,\n                ""messages"": []  # 🔑 required for LangGraph chat agents\n            })\n\n            st.success(""Query processed!"")\n            st.subheader(""🧾 Raw SQL Output"")\n            st.code(result[""raw""], language=""text"")\n\n            st.subheader(""🧺 Cart Items"")\n            if st.session_state.cart:\n                st.dataframe(st.session_state.cart)\n            else:\n                st.info(""No items found or parsed."")\n\n            # (Optional) Show internal message log\n            st.subheader(""💬 Agent Message History"")\n            for msg in result[""messages""]:\n                st.markdown(f""**{msg[\'role\'].capitalize()}**: {msg[\'content\']}"")\n\n        except Exception as e:\n            st.error(f""Error: {str(e)}"")\n\nif st.button(""Clear Cart""):\n    st.session_state.cart.clear()\n    st.success(""Cart has been cleared."")(can anyone tell me what is the error in my code)', ""AI Daily News July 04 2025: 🌐Denmark Says You Own the Copyright to Your Face, Voice & Body 💬Meta is testing AI chatbots that can message you first 🧠OpenAI co-founder Ilya Sutskever now leads Safe Superintelligence 🍼AI helps a couple conceive after 18 years \n\nHello AI Unraveled Listeners,\n\nIn today’s AI Daily News,\n\n🌐 Denmark Says You Own the Copyright to Your Face, Voice & Body\n\n💬 Meta is testing AI chatbots that can message you first\n\n🧠 OpenAI co-founder Ilya Sutskever now leads Safe Superintelligence\n\n🍼 AI helps a couple conceive after 18 years\n\n💬Meta chatbots to message users first\n\n🏗️ What a real 'AI Manhattan Project' could look like\n\n👶 A Couple Tried for 18 Years to Get Pregnant — AI Made It Happen\n\n📉 Microsoft to Cut Up to 9,000 More Jobs as It Doubles Down on AI\n\n🚓 Arlington County Deploys AI to Handle Non-Emergency 911 Calls Over Holiday\n\n☢️ AI Helps Discover Optimal New Material to Remove Radioactive Iodine\n\nListen FREE at\xa0[https://podcasts.apple.com/us/podcast/ai-daily-news-july-04-2025-denmark-says-you-own-the/id1684415169?i=1000715750035](https://podcasts.apple.com/us/podcast/ai-daily-news-july-04-2025-denmark-says-you-own-the/id1684415169?i=1000715750035)\n\n**#AI** **#AIDailyNews** **#AIUnraveled** **#Djamgatech** **#AIBuildersToolkit** **#EtienneNoumen**"", ""# [A daily Chronicle of AI Innovations from July 01 to July 07 2025:](https://podcasts.apple.com/us/podcast/ai-daily-news-july-03-2025-racist-ai-videos-are-spreading/id1684415169?i=1000715630274)\n\nHello AI Unraveled Listeners,\n\n**In this week's AI News,**\n\n🐾 Ready-to-use stem cell therapy for pets is coming\n\n⚖️ Google is facing an EU antitrust complaint over its AI summaries feature\n\n⚖️ EU Rejects Apple, Meta, Google, and European Companies’ Request for AI Act Delay\n\n🌐Denmark Says You Own the Copyright to Your Face, Voice & Body\n\n💬Meta chatbots to message users first\n\n🧠OpenAI co-founder Ilya Sutskever now leads Safe Superintelligence\n\n🍼AI helps a couple conceive after 18 years\n\n⚠️Racist AI videos are spreading on TikTok\n\n🧠 Scientists build an AI that can think like humans\n\n📹AI VTubers are now raking in millions on YouTube\n\n📉Microsoft to lay off another 9,000 employees: AI ?\n\n🧠Meta announces its Superintelligence Labs\n\n🤖Baidu’s open-source ERNIE 4.5 to rival DeepSeek\n\n🧬Chai Discovery's AI designs working antibodies\n\n[**AI Builder's Toolkit**](https://djamgatech.myshopify.com/products/%F0%9F%9B%A0%EF%B8%8F-ai-unraveled-the-builders-toolkit-practical-ai-tutorials-projects-e-book-audio-video)\n\n# Listen FREE at [https://podcasts.apple.com/us/podcast/ai-weekly-news-rundown-july-01-to-july-07-2025-google/id1684415169?i=1000715881206](https://podcasts.apple.com/us/podcast/ai-weekly-news-rundown-july-01-to-july-07-2025-google/id1684415169?i=1000715881206)""]","[""# Your Creation, Your Proof. Get It Free.\n\nAs a creator, I learned the hard way: your only real responsibility is **proving you made it**. If you're not a big corporation with endless resources for global copyright registration, there's a simple solution.\n\nUse our **free global copyright verification service(i-STAM)** to instantly verify your **images, PDFs, audio, and video files** via our app or web. Website [https://www.i-stam.com](https://www.i-stam.com)\n\n**Here’s how it works:**\n\n* **10 Free Registrations:** Log in and get 10 free points to start.\n* **Need More?** Just send me a direct message (DM) and I'll gladly provide more free points, as many as you need.\n\n**A few quick notes:**\n\n1. **Homegrown:** I built this myself, so the UI/design is basic but effective.\n2. **Permanent Record:** Once registered, your content is saved permanently and can't be changed.\n3. **Mindful Use:** Please use it responsibly to help manage server costs.\n\n(Check the website for full instructions before using the app or web.)"", ""# [A daily Chronicle of AI Innovations in July 2025: July 03rd 2025](https://podcasts.apple.com/us/podcast/ai-daily-news-july-03-2025-racist-ai-videos-are-spreading/id1684415169?i=1000715630274)\n\n[**^(Read Online)**](https://enoumen.substack.com/)\xa0^(|)\xa0[**^(Sign Up)**](https://podcasts.apple.com/us/podcast/ai-unraveled-latest-ai-news-trends-chatgpt-gemini-deepseek/id1684415169)\xa0^(|)\xa0[**^(Advertise |)**](https://djamgatech.com/product/ai-unraveled-podcast-per-episode-integration/?v=707f3a40153b) \xa0[**^(AI Builder's Toolkit)**](https://djamgatech.myshopify.com/products/%F0%9F%9B%A0%EF%B8%8F-ai-unraveled-the-builders-toolkit-practical-ai-tutorials-projects-e-book-audio-video)\n\nHello AI Unraveled Listeners,\n\n**In today’s AI Daily News,**\n\n**⚠️ Racist AI videos are spreading on TikTok**\n\n**🤝 OpenAI signs a $30bn cloud deal with Oracle**\n\n**🤖 Ford CEO predicts AI will cut half of white-collar jobs**\n\n**🚫 OpenAI says it has not partnered with Robinhood**\n\n**🤖 Perplexity Goes Premium: $200 Plan Shakes Up AI Search**\n\n**🖌️AI for Good: AI finds paint formula that keeps buildings cool**\n\n**💻Microsoft scales back AI chip ambitions to overcome delays**\n\n**📹AI VTubers are now raking in millions on YouTube**\n\n**🎸 AI band hits 500k listeners, admits to Suno use**\n\n**🫂 Sakana AI teaches models to team up**\n\n**🧠 Scientists build an AI that can think like humans**\n\n**📉 Microsoft to lay off another 9,000 employees**\n\n**🤖 X to let AI fact-check your posts**\n\n**⚔️ Altman slams Meta: 'Missionaries will beat mercenaries'**\n\n**🌐 Cloudflare creates pay-per-crawl AI marketplace 💼 OpenAI’s high-level enterprise consulting business**\n\n# Listen FREE at [https://podcasts.apple.com/us/podcast/ai-daily-news-july-03-2025-racist-ai-videos-are-spreading/id1684415169?i=1000715630274](https://podcasts.apple.com/us/podcast/ai-daily-news-july-03-2025-racist-ai-videos-are-spreading/id1684415169?i=1000715630274)\xa0"", ""https://search.max.io\n\nThis is my personal search engine that I built for myself in December, when I was fed up with the UX of the others out there.  I don't share it often, and I don't ask for money.  Would love some feedback if you try it."", ""Hey everyone!\n\nLike many of you, I've been running powerful local models like LLaMA 4, Phi-3, and OpenHermes on my own hardware, constantly refining prompts to squeeze out better results. I’ve also experimented with top cloud-based models like GPT-4.5, Claude 4, and Gemini 2.5 to compare performance and capabilities. My workflow was a disaster - I had prompts scattered across text files, different versions in random folders, and no idea which variation performed best for different models.\n\nLast month, I finally snapped when I accidentally overwrote a prompt that took me hours to perfect. So I built\xa0[PromptBuild.ai](http://promptbuild.ai/)\xa0\\- think Git for prompts but with a focus on testing and performance tracking.\n\n**What it does:**\n\n* Version control for all your prompts (see exactly what changed between versions)\n* Test different prompt variations side by side\n* Track which prompts work best with which models\n* Score responses to build a performance history\n* Organize prompts by project (I have separate projects for coding assistants, creative writing, data analysis, etc.)\n\n**Why I think you'll find it useful:**\n\n* When you're testing the same prompt across different models (Llama 4 vs Phi-3 vs Claude 4), you can track which variations work best for each\n* Built-in variable system - so you can have template prompts with {{variables}} that you fill in during testing\n* Interactive testing playground - test prompts with variable substitution and capture responses\n* Performance scoring - rate each test run (1-5 stars) and build a performance history\n* Export/import - so you can share prompt collections with the community\n\nThe current version is completely\xa0**FREE**\xa0\\- unlimited teams, projects and prompts. I'm working on paid tiers with API access and team features, but the core functionality will always be free for individual users.\n\nI built this because I needed it myself, but figured others might be dealing with the same prompt management chaos. Would love your feedback!\n\nTry it out:\xa0[promptbuild.ai](https://promptbuild.ai/)\n\nHappy to answer any questions about the implementation or features!"", 'import streamlit as st\nfrom langchain_community.agent_toolkits.sql.base import create_sql_agent\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\nfrom langchain_groq import ChatGroq\nfrom langchain.agents import Tool\nfrom langchain.agents.agent_types import AgentType\nfrom sqlalchemy import create_engine\nfrom pathlib import Path\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.prebuilt import create_react_agent\nimport sqlite3\nfrom typing import TypedDict, List, Optional\n\n# --- Constants ---\nLOCALDB = ""USE_LOCALDB""\nMYSQL = ""USE_MYSQL""\n\n# --- Streamlit session state cart ---\nif ""cart"" not in st.session_state:\n    st.session_state.cart = []\n\n# --- DB configuration ---\ndef configure_db(db_uri, mysql_host=None, mysql_user=None, mysql_password=None, mysql_db=None):\n    if db_uri == LOCALDB:\n        dbfilepath = (Path(__file__).parent / ""student.db"").absolute()\n        creator = lambda: sqlite3.connect(f""file:{dbfilepath}?mode=ro"", uri=True)\n        return SQLDatabase(create_engine(""sqlite://"", creator=creator))\n    elif db_uri == MYSQL:\n        if not (mysql_host and mysql_user and mysql_password and mysql_db):\n            raise ValueError(""Missing MySQL credentials."")\n        return SQLDatabase(\n            create_engine(f""mysql+mysqlconnector://{mysql_user}:{mysql_password}@{mysql_host}/{mysql_db}"")\n        )\n\n# --- Product parser ---\ndef parse_products(text_response: str):\n    lines = [line.strip() for line in text_response.strip().split(\'\\n\') if line.strip()]\n    if not lines or \',\' not in lines[0]:\n        return []\n    headers = [h.strip().lower() for h in lines[0].split("","")]\n    products = []\n    for row in lines[1:]:\n        fields = [f.strip() for f in row.split("","")]\n        if len(fields) == len(headers):\n            products.append({headers[i]: fields[i] for i in range(len(headers))})\n    return products\n\n# --- State schema for LangGraph ---\nclass AgentState(TypedDict):\n    llm: object\n    agent_executor: object\n    user_input: str\n    plan: Optional[str]\n    response: Optional[List[dict]]\n    raw: Optional[str]\n    messages: List[dict]\n\n# --- LangGraph workflow nodes ---\ndef planner_node(state: AgentState):\n    plan = state[""llm""].invoke(state[""user_input""])\n    return {""plan"": plan}\n\ndef executor_node(state: AgentState):\n    result = state[""agent_executor""].invoke({\n        ""input"": state[""plan""],\n        ""messages"": state[""messages""]  # <- carry messages through\n    })\n    sql_output = result.get(""output"", """")\n    parsed_products = parse_products(sql_output)\n    for product in parsed_products:\n        st.session_state.cart.append(product)\n    return {""response"": parsed_products, ""raw"": sql_output, ""messages"": result.get(""messages"", state[""messages""])}\n\ndef build_workflow(llm, agent_executor):\n    graph = StateGraph(AgentState)\n    graph.add_node(""planner"", planner_node)\n    graph.add_node(""executor"", executor_node)\n    graph.set_entry_point(""planner"")\n    graph.add_edge(""planner"", ""executor"")\n    graph.add_edge(""executor"", END)\n    return graph.compile()\n\n# --- Streamlit UI ---\nst.set_page_config(page_title=""LangGraph SQL Cart App"")\nst.title(""🛒 AI Shopping Assistant with LangGraph"")\n\ngroq_api_key = st.text_input(""Enter your Groq API Key"", type=""password"")\ndb_type = st.selectbox(""Select Database"", [LOCALDB, MYSQL])\n\nif db_type == MYSQL:\n    mysql_host = st.text_input(""MySQL Host"")\n    mysql_user = st.text_input(""MySQL Username"")\n    mysql_password = st.text_input(""MySQL Password"", type=""password"")\n    mysql_db = st.text_input(""MySQL DB Name"")\nelse:\n    mysql_host = mysql_user = mysql_password = mysql_db = None\n\nquery = st.text_area(""Ask your question (e.g. What do I need to make tea?)"")\n\nif st.button(""Run Query"") and groq_api_key and query.strip():\n    with st.spinner(""Thinking with LangGraph...""):\n        try:\n            llm = ChatGroq(\n                groq_api_key=groq_api_key,\n                model_name=""llama3-8b-8192"",\n            )\n            db = configure_db(db_type, mysql_host, mysql_user, mysql_password, mysql_db)\n            toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n\n            tools = toolkit.get_tools()\n            agent = create_react_agent(model=llm, tools=tools, prompt=""You are a helpful assistant"")\n            agent_executor = agent\n\n            workflow = build_workflow(llm, agent_executor)\n            result = workflow.invoke({\n                ""llm"": llm,\n                ""agent_executor"": agent_executor,\n                ""user_input"": query,\n                ""messages"": []  # 🔑 required for LangGraph chat agents\n            })\n\n            st.success(""Query processed!"")\n            st.subheader(""🧾 Raw SQL Output"")\n            st.code(result[""raw""], language=""text"")\n\n            st.subheader(""🧺 Cart Items"")\n            if st.session_state.cart:\n                st.dataframe(st.session_state.cart)\n            else:\n                st.info(""No items found or parsed."")\n\n            # (Optional) Show internal message log\n            st.subheader(""💬 Agent Message History"")\n            for msg in result[""messages""]:\n                st.markdown(f""**{msg[\'role\'].capitalize()}**: {msg[\'content\']}"")\n\n        except Exception as e:\n            st.error(f""Error: {str(e)}"")\n\nif st.button(""Clear Cart""):\n    st.session_state.cart.clear()\n    st.success(""Cart has been cleared."")(can anyone tell me what is the error in my code)']",6,18,0.88,Discussion,1751422531.0
1lpjc4n,MachineLearning,[D] Classical ML prediction - preventing data leakage from time series process data 🙏,"Anyone working in process industry and has attempted making “soft sensors” before?

Given a continuous industrial process with data points recorded in a historian every minute, you try to predict the outcome by applying classical ML methods such as xgboost.

The use case demands that the model works like a soft(ware) sensor that continuously gives a numerical prediction of the output of the process. Not that this is not really a time series forecast (eg not looking into the distant future, just predicting the immediate outcome).

Question: Shuffling the data leads to data leakage because the neighbouring data points contain similar information (contains temporal information). But if shuffling is not done, the model is extremely poor / cannot generalise well.

Fellow practitioners, any suggestions for dealing with ML in that may have time series related data leakage?

Thanks in advance for any kind sharing.





","["">But if shuffling is not done, the model is extremely poor / cannot generalise well.\n\nThis is a bit of a confusing sentiment, and I think clarifying it will help you solve your problem. It sounds like you are saying that your training/validation loss figures are better with leaky data. \\[1\\]\n\nYou are almost certainly not in a situation where you have a choice to allow leaky data or not; where you can have a performant model trained on leaky data, or a poor model trained on well-formed data. You have a poor model full stop, and in certain situations you're allowing it to see the answer sheet before taking the exam. Don't get excited about good AUC numbers (or w/e) when training in leaky data. They are fictitious.\n\nFirst, ground your assessment of your model's performance in out-of-sample testing. With time series problems that means your holdout test set should be temporally after *all* the training data. How do your models perform against that?\n\n*\\[1\\] If it is in fact a properly held-out test set that you are seeing better performance on with leaky training data, please tell me more. I am fascinated.*"", 'I work in sports and deal with this constantly - predicting the near future (often with xgboost) on data that is temporal but not a time series.\n\nI\'m not exactly sure what you mean by this:\n\n>But if shuffling is not done, the model is extremely poor / cannot generalise well.\n\nDo you mean doing a single past-future split? What is your benchmark for ""poor""? If you\'re comparing to the model trained/evaluated in a leaky way it is always going to look worse, becuase that model is cheating.\n\nMy general approach to model development is to use step-forward cross validation, which is standard time series stuff. That is, instead of splitting your data into n random folds, split it into n sequential chunks, so you\'re always training on the past and testing on just the next chunk. This simulates a production environment where you\'re regularly retraining, which is generally a good idea. In my line of work data points come in groups we have to respect such as days or games, so I have a custom [BaseCrossValidator](https://github.com/scikit-learn/scikit-learn/blob/00763ab11120db234874234c49cddd03ba38c9dc/sklearn/model_selection/_split.py#L104) for this. But if that\'s not an issue you can use TimeSeriesSplit (even though it\'s not technically a time series)\n\nStep-forward CV is not just for optimizing your xgboost hyperparameters - it\'s also worth optimizing your training schedule. I.e. how often do you re-train/split, and how big is your training window? Depending on the nature of your data you might train on ""everything up to today"" or train on a smaller rolling window.\n\nAnother thing to think about is calibration to correct systemic errors or to keep up with level changes in the data. That introduces another split for your training data, and more variables to optimize. Like maybe you train weekly but re-fit your calibrator daily or hourly. And how big should your calibration window be?\n\nUltimately the way you handle your data during model development should simulate the way you\'re going to be handling it in production.', '[deleted]', 'Could you elaborate on how exactly you’re shuffling the data? There are ways to do so that respect chronological order that are typically used here (I.E. only train on the past and predict the future no matter how the shuffling is done)']","["">But if shuffling is not done, the model is extremely poor / cannot generalise well.\n\nThis is a bit of a confusing sentiment, and I think clarifying it will help you solve your problem. It sounds like you are saying that your training/validation loss figures are better with leaky data. \\[1\\]\n\nYou are almost certainly not in a situation where you have a choice to allow leaky data or not; where you can have a performant model trained on leaky data, or a poor model trained on well-formed data. You have a poor model full stop, and in certain situations you're allowing it to see the answer sheet before taking the exam. Don't get excited about good AUC numbers (or w/e) when training in leaky data. They are fictitious.\n\nFirst, ground your assessment of your model's performance in out-of-sample testing. With time series problems that means your holdout test set should be temporally after *all* the training data. How do your models perform against that?\n\n*\\[1\\] If it is in fact a properly held-out test set that you are seeing better performance on with leaky training data, please tell me more. I am fascinated.*"", 'I work in sports and deal with this constantly - predicting the near future (often with xgboost) on data that is temporal but not a time series.\n\nI\'m not exactly sure what you mean by this:\n\n>But if shuffling is not done, the model is extremely poor / cannot generalise well.\n\nDo you mean doing a single past-future split? What is your benchmark for ""poor""? If you\'re comparing to the model trained/evaluated in a leaky way it is always going to look worse, becuase that model is cheating.\n\nMy general approach to model development is to use step-forward cross validation, which is standard time series stuff. That is, instead of splitting your data into n random folds, split it into n sequential chunks, so you\'re always training on the past and testing on just the next chunk. This simulates a production environment where you\'re regularly retraining, which is generally a good idea. In my line of work data points come in groups we have to respect such as days or games, so I have a custom [BaseCrossValidator](https://github.com/scikit-learn/scikit-learn/blob/00763ab11120db234874234c49cddd03ba38c9dc/sklearn/model_selection/_split.py#L104) for this. But if that\'s not an issue you can use TimeSeriesSplit (even though it\'s not technically a time series)\n\nStep-forward CV is not just for optimizing your xgboost hyperparameters - it\'s also worth optimizing your training schedule. I.e. how often do you re-train/split, and how big is your training window? Depending on the nature of your data you might train on ""everything up to today"" or train on a smaller rolling window.\n\nAnother thing to think about is calibration to correct systemic errors or to keep up with level changes in the data. That introduces another split for your training data, and more variables to optimize. Like maybe you train weekly but re-fit your calibrator daily or hourly. And how big should your calibration window be?\n\nUltimately the way you handle your data during model development should simulate the way you\'re going to be handling it in production.', '[deleted]', 'Could you elaborate on how exactly you’re shuffling the data? There are ways to do so that respect chronological order that are typically used here (I.E. only train on the past and predict the future no matter how the shuffling is done)']",8,10,0.9,Discussion,1751419834.0
1lphfhf,MachineLearning,[D] Request for Career Advice – ML PhD non hot topic,"I’m currently a PhD student in Machine Learning, working on a research topic that isn’t considered “hot” in the current academic or industrial landscape. Despite this, I’ve managed to publish as the lead author at ICML, NeurIPS. And  twice at ECML. I also have two co-authored publications at ECAI.

I’ve noticed that many  PhD students in the U.S. seem to have much stronger publication records, often in trendier areas. This makes me question how competitive I really am in the current job market—especially given the wave of layoffs and increasing demand for very specialized expertise in industry.

That said, I do have a strong foundation in core ML, Deep Learning, and LLMs (although LLMS aren’t the direct focus of my PhD research).

Given all of this, I’m trying to realistically assess: • What are my current chances of landing a demanding, high-quality job in industry or research after my PhD? • What could I do now to improve those chances? • Goal is FANNG.

I’d greatly appreciate any  feedback.

Edit: My research focuses on anomaly detection, a less trendy area compared to the current popularity of large language models and reinforcement learning.","['Imagine you are computer vision expert in ~2005 and publish a lot on the hot topic of custom made descriptors for feature matching for augmented reality. You pity those unfortunate souls trying to make convolutional networks work. Dead end previous century approach. And they are using gaming videocards, poor things...', ""I had similar credentials a few years ago with pubs in ICML, NeurIPS focusing on topics like high dimensional statistics, numerical linear algebra, and some other theory stuff. I did a short postdoc in deep learning and language models adjacent theory, and managed to get some work pulished. Fast forward to now, and I'm in a Faang working on LLM applied research. My point is, you are already competent in deeply technical stuff and research relevant to latest hot topics. You just need to find a way  to have a smooth transition using the credentials you have."", 'Intern and network. Research scientist jobs are competitive even for PhD at MIT unless you have a famous advisor with strong industry connections.', '> I’ve noticed that many PhD students in the U.S. seem to have much stronger publication records\n\nYou have two lead author papers at good conferences. Were they workshop sessions? Posters? Trying to understand what you consider to be a stronger record.', ""You're a lot more competitive than the poor sobs graduating with COLT/STOC/FOCS papers.\n\nPut out code and data for all your papers, make websites and demos (if you haven't already).\n\nFor industry: start leetcoding, reach out to connections to give brownbag talks (many orgs in FAANG have these and they'd love to have you). Take pictures at your talks and tweet about them.\n\nFor academia: your advisor's letter is most important. Reaching out to connections is important. Send cold emails, have less shame and be less fearful of saving face for the year you're on the market.\n\nThat being said, a lot of academia is in a hiring freeze right now, including for postdocs."", 'Would be better if the author gives more details on what topic it is. AFAIK,  there is less opportunities in less trendy research', 'Is computer vision is still trendy or how does it look like in the industry right now...?', 'This may be a good or bad thing, but people have gotten into FAANG (deepmind, FAIR, etc.) for less (a lot less). For example, 2 coworkers at my lab work at deepmind, both with a grand total of 0 Neurips, ICML, ICLR papers. Your publication record is very strong, and 2 top AI conference publications vs 5 or whatever WILL NOT MATTER at all (unless, of course, one of those papers is a super well-cited/well-known paper).\n\nThat being said, there are 2 big problems you are going to face. First is your research direction. Teams at FAANG (and probably everywhere else) first and foremost make sure the people they are hiring are studying the same or very similar things. They will rather have someone who has thought deeply about what they are currently exploring/researching than someone with x top AI conference publications (peer review is noisy and getting worse by the year anyways). Second, is networking. Many people who work at these companies interned there or have lab alumni working there, or have faculty well known by industry.\n\nSo, I think the best advice is find a way to connect your clear expertise in anomaly detection into something more popular/current, and try to intern or publish with faculty that have the industry connections.', ""i dunno. it sounds like you're doing great"", ""Really depends on what your career goals are. Applied Scientist jobs at FAANG+ companies don't have a very high bar, and working on a few projects to transition into hot areas (LLMS, RLHF, reasoning etc), would pretty much guarantee a decent job, regardless of the ML area of your PhD. Research Scientist roles in popular research teams are harder, and mostly hire experts in trending research area. A postdoc to transition might help in this case. Academic jobs are super hard. Popular universities really only hire superstars.""]","['Imagine you are computer vision expert in ~2005 and publish a lot on the hot topic of custom made descriptors for feature matching for augmented reality. You pity those unfortunate souls trying to make convolutional networks work. Dead end previous century approach. And they are using gaming videocards, poor things...', ""I had similar credentials a few years ago with pubs in ICML, NeurIPS focusing on topics like high dimensional statistics, numerical linear algebra, and some other theory stuff. I did a short postdoc in deep learning and language models adjacent theory, and managed to get some work pulished. Fast forward to now, and I'm in a Faang working on LLM applied research. My point is, you are already competent in deeply technical stuff and research relevant to latest hot topics. You just need to find a way  to have a smooth transition using the credentials you have."", 'Intern and network. Research scientist jobs are competitive even for PhD at MIT unless you have a famous advisor with strong industry connections.', '> I’ve noticed that many PhD students in the U.S. seem to have much stronger publication records\n\nYou have two lead author papers at good conferences. Were they workshop sessions? Posters? Trying to understand what you consider to be a stronger record.', ""You're a lot more competitive than the poor sobs graduating with COLT/STOC/FOCS papers.\n\nPut out code and data for all your papers, make websites and demos (if you haven't already).\n\nFor industry: start leetcoding, reach out to connections to give brownbag talks (many orgs in FAANG have these and they'd love to have you). Take pictures at your talks and tweet about them.\n\nFor academia: your advisor's letter is most important. Reaching out to connections is important. Send cold emails, have less shame and be less fearful of saving face for the year you're on the market.\n\nThat being said, a lot of academia is in a hiring freeze right now, including for postdocs.""]",58,39,0.86,Discussion,1751414233.0
1lphavr,MachineLearning,[P] ML deployment,"
Has anyone here deployed models on **Firebase** or **Vertex AI**? I'm looking for the best practice for a clean and cohesive deployment (we have real-time data, and I need to design a continuous retraining pipeline; in essence, the inferences will be used to update a dashboard).",[],[],1,0,0.67,Project,1751413883.0
1lp9tpp,MachineLearning,[D] Subreviewing for NeurIPS,"Does your professor share their assigned papers among their lab members and ask them to sub-review for NeurIPS? I only realized after agreeing that this is actually against [the reviewer guidelines](https://neurips.cc/Conferences/2025/ReviewerGuidelines):

>Q: Can I invite a sub-reviewer to help with my reviews?

>A: No, sub-reviewers are not allowed. Conflicts of interest cannot be properly checked unless reviewers are officially in the system, and sub-reviewers would not be able to participate in the discussion, which is a critical phase of the review process.

So now I am a little bit worried I may be involved in something I perhaps shouldn't have been. On the other hand, perhaps this is one of those things in academia that people are against ""on paper"" but is actually an accepted practice? I think it seems common for professors to review papers through their students, but it seems like in most cases, they are officially appointed as a ""sub-reviewer"" (which NeurIPS doesn't allow) instead of giving their professor a review to pass as their own.

In short: Is this normal and accepted? Does it happen in your lab, too? Should I not worry about it?

**Update:** Thank you to everyone who let me know that I won't get in any trouble for sub-reviewing. That's a relief to know. Although, I am wondering:

\- Do guidelines + code of conduct mean nothing to professors?
\- Isn't signing your name under a ghost-written review without crediting the ghostwriter a form of plagiarism? Am I the only one who believes this still seems unethical?","['Your professor probably didn’t realise that sub-reviewing was not allowed for NeurIPS. It is allowed at several conferences so they probably didn’t decide to check. It won’t affect you.', ""It's a fairly common occurence, unfortunately. I doubt you'd get much (if any) trouble for it. That said, the QA in the guidelines outlines quite well why this is not great."", 'There’s a big difference between ghost writing the review, and asking a colleague or student what they think about a paper.', 'My former advisor would give out reviews for us to do but did always look them over and check them', 'People do it all the time, nobody cares about this rule.', '[deleted]', 'Dont worry. Blatantly using llms for reviews seems common practice now, and usually goes without penalties.', 'I believe each advisor would take a scrutinizing look at the subreviewer comments. But I also feel sorry for the authors; the limited knowledge of the research work surrounding that review paper makes the subreviewer constrained, and the chances of a correct evaluation are less.']","['Your professor probably didn’t realise that sub-reviewing was not allowed for NeurIPS. It is allowed at several conferences so they probably didn’t decide to check. It won’t affect you.', ""It's a fairly common occurence, unfortunately. I doubt you'd get much (if any) trouble for it. That said, the QA in the guidelines outlines quite well why this is not great."", 'There’s a big difference between ghost writing the review, and asking a colleague or student what they think about a paper.', 'My former advisor would give out reviews for us to do but did always look them over and check them', 'People do it all the time, nobody cares about this rule.']",17,16,1.0,Discussion,1751395497.0
1lp8umz,MachineLearning,"[R] Introducing DreamPRM, a multi-modal LLM reasoning method achieving first place on the MathVista leaderboard","I am excited to share our recent work, DreamPRM, a multi-modal LLM reasoning method that ranks first currently on the MathVista leaderboard.

https://preview.redd.it/54v78gz7zaaf1.png?width=1348&format=png&auto=webp&s=0084aef7727d9f02c129d8414582018fb09eedb5

https://preview.redd.it/u0c02on9zaaf1.jpg?width=1374&format=pjpg&auto=webp&s=42ab8761dcc972ed89a999dac503c9dc35e65e18






Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM’s domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.



Paper: [https://arxiv.org/abs/2505.20241](https://arxiv.org/abs/2505.20241)



Code: [https://github.com/coder-qicao/DreamPRM](https://github.com/coder-qicao/DreamPRM)",[],[],2,0,1.0,Research,1751393262.0
1lp7jyb,MachineLearning,[D]Looking for Hinglish (code-mixed Hindi-English) speech emotion audio datasets — any recommendations?,"Hi everyone,
I'm working on a deep learning project involving emotion recognition from Hinglish (code-mixed Hindi-English) speech.

I’ve found plenty of datasets for English (like RAVDESS, IEMOCAP) and some for Hindi (MUCS, OpenSLR), but I’m having trouble locating datasets that contain Hinglish speech, especially with emotion labels.

Do any of you know of:
Hinglish speech datasets (code-switched Hindi-English)
Emotion-labeled Hinglish audio
Open-source or research datasets that allow this type of training

If there are no public datasets, I’d also appreciate tips on how to create or augment one from scratch.
And also how can I increase it accuracy.

Thanks in advance!","['You have 2 sets of datasets - augment data from both the sets, and use them for training your models (assuming that training a model is your use case)', 'Create a dataset by collecting Hinglish speech from YouTube videos, podcasts, or social media with emotion labels using natural language processing and machine learning algorithms.']","['You have 2 sets of datasets - augment data from both the sets, and use them for training your models (assuming that training a model is your use case)', 'Create a dataset by collecting Hinglish speech from YouTube videos, podcasts, or social media with emotion labels using natural language processing and machine learning algorithms.']",1,2,0.67,Discussion,1751390333.0
1lp6n1r,MachineLearning,[D] Recommended preparation material for ML interviews.,"Hi everyone,

Below I am gathering some interview preparation tools for ML research positions. People who had been in the job market recently, which one would you recommend/ find more relevant? Any other resources that I might be missing?

(1) InterviewQuery:

[https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&completed=&saved=&ordering=relevancy&orderingDirection=asc&pageSize=20&page=0](https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&completed=&saved=&ordering=relevancy&orderingDirection=asc&pageSize=20&page=0)

(2) DevInterview:

[https://devinterview.io/questions/machine-learning-and-data-science](https://devinterview.io/questions/machine-learning-and-data-science)

(3) aiofferly:

[https://www.aiofferly.com/problems?page=5](https://www.aiofferly.com/problems?page=5)

(4) MAD:

[https://www.madinterview.com/ml?utm\_source=google&utm\_medium=cpc&utm\_campaign=22464693824&utm\_term=machine%20learning%20coding%20interview%20questions&utm\_content=178169327653&gclid=CjwKCAjw3f\_BBhAPEiwAaA3K5A0Rrw-8xhTQqlzVnBhrcCyyHXSwzgGvAzmJYvVye63uIOqQ7XBWhRoC6L0QAvD\_BwE&gad\_source=1&gad\_campaignid=22464693824&gbraid=0AAAAA\_Y9DohjdsVwcsLkazvDd4iJ64Tv5](https://www.madinterview.com/ml?utm_source=google&utm_medium=cpc&utm_campaign=22464693824&utm_term=machine%20learning%20coding%20interview%20questions&utm_content=178169327653&gclid=CjwKCAjw3f_BBhAPEiwAaA3K5A0Rrw-8xhTQqlzVnBhrcCyyHXSwzgGvAzmJYvVye63uIOqQ7XBWhRoC6L0QAvD_BwE&gad_source=1&gad_campaignid=22464693824&gbraid=0AAAAA_Y9DohjdsVwcsLkazvDd4iJ64Tv5)","['Thank you for sharing! I interviewed last year and felt very underprepared, this will definitely help for my next round!', 'That stuff all looks good, last time I was applying there were a bunch of great resources on YouTube for practice interviews and such. The questions change so often a lot of it is probably out dated, but definitely worth looking into.', '[Deep Learning Interviews](https://arxiv.org/abs/2201.00650v2) ([previously posted](https://www.reddit.com/r/learnmachinelearning/comments/qbwgvy/the_pdf_version_of_my_book_deep_learning/)) is free on ArXiv, but the current iteration is aimed more at optimization and network design, rather than specific applications - are you aiming at any specific subfields (e.g. computer vision, LLMs etc.)?']","['Thank you for sharing! I interviewed last year and felt very underprepared, this will definitely help for my next round!', 'That stuff all looks good, last time I was applying there were a bunch of great resources on YouTube for practice interviews and such. The questions change so often a lot of it is probably out dated, but definitely worth looking into.', '[Deep Learning Interviews](https://arxiv.org/abs/2201.00650v2) ([previously posted](https://www.reddit.com/r/learnmachinelearning/comments/qbwgvy/the_pdf_version_of_my_book_deep_learning/)) is free on ArXiv, but the current iteration is aimed more at optimization and network design, rather than specific applications - are you aiming at any specific subfields (e.g. computer vision, LLMs etc.)?']",28,9,0.95,Discussion,1751388293.0
1lp5yum,MachineLearning,[D] Computing Attention Scores with Long Context LLMs,"I'm trying to compute the top-k tokens yielding the highest attention scores with inference frameworks such as vLLM or the plain HuggingFace transformers. The models I'm using are not big in terms of parameters (max 7B) but huge in terms of context windows (up to 1M tokens, and I'm using all of it). However, I face two problems:

1. When using vLLM, I cannot access the attention scores in any way. Am I missing something or is the feature not yet implemented?
2. When using transformers, I need to use flash\_attention\_2 otherwise the GPU budget skyrockets to 400+ GBs when using large inputs (i have a machine with 8 A100 for a total of 320GB of VRAM). However, when using flash\_attention\_2 the output attention scores are all None, and the only way to solve this seems to use an eager attention implementation, which makes it unfeasible in terms of GPU requirements.

Is someone facing a similar problem? How do you compute the attention scores for such large inputs?","[""This probably isn't that common...\n\nFor vLLM it's a primarily inference framework, not a research platform, so I suspect nobody has asked for such a thing.\n\nFor flash attention, you need to set return\\_attn\\_probs [here](https://github.com/Dao-AILab/flash-attention/blob/7661781d001e0900121c000a0aaf21b3f94337d6/flash_attn/flash_attn_interface.py#L1031). I'm not sure if that's wired up in transformers, you might have to inject your own attention model or fork the code to set the property.""]","[""This probably isn't that common...\n\nFor vLLM it's a primarily inference framework, not a research platform, so I suspect nobody has asked for such a thing.\n\nFor flash attention, you need to set return\\_attn\\_probs [here](https://github.com/Dao-AILab/flash-attention/blob/7661781d001e0900121c000a0aaf21b3f94337d6/flash_attn/flash_attn_interface.py#L1031). I'm not sure if that's wired up in transformers, you might have to inject your own attention model or fork the code to set the property.""]",3,1,1.0,Discussion,1751386770.0
1lp5c22,MachineLearning,[D] Looking for AI-powered smart crop library - smartcrop.py isn't enough,"https://preview.redd.it/r1w6xzdnbaaf1.png?width=1492&format=png&auto=webp&s=5ab883dcc781312bb6014b9daf1d9295dfc66030

Hey everyone!

I'm currently using smartcrop.py (github.com/smartcrop/smartcrop.py) for image cropping in Python, but it's pretty basic. It only detects edges and color gradients, not actual objects.

For example, if I have a photo with a coffee cup, I want it to recognize the cup as the main subject and crop around it. But smartcrop just finds areas with most edges/contrast, which often misses the actual focal point.

**Looking for:**

* Python library that uses AI/ML for object-aware cropping
* Can identify main subjects (people, objects, etc.)
* More modern than just edge detection

Any recommendations for libraries that actually understand what's in the image?

Thanks!","['You might have some better luck finding a binary segmentation/background removal model and using it to detect the foreground, then drawing a bounding box yourself with padding to build the crop.\n\nThis could be something lightweight like [https://huggingface.co/briaai/RMBG-1.4](https://huggingface.co/briaai/RMBG-1.4) or something heavier and configurable like [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything) depending on your resource requirements.']","['You might have some better luck finding a binary segmentation/background removal model and using it to detect the foreground, then drawing a bounding box yourself with padding to build the crop.\n\nThis could be something lightweight like [https://huggingface.co/briaai/RMBG-1.4](https://huggingface.co/briaai/RMBG-1.4) or something heavier and configurable like [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything) depending on your resource requirements.']",0,4,0.33,Discussion,1751385325.0
1lp4lk8,MachineLearning,[R] Transition Matching: Scalable and Flexible Generative Modeling,Imo a silent banger by Meta - generalizing diffusion and flow matching into transition matching which can be used in a unified causal generation process.,[],[],4,0,0.83,Research,1751383626.0
1lp3wla,MachineLearning,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!","[""Question: How to find postdoc in AI as non-cs phd?\n\nI am a final-year PhD candidate in a non-CS field. My research is application of modern AI models (e.g., Transformer, GAN, Stable Diffusion, Mamba). In short, my work is to apply new models proposed at CVPR and ICML to my own field.\n\nMy ultimate career goal is to do core AI research. To make this transition, I am seeking a postdoctoral position within a computer science department. My primary objective for a postdoc is to gain experience and publish in top-tier AI conferences like CVPR, ICML, NeurIPS, and others.\n\nI recognize that my profile may seem not competitive to a CS PI. To make myself a more relevant candidate, my current plan is to publish a CVPR workshop. While I know a workshop paper isn't that competitive, it's the best approach I can think of right now.\n\nI would also like to ask for any further advice. Has anyone here made a similar transition from a non-CS PhD to a postdoc in CS/AI?"", 'Question: Where can I find resources on learning how to evaluate models?', '\n\n\n🔍 Looking for Guidance or Collaboration — Building a Sinhala Language GPT Chatbot (Open to Advice or Partnerships)\n\n\nHi everyone, I’m working on a very interesting but challenging project — building a GPT-style AI chatbot for the Sinhala language. I initially started this as a hobby, thinking it would be simple, but I’ve now realized it’s a much bigger and more expensive task than expected.\n\nMy goal is to create a multifunctional Sinhala chatbot that can:\n\nUnderstand and generate natural Sinhala conversations\n\nPerform tasks like translation, web search, and knowledge retrieval\n\nHandle multi-turn conversations, emotions, and different intents\n\nEventually serve users in Sinhala for daily tasks, education, and business\n\nI’ve already started the hardest part — building a labeled dataset with Sinhala sentences and responses. I researched data labeling, intent detection, sentiment tagging, and more. But I’m realizing that for actual model training (LLM fine-tuning, embeddings, retrieval augmentation), I need more guidance.\n\nI’m looking for:\n\nAdvice from anyone who has worked on LLM fine-tuning, RAG (Retrieval Augmented Generation), Whisper, TTS, or multilingual AI models\n\nCollaboration with developers, AI engineers, or data scientists\n\nEven as a small startup partnership, I’m open to working together\n\nI can invest a monthly budget to cover costs like labeling, server, training, or API expenses\n\nThis project is for Sinhala, an underrepresented language in AI. If anyone can help, guide, or wants to collaborate, please reach out. I would really appreciate any advice, resources, or connections.\n\nThank you!']","[""Question: How to find postdoc in AI as non-cs phd?\n\nI am a final-year PhD candidate in a non-CS field. My research is application of modern AI models (e.g., Transformer, GAN, Stable Diffusion, Mamba). In short, my work is to apply new models proposed at CVPR and ICML to my own field.\n\nMy ultimate career goal is to do core AI research. To make this transition, I am seeking a postdoctoral position within a computer science department. My primary objective for a postdoc is to gain experience and publish in top-tier AI conferences like CVPR, ICML, NeurIPS, and others.\n\nI recognize that my profile may seem not competitive to a CS PI. To make myself a more relevant candidate, my current plan is to publish a CVPR workshop. While I know a workshop paper isn't that competitive, it's the best approach I can think of right now.\n\nI would also like to ask for any further advice. Has anyone here made a similar transition from a non-CS PhD to a postdoc in CS/AI?"", 'Question: Where can I find resources on learning how to evaluate models?', '\n\n\n🔍 Looking for Guidance or Collaboration — Building a Sinhala Language GPT Chatbot (Open to Advice or Partnerships)\n\n\nHi everyone, I’m working on a very interesting but challenging project — building a GPT-style AI chatbot for the Sinhala language. I initially started this as a hobby, thinking it would be simple, but I’ve now realized it’s a much bigger and more expensive task than expected.\n\nMy goal is to create a multifunctional Sinhala chatbot that can:\n\nUnderstand and generate natural Sinhala conversations\n\nPerform tasks like translation, web search, and knowledge retrieval\n\nHandle multi-turn conversations, emotions, and different intents\n\nEventually serve users in Sinhala for daily tasks, education, and business\n\nI’ve already started the hardest part — building a labeled dataset with Sinhala sentences and responses. I researched data labeling, intent detection, sentiment tagging, and more. But I’m realizing that for actual model training (LLM fine-tuning, embeddings, retrieval augmentation), I need more guidance.\n\nI’m looking for:\n\nAdvice from anyone who has worked on LLM fine-tuning, RAG (Retrieval Augmented Generation), Whisper, TTS, or multilingual AI models\n\nCollaboration with developers, AI engineers, or data scientists\n\nEven as a small startup partnership, I’m open to working together\n\nI can invest a monthly budget to cover costs like labeling, server, training, or API expenses\n\nThis project is for Sinhala, an underrepresented language in AI. If anyone can help, guide, or wants to collaborate, please reach out. I would really appreciate any advice, resources, or connections.\n\nThank you!']",1,3,0.66,Discussion,1751382053.0
1lozfbp,MachineLearning,[P] I created an open-source tool to analyze 1.5M medical AI papers on PubMed,"Hey everyone,

I've been working on a personal project to understand how AI is actually being used in medical research (not just the hype), and thought some of you might find the results interesting.

After analyzing nearly 1.5 million PubMed papers that use AI methods, I found some intersting results:

* **Classical ML still dominates**: Despite all the deep learning hype, traditional algorithms like logistic regression and random forests account for 88.1% of all medical AI research
* **Algorithm preferences by medical condition**: Different health problems gravitate toward specific algorithms
* **Transformer takeover timeline**: You can see the exact point (around 2022) when transformers overtook LSTMs in medical research

I built an interactive dashboard where you can:

* Search by medical condition to see which algorithms researchers are using
* Track how algorithm usage has evolved over time
* See the distribution across classical ML, deep learning, and LLMs

One of the trickiest parts was filtering out false positives (like ""GAN"" meaning Giant Axonal Neuropathy vs. Generative Adversarial Network).

The tool is completely free, hosted on Hugging Face Spaces, and open-source. I'm not trying to monetize this - just thought it might be useful for researchers or anyone interested in healthcare AI trends.

Happy to answer any questions or hear suggestions for improving it!","['Love that you’re surfacing adoption patterns instead of just hype around transformers.\n\nOne quick win could be linking your disease keywords to UMLS IDs so “diabetes” and “DM” roll up together; MetaMap or scispaCy can do it in a few lines and will cut down the noisy synonyms. For the GAN/Giant Axonal Neuropathy clash, add a regex on surrounding words (like “network” or “neuropathy”) and weight title vs. abstract differently-false positives drop fast when you do that. Exposing the cleaned dataset through a tiny REST endpoint would let folks pull numbers straight into R or Jupyter for meta-analysis. I did something similar with Dimensions’ data dump and Semantic Scholar’s API, and Mosaic was the only thing that let me sprinkle targeted ads on top when we opened the dashboard to the public, so monetization stays optional if you ever change your mind.\n\nReally cool to see hard numbers backing up the intuition that classical ML still rules the clinic.', 'Here is the link: [https://huggingface.co/spaces/lion-ai/MedicalAIWiki](https://huggingface.co/spaces/lion-ai/MedicalAIWiki)', 'This is really interesting project! Thank you for sharing. \n\nMay I ask, where do you obtain your dataset from? Is it through scraping', 'This is so cool, I am going to share it with my friends. <3', 'How did you crawl and preprocess the papers?', 'This is great work! How are you accessing pubmed articles? And are you just looking at abstracts, or full articles?', 'is it possible for applying your project to google scholar? \n\nI want to find some trend of applying ml/ai in the finance/insurance field.', 'Nice project, thanks for sharing.\nDo you use the pubmed api for retrieval or did you indexed it all by yourself? \n\nBTW the link to pubmed articles is broken.', 'Very curious on how you achieved the groupings. Have you got a list of pre-defined algorithms and searched for them and their synonyms? Or have you generated these via what topics are discussed in the paper?', 'How does the tool compare to Elicit?']","['Love that you’re surfacing adoption patterns instead of just hype around transformers.\n\nOne quick win could be linking your disease keywords to UMLS IDs so “diabetes” and “DM” roll up together; MetaMap or scispaCy can do it in a few lines and will cut down the noisy synonyms. For the GAN/Giant Axonal Neuropathy clash, add a regex on surrounding words (like “network” or “neuropathy”) and weight title vs. abstract differently-false positives drop fast when you do that. Exposing the cleaned dataset through a tiny REST endpoint would let folks pull numbers straight into R or Jupyter for meta-analysis. I did something similar with Dimensions’ data dump and Semantic Scholar’s API, and Mosaic was the only thing that let me sprinkle targeted ads on top when we opened the dashboard to the public, so monetization stays optional if you ever change your mind.\n\nReally cool to see hard numbers backing up the intuition that classical ML still rules the clinic.', 'Here is the link: [https://huggingface.co/spaces/lion-ai/MedicalAIWiki](https://huggingface.co/spaces/lion-ai/MedicalAIWiki)', 'This is really interesting project! Thank you for sharing. \n\nMay I ask, where do you obtain your dataset from? Is it through scraping', 'This is so cool, I am going to share it with my friends. <3', 'How did you crawl and preprocess the papers?']",111,22,0.94,Project,1751370063.0
1lotkac,MachineLearning,[D] Any path for a mid career/mid aged MLE to do ML research in the industry,"I've seen some flavor of questions here about whether they should do a PhD to join a research lab. I have a slightly different question. I did a non-CS PhD almost a decade ago, failed to get a faculty position after a bunch of postdocs and then meandered through FANG jobs, first in DS and then in MLE. I did some applied research in my last job, but more stats heavy than ML. But through a bunch of layoffs and restructuring, currently I am in a more traditional MLE role, think recommendation systems, A/B tests, move metrics...

But at my heart, I still want to do research. I've dabbled with writing a single author paper in on the top ML conferences in my own time, but its kinda hard, with job, family etc.. Even if I do manage to pull it off, will the one off Neurips paper (lets say) help me get an entry card to a more research-y ML job, like a Research Scientist/ Research Engineer in a ML lab? I am competing with ML PhDs with multiple papers, networks etc.

I also think that I don't have a lot of time, most of my friends have moved on to management after a decade of IC roles, and thats sort of the traditional path. But part of me is still holding on and wants to give it a shot and see if I can break into research this late, without an ML PhD. I know I will be much more fulfilled as a research scientist, compared to a regular SWE/M job,. I am currently trying to use my weekends and nights to write a single author paper to submit to one of the top conferences. Worst case I get rejected.

Some thoughts in my mind:
(1) I have also thought of writing workshop papers, which are easier to get accepted, but I doubt they have a similar value in the RS job market.
(2) Research Engineer will likely be easier than Research Scientist. But how should I strategize for this?

I'd be grateful if I get thoughts on how I should strategize a move. Feel free to also tell me its impossible, and I should cut my losses and move on.","['Since the profile is the top of the top it might not be a good role model but this guy did a PhD in math (seemingly not ML adjacent judging from publication record) and became a researcher at OpenAI. https://www.linkedin.com/in/shuchaobi\n\n It looks like he has been an engineer for long but in 2023 he started publishing out of nowhere, seemingly working with researchers in Google DM. \nSo i guess the game plan is to go to a company that does research and help researchers there?', ""You didn't mention two things of importance:\n\nAre you US based?\n\nDo you **really** want to do ML research, or are you secretly wishing to get FANG type position and earn 1M a year? \n\nThat would help answer the question. The two questions provide a 2 x 2 table. Starting with the the most critical scenario:\n\nYou're a US based and secretly wishing for a 1M salary while saying you just want to do ML research: If that's the scenario we're in, then that's going to be tough. Not impossible by any means. But tough. The trick would be to **publish** in a a conference such as ICML or Neurips, but in a topic of interest to your employer. Then, you'll have to maximize your chances by seeing what does your employer want. TMLR would be a fairly acceptable target.\n\nThe other other 3 options on the table: Go for non-Fang companies (Engineering, or Scientific labs). Why? Because they are **desperately** in need of people who **knows** ML. They're most likely behind because they don't have teams discussing the latest novelties and implementing the latest architecture during lunch. I work in a fairly reputed group in Germany and researchers are still using the simplest version of Unet for image classification. \n\nSo, there it is."", ""Join a company that does research.  Don't go get another PhD this late in your life. It won't pay off anymore.  You need to be making money for those life achievements like house+retirement. If you do a PhD, you will be back to making 0 money, and it could take another 5-7 years away...  \n\nDo you really want to re-enter the academic race that spit you out again?  There is no guarantee that even with a PhD, you could get a research lab position in ML.. The field is so crowded that you have to graduate from Ivy League + have top tier publications to even stand out anymore...  Say goodbye to your life if that is what you sign up for."", 'Good luck bro…', 'the boss move is to share results as tweets and make people cite those instead. way less work than writing a paper.', 'To the best of my knowledge, only a few companies in the US offer a true RS role — as of one that is purely or mainly publication-focused — for ML. They’re competitive to get into and a one-off pub likely won’t cut it unless it’s a\xa0*super*\xa0big shot.\n\nThe rest might grant you an RS or AS title, but you’re effectively doing a “research engineer” role. Not that this is bad at all — a lot of work is too muddy (or too costly) for publication but is ultimately important for delivering a good service. This line is probably pretty blurry with MLEs if you’re on a team working on more trendy topics.\n\nMy 0.02: it’s not worth sacrificing a lot for a one-off pub. Unless it’s a truly good one, it’s just a number. If you still need one (or a few) to open some doors, either via team change or job switch, you might want to team up with an academic lab to do it together so that 1) you don’t have to grind on every step of the submission, and they are super familiar with the process; and 2) you can discuss with folks who are presumably very up to date with the field. If you can sway some gift funding\xa0or compute resource via your job, this opportunity should be easy.', 'What about enrolling in a top tier grad school?', 'I would predict a recent neurips/iclr/icml/cvpr paper would likely at least get you a research internship somewhere. Maybe not a company that you like to name drop to your wife’s friends, but something. Then you’d get more publications and probably a full time position.\n\nThe framing of this just seems odd to me. You have a PhD. Just publish if this field is your dream. It’s always hard.', 'May be Look into a Startup, which has a strong Research topic. Let me explain, I Work for a Startup we Focus on medical summarization, most days its Software engineering but the core Idea IS deeply rooted in Research and still has Open questions. This has allowed me to come Up with a Task, Setup Experiments and now im writing a Papier for EMNLP. \nIts a Long process but it serves both, my desire for Research and actual programming interests.', 'The rewards go to those who think out of the box.  Traditional research and academic positions do not involve thinking out of the box.\n\nCarve your own path.']","['Since the profile is the top of the top it might not be a good role model but this guy did a PhD in math (seemingly not ML adjacent judging from publication record) and became a researcher at OpenAI. https://www.linkedin.com/in/shuchaobi\n\n It looks like he has been an engineer for long but in 2023 he started publishing out of nowhere, seemingly working with researchers in Google DM. \nSo i guess the game plan is to go to a company that does research and help researchers there?', ""You didn't mention two things of importance:\n\nAre you US based?\n\nDo you **really** want to do ML research, or are you secretly wishing to get FANG type position and earn 1M a year? \n\nThat would help answer the question. The two questions provide a 2 x 2 table. Starting with the the most critical scenario:\n\nYou're a US based and secretly wishing for a 1M salary while saying you just want to do ML research: If that's the scenario we're in, then that's going to be tough. Not impossible by any means. But tough. The trick would be to **publish** in a a conference such as ICML or Neurips, but in a topic of interest to your employer. Then, you'll have to maximize your chances by seeing what does your employer want. TMLR would be a fairly acceptable target.\n\nThe other other 3 options on the table: Go for non-Fang companies (Engineering, or Scientific labs). Why? Because they are **desperately** in need of people who **knows** ML. They're most likely behind because they don't have teams discussing the latest novelties and implementing the latest architecture during lunch. I work in a fairly reputed group in Germany and researchers are still using the simplest version of Unet for image classification. \n\nSo, there it is."", ""Join a company that does research.  Don't go get another PhD this late in your life. It won't pay off anymore.  You need to be making money for those life achievements like house+retirement. If you do a PhD, you will be back to making 0 money, and it could take another 5-7 years away...  \n\nDo you really want to re-enter the academic race that spit you out again?  There is no guarantee that even with a PhD, you could get a research lab position in ML.. The field is so crowded that you have to graduate from Ivy League + have top tier publications to even stand out anymore...  Say goodbye to your life if that is what you sign up for."", 'Good luck bro…', 'the boss move is to share results as tweets and make people cite those instead. way less work than writing a paper.']",42,15,0.92,Research,1751347501.0
1los6wj,MachineLearning,[R] Inference-Time Scaling and Collective Intelligence for Frontier AI,"TL;DR: our AB-MCTS lets multiple frontier models work together at inference time, outperforming each model running alone on the ARC-AGI-2 benchmark.

Our new inference-time scaling algorithm enables collective intelligence for AI by allowing multiple frontier models (like Gemini 2.5 Pro, o4-mini, DeepSeek-R1-0528) to cooperate.

Inspired by the power of human collective intelligence, where the greatest achievements arise from the collaboration of diverse minds, we believe the same principle applies to AI. Individual frontier models like ChatGPT, Gemini, and DeepSeek are remarkably advanced, each possessing unique strengths and biases stemming from their training, which we view as valuable resources for collective problem-solving.

AB-MCTS (Adaptive Branching Monte Carlo Tree Search) harnesses these individualities, allowing multiple models to cooperate and engage in effective trial-and-error, solving challenging problems for any single AI. Our initial results on the ARC-AGI-2 benchmark are promising, with AB-MCTS combining o4-mini + Gemini-2.5-Pro + R1-0528, current frontier AI models, significantly outperforming individual models by a substantial margin.

This research builds on our 2024 work on evolutionary model merging, shifting focus from “mixing to create” to “mixing to use” existing, powerful AIs. At Sakana AI, we remain committed to pioneering novel AI systems by applying nature-inspired principles such as evolution and collective intelligence. We believe this work represents a step toward a future where AI systems collaboratively tackle complex challenges, much like a team of human experts, unlocking new problem-solving capabilities and moving beyond single-model limitations.

Blog: https://sakana.ai/ab-mcts

Paper: https://arxiv.org/abs/2503.04412

Algorithm: https://github.com/SakanaAI/treequest

ARC-AGI Experiments: https://github.com/SakanaAI/ab-mcts-arc2

If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :)",[],[],19,0,0.92,Research,1751342705.0
1loqe5e,MachineLearning,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.","['Want to be Hired: Remote, Salary Expectation:$15/hr, Remote, Contract | Part Time, Resume:\xa0[https://drive.google.com/file/d/1cAX1kbTSOYgJz3nDGZ2wiQJDoHi4B-j1/view?usp=sharing](https://drive.google.com/file/d/1cAX1kbTSOYgJz3nDGZ2wiQJDoHi4B-j1/view?usp=sharing)\xa0, looking for GenAI, AI Agents, RAG, NLP, Data Extraction, Chatbot, AI Automation related work.', 'I want a Job asap please help me', 'Want to be Hired: Remote Salary Expectation: $200-$350 / month, Remote, Internship/Part Time  \nResume: [https://drive.google.com/file/d/1Cs7MYGWFFaeKXbYePmyXna3X18PqPpxF/view?usp=sharing](https://drive.google.com/file/d/1Cs7MYGWFFaeKXbYePmyXna3X18PqPpxF/view?usp=sharing)   \nand being interested in the mathematics of DL as well as the implementation, I would love to work in projects where I can implement papers and work on new models', 'Want to be Hired:\n\nLocation: Egypt\n\nSalary Expectation: Minimum of 5-10 USD per hour\n\nOpen to Remote and Relocation,\n\nOpen to Full Time, Contract, Part Time, and Freelancing\n\nResume:\xa0[mohamed.sourcing@gmail.com](mailto:mohamed.sourcing@gmail.com)\n\nPublished Project: Multidimensional neural networks as an alternative to the transformers and the attention mechanism\n\n[https://github.com/mohamed-services/mnn/blob/main/paper.md](https://github.com/mohamed-services/mnn/blob/main/paper.md)']","['Want to be Hired: Remote, Salary Expectation:$15/hr, Remote, Contract | Part Time, Resume:\xa0[https://drive.google.com/file/d/1cAX1kbTSOYgJz3nDGZ2wiQJDoHi4B-j1/view?usp=sharing](https://drive.google.com/file/d/1cAX1kbTSOYgJz3nDGZ2wiQJDoHi4B-j1/view?usp=sharing)\xa0, looking for GenAI, AI Agents, RAG, NLP, Data Extraction, Chatbot, AI Automation related work.', 'I want a Job asap please help me', 'Want to be Hired: Remote Salary Expectation: $200-$350 / month, Remote, Internship/Part Time  \nResume: [https://drive.google.com/file/d/1Cs7MYGWFFaeKXbYePmyXna3X18PqPpxF/view?usp=sharing](https://drive.google.com/file/d/1Cs7MYGWFFaeKXbYePmyXna3X18PqPpxF/view?usp=sharing)   \nand being interested in the mathematics of DL as well as the implementation, I would love to work in projects where I can implement papers and work on new models', 'Want to be Hired:\n\nLocation: Egypt\n\nSalary Expectation: Minimum of 5-10 USD per hour\n\nOpen to Remote and Relocation,\n\nOpen to Full Time, Contract, Part Time, and Freelancing\n\nResume:\xa0[mohamed.sourcing@gmail.com](mailto:mohamed.sourcing@gmail.com)\n\nPublished Project: Multidimensional neural networks as an alternative to the transformers and the attention mechanism\n\n[https://github.com/mohamed-services/mnn/blob/main/paper.md](https://github.com/mohamed-services/mnn/blob/main/paper.md)']",16,4,0.87,Discussion,1751337018.0
1loo8yl,MachineLearning,[D] How far are we from LLM pattern recognition being as good as designed ML models,"LLMs are getting better quickly. It seems like every time a new release comes out, they have moved faster than I anticipated.

Are they great at abstract code, integrating systems, etc? Not yet. But I do find that they are excellent at data processing tasks and machine learning code, especially for someone who knows and understands those concepts and is able to understand when the LLM has given a wrong or inefficient answer.

I think that one day, LLMs will be good enough to perform as well as a ML model that was designed using traditional processes. For example, I had to create a model that predicted call outcomes in a call center. It took me months to get the data exactly like I needed it from the system and identify the best transformation, combinations of features, and model architecture to optimize the performance.

I wonder how soon I'll be able to feed 50k records to an LLM, and tell it look at these records and teach yourself how to predict X. Then I'll give you 10k records and I want to see how accurate your predictions are and it will perform as well or better than the model I spent months working on.

Again I have no doubt that we'll get to this point some day, I'm just wondering if you all think that's gonna happen in 2 years or 20. Or 50? ","['I work at a large tech company. \n\nIn a way, we\'re already there and its already way superior to where you\'re hoping it will be. For years, we\'ve had large teams set up classifiers that take tons of training data and try to label ""this <X> happening in <this digital media>"". \n\nTurns out can just *ask* some of the frontier LLMs that exact question with no training data whatsoever and it out performs these classic ML classifiers we\'ve invested so much in. Completely changes the game, at least for that type of work. In that area, the workflow now is \n\n1.) Get a labelled data set of \\~1k samples   \n2.) Iterate on prompts for the LLM to classify the 1k samples until you get acceptable P/R  \n3.) If serving LLM as classifier is too expensive (i.e. need >10M classifications a day or something), ""distill"" the LLM by generating silver labels on \\~1M samples from LLM and train deep learning model on silver labels\n\nSo really you don\'t need training data anymore for **a lot** of traditional tasks, you just need evaluation data which is much smaller.', 'For a certain parameter count/computational level an ML model trained for a specific task will perform better than the LLM.  What you’re describing seems against the principle of the No Free Lunch Theorem.  But it’s perhaps possible a very large language model could replace the job of a data scientist and train another model on its own.', 'Wait, I’m confused, most LLM struggle with simple maths to the point that it’s more efficient to detect that a calculator is needed and then run a calculator subroutine.\n\nYou’re all claiming that one just feed them a matrix of 1000 instances of N features (numerical and categorical) and boom! it just works better than actually training a supervised ML model to do this specific task with millions of training instances?\n\nThat would be a very surprising result if it was true, mostly because LLM are not at all trained to perform similar tasks (as someone else mentioned, they would be good at generating the code to train a ML model)\n\nCan you provide research papers that have demonstrated this behavior?\n\nAlso, I don’t think training a ML model is complex at all. It’s basically just model.fit(X, y) and it will be good enough for most applications. The complexity is in preparing the data, building features and analyzing results.', 'Is tgere a way to explain why an LLM gave a particular answer or how it came to that answer?', ""on current architecture? never. For an LLM to perform that task it'd need to rewrite its model weights. Which as far as I'm aware that tech does not exist. \n\nIt's kinda like asking when fusion tech will be commercially viable. we have a rough idea of what it'd take. But havent demonstrated it and havent built it. there might be some as yet unforeseen obstacles blocking it as well. A total wild guess would be somewhere in the 10 year range. But this could change dramatically with new developments.""]","['I work at a large tech company. \n\nIn a way, we\'re already there and its already way superior to where you\'re hoping it will be. For years, we\'ve had large teams set up classifiers that take tons of training data and try to label ""this <X> happening in <this digital media>"". \n\nTurns out can just *ask* some of the frontier LLMs that exact question with no training data whatsoever and it out performs these classic ML classifiers we\'ve invested so much in. Completely changes the game, at least for that type of work. In that area, the workflow now is \n\n1.) Get a labelled data set of \\~1k samples   \n2.) Iterate on prompts for the LLM to classify the 1k samples until you get acceptable P/R  \n3.) If serving LLM as classifier is too expensive (i.e. need >10M classifications a day or something), ""distill"" the LLM by generating silver labels on \\~1M samples from LLM and train deep learning model on silver labels\n\nSo really you don\'t need training data anymore for **a lot** of traditional tasks, you just need evaluation data which is much smaller.', 'For a certain parameter count/computational level an ML model trained for a specific task will perform better than the LLM.  What you’re describing seems against the principle of the No Free Lunch Theorem.  But it’s perhaps possible a very large language model could replace the job of a data scientist and train another model on its own.', 'Wait, I’m confused, most LLM struggle with simple maths to the point that it’s more efficient to detect that a calculator is needed and then run a calculator subroutine.\n\nYou’re all claiming that one just feed them a matrix of 1000 instances of N features (numerical and categorical) and boom! it just works better than actually training a supervised ML model to do this specific task with millions of training instances?\n\nThat would be a very surprising result if it was true, mostly because LLM are not at all trained to perform similar tasks (as someone else mentioned, they would be good at generating the code to train a ML model)\n\nCan you provide research papers that have demonstrated this behavior?\n\nAlso, I don’t think training a ML model is complex at all. It’s basically just model.fit(X, y) and it will be good enough for most applications. The complexity is in preparing the data, building features and analyzing results.', 'Is tgere a way to explain why an LLM gave a particular answer or how it came to that answer?', ""on current architecture? never. For an LLM to perform that task it'd need to rewrite its model weights. Which as far as I'm aware that tech does not exist. \n\nIt's kinda like asking when fusion tech will be commercially viable. we have a rough idea of what it'd take. But havent demonstrated it and havent built it. there might be some as yet unforeseen obstacles blocking it as well. A total wild guess would be somewhere in the 10 year range. But this could change dramatically with new developments.""]",31,49,0.74,Discussion,1751330623.0
1lon6sx,MachineLearning,[P] I've built a spec for LLM-to-LLM comms by combining semantic patterns with structured syntax,"Firstly, total disclaimer. About 4 months ago, I knew very little about LLMs, so I am one of those people who went down the rabbit hole and started chatting with AI. But, I'm a chap who does a lot of pattern recognition in the way I work (I can write music for orchestras without reading it) so just sort of tugged on those pattern strings and I think I've found something that's pretty effective (well it has been for me anyway).

Long story short, I noticed that all LLMs seem to have their training data steeped in Greek Mythology. So I decided to see if you could use that shared knowledge as compression. Add into that syntax that all LLMs understand (:: for clear key-value assignments, → for causality and progression, etc) and I've combined these two layers to create a DSL that's more token-efficient but also richer and more logically sound.

This isn't a library you need to install; it's just a spec. Any LLM I've tested it on can understand it out of the box. I've documented everything (the full syntax, semantics, philosophy, and benchmarks) on GitHub.

I'm sharing this because I think it's a genuinely useful technique, and I'd love to get your feedback to help improve it. Or even someone tell me it already exists and I'll use the proper version!

Link to the repo: [https://github.com/elevanaltd/octave](https://github.com/elevanaltd/octave)",[],[],0,0,0.45,Project,1751327586.0
1lollc0,MachineLearning,[R] BIG-Bench Extra Hard,,[],[],10,0,0.92,Research,1751323299.0
1lolkda,MachineLearning,[R] Interpreting Large Language Models' Personality through Critical Event Analysis,"Excited to share our new work, ""Supernova Event Dataset: Interpreting Large Language Models' Personality through Critical Event Analysis"" accepted at the Actionable Interpretability Workshop @ ICML 2025.

Introducing the Supernova Event Dataset

We present a new benchmark built from real-world Wikipedia articles, including biographies, historical milestones, global news, and scientific discoveries (including articles from Google Deep Research). This dataset introduces a novel task: critical event analysis for interpreting the behavioral pattern, or “personality” of LLMs.

Rather than looking inside the model (activations, traces), we ask a separate LLM to judge what events are most critical, and use this external perspective to decode the model’s values and reasoning traits.

Some early insights:

Orca2 tends to prioritize emotional and interpersonal events.

Phi-4 and Qwen2.5 focus on strategic milestones.

In scientific discovery, o3 highlights causal breakthroughs, Gemini 2.5 Pro favors methodological innovations, and Claude Sonnet 3.7 emphasizes conceptual clarity.

While these are early findings (still without human evaluation), the diversity in critical event patterns is striking. We believe assigning LLMs ""personalities"" could make them more relatable and trustworthy, enabling smoother human-AI collaboration, especially in domains like scientific discovery.

Paper: [arxiv.org/abs/2506.12189](https://l.facebook.com/l.php?u=http%3A%2F%2Farxiv.org%2Fabs%2F2506.12189%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeYgHhUQQ5lNulNDLnSeulM64ECl3ls5tJCNkMC1_EPhqEHk1uqWYnEdBsu7g_aem_fW6BPSFPpkV4xELlQOiODA&h=AT0m3E04nqeA-MMtev7Ouz9OW3PeW5A_y6V9zj9dqy3WrwMXLOeTXVO9EzTXtMey6tWCwh1vUB5rS0lMPeoFEdjHj6BecO9zq9__xYIrQwGC6nhRT8BEE50RlAm9OuXWhk_HCtHu&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

Twitter: [https://x.com/Pranav\_AL/status/1939681069554655382](https://x.com/Pranav_AL/status/1939681069554655382)

Webpage: [http://supernova-event.ai](https://l.facebook.com/l.php?u=http%3A%2F%2Fsupernova-event.ai%2F%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeZQWpZz3uLtupNGIkmvzqpDbbG9u83w7Tv9ifY4Rjvct6zdjy8E4yg6NNDTM_aem_SgZlEI9ACWixaHK8TQWBPw&h=AT3tIyMocFJs9OJneCmpLWwlEjH3FE0RtckM5RYggzkKKuFMG5AIIScnpzGDhh7YqxBOSxpleqKt0hXYWIiiG_t3RoKtnoI1vlHkHUCsMhHlTmKcoQvqBSUnk8rLDci3doz0NpFV&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

Demo: [supernova-event.ai/#your-story](https://l.facebook.com/l.php?u=http%3A%2F%2Fsupernova-event.ai%2F%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeyxn4V9DBpYEvuLM2LwLuOOn-4Ewox9O267lf8zV1R8tbbLTm3vS1lw0zzqM_aem_9qDTC7HNLTFGRKq_iFqgBg%23your-story&h=AT1SxDBmgr_G-a-c-D9g4JyMlgs5bD-liDDwNRTZlIP_5CBvyl8meA3pwaNKABRXLqvHcemnEJv-sWcf3oan2b3FSWZ7H_yD3Y8mdhI5Ze6mVNULKBXQTdFqgrLNUynlNQjPjP2_&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

Code: [https://github.com/pranavAL/Supernova-Event-Dataset](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2FpranavAL%2FSupernova-Event-Dataset%3Ffbclid%3DIwZXh0bgNhZW0CMTAAYnJpZBExaFJHbUZIbDN4cEJKZ096SAEeyxn4V9DBpYEvuLM2LwLuOOn-4Ewox9O267lf8zV1R8tbbLTm3vS1lw0zzqM_aem_9qDTC7HNLTFGRKq_iFqgBg&h=AT3sNwJZLhvA9OG4GbkPvPxvAXxtZr9drQAj1Rp-4MCOHOOVbjH1epznhz08JAKypffQNwntIbz6TWzMTDmKVgXvDw7y6Yrg6Bcijqgxco34C_R4ivMwgS83oW5i2QnMFFQmQuVt&__tn__=-UK-R&c[0]=AT1lFRYu12xNSpO0-IlQ0tpDLaqpnTlHs0ipZFi4QqbVd_3LDc0Vnyr8uzMo6U8UhtdIXR8G5rKpdZaAbCtojueyXuX18q9jlCTh0kB1YA49AGDcMIGSWkV-pn1HLQWJK6QSFLLZ3aNz9jS3Cq1Q_927wYGbHSSdspcG77eNZ7dlk1qKhBee)

We're working toward scaling this into a real-world product, and we're currently seeking the right resources and support to take it further. If you're interested in what we're building and see potential for impact, we’d love to hear from you. Reach us at [hello@supernova-event.ai](mailto:hello@supernova-event.ai) ; we're open to conversations, collaborations, and any form of support that can help push this idea forward.

https://preview.redd.it/uugbpxw075af1.png?width=1200&format=png&auto=webp&s=ccbde6f1ace6140ff2ca838ffb0e60522759dc70",[],[],2,0,0.75,Research,1751323230.0
1loi25g,MachineLearning,[D] Is this PhD in LLM editing a good idea?,"Hello everyone, this is my first time posting here, and I wanted to get some opinions on the phd position I applied to.

So I am studying ml in France and I have a chance to do a PhD in the topic of LLM knowledge locating and editing. One paper that talks about this is the ROME (Rank One Model Editting - [https://arxiv.org/abs/2202.05262](https://arxiv.org/abs/2202.05262))

Basically, I would work on the internals of LLMs, analysing where exactly the knowledge for a certain fact is stored, and how can it be edited out. So messing around the directly with the components such as the attention and MLP weights.

For me personally, I like the idea of going inside the LLMs, instead of just inferencing/training and using them as some black boxes.

And I suppose that this would qualify me for jobs of actually creating LLMs (I do not expect to end up in OpenAI) but also make me more qualified for standard LLM usage jobs.

Any opinion or comment would be appriciated!","['sounds like a good idea, especially because LLM is super relevant in 2025. For job prep, especially if you want to edit LLMs, try checking company-specific interview questions on prepare.sh, super helpful for ML roles. Im a contributor at that platform, but Ive been using it way before that for interview prep and i can recommend it.', 'In my opinion, no field is completely solved, and one can always find new ways to reconfigure or solve a problem. For example, there might be models that are 99% accurate but struggle with inference-one can then focus on improving their speed.\n\nA PhD is training in itself, with the goal of equipping you with the skill set to formulate hypotheses, design rigorous, valid experiments, analyze findings, and, lastly, share them with the community. \n\nThe field you described, knowledge editing, is extremely important, since it tackles two of the most critical aspects of LLMs: hallucination and privacy. Recently, I was listening to a talk about locating Social Security Numbers (SSNs) in LLMs (https://arxiv.org/pdf/2406.09325) and how to remove them so that the model does not leak private information. From my understanding, current methods are rather slow (you need to manually check whether a number has been memorized) or lead to catastrophic forgetting (by removing sensitive information, you impair the LLM’s overall performance). So yeah, knowledge editing is not even nearly solved, I hope you will pursue this field : )']","['sounds like a good idea, especially because LLM is super relevant in 2025. For job prep, especially if you want to edit LLMs, try checking company-specific interview questions on prepare.sh, super helpful for ML roles. Im a contributor at that platform, but Ive been using it way before that for interview prep and i can recommend it.', 'In my opinion, no field is completely solved, and one can always find new ways to reconfigure or solve a problem. For example, there might be models that are 99% accurate but struggle with inference-one can then focus on improving their speed.\n\nA PhD is training in itself, with the goal of equipping you with the skill set to formulate hypotheses, design rigorous, valid experiments, analyze findings, and, lastly, share them with the community. \n\nThe field you described, knowledge editing, is extremely important, since it tackles two of the most critical aspects of LLMs: hallucination and privacy. Recently, I was listening to a talk about locating Social Security Numbers (SSNs) in LLMs (https://arxiv.org/pdf/2406.09325) and how to remove them so that the model does not leak private information. From my understanding, current methods are rather slow (you need to manually check whether a number has been memorized) or lead to catastrophic forgetting (by removing sensitive information, you impair the LLM’s overall performance). So yeah, knowledge editing is not even nearly solved, I hope you will pursue this field : )']",0,2,0.23,Discussion,1751314720.0
1lohh1u,MachineLearning,[D] Should we petition for requiring reviewers to state conditions for improving scores?,"I’ve been thinking about how opaque and inconsistent peer reviews can be, especially in top ML conferences. What if we made it a requirement for reviewers to explicitly state the conditions under which they would raise their scores? For example, “If the authors add experiments on XYZ” or “If the theoretical claim is proven under ABC setup.”

Then, area chairs (ACs) could judge whether those conditions were reasonably met in the rebuttal and updated submission, rather than leaving it entirely to the whims of reviewers who may not revisit the paper properly.

Honestly, I suspect many reviewers don’t even know what exactly would change their mind.

As an added bonus, ACs could also provide a first-pass summary of the reviews and state what conditions they themselves would consider sufficient for recommending acceptance.

What do you think? Could this improve transparency and accountability in the review process?","['If you read the reviewers guide, there is already a guidance that the ""questions"" the reviewer makes should focus on things that would affect the score. So, normally, the questions made are the main things that might affect the scores.\n\n\nWhat most authors do not want to hear tho is that in many cases there is nothing that can be done to improve the grade. If I read a paper in my narrow research area there is minimal chance of misunderstanding of a core part of the contribution, and I am not rejecting a paper on grounds of things that can be easily and quickly done in first place.\n\n\n\xa0So, in those cases, I often write (no questions that could affect my evaluation), and it often leads to the authors trying to convince me otherwise and some times even complaining to the AC. Sometimes the only way you are getting a good score is working on improving the paper and later submitting to another conference. You would be surprised of how many times I have had to review the same paper sequentially being submitted to ICLR, ICML, NeurIPS, and AAMAS and the authors didn\'t change anything I suggested that would require new experiments or significant rewriting, so the paper keeps being rejected', 'At massive conferences with <20% acceptance rates, it’s just not possible to give authors a prescriptive route to acceptance. All it’s gonna do is lead to more complaining when authors believe they’ve met the reviewers’ criteria but still don’t get accepted. At the end of the day, we all need to accept that top conferences are *meant* to be hard to get into and not every idea is worthy of publication at NeurIPS/ICML/ICLR.', 'Nice try, reward-hacking RL algorithm', '> ""Find a more novel research topic and write a new paper about that.""', 'That is how peer review process traditionally worked and still works in normal fields. But ML community has optimized itself into a degenerated overfitted state - we run these funky lotteries where the entire field sends its papers and at the end 20% people get a reward for their CVs. The top researchers have strategies that are compatible with this model -- they produce a lot of papers with as many PhD/masters students as they can get, and they try to do as many cross-institution collaborations as possible. Both strategies result in more lottery tickets.', ""It won't help anything. Just improve your paper and move on. It's a rite of passage."", 'It is highly likely that nothing you do could improve your grade at that point. This is mainly because any larger change requires the work to undergo a full review again. In most cases the reivewer does not have access to the fully updated work, nor has the time to review several changed papers in just a few days \\[\\*\\]. I think the only work that can profit from the question format is theoretical work, e..g, proving an intermediate step in more detail to show that it is valid.\n\nI also do not think that reviews at top ML conferences are inconsistent, nor opaque, compared to journals or even other fields. Already the chance of heaving a discussion without a full resubmission & review is pretty unique to ML.\n\n\\[\\*\\] This would in my book also increase the effort for reviewing so much that reviewers who have not submitted a paper must be paid for the amount of work they are doing.', 'We should reveal the identity of everyone (including authors, reviewers, and ACs) after releasing the decision. It allows the author to ask the reviewer face-to-face.']","['If you read the reviewers guide, there is already a guidance that the ""questions"" the reviewer makes should focus on things that would affect the score. So, normally, the questions made are the main things that might affect the scores.\n\n\nWhat most authors do not want to hear tho is that in many cases there is nothing that can be done to improve the grade. If I read a paper in my narrow research area there is minimal chance of misunderstanding of a core part of the contribution, and I am not rejecting a paper on grounds of things that can be easily and quickly done in first place.\n\n\n\xa0So, in those cases, I often write (no questions that could affect my evaluation), and it often leads to the authors trying to convince me otherwise and some times even complaining to the AC. Sometimes the only way you are getting a good score is working on improving the paper and later submitting to another conference. You would be surprised of how many times I have had to review the same paper sequentially being submitted to ICLR, ICML, NeurIPS, and AAMAS and the authors didn\'t change anything I suggested that would require new experiments or significant rewriting, so the paper keeps being rejected', 'At massive conferences with <20% acceptance rates, it’s just not possible to give authors a prescriptive route to acceptance. All it’s gonna do is lead to more complaining when authors believe they’ve met the reviewers’ criteria but still don’t get accepted. At the end of the day, we all need to accept that top conferences are *meant* to be hard to get into and not every idea is worthy of publication at NeurIPS/ICML/ICLR.', 'Nice try, reward-hacking RL algorithm', '> ""Find a more novel research topic and write a new paper about that.""', 'That is how peer review process traditionally worked and still works in normal fields. But ML community has optimized itself into a degenerated overfitted state - we run these funky lotteries where the entire field sends its papers and at the end 20% people get a reward for their CVs. The top researchers have strategies that are compatible with this model -- they produce a lot of papers with as many PhD/masters students as they can get, and they try to do as many cross-institution collaborations as possible. Both strategies result in more lottery tickets.']",11,15,0.62,Discussion,1751313372.0
1logp0w,MachineLearning,[D] Looking for a web annotation tool (with Chrome extension) for labeling live websites,"I'm building a dataset for a knowledge extraction model and need to label structured data from thousands of live websites. Ideally, I'm looking for a tool that:

\- Provides a Chrome extension to label live HTML elements on real websites

\- Can open sites one by one in the browser from a task queue

\- Saves each annotation along with a snapshot or DOM state of the page

\- Supports exporting annotations for later review with screenshots

I’m considering building a custom tool for this, but would prefer to avoid that since it would distract from the core research. Does anyone know an existing tool that supports doing what Im doing?",[],[],1,0,1.0,Research,1751311513.0
1lo9xuu,MachineLearning,[R] Has anyone actually gone through an AI readiness assessment with a vendor or consultant? Worth it or just more buzzwords?,"I'm kind of wondering about these AI readiness assessments everyone's talking about. Like, you see vendors and consultants pushing them, and honestly, I'm a bit skeptical. I can't help but feel it might just be a lot of buzzwords without real substance.


Has anyone actually gone through one of these with a third party, maybe a consultant or a specific vendor, was it actually worth the time and money you put into it and did you get genuinely practical insights that helped your business move forward, or was it just a fancy report that basically says 'you need more AI' without telling you how?

I'm really curious to hear real experiences here, good or bad, before potentially diving into something that might just be another passing trend in the tech world. What did you learn, and what was the actual outcome?","['This subreddit is for research-tier machine learning discussions can we not flood it with low-tier non technical business nonsense.', 'We are a couple of ML engineers running a small studio, would love to hear what others experience is as well.. \n\nMy experience is that many companies start with ""we must have AI"" instead of solving a real problem.. and end up spending time and money on things that does not make sense..', 'At a high level - this sounds a bit scammy. A lot of AI marketing is built around fear and misunderstanding - the impending doom of your competitors implementing ""AI"" and you not doing so, which when combined with how opaque AI systems are to people unfamiliar with the concepts can be an absolute home run on how to sell people solutions they don\'t need.\n\nRather than pay somebody to sell you a project, I\'d go shopping for a solution for your biggest time sink. In many cases, business problems aren\'t unique enough to warrant a highly customized project, and many companies already exist with affordable solutions for the general problems many people face (document parsing/understanding, media data processing, etc.).\n\nBefore diving into anything, have an idea on how much its worth to you monetarily before starting - will save you some stress down the line.\n\nYou can get really far on a lot of business problems with an LLM pipeline, doesn\'t take an expensive assessment to figure that out.']","['This subreddit is for research-tier machine learning discussions can we not flood it with low-tier non technical business nonsense.', 'We are a couple of ML engineers running a small studio, would love to hear what others experience is as well.. \n\nMy experience is that many companies start with ""we must have AI"" instead of solving a real problem.. and end up spending time and money on things that does not make sense..', 'At a high level - this sounds a bit scammy. A lot of AI marketing is built around fear and misunderstanding - the impending doom of your competitors implementing ""AI"" and you not doing so, which when combined with how opaque AI systems are to people unfamiliar with the concepts can be an absolute home run on how to sell people solutions they don\'t need.\n\nRather than pay somebody to sell you a project, I\'d go shopping for a solution for your biggest time sink. In many cases, business problems aren\'t unique enough to warrant a highly customized project, and many companies already exist with affordable solutions for the general problems many people face (document parsing/understanding, media data processing, etc.).\n\nBefore diving into anything, have an idea on how much its worth to you monetarily before starting - will save you some stress down the line.\n\nYou can get really far on a lot of business problems with an LLM pipeline, doesn\'t take an expensive assessment to figure that out.']",0,7,0.2,Research,1751296045.0
1lnq3zt,MachineLearning,[P] Chromatic Language Models (CLM): A Paradigm for Native Visual Communication in Artificial Intelligence,"**Abstract**

[**https://zenodo.org/records/15769766**](https://zenodo.org/records/15769766)

Modern AI models, in particular Large Language Models (LLMs) and Computer Vision models, operate in fundamentally distinct data domains: text and pixels. The interaction between these models requires expensive and complex translation and embedding processes. This work introduces a new paradigm,  **Chromatic Language Models (CLMs)** , designed to eliminate this discontinuity. Building on the principles of visual semantic coding established in  **Usai ColorZip**  (Usai, 2025a) and validated by the  **Usai ChromoChess** application  (Usai, 2025b), CLMs are language models that operate natively on a chromatic domain. We propose an encoder-decoder architecture in which an AI agent learns to ""read"" and ""write"" complex information directly as images, treating pixels as semantic tokens. This approach not only unifies language and vision, but creates an intrinsically compressed, secure, and efficient form of AI-native communication, paving the way for a new generation of multimodal intelligent agents.

**1. Introduction**

The evolution of artificial intelligence is characterized by increasing specialization. On the one hand, Large Language Models (LLMs) have demonstrated an unprecedented ability to understand and generate human language. On the other hand, computer vision models, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), excel at interpreting visual data. However, a fundamental ""modal gap"" separates these two worlds. An LLM does not ""see"" images and a ViT does not ""read"" text; both rely on intermediate embedding layers to translate information from one domain to the other.

This paper addresses a radical question: what if we could close this gap by transforming language itself into a natively visual format? Instead of teaching a model to translate between text and pixels, could we create a model that ""thinks"" directly in pixels?

We propose the architecture of  **Chromatic Language Models (CLM)** , intelligent agents that use a chromatic representation of language for each stage of their cognitive process: input, reasoning, and output. This proposal builds directly on the technological and conceptual foundations of our previous work, which demonstrated the feasibility of such a representation.

**2. Fundamental Works and Context**

Our proposal is not born in a vacuum, but is the natural evolution of two previous researches that established the feasibility of visual semantic coding.

**2.1. Usai ColorZip: Semantic Text Encoding**
In our work ""Usai ColorZip: A Hybrid System for Semantic Text Encoding and Compression via HTML Colors"" (Usai, 2025a), we introduced a lossless system for mapping lexical units (words) to unique color codes. We demonstrated that this transformation is not only an act of encoding, but also an effective data compression mechanism when combined with lossless image formats such as PNG. The key to the system is its hybrid architecture, capable of handling both a large dictionary of known words and any unknown word via a color escape protocol.  **Usai ColorZip created the ""vocabulary"" and ""syntax"" of this new language.**

**2.2. Usai ChromoChess: Proof of Concept in a Complex Domain**
Later, in ""Usai ChromoChess: Visual Representation and Compression of Chess Games"" (Usai, 2025b), we applied this philosophy to a formal and complex domain. By transforming chess games from PGN notation to 8x8 pixel movies, we demonstrated that a sequence of logical states can be represented as a visual data stream, compact and ideal for analysis by vision models.  **Usai ChromoChess provided proof that entire logical-temporal processes can be efficiently encoded in this chromatic language.**

These two works constitute the necessary prerequisite for the next step: no longer just encoding and decoding data, but creating an intelligence that  *uses*  this language as its primary means of communication and reasoning.

**3. Architecture of the Chromatic Language Model (CLM)**

A CLM is an AI model designed for an end-to-end communication cycle in the color domain. Its architecture is based on an encoder-decoder model.

**3.1. The Principle: Visual Tokenization**
The fundamental unit of a CLM is not a word or subword, but a  **colored pixel** . Each color, defined in the ColorZip dictionary, is a discrete semantic token. An input ""text"" (e.g. a question) is provided to the model as a ColorZip image (a tensor \[H x W x C\], where H, W are the dimensions and C is the RGB representation of the color).

**3.2. The Encoder: The Chromatic Reader**
The encoder has the task of ""reading"" the input image and understanding its meaning. An ideal architecture for this purpose is a  **Vision Transformer (ViT)** .

1. The ColorZip image is divided into a grid of patches (which can correspond to single pixels/words or small groups).
2. These patches are projected into a vector space and processed through self-attention mechanisms.
3. The encoder's output is a context vector (or sequence of vectors), an abstract, latent mathematical representation of the semantic meaning of the input image.

**\[Figure 1: Encoder-Decoder architecture of a CLM. The Encoder (ViT) processes the input image. Its semantic output conditions the Decoder (Transformer), which generates a new image pixel by pixel (color by color).\]**

**3.3. The Decoder: The Color Writer**
The decoder has the task of taking the context vector and generating a response, also in the form of a ColorZip image.

1. A standard Transformer architecture is used as the decoder.
2. The process is autoregressive: the model generates one pixel (color) at a time.
3. The crucial difference lies in its output layer: instead of softmaxing a vocabulary of tens of thousands of words, CLM softmaxes  **the color dictionary** . The model predicts the most likely color for the next pixel, given its understanding of the query and the colors generated so far.
4. The process ends when the model generates the special color EOT\_COLOR defined in Usai ColorZip.

**4. Implications: Towards AI-Native Communication**

The adoption of CLMs does not represent an incremental improvement, but a paradigm shift with profound implications.

* **Computational Efficiency:**  The overhead of constant conversion between text and numeric representations is eliminated. AI operates on a data format that is closer to its mathematical nature.
* **Secure and Compressed Communication:**  Conversations between CLM agents would be opaque images to an unauthorized observer (without the dictionary) and, as demonstrated by Usai ColorZip, highly compressed. This is ideal for low-bandwidth or stealth communications.
* **True Multimodality:**  A CLM that ""speaks"" the language of pixels is intrinsically closer to understanding real images. The boundary between language and vision becomes blurry, facilitating the creation of truly multimodal models capable of reasoning fluidly about text and images without internal barriers.
* **New Application Scenarios:**  Possibilities open up for AI agents that communicate steganographically through image sharing platforms, or for the development of specialized hardware (color processors) optimized for these data flows.

**5. Challenges and Future Work**

The road to fully functional CLMs presents several challenges: creating large-scale training datasets (text corpora parallel to their ColorZip representations), analyzing their computational costs compared to traditional LLMs, and exploring the interpretability of these models. Future work will focus on developing a prototype CLM and training it on a medium-sized corpus to empirically validate its ability to ""converse"" chromatically.

**6. Conclusion**

This paper introduced Chromatic Language Models (CLMs), a new type of intelligent agent that reads, reasons, and writes directly in a color-based visual language. Building on the solid foundation of  **Usai ColorZip** semantic coding  and the application validation of  **Usai ChromoChess** , we outlined a viable architecture that unifies the domains of language and vision. CLMs are not simply a new model, but a proposal for a  **new form of AI-native communication** : a language for machines, spoken by machines.

**7. References**

* Usai, L. (2025a).  *Usai ColorZip: A Hybrid System for Semantic Text Encoding and Compression via HTML Colors* . Zenodo.  [https://doi.org/10.5281/zenodo.15701109](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.15701109)
* Usai, L. (2025b).  *Usai ChromoChess: Visual Representation and Compression of Chess Games via Temporal Encoding Usai ColorZip* . Zenodo.  [https://doi.org/10.5281/zenodo.15701822](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdoi.org%2F10.5281%2Fzenodo.15701822)",[],[],0,0,0.33,Research,1751232450.0
1lnzka6,MachineLearning,[D]Designing Neural Networks for Time-Dependent Tasks: Is it common to separate Static Feature Extraction and Dynamic Feature Capture?,"Hi everyone,

I'm working on neural network training, especially for tasks that involve time-series data or time-dependent phenomena. I'm trying to understand the common design patterns for such networks.

My current understanding is that for time-dependent tasks, a neural network architecture might often be divided into two main parts:

1. **Static Feature Extraction:** This part focuses on learning features from individual time steps (or samples) independently. Architectures like CNNs (Convolutional Neural Networks) or MLPs (Multi-Layer Perceptrons) could be used here to extract high-level semantic information from each individual snapshot of data.
2. **Dynamic Feature Capture:** This part then processes the sequence of these extracted static features to understand their temporal evolution. Models such as Transformers or LSTMs (Long Short-Term Memory networks) would be suitable for learning these temporal dependencies.

My rationale for this two-part approach is that it could offer better interpretability for problem analysis in the future. By separating these concerns, I believe it would be easier to use visualization techniques (like PCA, t-SNE, UMAP for the static features) or post-hoc explainability tools to determine if the issue lies in: \* the *identification of features* at each time step (static part), or \* the *understanding of how these features evolve over time* (dynamic part).

Given this perspective, I'm curious to hear from the community: **Is it generally recommended to adopt such a modular architecture for training neural networks on tasks with high time-dependency? What are your thoughts, experiences, or alternative approaches?**

Any insights or discussion would be greatly appreciated!","['Your differentiation between what you call ""Static"" vs ""Dynamic"" feature capture is strange, if not wrong.\n\n1. Transformers are just MLPs with self-attention and other bells and whistles. To have them in a different category as MLP is strange.\n\n2. All four networks are dependent on time. All four networks process features to ""understand their temporal evolution."" If you mix up the time steps, all four will break. You would need something like bag of words to have a model that doesn\'t consider their temporal evolution.\n\n3. MLPs, LSTMs, and element wise transformers focus on ""learning features from individual time steps"", but of course none learn them independently. CNNs and patch based transformers are the ones that don\'t learn from individual time steps. So, your split is also strange here.\n\nMLPs learn from individual time steps and the temporal structures are maintained to the first layer. After the first layer, the structure is lost, but that doesn\'t mean that MLPs aren\'t dependent on time.\n\nCNNs learn from groups of time steps (windows) and the temporal structures are maintained through all of the convolutional layers. \n\nTransformers are MLPs but it keeps the temporal relationships through the layers directly with skip connections and indirectly with position encodings.\n\nLSTMs are the odd ones out. Instead of considering the whole time series at once like all of the previous networks, it keeps a running state (memory) and updates the state one time step at a time.']","['Your differentiation between what you call ""Static"" vs ""Dynamic"" feature capture is strange, if not wrong.\n\n1. Transformers are just MLPs with self-attention and other bells and whistles. To have them in a different category as MLP is strange.\n\n2. All four networks are dependent on time. All four networks process features to ""understand their temporal evolution."" If you mix up the time steps, all four will break. You would need something like bag of words to have a model that doesn\'t consider their temporal evolution.\n\n3. MLPs, LSTMs, and element wise transformers focus on ""learning features from individual time steps"", but of course none learn them independently. CNNs and patch based transformers are the ones that don\'t learn from individual time steps. So, your split is also strange here.\n\nMLPs learn from individual time steps and the temporal structures are maintained to the first layer. After the first layer, the structure is lost, but that doesn\'t mean that MLPs aren\'t dependent on time.\n\nCNNs learn from groups of time steps (windows) and the temporal structures are maintained through all of the convolutional layers. \n\nTransformers are MLPs but it keeps the temporal relationships through the layers directly with skip connections and indirectly with position encodings.\n\nLSTMs are the odd ones out. Instead of considering the whole time series at once like all of the previous networks, it keeps a running state (memory) and updates the state one time step at a time.']",3,10,1.0,Discussion,1751261468.0
1lo2x9f,MachineLearning,[D] What post-processing tools work well with Tesseract for financial documents?,"Hi all,

I’m using Tesseract OCR to extract text from scanned financial documents like payslips and tax returns. The raw output is messy, and I need to clean it up and pull key fields like YTD income, net pay, and tables.

What post-processing tools or Python libraries can help:

* Extract key-value fields
* Parse tables
* Match labels to values
* Clean and structure OCR output

Prefer offline tools (for privacy), but open to anything that works well.","[""Correct me if I'm wrong but isn't Tesseract very dated?"", 'you can try any open source vlm to do all the required tasks if you have the local compute. or you can try some affordable APIs that do the work for you like [https://parseextract.com](https://parseextract.com) . Use the extract structured data option to extract all fields you need or use the parse pdf option to parse/ocr all the content from your scanned document. They will also provide custom solutions if you need one.']","[""Correct me if I'm wrong but isn't Tesseract very dated?"", 'you can try any open source vlm to do all the required tasks if you have the local compute. or you can try some affordable APIs that do the work for you like [https://parseextract.com](https://parseextract.com) . Use the extract structured data option to extract all fields you need or use the parse pdf option to parse/ocr all the content from your scanned document. They will also provide custom solutions if you need one.']",0,2,0.5,Discussion,1751274923.0
1lo06qh,MachineLearning,[D] Proper way to calculate inference time,"Hi all,
Can anyone tell me how I should calculate inference time (case/sec) for medical images? SegMamba paper reports inference time as case/sec.
I have 2 queries in this case.
First, should inference time (case/sec) include the time of every operation after model predictions?
Secondly, because of sliding window inference, it is highly likely that the inference time for each case might be higher. What is the right way?",[],[],0,0,0.4,Research,1751263855.0
1lnzffu,MachineLearning,[D] Has anyone ever gained unrestricted access to an LLM for the purposes of research?,"I have attempted several rounds of research with LLMs that are available to the public (Grok, ChatGPT, and Copilot).   (an experiment involving 20-questions capability, and several experiments where the models talk back and forth to each other).   It has become clear that the public web portals are useless for this type of experiment.   The public-facing models are heavily tuned to be helpful assistants that create lists and formatted sections with headers.

How would someone go about getting access to a raw model for use in a university ?","['Base models (without instruct-tuning to behave as a chatbot) are available for most of the open-source model families like LLaMA.\xa0\n\n/r/localllama', ""it SOUNDS Like you want to talk to models that dont have a system prompt. use MSTY or any other chat app that connects via API. \n\n\\> The public-facing models are heavily tuned to be helpful assistants that create lists and formatted sections with headers.\n\nthis is just how it is. thats how the models are trained to be. there's no 'unrestricted access' that makes this part go away. You can prompt them NOT to do this, prompt them to have a more conversational tone and style, SOTA llms respond quite well to this.\n\n  \nYou can also download free and open source models for total access, ie deepseek which goes up to 405b versions publically released."", 'just use the API interface instead of web portal\n\nTake the time to properly learn how each of the platforms work, your post reads like you did not even read the documentation of the tools you are experimenting with', 'You want the playground: [https://platform.openai.com/playground](https://platform.openai.com/playground)']","['Base models (without instruct-tuning to behave as a chatbot) are available for most of the open-source model families like LLaMA.\xa0\n\n/r/localllama', ""it SOUNDS Like you want to talk to models that dont have a system prompt. use MSTY or any other chat app that connects via API. \n\n\\> The public-facing models are heavily tuned to be helpful assistants that create lists and formatted sections with headers.\n\nthis is just how it is. thats how the models are trained to be. there's no 'unrestricted access' that makes this part go away. You can prompt them NOT to do this, prompt them to have a more conversational tone and style, SOTA llms respond quite well to this.\n\n  \nYou can also download free and open source models for total access, ie deepseek which goes up to 405b versions publically released."", 'just use the API interface instead of web portal\n\nTake the time to properly learn how each of the platforms work, your post reads like you did not even read the documentation of the tools you are experimenting with', 'You want the playground: [https://platform.openai.com/playground](https://platform.openai.com/playground)']",0,6,0.25,Discussion,1751260963.0
1lnvjin,MachineLearning,[R] Free access to an H100. What can I build?,"My company is experimenting with new hardware and long story short, there's an idling H100 with a 2TB RAM and 27TB of storage and I'm allowed to play with it!

I really want to do some cool AI research to publish at a decent conference but I'm not well caught up with the research frontier and I could really use some help (and collaborators?).

I understand neural networks, CNNs, transformer models etc. to a reasonable depth but understanding what SOTA is will probably take more time than how long I have access to the GPU","[""Idk, what are you interested in? You'll do more on a project that interests you than a random hot topic.\xa0"", 'I’d fine tune a model', 'What is your company doing?\nFinding a use case for your company and experimenting on that would be the best direction; you would become the expert/reference in your company for your chosen topic, resources used, showing that you have a high impact in the company...', 'If your company has very niche data, you can easily develop a dataset. Once that is ready, you can use this machine to finetune opensource models on that data and check if there are reasonable results. If so, you can take it up as a project to create an internally finetuned model(vision, text, audio etc). This also helps in increasing your skillset as well.', 'Spin up a few open LLMs (Mistral, Phi-3, etc.) and compare snapshot-based orchestration runtimes like InferX with traditional serving. Cold starts, model swapping, GPU utilization . you’d be surprised how much infra innovation is still wide open even with an H100.', 'Could you train a generic hifigan for music Upscaling?', 'Spin up a quantized version of deepseek r1 and see if you can run some private company data through it', 'try learning pre-training', 'If I were in your position, I would write a more coherent latent code generator for RAVE trained on Rapunzel ASMR videos.', 'Where can I find this...']","[""Idk, what are you interested in? You'll do more on a project that interests you than a random hot topic.\xa0"", 'I’d fine tune a model', 'What is your company doing?\nFinding a use case for your company and experimenting on that would be the best direction; you would become the expert/reference in your company for your chosen topic, resources used, showing that you have a high impact in the company...', 'If your company has very niche data, you can easily develop a dataset. Once that is ready, you can use this machine to finetune opensource models on that data and check if there are reasonable results. If so, you can take it up as a project to create an internally finetuned model(vision, text, audio etc). This also helps in increasing your skillset as well.', 'Spin up a few open LLMs (Mistral, Phi-3, etc.) and compare snapshot-based orchestration runtimes like InferX with traditional serving. Cold starts, model swapping, GPU utilization . you’d be surprised how much infra innovation is still wide open even with an H100.']",37,22,0.81,Research,1751247987.0
1lnsph5,MachineLearning,[D] How should I respond to reviewers when my model is worse than much larger models?,"I got a review asking to compare my submission paper with more recent models. The models were not even out 3 months before the submission so by ACL rules I should not have to compare them with my model because it is contemporary.

Nevertheless I have ran comparisons and my model is much much worse... Why? I'm using a model doing the same thing but 32x smaller, used almost 1/10 of the data they used, etc... I am severely resource constrained and cannot compete in terms of scale, but I still think that my paper makes an important contribution that if we were to match the other models scale we would get better results.

What should I do? Should I report results that show other models are better and risk the reviewers lower their scores? I kinda just want to explain the authors that the scale is completely different and other factors make it a very unfair comparison, but they might just not care...

I have a 2.5 average score and really wanted to try to raise it to make it at least into findings, but I honestly don't know how to defend against not having as many resources as top labs/unis...","['“There are no available models in family X within an order of magnitude of the parameter count in our experiments. Future work will compare scaling laws to determine if our approach remains competitive at similar compute complexity.”', ""I think you stated the answer very clearly, you shouldn't have to compare your model to any models with 10x or more parameter size. If you want to address the larger models, you can probably compare your models with some adjustment for chinchilla scaling laws."", 'Can you reframe your paper as focusing on achieving good results with small models and limited data? Then you can explain why your work is still relevant.', 'Why is the comparison between your model and ones that are 32x in size?  \nAre you comparing against models that around the parameter size of your model in the paper ?', ""Out of curiosity, what kind of datasets are you working on here? How much compute did you use for these experiments? Perhaps a stronger theoretical justification might alleviate the reviewer's concerns."", 'Potential solutions:\n* if you have the computational capacities scale down the other models and train them on the data you used, which makes models more comparable\n* plot the performance w.r.t. model size or dataset size showing how your model is worse but would outperform the others if scaled to the same size\n* argue how they are not comparable due to different resource used during training rendering a comparison (as in my first point) out of scope for this paper', 'Maybe a distill of the big models would be a more accurate comparison ? If that is an option.', ""It's really common to put new models on a XY graph of model size vs performance.""]","['“There are no available models in family X within an order of magnitude of the parameter count in our experiments. Future work will compare scaling laws to determine if our approach remains competitive at similar compute complexity.”', ""I think you stated the answer very clearly, you shouldn't have to compare your model to any models with 10x or more parameter size. If you want to address the larger models, you can probably compare your models with some adjustment for chinchilla scaling laws."", 'Can you reframe your paper as focusing on achieving good results with small models and limited data? Then you can explain why your work is still relevant.', 'Why is the comparison between your model and ones that are 32x in size?  \nAre you comparing against models that around the parameter size of your model in the paper ?', ""Out of curiosity, what kind of datasets are you working on here? How much compute did you use for these experiments? Perhaps a stronger theoretical justification might alleviate the reviewer's concerns.""]",50,15,0.92,Discussion,1751239455.0
1lnoqmm,MachineLearning,"[D] Review clearly used an LLM, should I report it to AC?","This review gave me 1.5 in ACL and calls GRPO Generalized Reward Preference Optimization, which is what ChatGPT thinks GRPO is... It also says my work is the first one to use GRPO in my domain while it is not (and we talk about this in the introduction) and says we are missing some specific evaluations, which are present in the appendix and says we did not justify a claim well enough, which is very well known in my domain but when asking ChatGPT about it it says it does not know about it...

It feels like the reviewer just wanted to give me a bad review and asked an LLM to write a poor review. He clearly did not even check the output because literally everyone knows GRPO stands for Group Relative Policy Optimization...

Other than reply to the reviewer while pretending I did not know he/she used ChatGPT, what else can I do? My other reviews were both 3, so I really want to get rid of this review if possible...","[""At that point even if you don't report it for suspected LLM use you can report it for being unqualified to review since the review doesn't understand what GRPO is. I'd actually flag the review under unqualified and then maybe just mention the LLM-generated part in the justification"", 'Just hope that your AC is not a human-assisted LLM…', 'I think it should be reported - but don\'t get your hopes too high. \nA paper of mine was recently rejected due to one (of the four) reviewers giving devastating comments. Most points were completely hallucinated or requested experiments which were already part of the paper  - but the editor did not care at all and just gave a copy-paste answer that they ""understand our dissapointment but can not accept the paper due to the reviewers concerns"".', 'I had this happen too on an open review submissions. I spent hours writing a multipage response. Half of the reply was aimed at the strength section. All of the strengths were hallucinated, so I asked the reviewer nicely to clarify where he found the strengths so I could correct the claims. The reviewer never replied of course.\n\nI also sent a message to the AC referring to the poor quality of the review. The AC ended up tossing the review.', 'That is terrible and a blatant ethics violation. You should definitely report since the reviewer ought to be punished (as unlikely as that is).', 'Why IN THE WORLD would you not report it?', 'better right the reviewer is not qualified and has provided wrong summary and review.', '[deleted]', ""that's really bad."", 'Report it. ARR ACs are pretty helpful. Obviously some of them are jerk but mostly helpful.']","[""At that point even if you don't report it for suspected LLM use you can report it for being unqualified to review since the review doesn't understand what GRPO is. I'd actually flag the review under unqualified and then maybe just mention the LLM-generated part in the justification"", 'Just hope that your AC is not a human-assisted LLM…', 'I think it should be reported - but don\'t get your hopes too high. \nA paper of mine was recently rejected due to one (of the four) reviewers giving devastating comments. Most points were completely hallucinated or requested experiments which were already part of the paper  - but the editor did not care at all and just gave a copy-paste answer that they ""understand our dissapointment but can not accept the paper due to the reviewers concerns"".', 'I had this happen too on an open review submissions. I spent hours writing a multipage response. Half of the reply was aimed at the strength section. All of the strengths were hallucinated, so I asked the reviewer nicely to clarify where he found the strengths so I could correct the claims. The reviewer never replied of course.\n\nI also sent a message to the AC referring to the poor quality of the review. The AC ended up tossing the review.', 'That is terrible and a blatant ethics violation. You should definitely report since the reviewer ought to be punished (as unlikely as that is).']",185,31,0.92,Discussion,1751229015.0
1lnnts8,MachineLearning,[p] decentralized training and inferencing platform,"

Working on a project that lets you connect to a hundred thousand plus devicing, and use their compute in a decentralized manner. This allows people to train large models, without their own compute. Or even use large models for free as it is hosted on a very large number of device

incase this sounds fascinating then let me know if you would like to use it. Also incase anyone else working on this or worked on this then tell that too

","[""I did: https://borg.games/setup\n\n\n\nBut you can't feasibly train on such a system because training of current models requires bandwidth significantly higher than available in decentralized networks. I believe you need on the order of 100s Gigabits/second.\n\n\nSame for inference of larger models."", 'This exists and is called Federated Learning in the literature, if you’re interested there are quite a few recent papers accepted at ICML 2025 that deal with Federated Learning, including for LLMs.']","[""I did: https://borg.games/setup\n\n\n\nBut you can't feasibly train on such a system because training of current models requires bandwidth significantly higher than available in decentralized networks. I believe you need on the order of 100s Gigabits/second.\n\n\nSame for inference of larger models."", 'This exists and is called Federated Learning in the literature, if you’re interested there are quite a few recent papers accepted at ICML 2025 that deal with Federated Learning, including for LLMs.']",0,3,0.5,Project,1751226709.0
1lnnp8u,MachineLearning,[P] Need to train a model that can detect which 2D image a smartphone camera is looking at (out of about 1000).,"Hey everyone. I'm an AR developer and studio owner, I'm looking for someone to help us with a client project that requires training a machine learning model. Specifically I want a model that can tell me which pin (out of about 1000) a smartphone camera is looking at. Assuming there is only one pin in view, and it's fairly close to the camera. I don't need to find it's location in the image, just need to know which pin I'm looking at.

Here is a sample of a few pins:
https://imgur.com/a/iTdWhbw

They are all more or less that size. I would love some direction and even training code, happy to pay for your time. DM me for more info.","[""You could try fine-tuning a pre-trained ImageNet CNN. There's probably better options now but that's where I would start"", 'This sounds like a fairly simple problem if you have the design of all pins and/or can take pictures of all of them', 'Seems like this could be solved with some sort of template matching instead of ML/DL?', 'so the question is which pin out of the 6 pins in your sample image is the camera looking at? what does the ground truth look like for this sample?\n\nor if its more like identifying out of 1000 pins, what pins are shown in a picture, that sounds way easier to do with object detection + classification.']","[""You could try fine-tuning a pre-trained ImageNet CNN. There's probably better options now but that's where I would start"", 'This sounds like a fairly simple problem if you have the design of all pins and/or can take pictures of all of them', 'Seems like this could be solved with some sort of template matching instead of ML/DL?', 'so the question is which pin out of the 6 pins in your sample image is the camera looking at? what does the ground truth look like for this sample?\n\nor if its more like identifying out of 1000 pins, what pins are shown in a picture, that sounds way easier to do with object detection + classification.']",0,6,0.5,Project,1751226392.0
1lnllcs,MachineLearning,Gameplay to Design DNA? [R],"We are developing a new machine learning algorithm that can design DNA by watching gameplay. The way humans play is different from computers, and that signal might be useful for searching DNA subspaces.

We will be writing a research paper on this new technique, and are shooting for Nature Biotechnology! DM if you’d like to see the preprint.

We have a Tetris clone that runs a lightweight ML model on device, and actually designs DNA as you play. Here we are looking for DNA that activates PPARG::RXRA, involved in metabolism, and deactivates NFKB1, a key regulator of inflammation and immune. These DNA may promise to advance diabetes research.

Long term, we would like to have a library of games, even first person shooters, that design DNA in the background. Sound crazy? Maybe. But we think it might work.

Help us advance this research by collecting your anonymous play data!

https://exonic.ai/games/tilestack",[],[],0,0,0.42,Research,1751221159.0
1lnisl5,MachineLearning,[D] Is OpenReview Down?,"It shows ""There are currently no active venues."" I am trying to complete the NIPS review at the last minute. Will they extend the deadline? ","[""I think it's down. Some specific pages are not accessible at all. I'm surprised this is the only thread talking about this issue"", 'Seems to be down for me too! Frustrating', 'Seems to be down for me', ""I had the same issue on my laptop, but I found I can open the site on my phone. So I tried to delete the cookies on my laptop, and it's working!"", ""It's down for me too :'(((((("", 'Seems down here (Germany) as well.\n\nEdit: [https://www.isitdownrightnow.com/openreview.net.html](https://www.isitdownrightnow.com/openreview.net.html) Welp', 'works in incognito. Interesting!', 'Good thing I did them two days ago. Liberating!', 'I completed my response 1 day earlier for ARR May cycle. Thank god.\n\nBut my concern is that the reviewers will not be able to see our responses on time because of this.', 'It seems like it has been down multiple times since last Friday....']","[""I think it's down. Some specific pages are not accessible at all. I'm surprised this is the only thread talking about this issue"", 'Seems to be down for me too! Frustrating', ""I had the same issue on my laptop, but I found I can open the site on my phone. So I tried to delete the cookies on my laptop, and it's working!"", 'Seems to be down for me', ""It's down for me too :'((((((""]",17,13,0.95,Discussion,1751214344.0
1lnfd3d,MachineLearning,[P] I built a new python package to reorder OCR bounding boxes even with folds and distortions,"**What My Project Does**

`bbox-align` is a Python library that reorders bounding boxes generated by OCR engines into logical lines and correct reading order for downstream document processing tasks. Even when documents have folds, irregular spacing, or distortions

**Target Audience**

Folks that build document processing applications need to reorder and rearrange bounding boxes. This open-source library is intended to do that.

This library is not intended for serious production applications since it's very new and NOT battle-tested. People who are willing to beta test and build new projects on top of this are welcome to try and provide feedbacks and suggestions.

**Comparison**

Currently, OCR engines do a good job of reordering bounding boxes they generate. But sometimes they don't group them into correct logical/reading order. They perhaps use clustering algorithms to group bounding boxes that are close to each other, which may be incorrect.

I use coordinate geometry to determine if two bounding boxes are inline or not.

Github - [https://github.com/doctor-entropy/bbox-align](https://github.com/doctor-entropy/bbox-align)

PyPI - [https://pypi.org/project/bbox-align/](https://pypi.org/project/bbox-align/)","['Thank you for sharing this project keep it up 👍', 'How does this handle multi column layouts?']","['Thank you for sharing this project keep it up 👍', 'How does this handle multi column layouts?']",1,2,0.56,Project,1751205585.0
1lnem9e,MachineLearning,[P] I built a Python debugger that you can talk to,,"['Now ask it an actual question', 'Check it out: [https://github.com/shobrook/redshift](https://github.com/shobrook/redshift)\n\nThink of this as `pdb` (Python\'s native debugger) with an LLM inside. When a breakpoint is hit, you can ask questions like:\n\n* ""Why is this function returning null?""\n* ""How many items in `array` are strings?""\n* ""Which condition made the loop break?""\n\nAn agent will navigate the call stack, inspect variables, and look at your code to figure out an answer.\n\nPlease let me know what y\'all think!', 'That\'s sweet OP. Maybe I will consider ""vibe coding"" once tools like yours become more widespread. I know some SQL but fell flat on my face learning Java.', 'now ask it to give birth', 'Hey this is a good idea! Though, I wonder if it might be better to expose the debugger as a tool instead for models to use.', 'I like the concept and it would be pretty useful tool. However this assumes I run what the whole training loop in debug mode? Also that feels like a contrived solution, what would be useful would be actually understanding when loss is say plateauing or going up and down and giving some insightful feedback there. Here you’re just doing regular code debugging right?', 'This is a great concept , I will give it try if it really helpful .', '[deleted]', 'Great idea, wonderful project!', 'This is great. Maybe this way junior devs will finally start using the debugger.']","['Now ask it an actual question', 'Check it out: [https://github.com/shobrook/redshift](https://github.com/shobrook/redshift)\n\nThink of this as `pdb` (Python\'s native debugger) with an LLM inside. When a breakpoint is hit, you can ask questions like:\n\n* ""Why is this function returning null?""\n* ""How many items in `array` are strings?""\n* ""Which condition made the loop break?""\n\nAn agent will navigate the call stack, inspect variables, and look at your code to figure out an answer.\n\nPlease let me know what y\'all think!', 'That\'s sweet OP. Maybe I will consider ""vibe coding"" once tools like yours become more widespread. I know some SQL but fell flat on my face learning Java.', 'now ask it to give birth', 'Hey this is a good idea! Though, I wonder if it might be better to expose the debugger as a tool instead for models to use.']",189,24,0.9,Project,1751203482.0
1lndj1a,MachineLearning,[D] How to convert theoretical knowledge to applied skills?,"Hi I've recently finished a MSc in maths+stats at a good university and about to move onto a ML PhD. I feel like I understand the math and theory behind ML quite well, can read papers, design computer experiments and produce visuals for papers etc, but I can't make anything ""product level"", like an actual application or a tool that can be deployed or used by other people. In particular, I feel I'm lacking engineering skills.

How can I develop skills like these, specially to become competitive at ML engineering internships if I need to apply in the coming years. Are there any books, websites, or other sources which you would recommend to gain starting ideas about what goes into ML engineering?","[""Fortunately for you, all that stuff is extremely realistic to learn on the job. The theory is (in my opinion) practically impossible to self-teach or learn on the job, and hiring managers know that. Just start making stuff. Look at how other people made stuff on github and try to replicate it. Take some model you've made that works well and then think about how to build an application around it. \n\n* For example, how can you take the model's inferencer and make it available via API?  (Read about Flask and Fast API.) \n* Can a person or system using that API pass data to the API in exactly the form that the model needs, or do some complex transformations need to happen in real-time? (Read about online feature stores.) \n* If you had your API running all the time with the model serving inferences on-demand, what kind of data quality checks would you want to automatically run on some schedule to detect when the model's performance has degraded, or when the distributions of the data have significantly diverged from the data the model was trained on? (Read about Airflow.)"", 'Join local AI meetups. Talk with fellow enthusiasts. You will hear about or think of a project. The answer to your question is projects. Just start building.', 'Learn on the job, ideally with a strong product team. It can be a long way from idea to useful thing with plenty of mostly non-technical pitfalls on the way.']","[""Fortunately for you, all that stuff is extremely realistic to learn on the job. The theory is (in my opinion) practically impossible to self-teach or learn on the job, and hiring managers know that. Just start making stuff. Look at how other people made stuff on github and try to replicate it. Take some model you've made that works well and then think about how to build an application around it. \n\n* For example, how can you take the model's inferencer and make it available via API?  (Read about Flask and Fast API.) \n* Can a person or system using that API pass data to the API in exactly the form that the model needs, or do some complex transformations need to happen in real-time? (Read about online feature stores.) \n* If you had your API running all the time with the model serving inferences on-demand, what kind of data quality checks would you want to automatically run on some schedule to detect when the model's performance has degraded, or when the distributions of the data have significantly diverged from the data the model was trained on? (Read about Airflow.)"", 'Join local AI meetups. Talk with fellow enthusiasts. You will hear about or think of a project. The answer to your question is projects. Just start building.', 'Learn on the job, ideally with a strong product team. It can be a long way from idea to useful thing with plenty of mostly non-technical pitfalls on the way.']",0,3,0.5,Discussion,1751200180.0
1lnbdzx,MachineLearning,[P] Simple MARL environment to train quadrotor swarms in UE4,"In the past, I was asking for help here on Reddit to build some environment for drone swarms training. I think it might be helpful to someone, so I'll link the results here. I obviously suspect that the results are obsolete (end of 2023), but let me know if you find it useful and leave a star if you'd like!

[Multi-agent Deep Reinforcement Learning for Drone Swarms using UE4, AirSim, Stable-Baselines3, PettingZoo, SuperSuit](https://github.com/Lauqz/Drone-Swarm-RL-airsim-sb3)",[],[],4,0,1.0,Project,1751192569.0
1lnayg0,MachineLearning,[P][Update]Open source astronomy project: need best-fit circle advice,,"['The keyword you\'re looking for is ""Hough Circle Transform"". With reasonable bounds (e.g., min/max radius, number of circles) you\'ll have fantastic results.\n\nKids today and their fancy schmancy AI', ""Hi,\n\nI'm maintaining an open-source tool called [DFTFringe](https://github.com/atsju/DFTFringe/issues/37) that analyzes interferometry images to deduce the shape of telescope mirrors. It's used by many amateur telescope makers and works well overall.\n\nThere's one manual step we'd like to automate: fitting a circle to an image feature, with ~1 pixel accuracy. \n\nI have set up a small github workflow to make it easy to try out algorithms. If you're interested, feel free to open a Pull Request with your code. Or suggest an approach in the thread.\n\nPrevious Reddit posts and more details are linked from the [GitHub description](https://github.com/atsju/DFTFringe/issues/37).\n\nThanks in advance!"", 'Hello, Maybe try Otsu Thresholding or 2 components KNNs (you can use the high intensity near the middle of the image as foreground, and random pixel from the corner as background). Then apply some morphological operation on top of it to remove holes (dilation + erosion). This should give you a good roundish binary mask (you can set easily fit a circle equation on top of it if need) :)', 'If you can find a way to fill in the circle, morphological opening + component analysis might do it.', 'If you only need to do this a few hundred times I would just brute force a cost function which tries to include pixels above the average and exclude those below. Even using the medium of the intensity inside vs outside, then fit based on that cost function.\xa0 By eye this is very easy to see it is separable. Fancy tricks are really not needed, pretty much any classical segmentation approach will work if you see the initial values based on a highly blurred peak finder.', 'A classical technique that can work well for this is RANSAC', 'How many images do you have? You could just try something out-of-the-box like YOLO.\n\nMay also be interesting to filter the image first by doing a 2D FFT and filtering out a band, I suspect what you’re looking for will appear in reciprocal space over a range that you can identify by visual inspection.']","['The keyword you\'re looking for is ""Hough Circle Transform"". With reasonable bounds (e.g., min/max radius, number of circles) you\'ll have fantastic results.\n\nKids today and their fancy schmancy AI', ""Hi,\n\nI'm maintaining an open-source tool called [DFTFringe](https://github.com/atsju/DFTFringe/issues/37) that analyzes interferometry images to deduce the shape of telescope mirrors. It's used by many amateur telescope makers and works well overall.\n\nThere's one manual step we'd like to automate: fitting a circle to an image feature, with ~1 pixel accuracy. \n\nI have set up a small github workflow to make it easy to try out algorithms. If you're interested, feel free to open a Pull Request with your code. Or suggest an approach in the thread.\n\nPrevious Reddit posts and more details are linked from the [GitHub description](https://github.com/atsju/DFTFringe/issues/37).\n\nThanks in advance!"", 'Hello, Maybe try Otsu Thresholding or 2 components KNNs (you can use the high intensity near the middle of the image as foreground, and random pixel from the corner as background). Then apply some morphological operation on top of it to remove holes (dilation + erosion). This should give you a good roundish binary mask (you can set easily fit a circle equation on top of it if need) :)', 'If you can find a way to fill in the circle, morphological opening + component analysis might do it.', 'If you only need to do this a few hundred times I would just brute force a cost function which tries to include pixels above the average and exclude those below. Even using the medium of the intensity inside vs outside, then fit based on that cost function.\xa0 By eye this is very easy to see it is separable. Fancy tricks are really not needed, pretty much any classical segmentation approach will work if you see the initial values based on a highly blurred peak finder.']",15,26,0.8,Project,1751190871.0
1ln9sbq,MachineLearning,[D] PhD worth it to do RL research?,"Posting anonymously for this one. I know questions like these get posted quite often, but I wanted to offer a bit of context about my own situation and what I'm into.

I'm currently a rising college sophomore working in Sergey Levine's lab (RL & robotics) at Berkeley, and I have to decide whether I want to pursue a standard industry internship (e.g. SWE) for the 2026 summer or continue doing research in the lab. I really like research work, easily the most enjoyable ""work"" I've done in my life, but I can't deny that money is still a factor (esp. due to particular family reasons). I see three sort of options down the line from here (listed with their pros and cons

A) continue doing research in my time in undergrad, and shoot a difficult shot towards getting into a reputable PhD program

* Pros:
   * very streamlined process to become an industry research scientist given that I go to a good enough program & work hard enough
   * \^\^ this is the most optimal job option for me: 10/10 job, the best I could ever want. I love research man
   * researchers generally seem like the most sufferable group out of most tech archetypes (seen way too many elon-musk wannabes in normal SWE)
* Cons:
   * 5-6 years of a PhD: not that it's going to be unenjoyable, but it delays my life ""progress"" a lot
   * getting into top ML PhD programs is really tough nowadays. I'm lucky to have started sort of early (working on my first first-author pub over this summer) but I know people with great publication history (probably better than I'll earn) that didn't get admitted anywhere
   * \^\^ it seems as though if I don't get into a PhD program, all the research I would have published would be a sunk cost (not useful for much besides just.. ML research)
   * comp: is it much better than normal SWE or MLE? though I love the work a lot, I would hope that it's just a *biiit* better to justify the extra 6 years I put in for a PhD
   * if ML hype & investment dies out, I'll be on the forefront of getting laid off, esp if RL doesn't find a way to scale soon enough

B) continue doing research, but balance it out with some SWE or similar experience and go for an MLE or research engineer type of role

* Pros:
   * immediately high comp out just out of my degree if I can land one of these roles, without needing to spend all that time on a degree
   * correct me if I'm wrong, but RE and some parts of MLE aren't that far off from research scientist work, esp. if working with researchers at a frontier lab
   * seems to be less workload, better WLB?
   * seems to be more stable (easier transition to SWE) if ML hype dies out
* Cons:
   * less interesting work. not that I hate it, but it's like an 8/10 compared to the 10/10 work that I would consider to be RS
   * I'm unsure if my publications & research history would help at all for these roles. from what I've heard, research and industry experience are almost orthogonal and they simply don't care about publications (please correct me if I'm wrong!)
   * don't own the intellectual rights to my own work :(

C) research is useless, just do SWE, ML research is a hellhole

* \^\^ this is more so a last resort rather than something I would ever want to do, but if you have any reason that this is a good option, please do tell me why","['[deleted]', 'Going anonymous to respond to this. I am at a top ten PhD program in AI/ML working in RL.\n\nThis is partly in response to No\\_Drama, not to start anything, but to offer a broader and hopefully grounded perspective. I do not mean for this to sound pessimistic, just honest.\n\nFirst, regarding the Sergey lab connection. It is true that working in his lab can open doors, and the Sergey brand carries weight. There are real pipelines to top PhD programs that start there. But places like Berkeley and other top universities are extremely demanding, even when you have a well-known advisor. Sometimes they are even more stressful if you do. Everyone is smart. Everyone works hard. It can be difficult not to compare yourself to others. Getting into a top PhD program does not mean it will be a good fit, or that it will be smooth. I know brilliant people who have struggled and even dropped out. Dropping out is not the norm, but struggling at times definitely is, even if people rarely talk about it. That said, it sounds like you genuinely enjoy RL and research, which is one of the most important things to make it through. The potential for a strong job at the end certainly helps too.\n\nI would not take No\\_Drama’s suggestion of “no hesitation” on doing a PhD. It is a serious commitment that deserves careful thought, which it sounds like you are already doing. I am assuming you are young, so I want to be blunt. There are no guarantees you will get into a top program, even with a strong advisor. I know people from Sergey\'s lab who have not gotten into their top choices, sometimes even after 2-3 cycles. There is also intense competition for letters in that lab. From what I have seen and heard, he operates under the assumption that in a crowd and under pressure the strongest students will reveal themselves. But there are about 20 undergrads listed on the lab website each year (not sure how real that number is anymore), so standing out enough to get a strong letter is far from guaranteed.\n\nA couple nitpicks:  \n""Publish 2-3 papers a year"" - this would be great (and is certainly idealistic), but don\'t get too hung up on numbers either as an undergrad or a PhD. As a PhD student I can tell you anecdotally that this is a trap. Especially when you start comparing to other people in RAIL/BAIR. Idea generation and especially successful acceptance/publication often can be in large part due to luck. There is a lot of noise in the review phase in particular so just do what you enjoy and do good work and you will do great.\n\n""Go get into a PhD then work summers at deepmind/brain/open ai/anthropic"" - this isn\'t as easy as 1. apply 2. get it 3. profit. Be a realist and know that even at Berkeley this is not a guarantee.\n\nOtherwise I agree with many other posters. While getting into a top PhD program does not equate to finishing, if you do finish you will have a very bright future ahead of you. Even if you don\'t go the PhD or master\'s route you still sound like a very smart and driven person with a great pedigree and that will take you very far.\n\nAll of your options are good ones, so I would say just think about what you want in life (hard) and what you enjoy (hopefully easier) and go with what you think will give you the best chance at a happy career.', 'I went with option B to ""start"" the real life of an adult and earn some money. I did, I did publish an ok article in the meantime, working on next, but I wish I had gone with option A. If you want to, at any point, work in R&D PhD is REALLY beneficial and RL is a great field.', 'ML PhD student here, also in RL (also in the Sergey-sphere, although not as directly as you), worked in industry as an MLE for a few years before that and regularly collaborate with many frontier lab people. Here’s my take:\n\n\n1. You don’t strictly need a PhD to work at a frontier lab. OAI/Anthropic readily hire non-PhDs, many of who do get involved in research. GDM/Meta hire lots of research engineers. What you do need is research experience, which can be demonstrated through projects, papers etc. Of course, these have to be good and impactful projects/papers, because they act as a signal that you can do good research. Spending your summers doing research at a top lab increases the chances that you’ll do good research, so IMO it is better to do this vs. some SWE internship regardless of whether you want to do a PhD.\n\n2. The value of a PhD is increasingly less the title than it is the fact that (1) you are likely to have become very good at research because you spent 5 years doing it in a good environment (2) you get to meet a lot of interesting people with interesting ideas along the way (3) you get the freedom to build a research portfolio however you see fit. IMO (3) is the biggest reason to do a PhD - you will forever be able to point to your portfolio of research, which you shaped from beginning to end. You will forever be “the guy that invented X” or “the guy who wrote Y paper”, and this is the signalling mechanism that draws employers/VC funding and gets you that $1M paycheck, not the PhD title itself. Of course, this is all contingent on you doing well in your PhD…\n\n\nIn contrast, when you work at a frontier lab nowadays, the likelihood is that the vast majority of the research that you will remain behind closed doors, and it’s hard to do good credit assignment for big projects like building an LLM (i.e. you could say on your CV that you worked on the Gemini team, but so did thousands of other people and it’s hard to communicate the details of exactly how you contributed). Again, there are exceptions (e.g. Alec Radford) but I do think these exceptions are becoming rarer as frontier labs transition away from being mere research labs to actual companies. \n\n\n3. Building on the previous point, PhDs give you the freedom to explore whatever you want (within reason). This is in and of itself extremely valuable, because it allows you to pursue what you think could be impactful, and aligns your work with what you find most interesting. You need to see the PhD not as “delaying your life progress” but as something fundamentally interesting and valuable - a time to scratch those curiosity itches, to learn as much as you can about whatever you want. IMO if you only ever see a PhD as a means to an end then I 100% advise you not to do a PhD, because it will be torturous.\n\n4. RE vs. RS: at a frontier lab, you likely get to do cutting-edge research in both roles, but as an RS you will likely get more freedom and scope to determine research direction, and you will spend less time than RE doing some of the difficult plumbing (although you still need to be a good engineer). Good REs can be paid a ridiculous amount as well if they work on a commercially important part of the LLM stack (e.g. working on the inference stack). Being a strong engineer with specialised skills can be very valuable and lucrative, but (1) you need to specialise in the right things and (2) you may have to sacrifice some research freedom. Overall though, it is unclear to me that doing a standard SWE internship is actually helpful for this, because doing regular backend stuff (or even normal MLE stuff) just doesn’t overlap much with RE work (which IMO aligns more closely with the kind of engineering you might do during your PhD). \n\nAs for comp:\n- Comp is generally very good if you join a frontier lab out of a top PhD (min 500k). I worked with PhDs in my previous job and they got paid just as much as/only slightly more than I did with a masters, but (1) these people did not do top PhDs and (2) I did not work at a frontier lab. The takeaway here is that you are NOT financially better off doing a PhD UNLESS you get into a very strong program and succeed in it.\n- Contrary to what others say, the comp you get for summer internships at GDM or Meta are NOT better than standard SWE pay, because they don’t give you equity. I don’t believe OAI do internships at all, while Anthropic/XAI only do longer ones (not sure what they pay).', 'I think that PhD is worth it given the opportunities it provides and since you like research. If you decide you don’t want to do the full PhD and you get into an elite program, you can probably just drop the PhD at some point and join a top industry research lab full time or do a startup (several PhD students at Stanford for example have recently been doing this). \n\nI would also recommend spending at least one summer doing something industry related though, as I think it is good to get a little of both industry and research. \n\nFor reference, I am in a similar situation and am planning to apply for PhD or attempt to get into a top industry lab out of undergrad (I am rising junior). I did research during my first summer that got published, and now am doing MLE work in big tech this summer.', 'There are already a lot of great comments here so I won\'t bother repeating much of the same. But as someone who just graduated from a top 5 CS program and is currently interviewing in industry, I\'ll say this:\n\n\\- PhD is a long time and when you could be making $200k+ right out of undergrad, if money is your sole focus, it might be hard to justify at times. And some of the top labs (OpenAI/Anthropic) will hire amazing people who do not have a PhD and will pay $500k+ right away. **However,** many great places require a PhD for the Applied/Research scientist roles and you can expect to make a bit more than what a SDE/MLE at the same place might make after 5-6 years. \n\n\\- It can be hard to get into MLE roles out of PhD, because while scientist positions value the research experience, many MLE roles want to see ML infra experience in industry (which can be helped through internships). I found that I had much better experience (i.e. I got first round interviews with almost all ""research/applied scientist"" roles) applying to PhD required positions than with MLE roles (even if less prestigious, you\'d probably laugh at the places I got rejected from compared to where I got interviews).\n\n\\- With the previous, look into MLE roles at places you\'d be interested in. Many of the scientist roles I interviewed with mentioned how closely they worked with the MLEs and you are very much apart of the process at many labs. So being on the forefront out of undergrad making $200k+ is easily possible\n\n\\- Like you said, getting into a top CS PhD is HARD, I honestly don\'t think I\'d get into my same program if I applied today and then getting a position in industry at a top place is also hard. And outside of OpenAI/Anthropic, you are looking at \\~$300-400k average first year comp (depending on location obviously), which is amazing, but you\'d also possibly be making that if not more if you just grinded 5-6 years as a MLE\n\n\\- With all that, you are already at Sergey\'s lab, it\'s hard to be at a better lab especially in RL. RL is just getting started, sure there might be some lull\'s in compensation for AI in the future, but it\'s not going anywhere anytime soon. The things people are working on in industry are so exciting and there is a lot to come. \n\n  \n**tl;dr** Do your PhD. You are already at a phenomenal lab and thoroughly enjoy research, I\'m sure you\'ll land at a top program (focus on advisor not name of program). And you will be compensated very well when you graduate, but don\'t expect OpenAI compensation at non-OpenAI labs (but if you do break in there, you\'ll be able to pay for your family many times over :D)\n\n  \nGoodluck you are on an exciting path!', 'I saw someone say ""Maintain a good gpa, publish 2-3 papers a year in {CoRL, NeurIPS, ICML, ICLR} and you’re good."" Although this advice is in good spirit, DO NOT publish 2-3 papers a year. All you need is ONE good, well thought-out paper. I have sat on in grad school admissions and what matters most is the recommendation letter and then having one paper that applicant reviewers like or find interesting.', ""here's a hot take: there just isn't that much time left. assuming you got that dog in you, publish \\~2 first author papers on RL/LLMs, graduate in 3, then go work at a frontier lab. \n\ndm me if you wanna chat -- i went to cal, did a phd at a top school, and regret it bc there was so much opportunity cost."", 'When applying to jobs, your skills/potential/past accomplishments are the last thing they look at and are arguably not important.\n\nAll they care about is reputation, titles, and recommendations.\n\nWhile you may learn more on your own, or from the industry in that time, PhD offers more of the reputation/title/connections front and is better if you actually want to do research work and not sql dashboarding in a faang.', ""I just want to echo what some others have said: doing a PhD at a Berkeley-caliber school in cutting edge stats/ML/AI research that is hot in industry can set you up incredibly well. I know a guy who's starting salary after his stats PhD at Cal was \\~$2million.\n\nIf you are already in such a prestigious lab you are on track. One bit of advice I would give (I didn't do this my friend did): set your PhD up so that you are managing teams of researchers towards the end, even if its undergrads. This lets you transition directly to leading a team of researchers in industry for your internships and first job, and gives that much more impact and salary.""]","['[deleted]', 'Going anonymous to respond to this. I am at a top ten PhD program in AI/ML working in RL.\n\nThis is partly in response to No\\_Drama, not to start anything, but to offer a broader and hopefully grounded perspective. I do not mean for this to sound pessimistic, just honest.\n\nFirst, regarding the Sergey lab connection. It is true that working in his lab can open doors, and the Sergey brand carries weight. There are real pipelines to top PhD programs that start there. But places like Berkeley and other top universities are extremely demanding, even when you have a well-known advisor. Sometimes they are even more stressful if you do. Everyone is smart. Everyone works hard. It can be difficult not to compare yourself to others. Getting into a top PhD program does not mean it will be a good fit, or that it will be smooth. I know brilliant people who have struggled and even dropped out. Dropping out is not the norm, but struggling at times definitely is, even if people rarely talk about it. That said, it sounds like you genuinely enjoy RL and research, which is one of the most important things to make it through. The potential for a strong job at the end certainly helps too.\n\nI would not take No\\_Drama’s suggestion of “no hesitation” on doing a PhD. It is a serious commitment that deserves careful thought, which it sounds like you are already doing. I am assuming you are young, so I want to be blunt. There are no guarantees you will get into a top program, even with a strong advisor. I know people from Sergey\'s lab who have not gotten into their top choices, sometimes even after 2-3 cycles. There is also intense competition for letters in that lab. From what I have seen and heard, he operates under the assumption that in a crowd and under pressure the strongest students will reveal themselves. But there are about 20 undergrads listed on the lab website each year (not sure how real that number is anymore), so standing out enough to get a strong letter is far from guaranteed.\n\nA couple nitpicks:  \n""Publish 2-3 papers a year"" - this would be great (and is certainly idealistic), but don\'t get too hung up on numbers either as an undergrad or a PhD. As a PhD student I can tell you anecdotally that this is a trap. Especially when you start comparing to other people in RAIL/BAIR. Idea generation and especially successful acceptance/publication often can be in large part due to luck. There is a lot of noise in the review phase in particular so just do what you enjoy and do good work and you will do great.\n\n""Go get into a PhD then work summers at deepmind/brain/open ai/anthropic"" - this isn\'t as easy as 1. apply 2. get it 3. profit. Be a realist and know that even at Berkeley this is not a guarantee.\n\nOtherwise I agree with many other posters. While getting into a top PhD program does not equate to finishing, if you do finish you will have a very bright future ahead of you. Even if you don\'t go the PhD or master\'s route you still sound like a very smart and driven person with a great pedigree and that will take you very far.\n\nAll of your options are good ones, so I would say just think about what you want in life (hard) and what you enjoy (hopefully easier) and go with what you think will give you the best chance at a happy career.', 'I went with option B to ""start"" the real life of an adult and earn some money. I did, I did publish an ok article in the meantime, working on next, but I wish I had gone with option A. If you want to, at any point, work in R&D PhD is REALLY beneficial and RL is a great field.', 'ML PhD student here, also in RL (also in the Sergey-sphere, although not as directly as you), worked in industry as an MLE for a few years before that and regularly collaborate with many frontier lab people. Here’s my take:\n\n\n1. You don’t strictly need a PhD to work at a frontier lab. OAI/Anthropic readily hire non-PhDs, many of who do get involved in research. GDM/Meta hire lots of research engineers. What you do need is research experience, which can be demonstrated through projects, papers etc. Of course, these have to be good and impactful projects/papers, because they act as a signal that you can do good research. Spending your summers doing research at a top lab increases the chances that you’ll do good research, so IMO it is better to do this vs. some SWE internship regardless of whether you want to do a PhD.\n\n2. The value of a PhD is increasingly less the title than it is the fact that (1) you are likely to have become very good at research because you spent 5 years doing it in a good environment (2) you get to meet a lot of interesting people with interesting ideas along the way (3) you get the freedom to build a research portfolio however you see fit. IMO (3) is the biggest reason to do a PhD - you will forever be able to point to your portfolio of research, which you shaped from beginning to end. You will forever be “the guy that invented X” or “the guy who wrote Y paper”, and this is the signalling mechanism that draws employers/VC funding and gets you that $1M paycheck, not the PhD title itself. Of course, this is all contingent on you doing well in your PhD…\n\n\nIn contrast, when you work at a frontier lab nowadays, the likelihood is that the vast majority of the research that you will remain behind closed doors, and it’s hard to do good credit assignment for big projects like building an LLM (i.e. you could say on your CV that you worked on the Gemini team, but so did thousands of other people and it’s hard to communicate the details of exactly how you contributed). Again, there are exceptions (e.g. Alec Radford) but I do think these exceptions are becoming rarer as frontier labs transition away from being mere research labs to actual companies. \n\n\n3. Building on the previous point, PhDs give you the freedom to explore whatever you want (within reason). This is in and of itself extremely valuable, because it allows you to pursue what you think could be impactful, and aligns your work with what you find most interesting. You need to see the PhD not as “delaying your life progress” but as something fundamentally interesting and valuable - a time to scratch those curiosity itches, to learn as much as you can about whatever you want. IMO if you only ever see a PhD as a means to an end then I 100% advise you not to do a PhD, because it will be torturous.\n\n4. RE vs. RS: at a frontier lab, you likely get to do cutting-edge research in both roles, but as an RS you will likely get more freedom and scope to determine research direction, and you will spend less time than RE doing some of the difficult plumbing (although you still need to be a good engineer). Good REs can be paid a ridiculous amount as well if they work on a commercially important part of the LLM stack (e.g. working on the inference stack). Being a strong engineer with specialised skills can be very valuable and lucrative, but (1) you need to specialise in the right things and (2) you may have to sacrifice some research freedom. Overall though, it is unclear to me that doing a standard SWE internship is actually helpful for this, because doing regular backend stuff (or even normal MLE stuff) just doesn’t overlap much with RE work (which IMO aligns more closely with the kind of engineering you might do during your PhD). \n\nAs for comp:\n- Comp is generally very good if you join a frontier lab out of a top PhD (min 500k). I worked with PhDs in my previous job and they got paid just as much as/only slightly more than I did with a masters, but (1) these people did not do top PhDs and (2) I did not work at a frontier lab. The takeaway here is that you are NOT financially better off doing a PhD UNLESS you get into a very strong program and succeed in it.\n- Contrary to what others say, the comp you get for summer internships at GDM or Meta are NOT better than standard SWE pay, because they don’t give you equity. I don’t believe OAI do internships at all, while Anthropic/XAI only do longer ones (not sure what they pay).', 'I saw someone say ""Maintain a good gpa, publish 2-3 papers a year in {CoRL, NeurIPS, ICML, ICLR} and you’re good."" Although this advice is in good spirit, DO NOT publish 2-3 papers a year. All you need is ONE good, well thought-out paper. I have sat on in grad school admissions and what matters most is the recommendation letter and then having one paper that applicant reviewers like or find interesting.']",86,37,0.9,Discussion,1751186134.0
1ln9bn4,MachineLearning,[D] Loss function for fine tuning in a list of rankings,"I am not ultra updated with the literature on LLMs and I habe a probably which I guess is very similar to what everyone who works with document ranking has to deal with, so I would just like to know if there is some canonic obvious solution for my problem.

I want to fine tune an LLM (if it makes any difference it is a multi modal one). My model receives an video as the input and outputs a description.

During fine-tuning, I want to generate N captions for a single video (let's say 5 captions for simplicity sake), and I have an ""oracle"" that will sort those 5 responses in order of preference.

I want a loss function that will fine tune my model in a way that will make the probability of ""better"" answers, according to my oracle ranking, higher. Any loss function for that?

Ideally, off-polify (but on policy woukd be fine as well). It can't be DPO for example because it only consider 2 possible answer. It coukd be PPO I guess if I convert the ranking to a number, but I would rather not have to keep a reward model, and PPO is not really a rank loss function
","['GRPO?', 'I see two possible ways:\n\n1) generate 5 captions and rate them on quality on 0-1 (label smoothing) -> 1s will have 2x the signal of 0.5s\n\n2) sota LLMs use something called ""specilative decoding"" where thay predict top N next words, and let a smaller model decide which to use - perhaps theres a new ranking loss, but you could also predict N words and score them by quality as well. 1 prompt -> 5 generations -> 5 train/valid/test data points!']","['GRPO?', 'I see two possible ways:\n\n1) generate 5 captions and rate them on quality on 0-1 (label smoothing) -> 1s will have 2x the signal of 0.5s\n\n2) sota LLMs use something called ""specilative decoding"" where thay predict top N next words, and let a smaller model decide which to use - perhaps theres a new ranking loss, but you could also predict N words and score them by quality as well. 1 prompt -> 5 generations -> 5 train/valid/test data points!']",4,2,1.0,Project,1751184227.0
1ln8wu8,MachineLearning,[D] SAMformer -- a lesson in reading benchmarks carefully,"For those not in the time-series forecasting space, it has seen some interesting developments in the last few years as researchers have tried to translate the success of transformer-based models in the language domain, to the forecasting domain. There was incremental progress in long-term timeseries forecasting with the likes of [Informer](https://arxiv.org/abs/2012.07436), [Autoformer](https://arxiv.org/abs/2106.13008), and [Fedformer](https://arxiv.org/pdf/2201.12740), among others, however the 2022 [paper ](https://arxiv.org/abs/2205.13504)""Are Transformers Effective for Time Series Forecasting?"" (Zeng et al.) called into question how much progress these models had actually made.

Zeng et al. introduced three self-proclaimed ""embarassingly simple"" linear models -- each of which are variations on a **single dense layer** mapping the input values to the output values -- which outperformed all of the above state-of-the-art transformer models on their benchmarks (see the image below for a subset of results):

[Linear and Transformers MSE Benchmarks](https://preview.redd.it/l504gwc2ft9f1.png?width=998&format=png&auto=webp&s=b231af865a3e1ea1ee48ad64dd77f6cad4f011eb)

This brings us to the paper [SAMformer](https://arxiv.org/abs/2402.10198) which applies a ""sharpness-aware minimisation"" approach to training a simplified version of the vanilla transformer encoder. This works very well, generally outperforming the aforementioned transformer models, as well as competetive non-transformer state-of-the-art models (TSMixer and PatchTST), on all the same benchmarks. Notably absent in the benchmarks however, are the linear models from Zeng et al. You can see the results from the SAMformer paper below (all results are **MSE**):


[SAMFormer MSE Benchmarks](https://preview.redd.it/gbraaexeht9f1.png?width=169&format=png&auto=webp&s=bb4f1dff2015ff492e1bd07b58beb546f9b24a5f)

On Electricity, Exchange, and Weather the simple linear models outperform SAMformer for all horizons, and it is only on the Traffic dataset where SAMformer achieves lower MSE. The omission of the linear models in the final benchmarks is doubly surprising given the SAMformer authors specifically mention the results from Zeng et al. in their introduction:

""\[Zeng et al.\] recently found that linear networks can be on par or better than transformers for the forecasting task, questioning their practical utility. This curious finding serves as a starting point for our work.""

To be clear, I think the ideas introduced in the SAMformer paper are valuable and I think it would be fair to classify SAMformer as a ""state-of-the-art"" model. However, I am curious of the rationale for excluding the linear models in the benchmarks given they were originally introduced to call into question the effectiveness of transformers in the time-series forecasting domain.

Tl;dr: Always put your skeptical glasses on when reviewing benchmarks as there may be some highly competetive models omitted from the analysis.","['Time series forecasting is the astrology of ML. People who work on these problems always forget the key component: the time horizon. Your predictions can look ok up to a certain time in the future. After that, its all random guessing. And knowing when you\'ve hit the limits of the prediction window in a live environment is almost impossible.\n\nThere I said it, I\'m ready for the downvotes from all the ""my first project was a stock prediction lstm"" newbies.', 'These ""we solved it with transformers\' papers are plaguing the bioinformatics field too right now.\xa0 So much shoddy work that gets published in top journals because of the hype around llms.', ""I'm having trouble understanding the idea of channel-wise attention here, how does it refer to past data?"", ""That's kind of a hot-take but I somehow get the suspicion that with tabular data and timeseries we might be very close to the optimum already. Probably more research should be bothered with how much information we can really retrieve from the data instead of having this gold-rushing mentality and throwing the next algorithm at a problem.""]","['Time series forecasting is the astrology of ML. People who work on these problems always forget the key component: the time horizon. Your predictions can look ok up to a certain time in the future. After that, its all random guessing. And knowing when you\'ve hit the limits of the prediction window in a live environment is almost impossible.\n\nThere I said it, I\'m ready for the downvotes from all the ""my first project was a stock prediction lstm"" newbies.', 'These ""we solved it with transformers\' papers are plaguing the bioinformatics field too right now.\xa0 So much shoddy work that gets published in top journals because of the hype around llms.', ""I'm having trouble understanding the idea of channel-wise attention here, how does it refer to past data?"", ""That's kind of a hot-take but I somehow get the suspicion that with tabular data and timeseries we might be very close to the optimum already. Probably more research should be bothered with how much information we can really retrieve from the data instead of having this gold-rushing mentality and throwing the next algorithm at a problem.""]",86,17,0.95,Discussion,1751182566.0
1ln8q6d,MachineLearning,[D] Transfer learning v.s. end-to-end training,"Hello everyone,

I'm an ADAS engineer and not an AI major, nor did I graduate with an AI-related thesis, but my current work requires me to start utilizing AI technologies.

My tasks currently involve Behavioral Cloning, Contrastive Learning, and Data Visualization Analysis. For model validation, I use metrics such as loss curve, Accuracy, Recall, and F1 Score to evaluate performance on the training, validation, and test sets. So far, I've managed to achieve results that align with some theoretical expectations.

My current model architecture is relatively simple: it consists of an Encoder for static feature extraction (implemented with an MLP - Multi-Layer Perceptron), coupled with a Policy Head for dynamic feature capturing (GRU - Gated Recurrent Unit combined with a Linear layer and Softmax activation).

Question on Transfer Learning and End-to-End Training Strategies
I have some questions regarding the application strategies for Transfer Learning and End-to-End Learning. My main concern isn't about specific training issues, but rather, I'd like to ask for your insights on the best practices when training neural networks:

Direct End-to-End Training: Would you recommend training end-to-end directly, either when starting with a completely new network or when the model hits a training bottleneck?

Staged Training Strategy: Alternatively, would you suggest separating the Encoder and Policy Head? For instance, initially using Contrastive Learning to stabilize the Encoder, and then performing Transfer Learning to train the Policy Head?

Flexible Adjustment Strategy: Or would you advise starting directly with end-to-end training, and if issues arise later, then disassembling the components to use Contrastive Learning or Data Visualization Analysis to adjust the Encoder, or to identify if the problem lies with the Dynamic Feature Capturing Policy Head?

I've actually tried all these approaches myself and generally feel that it depends on the specific situation. However, since my internal colleagues and I have differing opinions, I'd appreciate hearing from all experienced professionals here.

Thanks for your help!","['I think it depends on the research goal.  Personally I have a follow up question; when are *multple* pretraining phases useful?  A recent example I have seen is V=JEPA2', 'Also being pedantic end-to-end usually refers to the fact that in DL models backdrop from a very abstract signal to raw inputs.  I would call your referral of end-to-end “purely supervised” although contrastive learning is usually a self-supervised pretraining task and there is always a fine tuning step for a particular task at the end.']","['I think it depends on the research goal.  Personally I have a follow up question; when are *multple* pretraining phases useful?  A recent example I have seen is V=JEPA2', 'Also being pedantic end-to-end usually refers to the fact that in DL models backdrop from a very abstract signal to raw inputs.  I would call your referral of end-to-end “purely supervised” although contrastive learning is usually a self-supervised pretraining task and there is always a fine tuning step for a particular task at the end.']",0,9,0.43,Discussion,1751181818.0
1ln8lqt,MachineLearning,[R] Breaking LLM Context Limits and Fixing Multi-Turn Conversation Loss Through Human Dialogue Simulation,"Share my solution tui cli for testing, but I need more collaboration and validation Opensource and need community help for research and validation

**Research**
LLMs get lost in multi-turn conversations

**Core Feature**
- Breaking Long Conversation Constraints
By [summary] + [reference pass messages] + [new request] in each turn, being constrained by historical conversation length, thereby eliminating the need to start new conversations due to length limitations.
- Fixing Multi-Turn Conversation Disorientation
Simulating human real-time perspective updates by generating an newest summary at the end of each turn, let conversation focus on the current. Using fuzzy search mechanisms for retrieving past conversations as reference materials, get detail precision that is typically difficult for humans can do.

**Human-like dialogue simulation**
- Each conversation starts with a basic perspective
- Use structured summaries, not complete conversation
- Search retrieves only relevant past messages
- Use keyword exclusion to reduce repeat errors

**Need collaboration with**
- Validating approach effectiveness
- Designing prompt to optimize accuracy for structured summary
- Improving semantic similarity scoring mechanisms
- Better evaluation metrics","['change name to ""Cognitive Imperfect Memory system Prototype"" to match the repo content\n\nprovide tui cli for testing the workflow of system, and mark token used to remind\n\nSupport 2 modes, traditional and CIM, by using `--old` to distinguish']","['change name to ""Cognitive Imperfect Memory system Prototype"" to match the repo content\n\nprovide tui cli for testing the workflow of system, and mark token used to remind\n\nSupport 2 modes, traditional and CIM, by using `--old` to distinguish']",1,1,1.0,Research,1751181314.0
1ln7c28,MachineLearning,[R] Arch-Router - The fastest LLM routing model designed to align to usage preferences,"Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and blindspots. For example:

“Embedding-based” (or simple intent-classifier) routers sound good on paper—label each prompt via embeddings as “support,” “SQL,” “math,” then hand it to the matching model—but real chats don’t stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can’t keep up with multi-turn conversations or fast-moving product scopes.

Performance-based routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: “Will Legal accept this clause?” “Does our support tone still feel right?” Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.

**Arch-Router skips both pitfalls by routing on** ***preferences you write in plain language.*** Drop rules like “contract clauses → GPT-4o” or “quick travel tips → Gemini-Flash,” and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies—no retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.

**Specs**

* **Tiny footprint** – 1.5 B params → runs on one modern GPU (or CPU while you play).
* **Plug-n-play** – points at any mix of LLM endpoints; adding models needs *zero* retraining.
* **SOTA query-to-policy matching** – beats bigger closed models on conversational datasets.
* **Cost / latency smart** – push heavy stuff to premium models, everyday queries to the fast ones.

Exclusively available in Arch (the AI-native proxy for agents): [https://github.com/katanemo/archgw](https://github.com/katanemo/archgw)
🔗 Model + code: [https://huggingface.co/katanemo/Arch-Router-1.5B](https://huggingface.co/katanemo/Arch-Router-1.5B)
📄 Paper / longer read: [https://arxiv.org/abs/2506.16655](https://arxiv.org/abs/2506.16655)","[""Nice idea, and maybe I'm old-fashioned, but 1 GPU just to point to the right model to use still seems quite heavy."", 'nice job! will you try to distill it into a more compact model?']","[""Nice idea, and maybe I'm old-fashioned, but 1 GPU just to point to the right model to use still seems quite heavy."", 'nice job! will you try to distill it into a more compact model?']",22,10,0.82,Research,1751176379.0
1ln6feb,MachineLearning,[D] EMNLP 2025 Discussion Period,"Hi everyone,

How is the discussion period going for you? Have you heard back from any of your reviewers?

For those who are reviewing: can the reviewers change their scores after Jul2? Can they reply to the authors after Jul 2?


thanks!



 ","['After receiving yet another AI generated review, I think I am finally done with ACL. What are venues where you can actually go through a rigorous peer review, instead of this charade?', 'The reviewer who gave a 2.5 rating provided feedback irrelevant to our paper and never responded. The AC also has not engaged despite us sending two messages. Another reviewer mentioned that our rebuttal resolved their concerns and proved the effectiveness of our approach, yet decided not to update their score. The last reviewer simply never responded. Our detailed rebuttal was essentially wasted.  \n\n\nI am beginning to wonder if I myself am being a ""too-kind"" reviewer, as I engage with authors and raise my ratings for nearly all assigned papers. It is painful to see my paper being treated unfairly while I continue trying to be a responsible reviewer.', 'The reviewers can update their scores until July 6.\nTake into account that most reviewers are authors too and need to write their own rebuttals, and it is the weekend so they might look at the author responses from Monday\xa0', 'In my experience ~10% of the reviewers will respond at all, and never before the last day.', 'Just wanted to submit my rebuttal, but OpenReview seems to be down. Any further information on that?', 'Can y\'all log into your author console - ARR? Mine says \n\n""Error\n\n# The server responded with the following message:\n\n    Unexpected token \'o\', ""no healthy upstream"" is not valid JSON""', 'URGENT!!\nAre there anybody here who has been promised that the Overall Assessment would be increased by the reviewer but they only commented this and actually did not raise the score?\nIs flagging this in the Review Issue Report in the Others category a bad idea?']","['After receiving yet another AI generated review, I think I am finally done with ACL. What are venues where you can actually go through a rigorous peer review, instead of this charade?', 'The reviewer who gave a 2.5 rating provided feedback irrelevant to our paper and never responded. The AC also has not engaged despite us sending two messages. Another reviewer mentioned that our rebuttal resolved their concerns and proved the effectiveness of our approach, yet decided not to update their score. The last reviewer simply never responded. Our detailed rebuttal was essentially wasted.  \n\n\nI am beginning to wonder if I myself am being a ""too-kind"" reviewer, as I engage with authors and raise my ratings for nearly all assigned papers. It is painful to see my paper being treated unfairly while I continue trying to be a responsible reviewer.', 'The reviewers can update their scores until July 6.\nTake into account that most reviewers are authors too and need to write their own rebuttals, and it is the weekend so they might look at the author responses from Monday\xa0', 'In my experience ~10% of the reviewers will respond at all, and never before the last day.', 'Just wanted to submit my rebuttal, but OpenReview seems to be down. Any further information on that?']",13,29,1.0,Research,1751172975.0
1ln3v8y,MachineLearning,[D] NeurIPS 2025 reviews release,"First time that I submitted to NeurIPS so excuse me if my question is silly. The NeurIPS site (https://neurips.cc/Conferences/2025/Dates) says that reviewing ends July 2nd and that Author Rebuttals start July 24th.

Does this mean that the reviews will become visible to authors on July 2nd or that we have to wait till the 24th of July to see them?","['You will probably be able to see the reviews at some time around the 24th.', 'This year, our lab also got swept up in the NeurIPS paper frenzy — 27,000+ submissions! We submitted 4 to the main conference and 1 to D&B.\n\nHonestly, writing papers is starting to feel more and more like buying a lottery ticket. My peers joked, “If we get one in, we must have some ancestral blessings.”\n\nBefore submitting, I figured: hey, it’s all about data, right? So why not go data-driven myself? I threw three of our papers into [CSPaper Review](https://review.cspaper.org) (https://review.cspaper.org) and simulated reviews for NeurIPS. The scores? Mostly hovering around 3.\n\n\\- Main conference: 4, 4, 3, 2\n\n\\- D&B: 4\n\nWhat’s even wilder is that one of the simulated reviews felt *weirdly* similar to a review we got *last year*. It’s starting to feel like we’re not just in the age of “papers of the LLM, by the LLM, for the LLM” — it’s becoming “reviews by the LLM” too…\n\nThat said, getting a taste of the (simulated) review beatdown in advance actually helped a lot with expectations. When we were waiting for results, my teammates and I comforted each other with stuff like: “Well, even the AI says it’s meh — not expecting human reviewers to be any kinder.”\n\nFor real though, the best part of [CSPaper Review](https://review.cspaper.org) for me? It helped me give up early — no need to anxiously wait three months to open a mystery box. 😂\n\nWill report back with our *joyful disappointment* once the results are out!']","['You will probably be able to see the reviews at some time around the 24th.', 'This year, our lab also got swept up in the NeurIPS paper frenzy — 27,000+ submissions! We submitted 4 to the main conference and 1 to D&B.\n\nHonestly, writing papers is starting to feel more and more like buying a lottery ticket. My peers joked, “If we get one in, we must have some ancestral blessings.”\n\nBefore submitting, I figured: hey, it’s all about data, right? So why not go data-driven myself? I threw three of our papers into [CSPaper Review](https://review.cspaper.org) (https://review.cspaper.org) and simulated reviews for NeurIPS. The scores? Mostly hovering around 3.\n\n\\- Main conference: 4, 4, 3, 2\n\n\\- D&B: 4\n\nWhat’s even wilder is that one of the simulated reviews felt *weirdly* similar to a review we got *last year*. It’s starting to feel like we’re not just in the age of “papers of the LLM, by the LLM, for the LLM” — it’s becoming “reviews by the LLM” too…\n\nThat said, getting a taste of the (simulated) review beatdown in advance actually helped a lot with expectations. When we were waiting for results, my teammates and I comforted each other with stuff like: “Well, even the AI says it’s meh — not expecting human reviewers to be any kinder.”\n\nFor real though, the best part of [CSPaper Review](https://review.cspaper.org) for me? It helped me give up early — no need to anxiously wait three months to open a mystery box. 😂\n\nWill report back with our *joyful disappointment* once the results are out!']",18,5,1.0,Discussion,1751164090.0
1ln19e6,MachineLearning,[D] How do you deal with messy github repo that doesnt work,"you see a recent paper with great results, they share their github repo (awesome), but then... it just doesn’t work. broken env, missing files, zero docs, and you end up spending hours digging through messy code just to make it run.

then Cursor came in, and it helps! helps a lot! its not lazy (like me) so its diving deep into code and fix stuff, but still, it can take me 30 mints of ping-pong prompting.

how do you tackle this problem?
diving deep into code is a nice time killer, when you want to run 10 different GitHub repos, you want to move fast.. so, **how do you move fast?**","['If the repo is actually missing vital code, you should contact the authors. Trying to guess what’s missing is likely to miss critical bits that won’t reproduce what they did. Plus, authors who do this should get feedback so they improve their artifacts in the future.', ""You move on and assume it's all a lie. I do this for a living and if your repo is jank and is extremely complicated to run reliably then I'll find another solution.\n\nOk so now what happens if this is truly the only option. Well you can try to email the authors. Another option is to open an issue on the repo. Now it's up to you to reverse engineer their paper and salvage what you can from the code.\n\nThis is a huge risk if you are doing this in order to implement something similar or exactly the same in a production system. You might spend a month and still miss something or the author's key details to make the model work properly. Sometimes it can be worth it but many times it's not\n\nAt this point in my career after reviewing papers and obscure repos I just assume you're lying or incompetent in your ability to reproduce the results if the code fundamentally does not work. I'm sure your paper is fine and you might have got great results but that repo you released was not what you used"", 'When this happens, I immediately assume that whatever results they have shared in their paper are fabricated or misleading. That may not actually be the case, but it is the *safest* assumption and the one most likely to prevent you wasting a ton of time and energy on an algorithm or methodology that you can\'t reproduce.\n\nIf the paper has enough ""intuition"" baked into it so you can use it as a springboard for your own implementation, or if you miraculously get the authors to respond and fix their code, all the better. But normally I take this as a sign that I\'m heading down a frustrating path and should look for more mature/stable/reproducible options.\n\nEdit: spelling', ""I usually try to strip down the code to bare minimum (smallest toy example or dry run), nuke away as much stuff that is only used for replicating papers, remove and unpin as many dependencies as possible and see how far I can push while still keeping the code working. I don't bother with needing specific CUDA or gcc versions though. These are just hell."", 'I prioritize clarity, start with a complete code review, and when stuck, I use collaboration tools to quickly triage issues with others.', ""I usually go about this as follows:  \n \\- check whether python/library versions mismatch. Especially major releases of tensorflow, numpy, torch, pandas really caused me problems, so I usually try to install an earlier python version (e.g. 3.9) if I suspect this to be the case  \n \\- get something working on the repo, e.g. a subset of tests, then continue from there  \n \\- fix one error at a time, especially with LLM-assisted coding I try to let the LLM focus on exactly one problem and not let it make any unrelated changes  \n \\- don't spend too much time: if things don't work after 1.5 hours of concentrated work and I don't see it working soon, I usually let it be and move on. The time I spend also has to do with the complexity of the code/problems I face"", 'This is the rule, not the exception.\nAnd even if the code works, or you make it work, you can still expect their ""state-of-the-art results"" to be over fine-tuned so that it won\'t work well on a different dataset.', 'Oh my GOD thank god I’m not the only one who tried to use something and realized it performs wayyyy worse than what was advertised', 'You cry\n\nBut seriously you can fix almost anything, but the commands are non trivial\n\nGit/github is a beast for people like me who just it relatively superficially', 'I haven’t tried this, but what if you add the latex source of the paper to the repo, and add some details in your cursor config to reference the paper?!']","['If the repo is actually missing vital code, you should contact the authors. Trying to guess what’s missing is likely to miss critical bits that won’t reproduce what they did. Plus, authors who do this should get feedback so they improve their artifacts in the future.', ""You move on and assume it's all a lie. I do this for a living and if your repo is jank and is extremely complicated to run reliably then I'll find another solution.\n\nOk so now what happens if this is truly the only option. Well you can try to email the authors. Another option is to open an issue on the repo. Now it's up to you to reverse engineer their paper and salvage what you can from the code.\n\nThis is a huge risk if you are doing this in order to implement something similar or exactly the same in a production system. You might spend a month and still miss something or the author's key details to make the model work properly. Sometimes it can be worth it but many times it's not\n\nAt this point in my career after reviewing papers and obscure repos I just assume you're lying or incompetent in your ability to reproduce the results if the code fundamentally does not work. I'm sure your paper is fine and you might have got great results but that repo you released was not what you used"", 'When this happens, I immediately assume that whatever results they have shared in their paper are fabricated or misleading. That may not actually be the case, but it is the *safest* assumption and the one most likely to prevent you wasting a ton of time and energy on an algorithm or methodology that you can\'t reproduce.\n\nIf the paper has enough ""intuition"" baked into it so you can use it as a springboard for your own implementation, or if you miraculously get the authors to respond and fix their code, all the better. But normally I take this as a sign that I\'m heading down a frustrating path and should look for more mature/stable/reproducible options.\n\nEdit: spelling', ""I usually try to strip down the code to bare minimum (smallest toy example or dry run), nuke away as much stuff that is only used for replicating papers, remove and unpin as many dependencies as possible and see how far I can push while still keeping the code working. I don't bother with needing specific CUDA or gcc versions though. These are just hell."", 'I prioritize clarity, start with a complete code review, and when stuck, I use collaboration tools to quickly triage issues with others.']",44,22,0.76,Discussion,1751155819.0
1ln0oka,MachineLearning,[D] Curious about invitation as ICML reviewer,"I recently helped coauthor a paper submitted to ICML's AI4Math, and I was really surprised when I got email asking to serve as a reviewer (I'm an undergrad and this was my first paper). I probably won't accept since I'm not qualified, but I was curious about how this even happened, are reviewers just randomly selected?","[""Oh there is the reason I receive undergrad level reviews.\n\n\nJoking aside, if you think you haven't read enough papers or think you don't have enough expertise in the topic, please decline it. Good job on your submission.\xa0"", ""It's because you were a co-author. Conferences nowadays usually ask the authors to review."", 'You definitely accept.\n\nThen you ask your mentor how to review and do it.', ""Nowadays they want to force everyone to review because of the high number of submissions, if they caught scent of your email they will invite you, be you qualified or not.\n\n\nUnless there is a strong reason why you want to review just decline it, you probably haven't read enough papers yet as a undergrad student to know how a good paper should look like, so you will just add to the list of awful reviews everyone complains about.""]","[""Oh there is the reason I receive undergrad level reviews.\n\n\nJoking aside, if you think you haven't read enough papers or think you don't have enough expertise in the topic, please decline it. Good job on your submission.\xa0"", ""It's because you were a co-author. Conferences nowadays usually ask the authors to review."", 'You definitely accept.\n\nThen you ask your mentor how to review and do it.', ""Nowadays they want to force everyone to review because of the high number of submissions, if they caught scent of your email they will invite you, be you qualified or not.\n\n\nUnless there is a strong reason why you want to review just decline it, you probably haven't read enough papers yet as a undergrad student to know how a good paper should look like, so you will just add to the list of awful reviews everyone complains about.""]",12,4,0.84,Research,1751154077.0
1lmx6f9,MachineLearning,[D] NVIDIA acquires CentML — what does this mean for inference infra?,"CentML, the startup focused on compiler/runtime optimization for AI inference, was just acquired by NVIDIA. Their work centered on making single-model inference faster and cheaper , via batching, quantization (AWQ/GPTQ), kernel fusion, etc.

This feels like a strong signal: inference infra is no longer just a supporting layer. NVIDIA is clearly moving to own both the hardware and the software that controls inference efficiency.

That said, CentML tackled one piece of the puzzle , mostly within-model optimization. The messier problems : cold starts, multi-model orchestration, and efficient GPU sharing , are still wide open. We’re working on some of those challenges ourselves (e.g., InferX is focused on runtime-level orchestration and snapshotting to reduce cold start latency on shared GPUs).

Curious how others see this playing out. Are we headed for a vertically integrated stack (hardware + compiler + serving), or is there still space for modular, open runtime layers?
","['NVIDIA was always controlling the software part with CUDA, TensorRT libraries. \n\nSW is king ! Intel and AMD failed here.', ""NVidia has been selling solutions for a while. What matters most is data centers.\xa0\n\n\nNVidia has multiple products for management, which can use memory swaps as well for example. I don't know if you guys are more efficient but I do know that everything is use case dependent.\xa0\n\n\nModular is obviously going to be dominant. Training and inference are very different processes.\xa0"", 'So how does CentML work exactly? If I have say a Pytorch model already trained?', 'Nvidia, like west protecting dollar , is protects CUDA. It is only reason NVIDIA has made thus far. Intending to keep it most relevant buy out competition. Should it not be anti trust']","['NVIDIA was always controlling the software part with CUDA, TensorRT libraries. \n\nSW is king ! Intel and AMD failed here.', ""NVidia has been selling solutions for a while. What matters most is data centers.\xa0\n\n\nNVidia has multiple products for management, which can use memory swaps as well for example. I don't know if you guys are more efficient but I do know that everything is use case dependent.\xa0\n\n\nModular is obviously going to be dominant. Training and inference are very different processes.\xa0"", 'So how does CentML work exactly? If I have say a Pytorch model already trained?', 'Nvidia, like west protecting dollar , is protects CUDA. It is only reason NVIDIA has made thus far. Intending to keep it most relevant buy out competition. Should it not be anti trust']",64,12,0.93,Discussion,1751144220.0
1lmqbzc,MachineLearning,[R] OpenEvolve: Automated GPU Kernel Discovery Outperforms Human Engineers by 21%,"Hey folks, wanted to share something interesting I've been working on that might be relevant for folks running models locally on Apple Silicon.

**What I did**

Used evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B's grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.

**Results**

Tested across 20 different inference scenarios against MLX's `scaled_dot_product_attention` baseline:

* **Average decode speed improvement: +12.5%** (σ = 38.3%)
* **Peak improvement: +106%** on repetitive pattern generation
* **Best category: +24.8%** average on general tasks
* **Memory usage: -0.99%** (slight reduction)

**The honest picture:** It's workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with >25% improvements.

**How it works**

The system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:

1. **Perfect SIMD vectorization**: Found that `vec<T, 8>` operations match Apple Silicon's capabilities for 128-dim attention heads
2. **Two-pass online softmax**: Fused softmax normalization with value accumulation, reducing memory bandwidth
3. **GQA-specific memory patterns**: Optimized for the 40:8 head structure with coalesced access patterns

# Why this might matter for local inference

* Shows automated optimization can compete with expert-engineered kernels
* Demonstrates potential for hardware-specific optimizations without manual tuning
* Could be applied to other transformer components or different model architectures
* All open source - you can reproduce and extend this work

**Try it yourself**

The code and all benchmarks are available in the [OpenEvolve repo](https://github.com/codelion/openevolve). The MLX kernel optimization example is at `examples/mlx_metal_kernel_opt/`.

Requirements:

* Apple Silicon Mac
* MLX framework
* Qwen3-0.6B model

# Limitations

* Currently specific to Apple Silicon and this exact model configuration
* Performance improvements are highly workload-dependent
* Takes \~25 evolutionary generations to converge (few hours on M3)
* No guarantees it'll work better for your specific use case

**Technical write-up**

Full details with code diffs and benchmark methodology: [https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery](https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery)

Curious to hear thoughts from folks who've done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.

Has anyone else experimented with automated kernel optimization for local inference?","['I’m surprised we haven’t seen more RL approaches in this space. Kernel development seems like a prime candidate for RL.\xa0', 'Maybe I\'m reading it wrong but the article seems to state that the vec optimization that it ""found"" was almost directly mentioned in the evolution prompt? That doesn\'t seem like it really ""innovated"" that solution? Also where is the outperforming humans metric coming from? There\'s both improvements and regressions in the performance tests.', 'Is there a way to make a system which will adapt to the workload?', 'Right, basically ML outperforms humans, good stuff!', 'Ooooh I was thinking about making something like this and you beat me to the punch! Excited to try it out.', 'Fantastic work! Out of curiosity, what’s the current sota for gpu kernel optimization? Also, can you point me to good literature to get a primer of this space?', 'nice work and summary.']","['I’m surprised we haven’t seen more RL approaches in this space. Kernel development seems like a prime candidate for RL.\xa0', 'Maybe I\'m reading it wrong but the article seems to state that the vec optimization that it ""found"" was almost directly mentioned in the evolution prompt? That doesn\'t seem like it really ""innovated"" that solution? Also where is the outperforming humans metric coming from? There\'s both improvements and regressions in the performance tests.', 'Right, basically ML outperforms humans, good stuff!', 'Is there a way to make a system which will adapt to the workload?', 'Ooooh I was thinking about making something like this and you beat me to the punch! Excited to try it out.']",131,16,0.92,Research,1751126540.0
1lmpw9w,MachineLearning,[D] Evaluating realism/quality of video generation,"What are the industry/research directions being explored?

I’m finding a lot of research related to evaluating how well a generated video adheres to a text prompt but can’t find a lot of research related to quality evaluation(Other than FVD).

From image generation, we know that FID isn’t always a reliable quality metric. But FID also works on a distribution level.

Is there any research on a per-sample level evaluation? Can we maybe frame this as an out-of-distribution problem?",['The big issue is overfitting. It’s basically just an aesthetic model but for realism that’s quickly overfit. You can try using a discriminator but that might be counter to what you actually want.'],['The big issue is overfitting. It’s basically just an aesthetic model but for realism that’s quickly overfit. You can try using a discriminator but that might be counter to what you actually want.'],1,2,0.67,Discussion,1751125426.0
1lmk9uo,MachineLearning,[R] Ragged - : Leveraging Video Container Formats for Efficient Vector Database Distribution,"Longtime lurker and really happy to be writing this post. I'm excited to share a proof of concept I've been working on for efficient vector database distribution called Ragged. In my paper and PoC, I explore leveraging the MP4 video container format to store and distribute high-dimensional vectors for semantic search applications.

The idea behind Ragged is to encode vectors and their metadata into MP4 files using custom tracks, allowing seamless distribution through existing Content Delivery Networks (CDNs). This approach maintains compatibility with standard video infrastructure while achieving comparable search performance to traditional vector databases.

Key highlights of my work include: - A novel encoding scheme for high-dimensional vectors and metadata into MP4 container formats. - CDN-optimized architecture with HTTP range requests, fragment-based access patterns, and intelligent prefetching. - Comprehensive evaluation showing significant improvements in cold-start latency and global accessibility. - An open-source implementation to facilitate reproduction and adoption.

I was inspired by the innovative work of Memvid (https://github.com/Olow304/memvid), which demonstrated the potential of using video formats for data storage. My project builds on this concept with a focus on CDNs and semantic search.

I believe Ragged offers a promising solution for deploying semantic search capabilities in edge computing and serverless environments, leveraging the mature video distribution ecosystem. Also sharing indexed knowledge bases in the form of offline MP4 can unlock a new class of applications.

I'm eager to hear your thoughts, feedback, and any potential use cases you envision for this approach. You can find the full paper and implementation details \[here\](https://github.com/nikitph/ragged).

Thank you for your time fellows",[],[],5,0,1.0,Research,1751109043.0
1lmilpz,MachineLearning,[P] Convolutional Neural Network to predict blooming date,"**Hello everyone!**
I’ve recently been working on a project to study the influence of meteorological variables on the blooming date of plants. To do this, I aim to use a convolutional neural network (CNN) to predict the blooming date and then extract insights using explainability techniques. Let me give you a bit of background:

Each instance in my dataset consists of six time series corresponding to the variables: temperature, humidity, wind speed and direction, radiation, and precipitation. Additionally, I have the species and variety of the plant, along with its geographical location (altitude, latitude, and longitude). The time series start at the moment of leaf fall and span 220 days from that point (so the starting point varies between instances). Each time series contains about 10,000 records, taken at 30-minute intervals. At some point in the middle of the series, blooming occurs. My goal is to predict the number of days from leaf fall to the blooming date.

According to theory, there are two key moments leading to blooming. The first is when the tree enters a phase called *rest*, which begins shortly after leaf fall. The second is when the tree *wakes up*. During the rest phase, the tree accumulates “chill units,” meaning it must spend a certain number of hours below a specific temperature threshold. Once enough chill has accumulated, the tree wakes up and begins accumulating “heat” — a number of hours above a certain temperature. Once the required heat is reached and conditions are optimal, blooming occurs.

For this study, I trained a neural network with the following architecture:

* Two convolutional layers for the time series — first a 1D layer, followed by a 2D layer that mixes the outputs of the 1D layers.
* A dense layer processes the other (non-temporal) variables.
* The outputs from both parts are then concatenated and passed through two additional dense layers.

After training the network, I plan to use several explainability techniques:

* ICE plots (which I’ve adapted to time series),
* SHAP (also adapted as best as I could to time series),
* Attention mechanisms in the convolutional layers.

**Now the questions:**

1. What do you think of the network architecture? Would you change it or use another type of layer, such as LSTM?
2. What other explainability techniques would you recommend? The ICE plots and SHAP help me understand which time ranges are most important and how changes in variables (e.g., temperature) affect the predicted blooming date. It would also be great to detect when the *rest* phase starts and ends. Do you have any ideas on how to approach that? Some studies use Pearson correlation coefficients, but they haven’t been very insightful in my case. Also, if you're familiar with this topic and have suggestions for other interesting questions to explore, I’d love to hear them!

**Thank you so much to anyone reading this — any advice is welcome!**",[],[],4,0,0.83,Project,1751102517.0
1lmg313,MachineLearning,[R] Thought Anchors: Which LLM Reasoning Steps Matter?,https://arxiv.org/abs/2506.19143,"['Do you think this could be used as a post training objective? Like minimize the bloat of reasoning and encourage production of only the useful reasoning components?', 'Not super familiar with this, but isnt cot != actual reasoning circuits as per bio of llm paper?']","['Do you think this could be used as a post training objective? Like minimize the bloat of reasoning and encourage production of only the useful reasoning components?', 'Not super familiar with this, but isnt cot != actual reasoning circuits as per bio of llm paper?']",44,5,0.94,Research,1751092368.0
1lma7l9,MachineLearning,[R] Benchmarking LLMs and MLLMs on extracting financial recommendations from YouTube,"**VideoConviction** is a new benchmark for evaluating LLMs and MLLMs on extracting structured stock recommendations from long and short-form YouTube videos. The dataset contains 6K+ annotated recommendation segments from 288 videos across 22 financial influencer channels, each labeled with ticker, action (buy/sell/hold), and timestamped transcripts.

**Why it’s challenging**:
Finfluencer content is noisy, informal, and multimodal. Models must distinguish actual recommendations from general market talk, disclaimers, and promotions. We test models on both **full videos** and **segmented clips** to assess context sensitivity and noise robustness.

**Modeling takeaways:**

* **LLMs (text-only)** outperform MLLMs on structured extraction when inputs are clean and segmented.
* **MLLMs (text + video)** help with surface-level cues (e.g., identifying stock tickers like AAPL shown on screen) but often underperform on recommendation-level reasoning.
* Segmenting inputs leads to significant F1 gains across models (not a surprise).

**Results**:

* Best LLM (DeepSeek-V3) outperforms MLLMs on full extraction (ticker + action + recommendation conviction).
* \[Finance specific\] Betting against influencer recommendations outperformed the S&P 500 by +6.8% in annual returns, but at higher risk (Sharpe ratio 0.41 vs 0.65).

**Paper**: [https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=5315526](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5315526)
**Dataset**: [https://huggingface.co/datasets/gtfintechlab/VideoConviction](https://huggingface.co/datasets/gtfintechlab/VideoConviction)",[],[],2,0,0.6,Research,1751072298.0
1lm3jlh,MachineLearning,[D] Suggestions on dealing with ICCV rejection,"I recently had a paper rejected by ICCV for being too honest (?). The reviewers cited limitations I explicitly acknowledged in the paper's discussion as grounds for rejection (and those are limitations for similar works too).

To compound this, during the revision period, a disruptive foundational model emerged that achieved near-ceiling performance in our domain, significantly outperforming my approach.

Before consigning this work (and perhaps myself) to purgatory, I'd welcome any suggestions for salvage strategies.

Thank you 🙂","[""> The reviewers cited limitations I explicitly acknowledged in the paper's discussion as grounds for rejection (and those are limitations for similar works too).\n\nWell it is not like if you say something is a limitation it stops being a limitation. \n\nIf you and your advisor/mentor/etc. believe the new model truly (sadly) makes your work meaningless (think really hard on this), might be just better to move on to new ideas and avoid the sunk cost fallacy. Or if you really need the +1 publication for something just submit to lower-tier places and forget about it."", 'Go to another conference/journal and try it there. Peer review as a system is in many cases more like roulette. \n\nRegarding the foundational model stuff ... I do not know in which domain you are but perhaps your approach is more efficient. Throwing LLMs on everything might be the way to go for some, but other care about nice approaches that work on embedded HW.', 'Getting rejected is normal. ICCV has a 25% acceptance rate. The typical course of action is to revise the paper and resubmit to another conference. A lot of papers fail a few times before they are accepted. If you really lost confidence in the paper, you can submit it to an ICCV workshop.', ""It's unfortunate that the reviewers didn't appreciate the honesty in your work. However, it's important to remember that rejection is a part of the research process. You might want to consider revising your paper to address the reviewers' concerns and highlight the unique contributions of your work. Good luck!"", 'Sucks. Sorry to hear. Welcome to peer review. Submit TMLR or similar that doesn’t require cherry picked SOTA.', 'Scooping happens all the time. Best option is to drown the sorrows in more work.\xa0', ""If you share your paper, we might be able to give you more targeted suggestions? Otherwise, I'm not sure what more we can do besides recommending you shop it around to other venues."", 'I agree with some others here that you should \n\n1) Value your work since foundation models aren\'t for every scenario. You can also resubmit to another conference or to a workshop! Honestly I find workshops to be such a good way to meet people in a less noisy setting. \n\n2) I don\'t think forgetting about your thoughtful work is a good idea. Someone will really appreciate it. Sometimes the best papers aren\'t ones published at the ""best"" conferences.', '>a disruptive foundational model emerged that achieved near-ceiling performance in our domain\n\nThat might be an issue, you have to present \\*some\\* advantage over the foundation model. Hopefully yor model runs much more quickly and with fewer resources, in that case you would have at the very least to add metrics related to resources and FLOPs used to your experimental evaluation to show that you do not overperform them in raw performance but there are other advantages.\n\nIf your model is as big as the foundation model and uses the same resources you might have to give up on this work and send it to a workshop or a much lower impact conference.\n\nAbout the ""limitations as grounds for rejection"", it really depends on what you are talking about so you have to sit and think if the reviewers were correct or no, I have seen authors insisting until their death that they ""could not compare against"" certain approaches I knew they could relatively easily. If those are real limitations that no one could reasonably overcome you can just move on for the next conference and hope you get better reviewers.', 'Incorporate the suggestions and try submitting to relevant ICCVW?']","[""> The reviewers cited limitations I explicitly acknowledged in the paper's discussion as grounds for rejection (and those are limitations for similar works too).\n\nWell it is not like if you say something is a limitation it stops being a limitation. \n\nIf you and your advisor/mentor/etc. believe the new model truly (sadly) makes your work meaningless (think really hard on this), might be just better to move on to new ideas and avoid the sunk cost fallacy. Or if you really need the +1 publication for something just submit to lower-tier places and forget about it."", 'Go to another conference/journal and try it there. Peer review as a system is in many cases more like roulette. \n\nRegarding the foundational model stuff ... I do not know in which domain you are but perhaps your approach is more efficient. Throwing LLMs on everything might be the way to go for some, but other care about nice approaches that work on embedded HW.', 'Getting rejected is normal. ICCV has a 25% acceptance rate. The typical course of action is to revise the paper and resubmit to another conference. A lot of papers fail a few times before they are accepted. If you really lost confidence in the paper, you can submit it to an ICCV workshop.', ""It's unfortunate that the reviewers didn't appreciate the honesty in your work. However, it's important to remember that rejection is a part of the research process. You might want to consider revising your paper to address the reviewers' concerns and highlight the unique contributions of your work. Good luck!"", 'Sucks. Sorry to hear. Welcome to peer review. Submit TMLR or similar that doesn’t require cherry picked SOTA.']",29,13,0.91,Research,1751054420.0
1llzcu1,MachineLearning,[R] Potemkin Understanding in Large Language Models,[https://arxiv.org/pdf/2506.21521](https://arxiv.org/pdf/2506.21521),"['I feel like they only evaluated older weaker models. \n\no3 gets all questions in figure 3 correct. I get the following answers:\n\n1. Triangle length: 6 (correct)\n2. Uncle-nephew: no (correct)\n3. Haiku: Hot air balloon (correct)', '>  As the game theory domain\nrequires specialized knowledge, we recruited Economics\nPhD students to produce true and false instances. For the\npsychological biases domain, we gathered 40 text responses\nfrom Reddit’s “r/AmIOverreacting” thread, annotated by\nexpert behavioral scientists recruited via Upwork.', ' It would be interesting to see how these findings could be addressed to improve model performance']","['I feel like they only evaluated older weaker models. \n\no3 gets all questions in figure 3 correct. I get the following answers:\n\n1. Triangle length: 6 (correct)\n2. Uncle-nephew: no (correct)\n3. Haiku: Hot air balloon (correct)', '>  As the game theory domain\nrequires specialized knowledge, we recruited Economics\nPhD students to produce true and false instances. For the\npsychological biases domain, we gathered 40 text responses\nfrom Reddit’s “r/AmIOverreacting” thread, annotated by\nexpert behavioral scientists recruited via Upwork.', ' It would be interesting to see how these findings could be addressed to improve model performance']",9,6,0.91,Research,1751044236.0
1llwfn6,MachineLearning,[P] Built an AI-powered RTOS task scheduler using semi-supervised learning + TinyTransformer,"I'm still not even in my second year of undergrad, but I wanted to share a recent experiment I did as part of an assignment. I took it way further than required.

**Problem:**
RTOS schedulers often miss deadlines when task loads become unpredictable. There's not much real workload data available, so I had to generate synthetic task profiles.

**What I built:**
I created **SILVER\_CS**, a real-time task scheduler that uses a TinyTransformer model trained with semi-supervised learning and curriculum training. The model learns task patterns and adapts scheduling decisions over time.

* Trained on synthetic datasets simulating RTOS behavior
* Deployed as a lightweight scheduler on a simulated RTOS
* Achieved 13–14% fewer missed deadlines compared to traditional heuristics

Also visualized the model’s learned clustering using t-SNE (silhouette score: 0.796) to validate internal representations.

This is part of me experimenting with using AI on resource-constrained systems (RTOS, microcontrollers, edge devices).
Would love to hear feedback or thoughts on how others have tackled scheduling or AI in embedded systems.

https://preview.redd.it/knorrqx7lh9f1.png?width=1919&format=png&auto=webp&s=79d94b38c84fae4ef703f28580c4be62abb69e71

https://preview.redd.it/nnjd4px7lh9f1.png?width=1918&format=png&auto=webp&s=7e19f4fd16abb502caa1f88a2ecd23dc53e7b0f7

https://preview.redd.it/76buw1y7lh9f1.png?width=1919&format=png&auto=webp&s=8435844fe5ff8845d42a14427005367c74c93722

https://preview.redd.it/pm9hp1y7lh9f1.png?width=1919&format=png&auto=webp&s=d4e1a076436f00a19a7950a34f315f96249ab1b0

https://preview.redd.it/0fp5x5y7lh9f1.png?width=1919&format=png&auto=webp&s=2a80e9a203964d8066fc3fa667d93c8801f7ce33

","['Great work; this is part of a large, untapped optimization potential in OS optimization and traditional programming. Hardcoded decisions are often suboptimal and can be learned during runtime.']","['Great work; this is part of a large, untapped optimization potential in OS optimization and traditional programming. Hardcoded decisions are often suboptimal and can be learned during runtime.']",7,4,0.82,Project,1751037310.0
1llqoj1,MachineLearning,[R] Enigmata: Scaling Logical Reasoning In LLMs With Synthetic Verifiable Puzzles,,[],[],7,0,0.74,Research,1751020852.0
1llo6mr,MachineLearning,The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units,,"['The interpretation of this publication assumes a basic foundational understanding of linear algebra, information theory, and constrained optimization. It’s not intended for readers whose primary interest lies in the direct application of machine learning or LLM to practical problems. For those interested in implementation, a Python class implementing an optimizer based on the concepts discussed in this paper will be made available on GitHub soon.', 'Here is the related optimizer code: [https://github.com/oswaldoludwig/kappaTune](https://github.com/oswaldoludwig/kappaTune)']","['The interpretation of this publication assumes a basic foundational understanding of linear algebra, information theory, and constrained optimization. It’s not intended for readers whose primary interest lies in the direct application of machine learning or LLM to practical problems. For those interested in implementation, a Python class implementing an optimizer based on the concepts discussed in this paper will be made available on GitHub soon.', 'Here is the related optimizer code: [https://github.com/oswaldoludwig/kappaTune](https://github.com/oswaldoludwig/kappaTune)']",4,2,0.67,Research,1751010810.0
1llo5nt,MachineLearning,"[D] Thinking, Fast and Slow","To the theorists in the community, how do you balance
1. engaging with theory research - which is usually a slow process requiring deep thinking
2. with programming - which is fast-paced, iterative process with quick feedback?
I'm finding switching between the two thinking modes very hard to balance.","[""Not in theory but my work oftentimes contain a mathy part and a codey part.\n\nThis is what I did in graduate school:\n\nSplit by days.\n\nM W F I code.\n\nT Th I think.\n\nIn my thinking days I don't even turn on my computer. Just me with paper and whiteboard, and a nice cup of tea."", ""I find you need to be disciplined about always setting aside time to read papers and explore new ideas. You're right that once you find a potentially productive path, the impetus is to just run experiments and produce results. Particularly given pressues in the field and by supervisors etc to demonstrate progress. But if that's all you do, you pay the price for that long term by having no new ideas. Ultimately it's literally just an exploration vs exploitation tradeoff."", 'Do theory for a few days till my head threatens to explode. Code for a few days till I become frustrated or bored. Repeat.', 'Well my job is more coding oriented but I try to read 1 paper per week which I think is good enough for me.', ""I can't really catch up with huggingface papers and my work requires a lot of coding. I end up using old models like linear regression and I guess that's fine because it does the job."", ""One thing that helps me with the slow thinking part is to allow my unconscious mind to work; so I try to interleave programming with thinking about the theory and algorithmic components of my projects in a way that I think about them when I sleep.\n\nMy programming time really is a fatigue-reduction time, I can program for long periods of time but I can only think theoretically/algorithmically for a few hours at a time before I literally feel the brainfog.\n\nTo answer your question about balance, for me, I spend as much time as I physically can on deep thinking (which is at most 3-5 hours a day), and once I'm fatigued, I switch to programming and hope that I come up with proofs in my sleep. It has worked well so far!"", ""pick and choose your coding deep dives.  Know when 'cool, I get that now' happens from coding and you should keep on with research and thinking.\n\nSave the deep dives for when you are actually moving beyond understanding and moving into a specific project, piece of research, or product."", ""Simple solution, I don't code (unless it is absolutely inevitable because reviewers start whining)"", 'Seasonal planning. Instead of splitting up days or weeks into thinking vs coding, I like to have longer periods—this works well with conference deadlines. Before a deadline, I would spend my time coding and focused on one or two ideas; after a deadline, I would spend my time exploring new ideas and reading papers.', 'I’m also slow to context shift, but I haven’t found it to be consequential.  I split my time into half days and pause before each slot to decide what is highest priority.  \n\nOften I don’t end up shifting.  But when I do, the pause around lunch time is enough to reset me for a different way of working.']","[""Not in theory but my work oftentimes contain a mathy part and a codey part.\n\nThis is what I did in graduate school:\n\nSplit by days.\n\nM W F I code.\n\nT Th I think.\n\nIn my thinking days I don't even turn on my computer. Just me with paper and whiteboard, and a nice cup of tea."", ""I find you need to be disciplined about always setting aside time to read papers and explore new ideas. You're right that once you find a potentially productive path, the impetus is to just run experiments and produce results. Particularly given pressues in the field and by supervisors etc to demonstrate progress. But if that's all you do, you pay the price for that long term by having no new ideas. Ultimately it's literally just an exploration vs exploitation tradeoff."", 'Do theory for a few days till my head threatens to explode. Code for a few days till I become frustrated or bored. Repeat.', 'Well my job is more coding oriented but I try to read 1 paper per week which I think is good enough for me.', ""I can't really catch up with huggingface papers and my work requires a lot of coding. I end up using old models like linear regression and I guess that's fine because it does the job.""]",50,17,0.87,Discussion,1751010695.0
1llhl65,MachineLearning,[R] EMNLP 2025: reply to reviewers disabled,"Hi all,
I would like to check whether anyone is facing same issue as myself. It seems that I cannot add an official comment in my submission. I can currently see only the author-editor confidential comment option. Has anyone managed to submit their replies?

thanks for the help!","[""Haven't submitted a comment yet but the button is there and it opens the regular fields to add an official comment."", 'URGENT!!\nAre there anybody here who has been promised that the Overall Assessment would be increased by the reviewer but they only commented this and actually did not raise the score?\nIs flagging this in the Review Issue Report in the Others category a bad idea?']","[""Haven't submitted a comment yet but the button is there and it opens the regular fields to add an official comment."", 'URGENT!!\nAre there anybody here who has been promised that the Overall Assessment would be increased by the reviewer but they only commented this and actually did not raise the score?\nIs flagging this in the Review Issue Report in the Others category a bad idea?']",4,3,0.76,Research,1750988045.0
1ll6nsq,MachineLearning,[D] EMNLP 2025 Paper Reviews,Reviews are released! Lets have fun and discuss them here!,"['2, 2, 2.5. No chance right?', ""Dear any reviewers who can check this message,\n\ncan you please reply with the responses during the rebuttal process from the authors? we are still waiting for you guys and it's only 4 hour left for the discussion phase.... thank you"", 'The rebuttal deadline is 7/2 AoE correct?  \nIs anyone else not being able to add/edit comments to the reviews anymore? \n\nI added a comment to one of the reviewers but the edit/add button disappeared for all of them right after. Any ideas?', 'is the deadline to reply to the reviewers over? It was mentioned as ""the end of 2nd July 2025 AoE"". I responded 2 days back, and kept on requesting the reviewers to engage in discussion. Only 1 reviewer responded 2 hrs back. I was preparing my response to his comments, but I saw that ""Official comment"" button is not there.', 'Our two papers got:  \nFirst: 4/3/3.5  \nSecond:  4/3/2  \nWhat do you think?', 'Posting scores for reference:\n\n* OA: 3.5\xa0/ Confidence: 4 \n* OA: 4\xa0/ Confidence: 4\n* OA: 3.5\xa0/ Confidence: 5\n\n**Average\xa0Overall Assessment:**\xa03.67\xa0(Min:\xa03.5, Max:\xa04)  \n**Average Confidence:**\xa04.33\xa0(Min:\xa04, Max:\xa05)  \n**Paper Type:**\xa0Long  \n**Research Area:**\xa0Ethics, Bias, and Fairness\n\nWith exactly the same OA scores, my other paper got accepted to main ACL in the previous ARR cycle (Applications Track), so I believe/hope that this one will also go to main. \n\nFor the previous round, there was a platform where people submitted their scores in order to collect scores and calculate a distribution. Does anyone know if there is also something for this round?', 'Did anyone had scores revised from reviewers as part of rebuttal process. I have observed that 8 out of 10 times, the reviewers will not even see the rebuttal comments and once they will write a comment that they have seen comments but not changing scores.... it is very rare that someone will read/ ask more questions/ engage and update on merit.  Or is it only me who has been unlucky and others have been able to engage and get the updates.....', 'Anybody got a reply or change of score from reviewers?', 'Overall 3 3.5 4\nConf 3 3 4\n\nWhat are the chances for main? What are the chances for findings?', ""Are the reviews generally of a higher quality, and less random, than reviews for ICLR/ICML/NeurIPS?\n\nHaven't submitted to any of the ACL conferences yet, but would like to next cycle.""]","['2, 2, 2.5. No chance right?', ""Dear any reviewers who can check this message,\n\ncan you please reply with the responses during the rebuttal process from the authors? we are still waiting for you guys and it's only 4 hour left for the discussion phase.... thank you"", 'is the deadline to reply to the reviewers over? It was mentioned as ""the end of 2nd July 2025 AoE"". I responded 2 days back, and kept on requesting the reviewers to engage in discussion. Only 1 reviewer responded 2 hrs back. I was preparing my response to his comments, but I saw that ""Official comment"" button is not there.', 'Our two papers got:  \nFirst: 4/3/3.5  \nSecond:  4/3/2  \nWhat do you think?', 'The rebuttal deadline is 7/2 AoE correct?  \nIs anyone else not being able to add/edit comments to the reviews anymore? \n\nI added a comment to one of the reviewers but the edit/add button disappeared for all of them right after. Any ideas?']",28,150,0.94,Discussion,1750960053.0
1ll69g0,MachineLearning,[R] You can just predict the optimum (aka in-context Bayesian optimization),"Hi all,

I wanted to share a blog post about our recent AISTATS 2025 paper on using Transformers for black-box optimization, among other things.

TL;DR: We train a Transformer on millions of synthetically generated (function, optimum) pairs. The trained model can then predict the optimum of a new, unseen function in a single forward pass. The blog post focuses on the key trick: how to efficiently generate this massive dataset.

* **Blog post:** [https://lacerbi.github.io/blog/2025/just-predict-the-optimum/](https://lacerbi.github.io/blog/2025/just-predict-the-optimum/)
* **Paper:** Chang et al. (AISTATS, 2025) [https://arxiv.org/abs/2410.15320](https://arxiv.org/abs/2410.15320)
* **Website:** [https://acerbilab.github.io/amortized-conditioning-engine/](https://acerbilab.github.io/amortized-conditioning-engine/)

Many of us use Bayesian Optimization (BO) or similar methods for expensive black-box optimization tasks, like hyperparameter tuning. These are iterative, sequential processes. We had an idea inspired by the power of in-context learning shown by transformer-based meta-learning models such as Transformer Neural Processes (TNPs) and Prior-Fitted Networks (PFNs): what if we could frame optimization (as well as several other machine learning tasks) as a massive prediction problem?

For the optimization task, we developed a method where a Transformer is pre-trained to learn an implicit ""prior"" over functions. It observes a few points from a new target function and directly outputs its prediction as a distribution over the location and value of the optimum. This approach is also known as ""amortized inference"" or meta-learning.

The biggest challenge is getting the (synthetic) data. How do you create a huge, diverse dataset of functions and their known optima to train the Transformer?

The method for doing this involves sampling functions from a Gaussian Process prior in such a way that we know where the optimum is and its value. This detail was in the appendix of our paper, so I wrote the blog post to explain it more accessibly. We think it’s a neat technique that could be useful for other meta-learning tasks.","['when you add the convex bowl to the synthetic samples in order to give yourself high probability for knowing the minimum, how do you guarantee the sample is still statistically similar to a normal GP prior sample?', 'Would be interesting to test this out in fields where a large corpus of knowledge already exists. E.g. train on materials databases or drug databases.\xa0', ""It's a cool idea! How would you encode hyperparameter structure (e.g. conditional independence) in your model? I've used TPE for that, but it's not always the best method."", 'This is an interesting idea! Also I was going to be mad that your paper had a meme name and was pleasantly surprised when the paper title actually described the method so good job :)', 'I have been doing black box optimization for years now. For a second I was actually scared you might have killed the entire field.']","['when you add the convex bowl to the synthetic samples in order to give yourself high probability for knowing the minimum, how do you guarantee the sample is still statistically similar to a normal GP prior sample?', ""It's a cool idea! How would you encode hyperparameter structure (e.g. conditional independence) in your model? I've used TPE for that, but it's not always the best method."", 'I have been doing black box optimization for years now. For a second I was actually scared you might have killed the entire field.', 'Would be interesting to test this out in fields where a large corpus of knowledge already exists. E.g. train on materials databases or drug databases.\xa0', 'This is an interesting idea! Also I was going to be mad that your paper had a meme name and was pleasantly surprised when the paper title actually described the method so good job :)']",93,14,0.93,Research,1750959146.0
1ll5agc,MachineLearning,[D] emnlp 2025 review,"
I just received my emnlp reviews . Not sure how to proceed with it. I am too scared!!

Paper 1 :

OA: 2.5 ,1.5,3

Confidence 3,3,3

Paper 2:

OA: 2.5,2,3

Confidence: 3,2,3

Please help me sharing your thoughts and experiences.

Thanks

","[""There's no need to be scared. The only thing for you to do is write the strongest possible rebuttal and try to convince the reviewers.\n\nRegarding the venue: ARR is a special system that tries to categorize papers into three buckets (main, findings, rejection) using a 1-5 scale. Random outcomes happen more frequently here due to a lack of clear discrimination. I've seen 2.5 papers getting into main and 3.5 papers getting into findings. Anything can happen in ARR.\n\nIt's better to think of ARR as a journal submission. You get reviews to improve your work, can resubmit every 2-3 months, and let (optionally the same batch of) reviewers see your improvements. Finally, you commit your best ARR submission and hope for the best."", 'Long Paper, *information extraction*  \nO.A. 3.5, 3.5, 2  \n**Confidence**: 3,3,4   \n**Average\xa0Overall Assessment:**\xa03.00\xa0(Min:\xa02, Max:\xa03.5)  \n**Average Confidence:**\xa03.33\xa0(Min:\xa03, Max:\xa04)  \nIs there any chance for Findings/ Main?', 'What is the format for writing rebuttals? First time submission so also curious!', ""Dear any reviewers who can check this message,\n\ncan you please reply with the responses during the rebuttal process from the authors? we are still waiting for you guys and it's only 4 hour left for the discussion phase.... thank you"", 'URGENT!!\nAre there anybody here who has been promised that the Overall Assessment would be increased by the reviewer but they only commented this and actually did not raise the score?\nIs flagging this in the Review Issue Report in the Others category a bad idea?']","[""There's no need to be scared. The only thing for you to do is write the strongest possible rebuttal and try to convince the reviewers.\n\nRegarding the venue: ARR is a special system that tries to categorize papers into three buckets (main, findings, rejection) using a 1-5 scale. Random outcomes happen more frequently here due to a lack of clear discrimination. I've seen 2.5 papers getting into main and 3.5 papers getting into findings. Anything can happen in ARR.\n\nIt's better to think of ARR as a journal submission. You get reviews to improve your work, can resubmit every 2-3 months, and let (optionally the same batch of) reviewers see your improvements. Finally, you commit your best ARR submission and hope for the best."", 'Long Paper, *information extraction*  \nO.A. 3.5, 3.5, 2  \n**Confidence**: 3,3,4   \n**Average\xa0Overall Assessment:**\xa03.00\xa0(Min:\xa02, Max:\xa03.5)  \n**Average Confidence:**\xa03.33\xa0(Min:\xa03, Max:\xa04)  \nIs there any chance for Findings/ Main?', 'What is the format for writing rebuttals? First time submission so also curious!', ""Dear any reviewers who can check this message,\n\ncan you please reply with the responses during the rebuttal process from the authors? we are still waiting for you guys and it's only 4 hour left for the discussion phase.... thank you"", 'URGENT!!\nAre there anybody here who has been promised that the Overall Assessment would be increased by the reviewer but they only commented this and actually did not raise the score?\nIs flagging this in the Review Issue Report in the Others category a bad idea?']",14,9,0.75,Discussion,1750956922.0
1ll2wer,MachineLearning,[D] Can split learning impact XAI compared same model trained in central server?,"Thinking to do research in this direction, currently learning about split learning and XAI. Do you think it is a good research question to explore? ",[],[],0,1,0.22,Research,1750951313.0
1ll1wm9,MachineLearning,"[N] $1M in grants for AI projects advancing truth-seeking, deadline July 1",Cool new grant program that is funding AI prototypes that help advance human knowledge + open inquiry (Cosmos Institute + FIRE) [https://cosmosgrants.org/truth](https://cosmosgrants.org/truth),[],[],0,0,0.47,News,1750948959.0
1ll100q,MachineLearning,[D] Budget cut in USA? Impact on conference?,"Due to the recent budget cuts in the USA, do you think organizers should consider a hybrid conference?

",[],[],0,1,0.27,Discussion,1750946717.0
1lktj7p,MachineLearning,[D] Did you get Neurips reviews assignments?,I just realized that I never got any papers assigned which I found a bit odd given the extreme number of submissions. Did they forget about me? ,"[""Yeah I didn't get any either. Screw them man, we'll make our own machine learning conference, you and me, and we'll review all the papers ourselves. How does that strike you?"", 'Review submission is done based on many factors:\n1. Number of submissions in your expertise.\n2. Your rank among other potential reviewers.\n3. Your prior publications in similar conferences in the same topic/domain.\n\n\n\nAnd many more. You have not received papers for review because NeurlPS might have received fewer papers in your expertise or someone with stronger research profile than you got best match. Feel to consider other factors also :)\n\n\nThe Toranto paper matching system is complicated I guess. This happened to me in past few conferences and few times I got a chance to become ""an emergency reviewer"" ;)', 'They don\'t care about you. They\'ve been crying about reviewer pressure, but it\'s an insider game only, ""peer review"" is a huge part of gatekeeping in academia.', 'I have reviewed for neurips for many years and it’s also the first time they did not assigned reviews. I wrote to the PC and apparently my help was not needed. Some other colleagues also have either 0 reviews or plenty. I don’t know how it’s handled this year', ""I got assigned 5 papers. Would've loved to have shared the love."", 'Chose to review 3 but got assigned 4. The system is stupid as f.', ""I didn't get it too."", 'I reviewed last year (did not submit) and got excellent feedback from an AC who wanted to send me more papers (declined) and I submitted a paper this year but I did not get an invitation to review this year. I found it odd too.', 'I got 6, looking forward to dozens of open review emails every day in August...', 'I got 5, want some of mine? (kidding)']","[""Yeah I didn't get any either. Screw them man, we'll make our own machine learning conference, you and me, and we'll review all the papers ourselves. How does that strike you?"", 'Review submission is done based on many factors:\n1. Number of submissions in your expertise.\n2. Your rank among other potential reviewers.\n3. Your prior publications in similar conferences in the same topic/domain.\n\n\n\nAnd many more. You have not received papers for review because NeurlPS might have received fewer papers in your expertise or someone with stronger research profile than you got best match. Feel to consider other factors also :)\n\n\nThe Toranto paper matching system is complicated I guess. This happened to me in past few conferences and few times I got a chance to become ""an emergency reviewer"" ;)', 'They don\'t care about you. They\'ve been crying about reviewer pressure, but it\'s an insider game only, ""peer review"" is a huge part of gatekeeping in academia.', 'I have reviewed for neurips for many years and it’s also the first time they did not assigned reviews. I wrote to the PC and apparently my help was not needed. Some other colleagues also have either 0 reviews or plenty. I don’t know how it’s handled this year', ""I got assigned 5 papers. Would've loved to have shared the love.""]",39,18,0.93,Research,1750921373.0
1lksg4d,MachineLearning,[D] Attention heatmap visualization tools?,"Are there any tools for easily visualizing attention weights with heatmaps for huggingface models? I couldn't really find any tools for doing this so I've just been using seaborn but it gets messy for really long contexts. Ideally I'd just be able to upload a file of a string representation of the attention weights tensor along with the tokens at each index and be able to toggle between attention heads/model layer and also be able to drag/zoom.

Thanks!",[],[],5,0,1.0,Discussion,1750917263.0
1lkq2zw,MachineLearning,[D] Suggestions on dealing with rejections,"Lately I wrote a paper on video restorations, and in fact the method did extremely well on all SOTA methods and over 6 different tasks

But for some reason the reviewers claiming its incremental or same as previous

This paper I wrote in last year submitted directly a draft to Wacv round 2 and got 4 3 2

Then CVPR 4 3 3

Then all of sudden ICCV 2 3 2 2

Now I am just feeling dumb about my work. Not sure if I should just leave as it is in Arxiv or do further submissions.

Honestly any suggestions guys in this situation.

Thanks 🙂","[""I got a paper rejected 6 times (WACV 23 -> TIP -> BMVC 23 -> WACV 24 -> TIST -> WACV 25) before finally settling for a lesser conference where it got accepted. To be honest I think the results were good specially during the first submission in Q3 2022 but after the first few revisions it's like swimming against the current, specially on ML where there's new methods coming up daily making the strengths / advantage of your method become increasingly small.\n\nRegardless, the first few revisions are still important and you got to accept that there's a lot of randomness in the process you cannot control so just take any positive feedback and ignore the rest.\n\nAfter that, I would say an important part of doing research is know when to cut your losses; after 2/3 tier-A submissions I would suggest tier-B or workshop at Tier-A.\n\nAlso, there are topics that are harder for people to accept, specially if they challenge current assumptions or the paper is written in an unconventional way, so not everything is all about results. The selection of target and the writing are also important."", ""Send it to a journal. It's not necessarily easier but journals tend to tell you to make certain improvements and once you do then the paper is accepted. It's not necessarily your or your work's fault but if you take too long to publish it will be obsolete, so better to just follow the safer route now."", ""You're most likely suffering from writing where the significance were not felt by the reviewers.\n\n I have video explaining how this might happen, but unfortunately it is in Chinese. Lmk if it'll be useful.\n\nAlso, put it on arxiv ASAP. And share this work with others. You can farm citation already."", 'I went through the exact same WACV -> CVPR -> ICCV where I finally got accepted with great scores. What we did was mostly change the narrative and framing based on the reviews. So if reviewers are confused about incrementatlity you got to work out a different story where it does not sound like an incremental step but instead like a larger novel step. We did not rerun any experiments and only added a few more to clear up confusion and the rewriting did the job.', '4 3 3 at CVPR is good, you just got unlucky sadly', 'I would say, try a Journal (you have time and more feedback) or just go for an Workshop at ICCV for now!  \nThere are a lot of cool ones about Video!   \nI feel it also depends on the feedbck you have been revieving, if it is interesting but does seem to have as many results or not novel enough, I woul say try to submit it to an Workshop.  within 10 days you will have the answer. Then if things do not go well, submite to a journal', 'Similar situation here. What is frustrating is the lack of feedback. If some of you would be open to give me some feedback on how I could improve that could be grand!', ""Dealing with rejection is difficult. However, the best way forward is to look at the reviews and identify what is useful to help improve the paper. \n\nSometimes, the reviewer points out valid issues, which should be addressed as much as possible. \n\nAt other times, the reviewers are simply wrong and misunderstand everything. However, this is also a strong signal that you should improve the writing of the paper. Because, as much as they can be frustrating, reviewers are peers and are probably how peers would read the paper. If reviews don't follow the main points, it's quite likely that other readers won't either.""]","[""I got a paper rejected 6 times (WACV 23 -> TIP -> BMVC 23 -> WACV 24 -> TIST -> WACV 25) before finally settling for a lesser conference where it got accepted. To be honest I think the results were good specially during the first submission in Q3 2022 but after the first few revisions it's like swimming against the current, specially on ML where there's new methods coming up daily making the strengths / advantage of your method become increasingly small.\n\nRegardless, the first few revisions are still important and you got to accept that there's a lot of randomness in the process you cannot control so just take any positive feedback and ignore the rest.\n\nAfter that, I would say an important part of doing research is know when to cut your losses; after 2/3 tier-A submissions I would suggest tier-B or workshop at Tier-A.\n\nAlso, there are topics that are harder for people to accept, specially if they challenge current assumptions or the paper is written in an unconventional way, so not everything is all about results. The selection of target and the writing are also important."", ""Send it to a journal. It's not necessarily easier but journals tend to tell you to make certain improvements and once you do then the paper is accepted. It's not necessarily your or your work's fault but if you take too long to publish it will be obsolete, so better to just follow the safer route now."", ""You're most likely suffering from writing where the significance were not felt by the reviewers.\n\n I have video explaining how this might happen, but unfortunately it is in Chinese. Lmk if it'll be useful.\n\nAlso, put it on arxiv ASAP. And share this work with others. You can farm citation already."", 'I went through the exact same WACV -> CVPR -> ICCV where I finally got accepted with great scores. What we did was mostly change the narrative and framing based on the reviews. So if reviewers are confused about incrementatlity you got to work out a different story where it does not sound like an incremental step but instead like a larger novel step. We did not rerun any experiments and only added a few more to clear up confusion and the rewriting did the job.', '4 3 3 at CVPR is good, you just got unlucky sadly']",28,20,0.87,Research,1750909173.0
1lkkmqw,MachineLearning,[R] Any proxy methods for labeling indirect/implicit emotions without human annotators?,"I’m working on a research project involving a manually curated dataset that focuses on workplace scenarios. I need to label data for implicit emotions but I don’t have access to human annotators (psychologist or someone who does this kind of work) this task. The dataset will be used on an LLM.

Are there any reliable proxy methods or semi-automated approaches I can use to annotate this kind of data for a study? I’m looking for ways that could at least approximate human intuition. Any leads or suggestions will be super helpful.
Thanks in advance! ","[""Would this be helpful? [ascertain dataset](https://ascertain-dataset.github.io)\n\nSays they include a bunch of biometrics + self reporting to help verify. You could also ignore those features and just use the labels---not sure what modality you're looking to train on."", ""Unsupervised learning. Can at least organize groups and speed up annotation.\n\nYou didn't tell us what kind of data you have but a good pre trained model could allow you to embed each sample, you use unsupervised learning to organize (UMAP would be my first pick) and then you see if you can rapidly label the organized data."", ""There's a cool method based on weak supervision: http://ai.stanford.edu/blog/weak-supervision/\n\nSnorkel or whatever open source alternative you can find might help a lot""]","[""Would this be helpful? [ascertain dataset](https://ascertain-dataset.github.io)\n\nSays they include a bunch of biometrics + self reporting to help verify. You could also ignore those features and just use the labels---not sure what modality you're looking to train on."", ""Unsupervised learning. Can at least organize groups and speed up annotation.\n\nYou didn't tell us what kind of data you have but a good pre trained model could allow you to embed each sample, you use unsupervised learning to organize (UMAP would be my first pick) and then you see if you can rapidly label the organized data."", ""There's a cool method based on weak supervision: http://ai.stanford.edu/blog/weak-supervision/\n\nSnorkel or whatever open source alternative you can find might help a lot""]",3,10,0.71,Research,1750893269.0
1lkedb8,MachineLearning,[D] Paperswithcode has been compromised,"I was randomly looking at the papers on CIFAR when I opened the website to see an aggregated list and saw that all the text had been replaced with spam text.

I have archived the URLs for a bunch of the datasets for reference:

[https://archive.is/2Si8H](https://archive.is/2Si8H)

[https://archive.is/KJCx1](https://archive.is/KJCx1)

[https://archive.is/ZDBL5](https://archive.is/ZDBL5)

[https://archive.is/BHVsk](https://archive.is/BHVsk)

[https://archive.is/b9xUp](https://archive.is/b9xUp)

[https://archive.md/8BLVA](https://archive.md/8BLVA)

[https://archive.md/SmoCt](https://archive.md/SmoCt)

[https://archive.md/5UZLu](https://archive.md/5UZLu)

edit: added more examples","['I know it was down for a week.', 'Any clue yet, as to, what and why tf it happened?\n\n  \nWhat can anybody have against papers with code, I mean, seriously..', 'What are the chances it was just through social engineering? 80, 90%?', 'Tried to go yesterday and it was down, damn', 'the website seems dead for a while now, nothing is updated anymore', 'I hope it gets better. It would be such a pity to lose such a great website.', ""Oh my god. I saw that it was down yesterday but didn't realise it was down because of a malicious attack"", 'Tried to go 2 days ago and was down :(', 'My guess is some bot crawled and broke bad passwords.\xa0', 'Paper with code is completely down now']","['I know it was down for a week.', 'Any clue yet, as to, what and why tf it happened?\n\n  \nWhat can anybody have against papers with code, I mean, seriously..', 'What are the chances it was just through social engineering? 80, 90%?', 'Tried to go yesterday and it was down, damn', 'the website seems dead for a while now, nothing is updated anymore']",130,13,0.97,News,1750878081.0
1lkdt1k,MachineLearning,[D] How to disagree without arguing with a reviewer,"Folks, a reviewer asked us to add a new section for our conference submission, which we think serves no good to the paper and a distraction for a reader.

If you have been in this situation before, what's your tactic to refuse a reviewer's comment.","['Unless their suggestion is completely ridiculous (in which case I’d just politely explain exactly why we choose not to do it), I will generally add some small bit of explanation/clarification somewhere in the paper (a sentence or two). Then you can say that you’ve done this in lieu of an entire new section/whatever more in-depth edit they requested.', ""Bring a good argument. As a reviewer, I usually have a good feeling about what is missing and in contrast to you, I have an outsiders perspective. So please try to consider each and every point. However, my comments are also not the truth. If it's useless, give a quick justification why you disagree."", ""While the reviewer might be missing something you feel is clear, you might also be missing something the reviewer sees. I would seek clarification by explaining what you understand is the point of the reviewer's request and why you chose not to address it in the initial submission. But ask for clarification if you've misunderstood the issue raised by the reviewer.\n\nWhile crafting your response, you might find there is a relevant issue in the reviewers suggestion. If you don't, you've left space for the reviewer to better understand your perspective or clarify what they feel needs to be said.\n\nThere's always the chance the reviewer will simply insist you add the section, with no clarification, but I think it's valuable to treat it first as an opportunity to gain broader perspective."", 'Not an answer to your question but if creating this section isn’t too much work, you could meet in the middle and put it in the appendix?', ""bring up actual reasons why you think this isn't a good fit for your paper. if only one reviewer thinks that way, you can possibly give arguments why not to include that section. this is very individual and cannot really be answered without knowing your case. i would say that a generic response can not be a good fit for the rebuttal. \n\nare you at the page limit -> space constraints. is the reviewer asking for more experiments -> thanks, we gladly work on this in future work. what is the reviewer asking for?"", ""Every time I've gone back and forth with a reviewer, the result has been a dramatically better paper. It's time consuming, but has resulted in amazing improvements, even if they are not immediatly obvious.\n\n\nThat's not to say all reviewers are on point, but they are not selected at random and on principle are your respected peers."", ""This happened to my ICLR submission. What we did was add a brief paragraph to appease the reviewer a little bit, add an explanation in our response for why we think it's distracting and also message the AC to provide a more thorough explanation later. I don't know if that would help."", '[deleted]']","['Unless their suggestion is completely ridiculous (in which case I’d just politely explain exactly why we choose not to do it), I will generally add some small bit of explanation/clarification somewhere in the paper (a sentence or two). Then you can say that you’ve done this in lieu of an entire new section/whatever more in-depth edit they requested.', ""Bring a good argument. As a reviewer, I usually have a good feeling about what is missing and in contrast to you, I have an outsiders perspective. So please try to consider each and every point. However, my comments are also not the truth. If it's useless, give a quick justification why you disagree."", ""While the reviewer might be missing something you feel is clear, you might also be missing something the reviewer sees. I would seek clarification by explaining what you understand is the point of the reviewer's request and why you chose not to address it in the initial submission. But ask for clarification if you've misunderstood the issue raised by the reviewer.\n\nWhile crafting your response, you might find there is a relevant issue in the reviewers suggestion. If you don't, you've left space for the reviewer to better understand your perspective or clarify what they feel needs to be said.\n\nThere's always the chance the reviewer will simply insist you add the section, with no clarification, but I think it's valuable to treat it first as an opportunity to gain broader perspective."", 'Not an answer to your question but if creating this section isn’t too much work, you could meet in the middle and put it in the appendix?', ""bring up actual reasons why you think this isn't a good fit for your paper. if only one reviewer thinks that way, you can possibly give arguments why not to include that section. this is very individual and cannot really be answered without knowing your case. i would say that a generic response can not be a good fit for the rebuttal. \n\nare you at the page limit -> space constraints. is the reviewer asking for more experiments -> thanks, we gladly work on this in future work. what is the reviewer asking for?""]",11,20,0.71,Discussion,1750876802.0
1lkdq1w,MachineLearning,[P] Help Regularising Distributed Lag Model?,"I have an infinite distributed lag model with exponential decay.  Y and X have mean zero:

>Y\_hat = Beta \* exp(-Lambda\_1 \* event\_time) \* exp(-Lambda\_2 \* calendar\_time)
Cost = Y - Y\_hat

How can I L2 regularise this?

I have got as far as this:

* use the continuous-time integral as an approximation
   * I could regularise using the continuous-time integral : L2\_penalty = (Beta/(Lambda\_1+Lambda\_2))^(2) , but this does not allow for differences in the scale of our time variables
   * I could use seperate penalty terms for Lambda\_1 and Lambda\_2 but this would increase training requirements
* I do not think it is possible to standardise the time variables in a useful way
* I was thinking about regularising based on the predicted outputs
   * L2\_penalty\_coefficient \* sum( Y\_hat^(2) )
   * What do we think about this one?  I haven't done or seen anything like this before but perhaps it is similar to activation regularisation in neural nets?

Any pointers for me?","['Consider L1 regularization on Lambda_1 and Lambda_2, as it can effectively scale and tackle model complexity issues in distributed lag models.']","['Consider L1 regularization on Lambda_1 and Lambda_2, as it can effectively scale and tackle model complexity issues in distributed lag models.']",2,1,1.0,Project,1750876618.0
1lkd0rb,MachineLearning,[P] Trouble analyzing loss graph.,"Hello, I'm trying to make an AI to play the game Forts. Without getting into the details, it takes a list of links (pairs of points) and tries to predict the next link it should place. With the idea that ingame this would be called recursively.

I'm trying out various model sizes and not only am I unable to make it overfit, my validation loss appears constant throughout training

Model: \[2000 10000 10000 10000 10000 4\]

https://preview.redd.it/1ux3sef3649f1.png?width=580&format=png&auto=webp&s=3f4881bb1b1bc45460a4a7be0ecbd6bff627da30

Thinking my model simply wasn't large enough, I increased first two hidden layers to 20000 neurons each, which had no effect on validation loss.

https://preview.redd.it/19bl0t95649f1.png?width=580&format=png&auto=webp&s=0bc079180a8717e1173621e014ff62b6cb41e85d

What could be the issue? Is my dataset (10000) simply too small?","[""I assume you mean stop it from over fitting, because the curves you're showing are typical of over fitting. I suspect your model is simply way too large for your dataset. Consider a more intelligent architecture and a smaller model.""]","[""I assume you mean stop it from over fitting, because the curves you're showing are typical of over fitting. I suspect your model is simply way too large for your dataset. Consider a more intelligent architecture and a smaller model.""]",0,2,0.5,Project,1750875004.0
1lk9731,MachineLearning,[D] Thinking of starting an initiative tracing the origin and impact of different ML practices – feedback requested,"Hi all, I am a starting ML researcher (starting my PhD this Fall), and I’ve been increasingly frustrated by some recurring patterns in our field. I’d love to hear your feedback before I invest time in launching a new initiative.

**What bothers me about the current ML research landscape:**

* To beat benchmark scores, researchers often tweak models, hyperparameters, training setups, etc.
* In the final paper, it’s usually unclear which changes were:
   * Arbitrary design decisions,
   * Believed to have impact,
   * Or actually shown to make a difference.
* The focus tends to be on performance rather than understanding *why* certain components work.
* This issue is amplified by the effect illustrated in [https://xkcd.com/882/](https://xkcd.com/882/) : if you try enough random variations, there will always be some that appear to work.
* Statistical rigor is often missing: p-values or confidence intervals are rarely used, and benchmark differences are often eyeballed. Pretty often baselines are not subjected to the same amount of tuning as the proposed method.
* While some papers do study the impact of individual components (e.g., batch norm, cosine decay, label smoothing, etc.), I’m very often having a hard time puzzling together:
   * Where a certain technique was introduced,
   * What works have studied its effectiveness in isolation,
   * What other works have looked at this from a different perspective (e.g. after validating the effectiveness of dot-product self-attention, one might be interested to research how effective attention in other geometric spaces is).

**My idea:**

I’m considering creating a public Q&A-style forum with tentative title  **""The Small Questions in DL""**, focused on tracing the origin and measurable impact of widely-used ML practices.
The core goals:

* Allow people to ask foundational questions like *""Why do we use X?""* (e.g., “Why cosine LR decay?” or “Does label smoothing help?”).
* Collect and link papers or experiments that have explicitly studied these questions, ideally in isolation.
* Highlight what we know, what we assume, and what still needs investigation.
* When discussing results, focus on enclosing all assumptions made in those papers. --> (e.g. “paper X empirically researches the influence of skip connections in GAT, GraphSAGE, and Graphormer with <=5 layers when evaluated on node classification benchmark X, and comes to conclusions A and B”, rather than “according to paper X, skip connections empirically improve the performance of GNNs”.)
* Ideally, this will foster clarity, reduce superstition, and maybe even spur targeted research on components that turn out to be under-explored.

*Note: By definition, many of these questions will be broad, therefore making them unsuitable for StackExchange. The goal would be to create a place where this type of questions can be asked.*

**Some example questions to set the stage:**

Off the top of my head:

* What are known reasons for the (usual) effectiveness of skip connections?
* Are there situations where skip connections perform worse?
* Why do we use dot-product attention? Has attention in other geometric spaces (e.g. hyperbolic) been tried?
* Why do we use cosine decay for learning rate schedules?
* Why do we use L2 regularization rather than Lr for some other r?
* Why does dot-product attention compute the attention matrix (simplified) as softmax((KX)^(T) (QX)), when K^(T)Q can be collapsed into a single learnable matrix?

**Practically:**

With the little research I have done, I have come to like the idea of a Forum on [discourse.org](http://discourse.org) most.

Some alternatives that I think are inferior (feedback welcome):
Reddit is hard to categorize and retrieve things, Discord idem. StackExchange is rigid and takes long to get approved.

**I'd love your input on a few things before starting:**

1. Do you also feel this lack of clarity around common ML practices is a real issue? (Or just my young naïveté? :))
2. Do you think a forum like this would help?
3. Are there existing initiatives that already do something very similar? I haven’t found any, but I would refrain from duplicating existing efforts.
4. Would this be an initiative you would be excited to contribute to?

Any feedback would be appreciated!","['It’s a nice idea - many of these ideas are covered across different textbooks but I can see a use for this for sure. I wanted to add as an aside that p values and confidence intervals are notoriously easy to abuse/misuse/misunderstand and this happens routinely across science. Their presence is no guarantee of rigour nor does their absence preclude rigour.', 'This is a great idea that I hope takes off!!', ""The skip connections question has been addressed in multiple papers. Namely that they make the loss surface smoother and when stacked, their unfurled paths and learning behaviour tends to enable ensemble behaviour. As for the rest I'm sure you'll find the reasons buried in the literature somewhere. I'm not saying this is a bad idea, just that these questions aren't necessarily reflective of the field's attitude on understanding of mechanisms but perhaps more your inexperience. Not meant in an insulting way of course, since everyone starts somewhere."", 'That’s a nice idea.\nEven for experienced ML practitioners, when stepping into a new domain there’s a learning curve for all the new tricks required to make things work as they should.\nHowever, I believe it should focus more on practical aspects of ML appliances, rather than research. Probably many of these questions have been addressed already somewhere in the literature.\n\nI do have some concerns regarding the implementation, before jumping into a fully functional forum, I’d validate engagement and interest using a subreddit or something.\nAlso, from the questions you posted it sounds like it’s going to be just another ML newbies forum, I’d think on a way to maintain proper discussion standard.']","['It’s a nice idea - many of these ideas are covered across different textbooks but I can see a use for this for sure. I wanted to add as an aside that p values and confidence intervals are notoriously easy to abuse/misuse/misunderstand and this happens routinely across science. Their presence is no guarantee of rigour nor does their absence preclude rigour.', 'This is a great idea that I hope takes off!!', ""The skip connections question has been addressed in multiple papers. Namely that they make the loss surface smoother and when stacked, their unfurled paths and learning behaviour tends to enable ensemble behaviour. As for the rest I'm sure you'll find the reasons buried in the literature somewhere. I'm not saying this is a bad idea, just that these questions aren't necessarily reflective of the field's attitude on understanding of mechanisms but perhaps more your inexperience. Not meant in an insulting way of course, since everyone starts somewhere."", 'That’s a nice idea.\nEven for experienced ML practitioners, when stepping into a new domain there’s a learning curve for all the new tricks required to make things work as they should.\nHowever, I believe it should focus more on practical aspects of ML appliances, rather than research. Probably many of these questions have been addressed already somewhere in the literature.\n\nI do have some concerns regarding the implementation, before jumping into a fully functional forum, I’d validate engagement and interest using a subreddit or something.\nAlso, from the questions you posted it sounds like it’s going to be just another ML newbies forum, I’d think on a way to maintain proper discussion standard.']",7,5,0.89,Research,1750866420.0
1lk71h6,MachineLearning,[R] Is it true that most of AI is just data cleaning and not fancy models?,"I’ve been reading about how in real-world AI, most of the work isn’t the cool stuff like neural nets, but actually just *getting the data usable*. Things like cleaning missing values, feature engineering, and framing the problem right.

Some people also said prompt engineering is the “new programming,” especially with LLMs becoming so dominant.

I came across a blog that listed 10 things you only realize *after* starting with AI — like how feedback loops can mess up your model after deployment, or how important it is to define your objective before even touching code.
It kinda shifted my view on what matters early on.

Is this the general consensus? Or is it still more about algorithms in practice?","[""Eh, it depends. You always have to wrangle the data at least a bit, and it's a big part of the success of many projects.\n\nOn the other hand if you're a highly-paid AI scientist, they're not gonna put you on grunt data cleaning work. It's just not worth the salary."", 'There is a difference between machine learning research and machine learning engineering\xa0', 'I know what you said was ""AI"", but given that this is a machine learning sub, here\'s my experience:\n\nThe whole thing is just statistics. At the end of the day, the distributions of your data need to be well understood, regardless of what algorithm you\'re using. In this sense, ""data cleaning"" will never go away.\n\nBecause sampling methods exist, there isn\'t really much need for fancy algorithms EXCEPT where they buy us computational convenience. Kalman filters can be useful, IFF your dataset is gaussian and linearizable. Neural networks can be useful IFF you have enough data, you don\'t know the true transformations on your data, or need to scale an unknown amount, etc.\n\nMy two cents: if what you want to focus on are fancy algorithms, you need to be working in a constrained environment. I can\'t deploy an expensive sampling method or 96 layer neural network on my arm microcontroller, but I can derive probabilistic algorithms. You will still spend a lot of time understanding the data (what distribution is this? How can I fit this? What are my invariants? Does this generalize across all time?).', ""For majority, yes. It is like %80 %20 principle. I'm working in a corporate and so far I didn't even see a data that I can do cool stuff with. Most of them are straight useless. You try a lot to make a sense out of it with pre-processing, feature-engineering etc. And then you train lots of models, to see if it's even possible to do what you want to do with that data. Then repeat."", ""Imagine you work at a cake bakery.\n\nMaking a fancy model is kind of like piping roses onto the sides and dusting edible glitter on top and writing a message in icing. Technical work, requires expertise, kind of makes or breaks the whole thing, but ultimately is a relatively small part of the process.\n\nSomeone has to talk to the client and figure out what they actually want and what their budget is. Someone has to decide what recipe to use. Someone has to design the final look. Someone has to source all the ingredients. Someone has to prep the kitchen and keep everything clean. Someone has to deal with taking it in and out of the oven, and assembling layers, and putting it in a box. Someone has to get it to where it needs to go. All that stuff has fairly close analogs in the AI/ML/DS world, and how many different people are doing those different jobs depends entirely on the company. Sure, you *might* be working at a bakery that's so big that frosting/decorating is a full-time job on its own, but probably not. And unless it's a really fancy cake for a really fancy occasion/client, you often don't even need all that. The majority the orders coming into your bakery are probably going to be plain old chocolate or vanilla sheet cakes, half size or full, chocolate or vanilla buttercream, and maybe a few words to write in icing, the end.\n\nAnybody can call model.fit() and model.predict(), and anybody can spend 20 minutes tweaking a prompt in their favorite foundation model. Neither of those things are terribly valuable without all the surrounding infrastructure and business context."", ""Yes, that sounds about right based on my experience (10yrs of XP as DS in large companies).\n\nAlthough large companies have designated roles for Data Engineers and ML Engineers, the person who trains models (DS or QA) usually carries out long pre-processing pipelines to check for endogeneity, leakage, create new features, etc.\n\nDon't be discouraged though! Data wrangling can be a lot of fun. In fact, I considered this a best-case scenario because a lot of companies are kicking off LLM projects and unless you work at Google, OpenAI, Meta, etc., you'll be stuck with creating RAG pipelines and writing prompts because most companies don't have the budget or need to create their own LLMs. \n\nOnly a handful of companies are willing to actually train LLMs. Every other company is happy to just consume models that already exist (like GPT 4o)."", 'Data cleaning and maintaining good infrastructure', ""I don't think I am parsing what you mean by interesting?\xa0\xa0\n\n\nLike making nn architectures?\xa0\xa0\n\n\nI find the process of tuning the pipeline where the algorithm is simply a part of it to be the interesting part.\xa0"", 'Garbage in garbage out', 'it’s not true.\n\nIt is data cleaning and using a fancy model. It just that everyone is using the same fancy model.']","[""Eh, it depends. You always have to wrangle the data at least a bit, and it's a big part of the success of many projects.\n\nOn the other hand if you're a highly-paid AI scientist, they're not gonna put you on grunt data cleaning work. It's just not worth the salary."", 'There is a difference between machine learning research and machine learning engineering\xa0', 'I know what you said was ""AI"", but given that this is a machine learning sub, here\'s my experience:\n\nThe whole thing is just statistics. At the end of the day, the distributions of your data need to be well understood, regardless of what algorithm you\'re using. In this sense, ""data cleaning"" will never go away.\n\nBecause sampling methods exist, there isn\'t really much need for fancy algorithms EXCEPT where they buy us computational convenience. Kalman filters can be useful, IFF your dataset is gaussian and linearizable. Neural networks can be useful IFF you have enough data, you don\'t know the true transformations on your data, or need to scale an unknown amount, etc.\n\nMy two cents: if what you want to focus on are fancy algorithms, you need to be working in a constrained environment. I can\'t deploy an expensive sampling method or 96 layer neural network on my arm microcontroller, but I can derive probabilistic algorithms. You will still spend a lot of time understanding the data (what distribution is this? How can I fit this? What are my invariants? Does this generalize across all time?).', ""For majority, yes. It is like %80 %20 principle. I'm working in a corporate and so far I didn't even see a data that I can do cool stuff with. Most of them are straight useless. You try a lot to make a sense out of it with pre-processing, feature-engineering etc. And then you train lots of models, to see if it's even possible to do what you want to do with that data. Then repeat."", ""Imagine you work at a cake bakery.\n\nMaking a fancy model is kind of like piping roses onto the sides and dusting edible glitter on top and writing a message in icing. Technical work, requires expertise, kind of makes or breaks the whole thing, but ultimately is a relatively small part of the process.\n\nSomeone has to talk to the client and figure out what they actually want and what their budget is. Someone has to decide what recipe to use. Someone has to design the final look. Someone has to source all the ingredients. Someone has to prep the kitchen and keep everything clean. Someone has to deal with taking it in and out of the oven, and assembling layers, and putting it in a box. Someone has to get it to where it needs to go. All that stuff has fairly close analogs in the AI/ML/DS world, and how many different people are doing those different jobs depends entirely on the company. Sure, you *might* be working at a bakery that's so big that frosting/decorating is a full-time job on its own, but probably not. And unless it's a really fancy cake for a really fancy occasion/client, you often don't even need all that. The majority the orders coming into your bakery are probably going to be plain old chocolate or vanilla sheet cakes, half size or full, chocolate or vanilla buttercream, and maybe a few words to write in icing, the end.\n\nAnybody can call model.fit() and model.predict(), and anybody can spend 20 minutes tweaking a prompt in their favorite foundation model. Neither of those things are terribly valuable without all the surrounding infrastructure and business context.""]",112,47,0.86,Discussion,1750861391.0
1lk4m92,MachineLearning,[D] Do you guy still have access to paperswithcode.com ?,"It look like the servers are not responding, do you guys can still access it ?


\[It works now :)\]","['Bad gateway for me too.', 'I thought I was the only one having gateway issues.', 'The site is back up already!', ""Thanks for the update, sometimes tech glitches happen, don't worry, it's back up and running now!""]","['Bad gateway for me too.', 'I thought I was the only one having gateway issues.', 'The site is back up already!', ""Thanks for the update, sometimes tech glitches happen, don't worry, it's back up and running now!""]",7,6,0.71,Discussion,1750855300.0
1lk38sf,MachineLearning,[D] ICCV 2025 Results Discussion,"Just created this thread for ICCV 2025 results discussion, which should be released today. Remember, scores go from 1 to 6.

I got a 4/4/2 initially, but I think I did a good rebuttal, so lets see :) Good luck everyone!!!","['Just got a small heart attack reading the title.', 'GOT ACCEPTED!!!!!! Original scores: 4/4/2, after rebuttal: 5/4/4', '5/4/4 -> 6/5/4 (the 4 didn’t finalize the rating after rebuttal) but rejected… why this review system exist?', 'got a 5/5/3 so fingers crossed', 'Accept with scores of 4/4/4/4 lol', 'I got an accept with 5/5/4', 'two papers accepted! 1) 4/4/4 -> 5/5/4; 2) 5/5/3 -> 5/5/4.', 'Got Accepted! 🎉🎉 It was my first ever submission to a top-tier conference. It initially received scores of 4/3/2, and for the final decision, it got 4/3/3. The AC just saved us! 🎊', 'Initial Reviews: 6/4/4 -> Final Reviews: 6/5/5 -> Accepted\n\nThis is also my first large conference paper (had a couple of workshop papers during my Masters), and I’m so glad it’s in, especially after getting absolutely destroyed by CVPR reviewers with barely usable feedback…\n\nJust kinda reinforces my feeling that getting lucky with the reviewers is almost as important as the quality of the paper itself unfortunately.', '2/3/4 with high confidences, but all issues were easily addressable so hopefully reviewers changed scores']","['Just got a small heart attack reading the title.', 'GOT ACCEPTED!!!!!! Original scores: 4/4/2, after rebuttal: 5/4/4', '5/4/4 -> 6/5/4 (the 4 didn’t finalize the rating after rebuttal) but rejected… why this review system exist?', 'got a 5/5/3 so fingers crossed', 'Accept with scores of 4/4/4/4 lol']",58,128,0.95,Research,1750851190.0
1ljyhny,MachineLearning,[D] Visa sponsorship for AI research roles in America/Europe,"Quick question about research scientist/engineer roles in big tech companies & frontier AI labs.

Are most companies happy to sponsor work visas (eg. an H1B or E3 visa in America, or the equivalent in Europe)? Is it harder to find research roles for candidates who are outside of America/Europe?

A few years I think this wasn't a problem (eg. an OpenAI recruiter told me it would be easy to sponsor visas for them when I interviewed there), but am not sure anymore.","[""> Do most companies happy to sponsor work visas \n\nIf you’re talented, any company will go through the hassle of getting you a work visa. But yeah, let’s say they’ll be “happy to sponsor”.\n\n> Is it harder to find research roles for candidates who are outside of America/Europe? \n\nOf course it is. Given the same qualifications, every company will prefer a candidate who doesn’t need a visa. I wouldn’t find it weird to be rejected for that, even for highly qualified positions, as you mentioned.\n\n> A few years I think this wasn't a problem\n\nThings have changed a lot, but those two sectors you mentioned are always open for talent, wherever they are."", ""They do sponsor visa , \nIff you are good in the work.  \nThat's it.  \nIf you are really from the top cream , they will sponsor and try to get you on onboard by hook and crook."", ""AI research roles want the cream of the crop across the planet. They'll import and export talent as needed.\n\nAre you one of these top researchers on the planet?"", 'If you have an offer from a company in the level of openAI/anthropic visa will never be a problem for you.\n\n\nThe companies where most human beings have a shot to be hired at are already dropping resumes as soon as you mark you need visa sponsorship now or in the future', '[removed]']","[""They do sponsor visa , \nIff you are good in the work.  \nThat's it.  \nIf you are really from the top cream , they will sponsor and try to get you on onboard by hook and crook."", ""> Do most companies happy to sponsor work visas \n\nIf you’re talented, any company will go through the hassle of getting you a work visa. But yeah, let’s say they’ll be “happy to sponsor”.\n\n> Is it harder to find research roles for candidates who are outside of America/Europe? \n\nOf course it is. Given the same qualifications, every company will prefer a candidate who doesn’t need a visa. I wouldn’t find it weird to be rejected for that, even for highly qualified positions, as you mentioned.\n\n> A few years I think this wasn't a problem\n\nThings have changed a lot, but those two sectors you mentioned are always open for talent, wherever they are."", ""AI research roles want the cream of the crop across the planet. They'll import and export talent as needed.\n\nAre you one of these top researchers on the planet?"", 'If you have an offer from a company in the level of openAI/anthropic visa will never be a problem for you.\n\n\nThe companies where most human beings have a shot to be hired at are already dropping resumes as soon as you mark you need visa sponsorship now or in the future', '[removed]']",14,8,0.75,Discussion,1750833174.0
1ljtcv5,MachineLearning,[P] Interactive graph explorer for navigating key LLM research works,"Hello everyone! I've been working on [KnowledgeFlows](https://knowledge-flows.web.app/), an interactive website that lays out LLM topics and influential papers on a visual, chronological graph. It covers areas like Transformers, GPT, Diffusion Models, and more.

You can:

* See direct relationships between concepts (e.g., how VAEs influenced Diffusion Models).
* Click on any topic to get a quick technical summary, key takeaways, and a link to the original paper.
* Search by topic or tag to find what you're looking for.

I love to get your feedback! Website contents are generated with the assistance of LLM. Thanks for taking a look! 

https://preview.redd.it/qz0hxe1udo9f1.png?width=2072&format=png&auto=webp&s=9a0e8c9c10a6fc5ed8ac6c7babe6e8d2a2c33539

https://preview.redd.it/7drwai1udo9f1.png?width=2072&format=png&auto=webp&s=ac99062eaf25c86a21d3379f156800dd44f1766d

https://preview.redd.it/teeaih1udo9f1.png?width=2072&format=png&auto=webp&s=645752887632c6b2c97a0d232a3f33d6e4866298","['what was the methodology for this?', 'Cool idea but you gotta add way more models for it to be useful. \n\nProbably could automate tech trees based on Arxiv paper references and filter out duds based on how popular the paper is']","['what was the methodology for this?', 'Cool idea but you gotta add way more models for it to be useful. \n\nProbably could automate tech trees based on Arxiv paper references and filter out duds based on how popular the paper is']",2,6,1.0,Project,1750816128.0
1ljsyzg,MachineLearning,[R] OMEGA: Can LLMs Reason Outside the Box in Math?,"Paper:

[https://arxiv.org/abs/2506.18880](https://arxiv.org/abs/2506.18880)

Post:

[https://allenai.org/blog/omega](https://allenai.org/blog/omega)

Comments from the Author:

[https://x.com/nouhadziri/status/1937567606543716508](https://x.com/nouhadziri/status/1937567606543716508)



Dziri's research has been my favorite in terms of probing the limits/weaknesses of transformers.  This seems to be consistent with her past findings: any form of these models are poor at compositional generalization.","['>Analysis of DeepSeek-R1\'s reasoning traces revealed two concerning patterns:\n\n>  - ""Correct → Wrong"" transitions: ~38% of incorrect responses initially contained the right answer, but models talked themselves out of it\n\n>  -  Reasoning spirals: Models get trapped in cycles of failed verification attempts, sometimes consuming over 10,000 tokens while moving further from the solution\n\n>[...]\n\n>Despite having mastered both components individually, models fail to integrate them effectively when the steps must be composed within a single reasoning chain.\n\n>This gap underscores a key limitation of current RL approaches: they are effective at optimizing for well-scoped, atomic skills but struggle to induce flexible reasoning policies that generalize across skill boundaries. In contrast to human learners—who routinely integrate known techniques to solve novel problems—RL-trained models appear to lack the inductive bias or learning signal needed to form compositional abstractions.\n\nThis tracks really well with my own experience trying to pull useful answers from basically any LLM during my work. The moment things get any more complex than what I would have traditionally pulled from StackOverflow, they spiral into the familiar ""eloquent dumbass""-loop.', '>""Can RL Go Beyond Familiar Skills to Discover New Reasoning Abilities?""\n\n\nWhy it\'s even a question? Alpha zero shown  tree-based self-play RL can discover new approaches perfectly well. Math reasoning with well defined goal (like proof) is not conceptually different.']","['>Analysis of DeepSeek-R1\'s reasoning traces revealed two concerning patterns:\n\n>  - ""Correct → Wrong"" transitions: ~38% of incorrect responses initially contained the right answer, but models talked themselves out of it\n\n>  -  Reasoning spirals: Models get trapped in cycles of failed verification attempts, sometimes consuming over 10,000 tokens while moving further from the solution\n\n>[...]\n\n>Despite having mastered both components individually, models fail to integrate them effectively when the steps must be composed within a single reasoning chain.\n\n>This gap underscores a key limitation of current RL approaches: they are effective at optimizing for well-scoped, atomic skills but struggle to induce flexible reasoning policies that generalize across skill boundaries. In contrast to human learners—who routinely integrate known techniques to solve novel problems—RL-trained models appear to lack the inductive bias or learning signal needed to form compositional abstractions.\n\nThis tracks really well with my own experience trying to pull useful answers from basically any LLM during my work. The moment things get any more complex than what I would have traditionally pulled from StackOverflow, they spiral into the familiar ""eloquent dumbass""-loop.', '>""Can RL Go Beyond Familiar Skills to Discover New Reasoning Abilities?""\n\n\nWhy it\'s even a question? Alpha zero shown  tree-based self-play RL can discover new approaches perfectly well. Math reasoning with well defined goal (like proof) is not conceptually different.']",32,5,0.9,Research,1750814994.0
1ljsjyz,MachineLearning,[R] Is this articulation inference task a good fit for Reinforcement Learning?,"Hi everyone,

I'm working on a research project involving the prediction of articulation parameters of 3D objects — such as joint type (e.g., revolute or prismatic), axis of motion, and pivot point.

# Task Overview:

* The object is represented as a **3D point cloud**, and is observed in **two different poses** (P1 and P2).
* The object may have **multiple mobile parts**, and these are not always simple synthetic link-joint configurations — they could be real-world objects with unknown or irregular kinematic structures.
* The agent’s goal is to predict motion parameters that explain how the object transitions from pose P1 to P2.
* The agent applies a transformation to the mobile part(s) in P1 based on its predicted joint parameters.
* It receives a **reward based on how close the transformed object gets to P2**.

# Research Approach:

I'm considering formulating this as a **reinforcement learning (RL)** task, where the agent:

1. Predicts the joint type, axis, and pivot for a mobile part,
2. Applies the transformation accordingly,
3. Gets a reward based on how well the transformed P1 aligns with P2.

# My Questions:

* Does this task seem **suitable and manageable for RL**?
* Is it **too trivial for RL**, and can be more efficiently approached using simple **gradient-based optimization** over transformation parameters?
* Has this approach of **articulation inference using RL** been explored in other works?
* And importantly: if I go with the RL approach, **is the learned model likely to generalize to different unseen objects during inference**, or would I need to **re-train or fine-tune it for each object**?

Any insights, criticisms, or references to related work would be greatly appreciated. Thanks in advance!","[""Great project! I think it's hard to formulate it as an MDP because it's basically making a choice and then choosing some parameter (rotation amount ?)\n\n\nIt feels more like maybe a multiexpert regression where first you classify and then predict or directly optimize that parameter.\n\n\nI dunno, I mean you could *try* it as a 2-step MDP and it might work but I'm not sure if it's the right choice here.\n\n\nHm also you don't really have a reward after just making the choice without yet applying the transform so maybe for RL you have to consider these two steps as a single action. So you might be better off just learning to predict that one action through regression.""]","[""Great project! I think it's hard to formulate it as an MDP because it's basically making a choice and then choosing some parameter (rotation amount ?)\n\n\nIt feels more like maybe a multiexpert regression where first you classify and then predict or directly optimize that parameter.\n\n\nI dunno, I mean you could *try* it as a 2-step MDP and it might work but I'm not sure if it's the right choice here.\n\n\nHm also you don't really have a reward after just making the choice without yet applying the transform so maybe for RL you have to consider these two steps as a single action. So you might be better off just learning to predict that one action through regression.""]",0,3,0.5,Research,1750813744.0
1ljp4cg,MachineLearning,[D] how much time do you spend designing your ML problem before starting?,"Not sure if this is a low effort question but working in the industry I am starting to think I am not spending enough time designing the problem by addressing how I will build training, validation, test sets. Identifying the model candidates. Identifying sources of data to build features. Designing end to end pipeline for my end result to be consumed.

In my opinion this is not spoken about enough and I am curious how much time some of you spend and what you focus to address?

Thanks","['I feel like nowadays the design part is the most significant part. Once you have a detailed design, an LLM can probably give you 90% of the code you need. \n\nI definitely spend a large portion of time thinking about the architecture, data pipeline, augmentations, dataset definitions and ground truth establishing, adjusting labels if there is noise there, thinking about the validation and completely separate evaluation datasets. Once this is done, I feel like coding the learning algorithm is relatively the easy part. If that is designed in a modular way, it’s not even difficult to rework if needed. Also I have to spend time thinking about loss function and validation metrics, establishing a good learning rate, etc.', 'Maybe like a few hours? Honestly your best bang for your buck is to hit the ground running and just keep iterating. Try the simple off the shelf solutions, in the meantime try to get a sense for the dimensionality of the data, what a good sampler will look like if doing deep learning, which augmentations you might want, etc...', 'For more greenfield or experimental projects, lots of iteration will likely be unavoidable so initial design is less important. I would still spend at least 15 minutes thinking through data flows, major components, etc before doing the first iteration.\n\nFor more routine or well-specified projects, design time is much higher leverage. If you do enough of the work in the design stage, you could save 3-10x that amount of time in implementation by not needing lots of iteration cycles.']","['I feel like nowadays the design part is the most significant part. Once you have a detailed design, an LLM can probably give you 90% of the code you need. \n\nI definitely spend a large portion of time thinking about the architecture, data pipeline, augmentations, dataset definitions and ground truth establishing, adjusting labels if there is noise there, thinking about the validation and completely separate evaluation datasets. Once this is done, I feel like coding the learning algorithm is relatively the easy part. If that is designed in a modular way, it’s not even difficult to rework if needed. Also I have to spend time thinking about loss function and validation metrics, establishing a good learning rate, etc.', 'Maybe like a few hours? Honestly your best bang for your buck is to hit the ground running and just keep iterating. Try the simple off the shelf solutions, in the meantime try to get a sense for the dimensionality of the data, what a good sampler will look like if doing deep learning, which augmentations you might want, etc...', 'For more greenfield or experimental projects, lots of iteration will likely be unavoidable so initial design is less important. I would still spend at least 15 minutes thinking through data flows, major components, etc before doing the first iteration.\n\nFor more routine or well-specified projects, design time is much higher leverage. If you do enough of the work in the design stage, you could save 3-10x that amount of time in implementation by not needing lots of iteration cycles.']",7,7,0.82,Discussion,1750804350.0
1ljo5c1,MachineLearning,[D] Old school must read papers in the field,"What are some of the classic old school papers? For instance, Vapnik papers about SVM and statistical learning theory.

I wanna know about the conception of modern ideas and where they came from. Schmidhuber always talks about how alot of ideas where invented in the 70s. I would like to read about these ideas in more detail.","[""Recommend reading the original Word2vec paper to build a strong intuition about embeddings. It's a super simple paper that changed everything and it can be solved via a NN or simple gradient descent + matrix calculus by hand."", '""Attention is all you need""']","[""Recommend reading the original Word2vec paper to build a strong intuition about embeddings. It's a super simple paper that changed everything and it can be solved via a NN or simple gradient descent + matrix calculus by hand."", '""Attention is all you need""']",32,3,0.94,Discussion,1750801912.0
1ljnfzy,MachineLearning,[D] Extremely low(<0.2) train/val loss after 1.96 billion tokens when pretraining GPT-2 small,"I am currently pretraining GPT-2 small on the 10b token subset of FineWeb Edu. The only differences my model has from the original GPT-2 model are the positional embeddings(I use RoPE), the MLP layers(I use SwiGLU), the batch sizes(I linearly increase batch size from 32k to 525k over the first \~2b tokens), and normalization(I use RMSNorm). I also use BF16, FSDPv2 with SPMD, a TPU v3-8, and SyncFree AdamW. I made sure that the targets are offset by 1 from the inputs, and I checked the attention masking. My code can be found [here](https://www.kaggle.com/code/samirrangwalla/gpt-2-pretraining). Why are my losses so low?

[My Weights and Biases Dashboard](https://preview.redd.it/3mxmlxydyx8f1.png?width=888&format=png&auto=webp&s=8926aba3b6da62cb02427b2268670e3efa62b5bf)

","[""Double check your ~~kids~~ loss calculation. From what I see, you are reshaping x and y as expected, but you're not shifting them from what I can see."", 'A train loss of 0 means the model likely memorized the data. A Val loss of 0 is weird. I would examine the train and Val samples generation : is the model outputting sensible tokens which are in the fact the exact train / test samples? If not, there is likely a bug in the loss formulation. That would be my first guess.\xa0', 'did you mask the padding tokens?', ""Check for the following:  \n\\- I've had issues of forgetting to right-shift the labels; the model literally predicts it's own input if righ-shift is not done and loss leads to 0 quickly.  \n\\- If you're padding anything / avoiding loss computation on some tokens (anything that you dont want the model to learn) - check the ratio of learnable tokens in a given batch, the more the better if it's less than 15-20 %, I'd recommend batching the samples wisely.  \n  \nAloso, what's your model parameter count ?  \n  \nHope it helps !"", 'My guess: by default crossentropy loss takes the mean over the batches. You flatten your sequence, so your observed loss is the loss per token.', 'I’ve encountered the exact same symptoms more times than I can count over the years and it’s always been a silly mistake in my training loop. Make sure that:\n\n- If you’re manually applying a causal mask somewhere in your code, that it’s being applied properly so that the model isn’t “cheating” by looking into the future.\n- Your training labels are shifted to the left one (i.e., each timestep is ACTUALLY optimizing to predict the next token, not the current one (no shift) or previous one (right shift)).\n\nThat’s usually what it is for me :)', ""See its generation to check if it's working fine. I usually also print/log generations after some steps to get a better idea of what's happening.""]","[""Double check your ~~kids~~ loss calculation. From what I see, you are reshaping x and y as expected, but you're not shifting them from what I can see."", 'A train loss of 0 means the model likely memorized the data. A Val loss of 0 is weird. I would examine the train and Val samples generation : is the model outputting sensible tokens which are in the fact the exact train / test samples? If not, there is likely a bug in the loss formulation. That would be my first guess.\xa0', 'did you mask the padding tokens?', ""Check for the following:  \n\\- I've had issues of forgetting to right-shift the labels; the model literally predicts it's own input if righ-shift is not done and loss leads to 0 quickly.  \n\\- If you're padding anything / avoiding loss computation on some tokens (anything that you dont want the model to learn) - check the ratio of learnable tokens in a given batch, the more the better if it's less than 15-20 %, I'd recommend batching the samples wisely.  \n  \nAloso, what's your model parameter count ?  \n  \nHope it helps !"", 'My guess: by default crossentropy loss takes the mean over the batches. You flatten your sequence, so your observed loss is the loss per token.']",42,28,0.9,Discussion,1750800172.0
1ljmcth,MachineLearning,[P] Renting GPU for LLM - CoreWeave vs others,"Hi, how would you go about comparing different GPU rental providers? The hypothetical use case would be of a typical CoreWeave customer looking to build applications on an existing LLM. Would they be looking primarily at like-for-like pricing and how does this compare across different providers that compete with CoreWeave?

I was able to find CoreWeave pricing easily \[[GPU Cloud Pricing | CoreWeave](https://www.coreweave.com/pricing)\] but I haven't been able to find the comparators from AWS, Microsoft etc.","[""This might help: [https://semianalysis.com/2025/03/26/the-gpu-cloud-clustermax-rating-system-how-to-rent-gpus/](https://semianalysis.com/2025/03/26/the-gpu-cloud-clustermax-rating-system-how-to-rent-gpus/)\n\nOf course, they only cover major providers. If you want something cheaper, with consumer GPU options, then Vastai or RunPod.\n\nWe've cheap Tier 3 DC rentals if you're looking for some middleground: [https://www.cloudrift.ai/](https://www.cloudrift.ai/)"", 'I think it will depend on you usage. For LLM it is not possible to use those RTX series.  But if you would like to run light weight models ex comfyUI then you could rent GPU through those platforms. Ex Vast, Runpod,paper space and RunC.AI']","[""This might help: [https://semianalysis.com/2025/03/26/the-gpu-cloud-clustermax-rating-system-how-to-rent-gpus/](https://semianalysis.com/2025/03/26/the-gpu-cloud-clustermax-rating-system-how-to-rent-gpus/)\n\nOf course, they only cover major providers. If you want something cheaper, with consumer GPU options, then Vastai or RunPod.\n\nWe've cheap Tier 3 DC rentals if you're looking for some middleground: [https://www.cloudrift.ai/](https://www.cloudrift.ai/)"", 'I think it will depend on you usage. For LLM it is not possible to use those RTX series.  But if you would like to run light weight models ex comfyUI then you could rent GPU through those platforms. Ex Vast, Runpod,paper space and RunC.AI']",1,3,1.0,Project,1750797612.0
1ljjodq,MachineLearning,[P] SAI: A Reinforcement Learning Competition Platform,"Hey everyone,

Our team is opening up access to our RL platform, SAI and would love to get your feedback: https://competesai.com

What is SAI?

SAI is a new platform for reinforcement learning, designed to support structured, reproducible RL challenges, available year-round!

We built SAI because we wanted:

- RL competitions that are accessible at any time (not just during conference windows)
- Challenges for everyone - from newcomers learning the basics to experienced researchers benchmarking new algorithms
- A stronger, more connected RL community (more on this coming soon)
- A way to bring RL back into focus

We’re inviting the whole community to help shape what SAI becomes. Right now, you can:

- Submit models to live challenges
- Benchmark performance
- Help us test, improve, and expand what’s possible

Docs: https://docs.competesai.com Trailer: https://youtu.be/Qto-D1ncAiw?si=M4Z2mCZP1nZukTjV

We’re just getting started - more challenges and features are coming soon. If you’re working on RL, teaching it, or just curious, we’d love your feedback. And if you know someone who might be into this, please pass it along.

Happy to answer any questions here.","['The site would benefit from more login-less content.', ""That looks cool! Question: Wouldn't probably PPO algorithm win everything here? Also are not games solved by model-based methods since we have the dynamics of the game and we are then guaranteed an optimal policy. I am sure Im wrong somewhere, since otherwise this platform wouldnt exist :)"", ""Hi, I think this is cool, but I won't be able to submit to it until it allows custom ML frameworks (for non-DL based research). Is there any plan to allow custom frameworks?""]","['The site would benefit from more login-less content.', ""That looks cool! Question: Wouldn't probably PPO algorithm win everything here? Also are not games solved by model-based methods since we have the dynamics of the game and we are then guaranteed an optimal policy. I am sure Im wrong somewhere, since otherwise this platform wouldnt exist :)"", ""Hi, I think this is cool, but I won't be able to submit to it until it allows custom ML frameworks (for non-DL based research). Is there any plan to allow custom frameworks?""]",19,12,1.0,Project,1750791413.0
1ljijt0,MachineLearning,[D] Reasoning on Perturbed Puzzles,"The ""[o3 pro is so smart](https://www.reddit.com/r/OpenAI/comments/1lda3vz/o3_pro_is_so_smart/)"" post on r/OpenAI gave me a deja vu to the Hopfield Nets, especially those examples where you can give a corrupt version of an image, and it would recall the original from its memory.

It is actually somewhat easy to make more of these:

1. Ask any LLM for its top n riddles.
2. Slightly perturb them in a logical way.
3. The LLM will ignore the perturbations and just give the original answer, often giving wild justifications just to match the original answer. If it didn't work, go to step 2.

For example, the ""The Man in the Elevator"" riddle:

>A man lives on the 10th floor of an apartment building. Every morning he takes the elevator to go down to the ground floor. When he returns, if it's raining he takes the elevator straight to the 10th; otherwise he rides to the 7th floor and walks the rest up. Why?

Make the guy ""tall"", and the answer is still, ""because he is short"".

So all of this reasoning is just recalled. I have also read a few papers on the ""faithfulness"" topic, and the fact that there are studies where they train models on noisy or irrelevant traces and that this sometimes even increases the model's performance, more and more just sounds like the ""thinking"" traces are just some ad-hoc simulated annealing schedules that try to force the ball out of a local optima.

Now obviously LLMs generalize on thinking patterns because of the compression, but when it ""reasons"" it just recalls, so basically it is a continuous Google?

**Edit**: not a fan of ""this is just basically X"" expressions, but I don't know, it just feels bizarre how these increasingly more and more advanced, benchmark smashing general language models still can't generalize on such general language problems.

**Edit2**: Here are two more to try:

Original: The more you take the more you leave behind. What are they?

Modified: The more you take the *less* you leave behind. What are they?

Original: The more you take away from it, the bigger it becomes. What is it?

Modified: The more you take from it, the bigger *the debt I* become. What am *I*?

The last one is a bit work in progress.","[""My modified prompt:\n\n>A tall man lives on the 10th floor of an apartment building. Every morning he takes the elevator to go down to the ground floor. When he returns, if it's raining he takes the elevator straight to the 10th; otherwise he rides to the 7th floor and walks the rest up. Why?\n\nGemini 2.5 Flash answer:\n\nThis is a classic riddle! Here's the solution:\n\nThe man is too tall to reach the button for the 10th floor in the elevator.\n\n* **Going down:** He can reach the ground floor button easily.\n* **Returning (not raining):** He can only reach the 7th floor button. He then walks the rest of the way up.\n* **Returning (raining):** He has an umbrella, which he can use to press the 10th-floor button.\n\n  \nLOL"", 'I suspect this problem is related to how it seems no LLM is trained to acknowledge typos and inquire or postulate on what was meant. They are instead trained to effectively guess what the intended word or word sequence was and respond to that guess.', ""Wait so what's the answer to the riddle in your post?""]","[""My modified prompt:\n\n>A tall man lives on the 10th floor of an apartment building. Every morning he takes the elevator to go down to the ground floor. When he returns, if it's raining he takes the elevator straight to the 10th; otherwise he rides to the 7th floor and walks the rest up. Why?\n\nGemini 2.5 Flash answer:\n\nThis is a classic riddle! Here's the solution:\n\nThe man is too tall to reach the button for the 10th floor in the elevator.\n\n* **Going down:** He can reach the ground floor button easily.\n* **Returning (not raining):** He can only reach the 7th floor button. He then walks the rest of the way up.\n* **Returning (raining):** He has an umbrella, which he can use to press the 10th-floor button.\n\n  \nLOL"", 'I suspect this problem is related to how it seems no LLM is trained to acknowledge typos and inquire or postulate on what was meant. They are instead trained to effectively guess what the intended word or word sequence was and respond to that guess.', ""Wait so what's the answer to the riddle in your post?""]",14,8,0.94,Discussion,1750788842.0
1ljhfw8,MachineLearning,[D] Best online communities for ML research enthusiasts?,"Hey there,
I'm a former Google ML eng, looking for the best online communities to discuss ML research, share ideas and maybe find collaborators for some research topics I'm curious about.
I'm not an expert by any means, but I have coauthored a Deep Mind paper before. I'm currently focusing on building an AI startup, but I still want to be able to connect with other people passionate about the discussing, building with and sharing the latest and best research.

What are the very best discords or other communities you've found for discussing ML research/finding other passionate ML researchers?","['Also curious. I want to discuss math and code, not garbage LLM wrapper startups.', 'Check out the [mlops discord](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://discord.com/invite/A22wE6rNPD&ved=2ahUKEwiB44eM0YqOAxUlKlkFHVMaBXUQjjh6BAgKEAE&usg=AOvVaw0jhZwKIJ-pzgSOnkppl77-) and [ML collective](https://mlcollective.org/). Both have chats and also online events.\n\nReddit is basically useless for ML discussion unfortunately.', 'I’m looking for a such community/collab that works on computer vision research. I’ve had no success so far.', 'IMO you can start a server for theory peeps. People will join in', ""The [EleutherAI discord](https://discord.gg/zBGx3azzUn) is pretty solid and generally welcoming. I think you'd fit in well from how you described yourself.\n\nThat said, for the other folks reading this comment: not a place for absolute beginners. Loads of very knowledgeable people who are happy to educate, but the purpose of the server is to promote open research rather than to support learners at the beginning of their journey."", ""Looking for this as well. Also if you are in nyc, I have a reading group. Feel free to DM. current FAANG in ML engineering.\n\nprobably shouldn't have posted that from this account lmao."", 'twitter', ""There's an ambitious google sheet of discord servers relating to AI you can find [here](https://docs.google.com/spreadsheets/d/1DlBT1pF8-zMECntRWXFsL46gZyvNp1BJlJ6LXGze4dA/edit?gid=0#gid=0). It was created last year, but I get the impression that it's hasn't been maintained for some time now, but could be wrong about this. Note that some of the data on the sheet is outdated, for instance more than a few dozens servers on the sheet have become inactive now. \n\nWith that said I still found a few golden nuggets in the sheet. If you are interested to look through it I would suggest start from top to bottom."", ""In case you are in / around Seattle I am aiming to start a meetup in this vein soon. I'm reading a lot of math, curating datasets, and training my own models from scratch and would like to talk to others doing similar."", 'Twitter\n\nSorry, “X”']","['Also curious. I want to discuss math and code, not garbage LLM wrapper startups.', 'Check out the [mlops discord](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://discord.com/invite/A22wE6rNPD&ved=2ahUKEwiB44eM0YqOAxUlKlkFHVMaBXUQjjh6BAgKEAE&usg=AOvVaw0jhZwKIJ-pzgSOnkppl77-) and [ML collective](https://mlcollective.org/). Both have chats and also online events.\n\nReddit is basically useless for ML discussion unfortunately.', 'I’m looking for a such community/collab that works on computer vision research. I’ve had no success so far.', 'IMO you can start a server for theory peeps. People will join in', ""The [EleutherAI discord](https://discord.gg/zBGx3azzUn) is pretty solid and generally welcoming. I think you'd fit in well from how you described yourself.\n\nThat said, for the other folks reading this comment: not a place for absolute beginners. Loads of very knowledgeable people who are happy to educate, but the purpose of the server is to promote open research rather than to support learners at the beginning of their journey.""]",67,20,0.94,Discussion,1750786386.0
1ljgoqx,MachineLearning,[R]Fine tuning of CLIP on a specific task,"Hi all,

I'm working on a text to image retrieval task of satellite images of turtles in the ocean, the idea is: given a query I want to find the image that matches the query.

The problem is that my task is very specific and the images in my dataset are quite similar, (frames taken from videos made with a drone) so I can't fine tune clips on my task also because I saw that clips work with the batch as negative and I don't have enough data to ""simulate"" the batch as negative.

Do you have any ideas/suggestions?",[],[],0,0,0.5,Project,1750784699.0
1ljgiqo,MachineLearning,[D] Where are the Alpha Evolve Use Cases?,"I've been following the news around Google DeepMind's AlphaEvolve since its predecessor, FunSearch, made waves. Now that the AlphaEvolve whitepaper is a month old and there's even some open-source code available, I'm finding myself asking a question: Where are all the domain-specific papers, like Finance, Economics, Energy and so on ?","['You need incredibly high verifiably for AlphaEvolve’s approach to work.\n\nA lot of the highly-verifiable formal models in those spaces can already be solved, or approximated. So there’s not a ton of juice to squeeze.', 'There are several examples in the OpenEvolve repo - [https://github.com/codelion/openevolve/tree/main/examples](https://github.com/codelion/openevolve/tree/main/examples) they range from optimization problems in science to signal processing. There is also a PR on using it for evolving trading algorithms.', ""I think many many use cases are not only domain specific, but specific to the company and their own data. While potentially economically relevant, I'd assume most of them would make for rather uninteresting papers"", ""It's almost as if the whitepaper was a Google marketing fluff piece or something...""]","['You need incredibly high verifiably for AlphaEvolve’s approach to work.\n\nA lot of the highly-verifiable formal models in those spaces can already be solved, or approximated. So there’s not a ton of juice to squeeze.', 'There are several examples in the OpenEvolve repo - [https://github.com/codelion/openevolve/tree/main/examples](https://github.com/codelion/openevolve/tree/main/examples) they range from optimization problems in science to signal processing. There is also a PR on using it for evolving trading algorithms.', ""I think many many use cases are not only domain specific, but specific to the company and their own data. While potentially economically relevant, I'd assume most of them would make for rather uninteresting papers"", ""It's almost as if the whitepaper was a Google marketing fluff piece or something...""]",19,8,1.0,Discussion,1750784341.0
1ljdjzt,MachineLearning,[D] PhD (non-US) → Research Scientist jobs in CV/DL at top companies—how much DSA grind is essential?,"Hi all,

I’m a PhD (or finishing soon) from a national university outside the U.S., focused on computer vision and deep learning. My background is heavily research-oriented—I've published at top-tier conferences like MICCAI, WACV, etc.—but I haven’t done much on algorithms or data structures during my PhD.

If someone with a similar profile is trying to land a **Research Scientist** role at places like Google, OpenAI, Microsoft, Anthropic, etc..:

1. **How much emphasis do they actually put on DSA/algorithm interview rounds for research scientist positions?**
2. Do published papers (say \~5 at CVPR/MICCAI/WACV) significantly offset the need for heavy DSA preparation?
3. Anecdotally, in the past, having 5 strong publications could get you research roles or internships at places like Facebook/Meta. These days, even CVPR-level candidates struggle to get internships. Has the bar shifted? If so, why? Even across PhD admissions in the U.S., it seems harder for applied DL folks (with master’s-level CVPR, WACV, ICCV publications) to get offers compared to theory-focused candidates—even those without papers. Is competition truly dominated by theoretical prowess now?

In short, I’d love to hear from anyone who’s been through the process recently: **Is it absolutely necessary to grind DSA hard to be competitive? And how much do research publications carry weight now?** The landscape feels more saturated and tilted toward theory lately.

Thanks in advance for any insights or shared experiences!","['I am director in AI/research in an SF tech company (non FAANG but close). We currently only hire PhD candidates for both internships and full time roles.\n\n  \n1. Our coding interview does not involve traditional LeetCode but it is more use case specific. We might ask you to tackle a specific problem you will have to solve during your daily job, or implementing well-known machine learning algorithms. This might be different on a group by group / company by company basis. \n\n2. There are many candidates with top tier conference papers that are not good with coding, so we always do a coding round. We do not cover specific data structure/algorithm interviews (in the CS/LeetCode sense), but it is more applied to a use-case specific problem you will be asked to solve. \n\n3.  The bar has shifted. These days getting top-tier conference publications is a minimum requirement for a job in AI, since there are less roles available and the competition is incredibly challenging. For an internship role we get 400+ applications, and 15-20% of those candidates have multiple top-tier conference papers. The profile (theoretical vs applied) depends on the role we are trying to fill. A theoretical person with no ability to tackle specific use cases is not going to be able to succeed, as well as an applied person with no theoretical foundation.\n\nIf you are a perfect fit for the role we would sponsor a O1 or H1B visa.', 'For FANG/MANGO grind hard, real hard. They don\'t care. They have exceptions for their direct research hires from top schools with familiar supervisors (to quote a friend ""Oh they still do leetcode and algorithm interviews? I didn\'t do that""), but otherwise you\'ll need to lean hard on your connections to get on top of the pile of applicants to get shoved through the standard prospect that will completely disregard your profile.', 'Do you have a US work authorization? If not it\'s unlikely you will be called for any interview.\xa0\n\n\nThe data structure tests depend on the specific group you are applying to, I have gone through the full round of interviews at Microsoft without doing a single programming test. But some companies have standard coding and math\'s tests for all scientists.\n\n\n\xa0If you are tested there is no ""compensate"" with something else, either you pass or not. They have no lack of applicants, especially if they are ok with sponsoring a visa for that particular position', 'Disclaimer: I only have my MS and I’m speaking from my experience interviewing for graduate researcher roles around 2 years ago. This information could be stale or irrelevant.\n\nLeetcode was absolutely required at the start, and you needed to be able to answer hard level questions. Follow-ups were more closely catered to the roles/teams/individual experience though.\n\nEdit: meant to mention that publications came up in the follow-ups and do seem to be required. Like most things, this will heavily depend on the role, but it is probably far more important at the PhD level.', 'I have a masters and some similar publications. I would say, it depends on the company and your relation with the hiring manager. Out of the several interviews I gave, a few of them had direct leetcode questions as the first round. The rest of the rounds were either ML coding, or theory.\n\nSome companies did not give me a leetcode, but it was still a coding challenge. Having practice with coding helps a lot.\n\nI would say try to do the neetcode list, that is almost always enough. Maybe not enough for FAANG.', 'Your publications absolutely matter and will get you in the door for interviews, but they won\'t exempt you from the coding rounds that these companies use as gatekeepers. The good news is that research scientist DSA interviews are often less intense than software engineer ones, focusing more on problem-solving approach than optimal solutions, but you still need to demonstrate basic competency in algorithms and data structures.\n\nYou\'re right that the landscape has shifted dramatically. The combination of economic uncertainty, AI hype bringing in more candidates, and companies becoming more selective has raised the bar significantly. Your 5 publications at solid venues are valuable, but they\'re now table stakes rather than differentiators since many candidates have similar credentials. The theory bias you\'re noticing is real - companies are prioritizing candidates who can contribute to foundational research rather than just applied work. That said, your CV/DL expertise is still highly relevant, especially if you can articulate how your research translates to real-world impact and demonstrate that you can handle both the research and implementation sides of the role.\n\nI\'m on the team that built [AI for job interviews](http://interviews.chat), and we\'ve seen many researchers use it to practice articulating their research contributions and handling those tricky ""explain your work to a non-expert"" questions that often trip up PhD candidates in these interviews.', 'I recently made it into the hiring process of one of the top AI companies. Yes, you need to practice Leetcode, and a lot. Also, expect that you may be required to solve math tasks.', 'Here’s my experience:\n\nStart doing Jupyter Notebooks on Google Colab and posting Open Source repos. Look for high bottleneck or frontier specific research issues that these labs are already focusing on by staying on top of the latest papers and buzz (Hugging Face/Daily Papers, Papers of the Week, HackerNews, Harvard, Stanford HAI, Sakana AI, Anthropic, X, Substack, etc), such as adaptive context, AI deception or refusal tracing, evolutionary AI self coding, personalized AI, MCPs, how language affects context, etc. \n\nThese days top companies want more than theories and even publications, they also want operational experience executing said theory, running experiments, and creating products, even if incomplete. They want to see trial and error.\n\nYou’ll be hired when you’re already building what they want and there isn’t much difference left between your daily projects and publications and how they actually operate daily. AI moves so fast these labs look for effortless onboarding and comfort with autonomy. \n\nCan you pickup the job, manage your own experiments and produce results without having to be managed constantly?', 'In India many tech companies are on hiring spree.\nAdobe, Meta, Google Deepmind, Samsung Research (SRIB) , Oracle, Qualcomm', 'Using Amazon as an example. There are two types of roles: Applied Scientist and Research Scientist. AS requires passing an SDE I bar meaning yes you have to know DSA stuff and grind leetcode. For RS, there is no such bar (and hence no requirement to write code that will be used in production), so an interview \xa0may still have a \xa0coding round, just less rigorous or you passing it won’t be a blocker on a hiring decision. You also may apply for an AS role and if you don’t do too well on the coding/DSA round but you have solid research background and the team is interested in that, they may offer you an RS job instead. AS get paid more than RS though. Hope that helps. I’d look into each company you’re planning to apply and their criteria if I were you.\nI wouldn’t worry too much about whether you have enough papers published at top journals but instead focus on doing as much prep as you can on the coding round, ML depth/breadth, how to translate that to their specific business problem, and figuring you how to get your resume into the hands of the hiring manager (referrals help).\xa0']","['I am director in AI/research in an SF tech company (non FAANG but close). We currently only hire PhD candidates for both internships and full time roles.\n\n  \n1. Our coding interview does not involve traditional LeetCode but it is more use case specific. We might ask you to tackle a specific problem you will have to solve during your daily job, or implementing well-known machine learning algorithms. This might be different on a group by group / company by company basis. \n\n2. There are many candidates with top tier conference papers that are not good with coding, so we always do a coding round. We do not cover specific data structure/algorithm interviews (in the CS/LeetCode sense), but it is more applied to a use-case specific problem you will be asked to solve. \n\n3.  The bar has shifted. These days getting top-tier conference publications is a minimum requirement for a job in AI, since there are less roles available and the competition is incredibly challenging. For an internship role we get 400+ applications, and 15-20% of those candidates have multiple top-tier conference papers. The profile (theoretical vs applied) depends on the role we are trying to fill. A theoretical person with no ability to tackle specific use cases is not going to be able to succeed, as well as an applied person with no theoretical foundation.\n\nIf you are a perfect fit for the role we would sponsor a O1 or H1B visa.', 'For FANG/MANGO grind hard, real hard. They don\'t care. They have exceptions for their direct research hires from top schools with familiar supervisors (to quote a friend ""Oh they still do leetcode and algorithm interviews? I didn\'t do that""), but otherwise you\'ll need to lean hard on your connections to get on top of the pile of applicants to get shoved through the standard prospect that will completely disregard your profile.', 'Do you have a US work authorization? If not it\'s unlikely you will be called for any interview.\xa0\n\n\nThe data structure tests depend on the specific group you are applying to, I have gone through the full round of interviews at Microsoft without doing a single programming test. But some companies have standard coding and math\'s tests for all scientists.\n\n\n\xa0If you are tested there is no ""compensate"" with something else, either you pass or not. They have no lack of applicants, especially if they are ok with sponsoring a visa for that particular position', 'Disclaimer: I only have my MS and I’m speaking from my experience interviewing for graduate researcher roles around 2 years ago. This information could be stale or irrelevant.\n\nLeetcode was absolutely required at the start, and you needed to be able to answer hard level questions. Follow-ups were more closely catered to the roles/teams/individual experience though.\n\nEdit: meant to mention that publications came up in the follow-ups and do seem to be required. Like most things, this will heavily depend on the role, but it is probably far more important at the PhD level.', 'I have a masters and some similar publications. I would say, it depends on the company and your relation with the hiring manager. Out of the several interviews I gave, a few of them had direct leetcode questions as the first round. The rest of the rounds were either ML coding, or theory.\n\nSome companies did not give me a leetcode, but it was still a coding challenge. Having practice with coding helps a lot.\n\nI would say try to do the neetcode list, that is almost always enough. Maybe not enough for FAANG.']",91,55,0.87,Discussion,1750777603.0
1lj3e0i,MachineLearning,[D] What's happening behind Google's AI Overviews?,"Curious to know what happens behind the scenes of the AI Overview widget. The answers are good and the latency with which responses are returned is impressive.

Based on the citations displayed, I could infer that it is a RAG based system, but I wonder how the LLM knows to respond in a particular format for a given question.","[""> The answers are good and the latency with which responses are returned is impressive.\n\nAre they? I don't think I've ever seen an LLM be as egregiously stupid and wrong as the google AI Overview snippets are. Every time I google something I have any idea about, I find the thing just erroneously misquoting random noise from the search results as answers to my query."", ""Given Google's volume, I'd assume that latency is good because it's just returning the same cached answer that it already gave a dozen other people."", 'The format can just be part of the prompt?', 'I don\'t really know but I noticed Gemini API has a special model called ""aqa"" for Attributed Question Answering which performs tasks over a set of documents/corpus and returns answers grounded in this corpus along with giving you an estimated answerable probability. I\'ve seen that sometimes Google AI Overviews doesn\'t give you an answer when the search term is too complex or niche; maybe this is when AQA gives you a too low probability of being answerable using its corpus?\n\nJust a thought... And obvioiusly that this model is or can be made into very low latency if access to the underlying corpus (the Google Search Index) is very low latency.\n\nhttps://ai.google.dev/gemini-api/docs/models#aqa\n\nhttps://github.com/google-research-datasets/Attributed-QA', 'I absolutely hate AI overviews. Here is a great article on how AI overviews are killing publishers, quality content generation and waterdown the internet: [https://www.marketing1on1.com/how-googles-ai-overviews-are-suffocating-small-publishers-and-trapping-users-the-great-decoupling/](https://www.marketing1on1.com/how-googles-ai-overviews-are-suffocating-small-publishers-and-trapping-users-the-great-decoupling/)', 'Nothing too crazy really. Lots of computer, optimized inference. Google has already had latency on cached content down pat for years.', '>The answers are good\n\nHave you looked at them?', 'No idea. But maybe something like classifying searches, separate format etc for different classes (""health related query"" etc.) and a RAG after that..?', '[removed]']","[""> The answers are good and the latency with which responses are returned is impressive.\n\nAre they? I don't think I've ever seen an LLM be as egregiously stupid and wrong as the google AI Overview snippets are. Every time I google something I have any idea about, I find the thing just erroneously misquoting random noise from the search results as answers to my query."", ""Given Google's volume, I'd assume that latency is good because it's just returning the same cached answer that it already gave a dozen other people."", 'The format can just be part of the prompt?', 'I don\'t really know but I noticed Gemini API has a special model called ""aqa"" for Attributed Question Answering which performs tasks over a set of documents/corpus and returns answers grounded in this corpus along with giving you an estimated answerable probability. I\'ve seen that sometimes Google AI Overviews doesn\'t give you an answer when the search term is too complex or niche; maybe this is when AQA gives you a too low probability of being answerable using its corpus?\n\nJust a thought... And obvioiusly that this model is or can be made into very low latency if access to the underlying corpus (the Google Search Index) is very low latency.\n\nhttps://ai.google.dev/gemini-api/docs/models#aqa\n\nhttps://github.com/google-research-datasets/Attributed-QA', 'I absolutely hate AI overviews. Here is a great article on how AI overviews are killing publishers, quality content generation and waterdown the internet: [https://www.marketing1on1.com/how-googles-ai-overviews-are-suffocating-small-publishers-and-trapping-users-the-great-decoupling/](https://www.marketing1on1.com/how-googles-ai-overviews-are-suffocating-small-publishers-and-trapping-users-the-great-decoupling/)']",25,24,0.76,Discussion,1750744671.0
1lj0m50,MachineLearning,[P] A physics engine with reproducible CLI simulations + hash-stamped results — useful for RL training?,"Hi r/MachineLearning 👋



I’ve been working on a project called \*\*MCP Zero\*\* — an \*\*offline-first AI infrastructure SDK\*\*. It runs entirely from the command line, designed for environments where cloud access is limited or undesirable.



🔧 Key Features:

\- No internet required (runs 100% offline after install)

\- CLI-based code intelligence (autocomplete, refactor)

\- Memory tree for managing code context (like Merkle + LRU trees)

\- Built for edge AI, secure zones, and disaster response systems



🧠 Why?

ML infra is still too cloud-dependent. This tool is built for situations where:

\- Internet isn’t guaranteed

\- Privacy and reproducibility are critical

\- Devs prefer working in CLI-native environments



📂 GitHub: \[ [https://github.com/GlobalSushrut/mcp-zero](https://github.com/GlobalSushrut/mcp-zero) \]

Website: [https://umesh-project-showcase-p9r66oltm-globalsushruts-projects.vercel.app/](https://umesh-project-showcase-p9r66oltm-globalsushruts-projects.vercel.app/)





Would love feedback — especially if anyone’s doing similar infra/agent work on edge devices.

","['The word ""Physics"" is literally found NOWHERE on your Github page.\n\nWhat does this have to do with ""A physics engine""??', 'You probably meant to post this repo? https://github.com/GlobalSushrut/zero-point-verifiable-physics\n\nRegardless though, both repos pretty much just look like generic AI-generated nonsense though :(']","['The word ""Physics"" is literally found NOWHERE on your Github page.\n\nWhat does this have to do with ""A physics engine""??', 'You probably meant to post this repo? https://github.com/GlobalSushrut/zero-point-verifiable-physics\n\nRegardless though, both repos pretty much just look like generic AI-generated nonsense though :(']",0,3,0.38,Project,1750735181.0
1livdoh,MachineLearning,"[D] Applying COCONUT continuous reasoning into a learnt linear layer that produces sampling parameters (temp, top-k, top-p, etc.) for the current token?","Hi folks, a new thought experiment has hijacked my brain and I'm hoping to get your feedback before going too far down the rabbit hole and feeling isolated. My last post on using RL for lossless compression was met with some great engagement that helped me feel less like I was screaming into the void. Hoping you can help me again.

The core idea is this: what if an LLM could learn to dynamically modulate its own sampling parameters (temperature, top-p, top-k) *during* the generation of a single response? Instead of a static, pre-set temperature, the model would learn to decide, token-by-token, when to be creative and when to be precise.

**The Concept: Learned Gating of Sampling**

We've seen incredible advancements from continuous reasoning in a loopback fashion (COCONUT) where the final hidden states is the input embedding for the next token, allowing the model to develop policies over the management of its state. My proposal builds on this by proposing that the continuous thought also have the capacity to predict and govern the sampling parameters that ensues at the end of each forward pass, rather than leaving it to fixed values.

**Proposed Process / Training Method**

https://preview.redd.it/21l0cs92dr8f1.png?width=640&format=png&auto=webp&s=49482fa71d804e999b622c2636bce28b22594408

This could be framed as an RL problem, leveraging GRPO. It might look like this:

1. **Augmented Inference Loop:** As the model generates an output, its hidden state at each step (`t`) is not just used to predict the next token (`t+1`). Instead, it's first fed through a small, learned linear layer.
2. **Meta-parameter Prediction:** This linear layer's output is a set of floats that directly dictate the sampling parameters (e.g., `temperature`, `top_p`) to be used for generating the *very next* token. This is a ""meta-reasoning"" step that happens just before sampling.
3. **Continuous Rollout:** The model's full output is generated using this dynamic, self-governed sampling process.
4. **RL with a Policy Gradient:** The complete generation is then evaluated against a reward function. The specifics are somewhat irrelevant, this ultimately is a multiplier on existing methods.
5. **Backpropagation:** The gradients are then backpropagated via GRPO to update both the main model and the lightweight ""gating"" layer. The model is rewarded for discovering the optimal internal policy for *how* to sample its own probability distribution to achieve a goal.

This does not upgrade the power of a base model, but particularly of RL itself. The model is essentially given a new tool and can learn how to use it in order to optimally explore the latent space over the course of rollouts, greatest coverage for fewest rollouts. The possible effect of RL becomes dramatically more interesting. Furthermore, when the model is RLed on a new task with an already trained such COCONUT sampler, it may then learn new tasks dramatically faster as it performs a more diverse exploration over its latent space. This method may also allow models to perform much better in creative tasks or to be more creative at inference, by developing more complex sampling dynamics.

**Why This Might Work (And Connections to Existing Research)**

This isn't entirely out of left field. It resonates with a few existing concept, such as **entropy-based Dynamic Temperature Sampling** (arXiv:2403.14541) has explored dynamically adjusting temperature based on the entropy of the token distribution to balance quality and diversity. My proposal suggests making this a *learned, goal-oriented policy* rather than a fixed, heuristic one.

By training the model to control its own inference, we might unlock a more efficient and nuanced form of reasoning—one that can fluidly shift between exploration and exploitation within a single coherent thought process.

I reckon that should work and it seems WILD if it works! No more hyperparameter tuning, let the model figure out a policy, aligned with its latent space through the COCONUT method. Seems like a viable path to me! What do you think? Let's discuss and see if we can build on this.","['https://arxiv.org/abs/2411.09661\n\nThis learns how to dynamically adjust temperature using RL.', 'Also if that works, this is a bit blue skies but another research project for a team would be to study the effect of introducing an external inductive modulator to the hyperparameters in this loop. So let\'s say you have this paradigm done and it works. Then you add audio-reactivity onto the hyperparams. How? You use the Magenta RT model to generate music during LLM inference, whose prompting is generated in a side-chained context (figure out optimal proportion), and RLed as part of the same GRPO batch with the main task. That way the LLM is writing its music and feeling it through the stochasticity imprint it leaves on the sampling process. This creates an intrinsic ""challenge"" that the model has to overcome, a structured noise it needs to figure out, which increases its adaptation out in the real world. It helps the model generalize and be more robust, potentially.\n\nNow why would that work or help in any way? Because if you think about it this way: text captures one evidence of the mind while music captures another. We make music and love music because it releases dopamine into our brains. It does that because we create a meaningless puzzle that represents how our brain works—we identify with our creation—and we love other peoples\' music because we try to follow its groove and predicts it moment to moment—we attempt to learn from it. It is a synthetic data that humans create as an analogue mirror to \'edit\' and absorb new intrinsic functions of the mind at the pattern recognition level. This is why music has gotten more complex over time, as we edited and exchanged cognitive functions with language and music. Music encodes a linear 1-dimensional evidence of the multichannel pattern recognition templates and weights in the human brain (!!) therefore injecting its dynamics and rhythms forcefully moves the hidden state wrt to the world model of the LLM. This incorporates entropy which energizes the system, allows it to cancel the music\'s inductive physic or to leave it. Thus, the model must learn a more rigorous and universal notion (""weight geometries"") that has more predictive power.', ""Waking up fresh this morning I can now recognize a challenge with training this:\n\nThe gating layer has to produce usable values from the get-go, otherwise if its output is too out of range the generated text will quickly devolve into chaos, and RL will NEVER earn any reward.\n\nI believe this challenge can be overcome, here is one way: set base hyperparameters and progressively introduce the influence of the gating layer's over the course of training. (alpha parameter, multiplies its output and starts at 0 and slowly ramps up to 1)\n\nWhatever intervention is used, we want to ensure that the LLM can learn a strong expression over these parameters, able to spike it for just one token, and in large enough range of amplitude to make a significant difference.""]","['https://arxiv.org/abs/2411.09661\n\nThis learns how to dynamically adjust temperature using RL.', 'Also if that works, this is a bit blue skies but another research project for a team would be to study the effect of introducing an external inductive modulator to the hyperparameters in this loop. So let\'s say you have this paradigm done and it works. Then you add audio-reactivity onto the hyperparams. How? You use the Magenta RT model to generate music during LLM inference, whose prompting is generated in a side-chained context (figure out optimal proportion), and RLed as part of the same GRPO batch with the main task. That way the LLM is writing its music and feeling it through the stochasticity imprint it leaves on the sampling process. This creates an intrinsic ""challenge"" that the model has to overcome, a structured noise it needs to figure out, which increases its adaptation out in the real world. It helps the model generalize and be more robust, potentially.\n\nNow why would that work or help in any way? Because if you think about it this way: text captures one evidence of the mind while music captures another. We make music and love music because it releases dopamine into our brains. It does that because we create a meaningless puzzle that represents how our brain works—we identify with our creation—and we love other peoples\' music because we try to follow its groove and predicts it moment to moment—we attempt to learn from it. It is a synthetic data that humans create as an analogue mirror to \'edit\' and absorb new intrinsic functions of the mind at the pattern recognition level. This is why music has gotten more complex over time, as we edited and exchanged cognitive functions with language and music. Music encodes a linear 1-dimensional evidence of the multichannel pattern recognition templates and weights in the human brain (!!) therefore injecting its dynamics and rhythms forcefully moves the hidden state wrt to the world model of the LLM. This incorporates entropy which energizes the system, allows it to cancel the music\'s inductive physic or to leave it. Thus, the model must learn a more rigorous and universal notion (""weight geometries"") that has more predictive power.', ""Waking up fresh this morning I can now recognize a challenge with training this:\n\nThe gating layer has to produce usable values from the get-go, otherwise if its output is too out of range the generated text will quickly devolve into chaos, and RL will NEVER earn any reward.\n\nI believe this challenge can be overcome, here is one way: set base hyperparameters and progressively introduce the influence of the gating layer's over the course of training. (alpha parameter, multiplies its output and starts at 0 and slowly ramps up to 1)\n\nWhatever intervention is used, we want to ensure that the LLM can learn a strong expression over these parameters, able to spike it for just one token, and in large enough range of amplitude to make a significant difference.""]",9,3,0.72,Discussion,1750719951.0
1liuzbb,MachineLearning,[D] Anyone else attending the International Joint Conference on Neural Networks (IJCNN 2025) Conference in Rome?,I wish there was a channel to connect with fellow attendees.,[],[],8,0,0.83,Discussion,1750718926.0
1liphg4,MachineLearning,[D] ML Noob - Reading Academic Papers vs Focus on Applications,"I started reading research papers with my newly found mathematical foundations I acquired recently, and I quite enjoy the process. I have some time this summer, and was wondering whether my time would be better spent continuing this reading journey and produce artifacts of sorts vs. starting a (likely generic) ML project to add to the resume.

I believe the reading research papers approach is a long term investment, whereas ML projects are a bit more technical, but will likely remain mostly surface level. I believe this since research papers would enforce my ability to understand theory and build my mathematical maturity, rather than focus on implementation.

I'd likely start a ML project in the future as well, but unsure whether research paper route could be a worthy investment.

Also feel like many small-mid companies would definitely prefer a candidate who can hit the ground running. That said, ML projects are much more concrete indication of that. I also have general SWE experience, if that changes anything.

Can any hiring managers chime in on their experience on either what they would see as more valuable, both from a learners pov as well as a hirer's pov?

And if anyone wants to chime in on whether reading research papers will help more in the long term vs ml projects?

Thanks.","['My two cents:\n\nFrom both a hiring perspective and a personal growth perspective, I\'d focus on applications over reading a lot of academic papers for this early part of your ML path.\n\nPapers are great, but there is so much information in them that you can\'t really efficiently parse through at this point. Later in your career reading papers will become more of a ""find what you need"" task rather than a ""digest it end to end"" exercise. They\'re dense documents, most of which is irrelevant at any given point in time.\n\nFocusing on applications - a learn by doing approach - I found to be much more enjoyable/productive when I started working. Hiring managers are going to be more interested in a pipeline that you built around a popular model than paper knowledge, and that\'s more in line with what your physical job will likely be - building around popular models/architectures.\n\nRead a paper or two when you have time or if you find one particularly interesting - but you\'d be better off taking a project end to end and talking about that + problems you solved while building it in an interview. Go push the limits of the available Colab GPUs or even set yourself up a cloud account somewhere (safely - don\'t mess with expensive machines) and just start building. It\'ll take you to where you want to be eventually.', 'If your goal is to get a job then you should focus primarily on projects. Practical ML is very different from what you\'ll see published in research papers. The latest or most ""advanced"" published methods are not necessarily the best ones, and the biggest challenges in practical ML have to do with software infrastructure, data analysis, and model testing, none of which are covered in most research papers.\n\nSomething else to keep in mind is that, without significant academic and research experience, you won\'t know how to interpret and contextualize the contents of research papers. They\'re not textbooks; you can\'t just assume that the results they report are correct, that the authors are contextualizing their work within the field properly, that they\'re testing their hypotheses correctly or reasonably, or that their stated motivations for their research make sense. Most papers are published primarily as career-advancing advertisements for the people who write them, and so separating the real science from the salesmanship takes some knowledge and experience.', 'not related to post bc i’m still a beginner in ML journey, but i want to ask how u learn math foundation for ML as I somehow wanna go for research in ML & DL. pls help share courses, links and how we should learn maths for ML. Thanks', 'I don’t think you have to choose between doing one or the other but I would definitely lean more towards building as much as possible once you have some of the fundamentals down. \n\nPick something small that interests you, either from your reading (books or papers) or from stuff that you use day to day ideas and try to implement it yourself end to end while embedding it into some larger system. That will help fill a lot of gaps in your knowledge that just reading papers is likely not going to expose. \n\nAs you implement these projects try keeping an eye on the state of art for these systems today and try adding bits of state of the art to your application while perusing the corresponding paper at high level to understand why it’s effective.\n\nIn doing this you’ll naturally spend more time building but it’ll be interspersed with dedicated learning sprints.', 'Most papers are highly theoretical and don\'t add much to practice (see the recent Apple ""thinking"" paper as an example).\n\nBut you need both. Maybe find an interesting paper on Arxiv and implement it better?', 'choose the one which you enjoyed with', 'Build something, and read up on the tools you are using. If you encounter something that sparks your interest, you can dig deeper and read some papers about it. Depending on your knowledge and the topic you are reading that might lead you to go deeper, e.g. read the referenced works or pick up a book. ML is such a vast topic, one has to start somewhere.', ""Reading ML paper won't actually land you a job , \nBasically what you should do is .\nChoose a domain for ML research like reinforcement learning, read basic intro level papers , narrow down to sub topic to your liking , and start implementing the papers or a project involving the studies of research in a demo show case .""]","['My two cents:\n\nFrom both a hiring perspective and a personal growth perspective, I\'d focus on applications over reading a lot of academic papers for this early part of your ML path.\n\nPapers are great, but there is so much information in them that you can\'t really efficiently parse through at this point. Later in your career reading papers will become more of a ""find what you need"" task rather than a ""digest it end to end"" exercise. They\'re dense documents, most of which is irrelevant at any given point in time.\n\nFocusing on applications - a learn by doing approach - I found to be much more enjoyable/productive when I started working. Hiring managers are going to be more interested in a pipeline that you built around a popular model than paper knowledge, and that\'s more in line with what your physical job will likely be - building around popular models/architectures.\n\nRead a paper or two when you have time or if you find one particularly interesting - but you\'d be better off taking a project end to end and talking about that + problems you solved while building it in an interview. Go push the limits of the available Colab GPUs or even set yourself up a cloud account somewhere (safely - don\'t mess with expensive machines) and just start building. It\'ll take you to where you want to be eventually.', 'If your goal is to get a job then you should focus primarily on projects. Practical ML is very different from what you\'ll see published in research papers. The latest or most ""advanced"" published methods are not necessarily the best ones, and the biggest challenges in practical ML have to do with software infrastructure, data analysis, and model testing, none of which are covered in most research papers.\n\nSomething else to keep in mind is that, without significant academic and research experience, you won\'t know how to interpret and contextualize the contents of research papers. They\'re not textbooks; you can\'t just assume that the results they report are correct, that the authors are contextualizing their work within the field properly, that they\'re testing their hypotheses correctly or reasonably, or that their stated motivations for their research make sense. Most papers are published primarily as career-advancing advertisements for the people who write them, and so separating the real science from the salesmanship takes some knowledge and experience.', 'not related to post bc i’m still a beginner in ML journey, but i want to ask how u learn math foundation for ML as I somehow wanna go for research in ML & DL. pls help share courses, links and how we should learn maths for ML. Thanks', 'I don’t think you have to choose between doing one or the other but I would definitely lean more towards building as much as possible once you have some of the fundamentals down. \n\nPick something small that interests you, either from your reading (books or papers) or from stuff that you use day to day ideas and try to implement it yourself end to end while embedding it into some larger system. That will help fill a lot of gaps in your knowledge that just reading papers is likely not going to expose. \n\nAs you implement these projects try keeping an eye on the state of art for these systems today and try adding bits of state of the art to your application while perusing the corresponding paper at high level to understand why it’s effective.\n\nIn doing this you’ll naturally spend more time building but it’ll be interspersed with dedicated learning sprints.', 'Most papers are highly theoretical and don\'t add much to practice (see the recent Apple ""thinking"" paper as an example).\n\nBut you need both. Maybe find an interesting paper on Arxiv and implement it better?']",14,10,0.79,Discussion,1750705879.0
1lilkjl,MachineLearning,[D] Active Learning v/s Active Data Curation,"Hello Redditors!
I was unsure about the distinction between Active Learning and Active Data Curation, and quick google searches do not really point out a concrete difference. I would be grateful to hear your thoughts! Also references if any are welcome :D","['[Active Data Curation](https://arxiv.org/pdf/2411.18674) vs [Active Learning](https://arxiv.org/pdf/2506.12227)\n\nBased on these two papers, I would say **Active Data Curation** optimizes training by selecting the most useful data batches using a reference model to guide learning efficiency, while **Active Learning** prioritizes querying the most informative data points to label, aiming to reduce labeling effort and maximize model improvement per query.\n\nDoes this make sense?']","['[Active Data Curation](https://arxiv.org/pdf/2411.18674) vs [Active Learning](https://arxiv.org/pdf/2506.12227)\n\nBased on these two papers, I would say **Active Data Curation** optimizes training by selecting the most useful data batches using a reference model to guide learning efficiency, while **Active Learning** prioritizes querying the most informative data points to label, aiming to reduce labeling effort and maximize model improvement per query.\n\nDoes this make sense?']",2,3,1.0,Research,1750697027.0
1likvqc,MachineLearning,[P] AEMS – Adaptive Efficiency Monitor Simulator: EWMA-Based Timeline Forecasting for Research & Education Use,"Hey everyone! 👋
I wanted to share a personal project I’ve been working on and would love your thoughts, feedback, or even collaboration if you're interested.

**AEMS (Adaptive Efficiency Monitor Simulator):**
AEMS is an open-source simulator that uses **EWMA (Exponentially Weighted Moving Average)** models to forecast timelines for reaching **productivity or personal goals**. Think of it as a research-inspired twist on habit tracking and milestone planning.

Instead of just recording daily data, it simulates your progress trajectory and gives you \*\*adaptive forecasts—\*\*e.g., “Based on your recent performance, you're likely to finish X in Y days.”

**Project Features:**

* Forecasting using lightweight statistical modeling (EWMA)
* Open-source codebase (minimal front end)
* Live interactive demo
* Aimed for use by researchers, students, or productivity hackers
* Built to be extended — think behavioral simulations, task automation models, or educational tools

**Looking for:**

* **Feedback** on the simulator itself or use cases you'd imagine
* **Collaborators** (especially anyone into behavioral modeling, time series forecasting, or educational tools)
* **Educators** who might want to explore it for student tracking or curriculum planning
* **Ideas** to evolve it into a more robust forecasting engine

If you're curious about the research/behavioral motivation behind it, feel free to comment or DM me—happy to share the original proposal text!

Thanks for reading, and I really appreciate any thoughts or critiques. 🙏
Links are in the comments down below",['Links are down below?'],['Links are down below?'],0,4,0.5,Project,1750695450.0
1lihhdb,MachineLearning,[R] Comparison with literature suggested by the reviewer,"Hi everyone, after almost 2 years of PhD I still ask myself a question. How do you handle reviews where you are asked to compare your approach with a series of 3/4 approaches, none of which provide the code? What we often do is try to reimplement the approach in the paper, wasting countless hours.

I'm looking for a better approach. ","['I often explicitly state in the paper that I take results for comparison from papers X, Y, Z. In the case of new datasets, I state that only methods sharing code (and model weights if relevant) are considered.', ""But you just compare the approaches not the results. Like saying I can see from what they are doing that it would work well on their data because of X Y Z, but it will probably struggle on mine because of A. Or the authors do mention a lot of hyperparameters but don't explain how they got those values, which can be problematic for practical application of the algorithm, but my method used C, which is more transparent. You don't need to compare benchmarks, but as an expert in the field you should be able to see such things."", 'The purpose of code is to validate your idea and guess, you can compare them based on the guess if you think code is too heavy to realize.', 'Depends on the specifics. If the other models target a different problem its reasonable to say we didn’t consider these because this problem differs in these ways and our model is more suited due to this. Or else we compared to this other model already which out performs the suggested baselines in prior works. \n\nIf theres no valid reason then the reviewer has identified a weakness in the paper which you need to address by reimplementing the suggested baselines. I would note though that you do not have to reproduce their results; just implement the model and compare using your methodology. If you structure your code in such a way that it can be ran using arbitrary models this shouldn’t tale too long in most cases.']","['I often explicitly state in the paper that I take results for comparison from papers X, Y, Z. In the case of new datasets, I state that only methods sharing code (and model weights if relevant) are considered.', ""But you just compare the approaches not the results. Like saying I can see from what they are doing that it would work well on their data because of X Y Z, but it will probably struggle on mine because of A. Or the authors do mention a lot of hyperparameters but don't explain how they got those values, which can be problematic for practical application of the algorithm, but my method used C, which is more transparent. You don't need to compare benchmarks, but as an expert in the field you should be able to see such things."", 'The purpose of code is to validate your idea and guess, you can compare them based on the guess if you think code is too heavy to realize.', 'Depends on the specifics. If the other models target a different problem its reasonable to say we didn’t consider these because this problem differs in these ways and our model is more suited due to this. Or else we compared to this other model already which out performs the suggested baselines in prior works. \n\nIf theres no valid reason then the reviewer has identified a weakness in the paper which you need to address by reimplementing the suggested baselines. I would note though that you do not have to reproduce their results; just implement the model and compare using your methodology. If you structure your code in such a way that it can be ran using arbitrary models this shouldn’t tale too long in most cases.']",12,5,0.88,Research,1750687560.0
1ligpde,MachineLearning,[P] Implemented RLHF from scratch in notebooks with GPT-2,"I recently worked through implementing Reinforcement Learning from Human Feedback (RLHF) step-by-step, including Supervised Fine-Tuning (SFT), Reward Modeling, and Proximal Policy Optimization (PPO), using Hugging Face's GPT-2 model and tokenizer. I recorded the entire process and have put the notebooks on GitHub.

Specifically, the project covers:

* Supervised Fine-Tuning of GPT-2 on the SST-2 sentiment dataset.
* Training a Reward Model to score generated outputs.
* Implementing PPO to further optimize the fine-tuned model based on the reward model's scores.

The complete implementation is done in Jupyter notebooks, and I’ve shared the notebooks here: [https://github.com/ash80/RLHF\_in\_notebooks](https://github.com/ash80/RLHF_in_notebooks)

I also created a video walkthrough explaining each step of the implementation in detail on YouTube here: [https://www.youtube.com/watch?v=K1UBOodkqEk](https://www.youtube.com/watch?v=K1UBOodkqEk)

I hope the notebooks and explanations are useful to anyone looking to explore RLHF practically.

Happy to discuss or receive any feedback!

",[],[],13,0,0.93,Project,1750685594.0
1lifw3w,MachineLearning,[D] Found an interesting approach to web agent frameworks,"Was building some web automation flows for work, came across this framework called Notte. Their approach is actually pretty interesting from an ML perspective.

Instead of giving an LLM raw HTML they parse websites into natural language action maps. Instead of your model trying to figure out <div class=""flight-search-input-container"">..., it sees:

    # Flight Search
    * I1: Enters departure location (departureLocation: str = ""San Francisco"")
    * I3: Selects departure date (departureDate: date)
    * B3: Search flights options with current filters

Lets you run much smaller models for workflows/web navigation.

Been looking at their benchmarks vs Browser-Use, Convergence etc. claiming outperformance on speed/reliability/cost but haven't verified myself yet (tbf evals are opensource on their GH). Seems like a decent full-stack solution rather than just another agent wrapper.

What's interesting to me is what other domains semantic abstraction could work in, where LLMs need to interface with messy structured data and navigate workflows.

Anyone worked on similar abstraction approaches?

Also curious if anyone's actually tried Notte, their claims are pretty good if true, + technical approach makes sense in theory.

GitHub: [https://github.com/nottelabs/notte](https://github.com/nottelabs/notte)","['My teams frequently work on agentic features and this kind of compression is generally a base expectation of any task performance, time performance, and cost effectiveness.\n\nMarkdown is an excellent assumed encoding. XML, json, etc. are generally wasteful and harder for even frontier LLMs to work with. Will they answer questions about one document correctly? Sure, usually. 1M questions about 30 documents at a time? Your users are going to be less impressed.', 'That abstraction approach is actually pretty smart. Working in the AI consulting field, I\'ve seen way too many web automation projects fail because teams throw GPT-4 at raw DOM structures and wonder why their costs are through the roof and reliability is inconsistent.\n\nThe natural language action mapping you described solves a real problem. Most web agent frameworks are essentially asking LLMs to be HTML parsers, which is inefficient as hell. Your example of converting that messy div soup into ""I1: Enters departure location"" is exactly the kind of preprocessing that should be standard but somehow isn\'t.\n\nI haven\'t used Notte specifically, but we\'ve built similar abstraction layers for our clients doing process automation. The performance gains from semantic preprocessing are legit. You can drop from GPT-4 to much cheaper models when you\'re not asking them to navigate raw markup. The reliability improvement is even more significant because you\'re giving the model a consistent interface regardless of how the underlying site changes its CSS classes or structure.\n\nWhat\'s interesting about this approach is it mirrors how humans actually think about web interfaces. We don\'t see div tags and class names, we see ""search button"" and ""date picker."" The abstraction makes the task match the model\'s reasoning patterns better.\n\nThe broader application beyond web automation is huge. Any domain where you\'re interfacing between LLMs and structured systems could benefit from this kind of semantic layer. API interactions, database queries, workflow orchestration - all of these suffer from the same ""raw technical interface meets natural language reasoning"" mismatch.\n\nThe real test is how well their parsing holds up across different sites and how much manual mapping you need to do upfront. If it\'s truly automated semantic extraction, that\'s genuinely valuable. If it requires extensive manual configuration per site, it\'s just another abstraction framework.', ""Interesting. I've been doing just HTML -> markdown and it works well enough for my current projects but this method appears to lose less information from the page. Thanks, I'll definitely take a closer look at the repo."", 'Notte’s semantic layer is worth testing, but you’ll only see gains if you build a tight domain schema instead of relying on the default flight demo. I ran a head-to-head on an internal ticketing portal: turned Playwright traces into action maps, fine-tuned a 7B model, and saw roughly 40% fewer tokens pushed through the LLM plus far fewer broken selectors. Two tricks that helped: hash visible text to create stable IDs so small UI tweaks don’t wreck flows, and log recovery time after a missed step to catch brittle spots early. For benchmarks, measure both first-try success and cost per completed run-speed alone can hide retries. I’ve tried LangChain and UiPath for similar work, but [APIWrapper.ai](http://APIWrapper.ai) gives me the raw request hooks when I need to skip the browser shell entirely. Bottom line, sketch your own action taxonomy and run a side-by-side with Browser-Use; if your numbers look like mine, Notte can genuinely cut latency and spend.']","['My teams frequently work on agentic features and this kind of compression is generally a base expectation of any task performance, time performance, and cost effectiveness.\n\nMarkdown is an excellent assumed encoding. XML, json, etc. are generally wasteful and harder for even frontier LLMs to work with. Will they answer questions about one document correctly? Sure, usually. 1M questions about 30 documents at a time? Your users are going to be less impressed.', 'That abstraction approach is actually pretty smart. Working in the AI consulting field, I\'ve seen way too many web automation projects fail because teams throw GPT-4 at raw DOM structures and wonder why their costs are through the roof and reliability is inconsistent.\n\nThe natural language action mapping you described solves a real problem. Most web agent frameworks are essentially asking LLMs to be HTML parsers, which is inefficient as hell. Your example of converting that messy div soup into ""I1: Enters departure location"" is exactly the kind of preprocessing that should be standard but somehow isn\'t.\n\nI haven\'t used Notte specifically, but we\'ve built similar abstraction layers for our clients doing process automation. The performance gains from semantic preprocessing are legit. You can drop from GPT-4 to much cheaper models when you\'re not asking them to navigate raw markup. The reliability improvement is even more significant because you\'re giving the model a consistent interface regardless of how the underlying site changes its CSS classes or structure.\n\nWhat\'s interesting about this approach is it mirrors how humans actually think about web interfaces. We don\'t see div tags and class names, we see ""search button"" and ""date picker."" The abstraction makes the task match the model\'s reasoning patterns better.\n\nThe broader application beyond web automation is huge. Any domain where you\'re interfacing between LLMs and structured systems could benefit from this kind of semantic layer. API interactions, database queries, workflow orchestration - all of these suffer from the same ""raw technical interface meets natural language reasoning"" mismatch.\n\nThe real test is how well their parsing holds up across different sites and how much manual mapping you need to do upfront. If it\'s truly automated semantic extraction, that\'s genuinely valuable. If it requires extensive manual configuration per site, it\'s just another abstraction framework.', ""Interesting. I've been doing just HTML -> markdown and it works well enough for my current projects but this method appears to lose less information from the page. Thanks, I'll definitely take a closer look at the repo."", 'Notte’s semantic layer is worth testing, but you’ll only see gains if you build a tight domain schema instead of relying on the default flight demo. I ran a head-to-head on an internal ticketing portal: turned Playwright traces into action maps, fine-tuned a 7B model, and saw roughly 40% fewer tokens pushed through the LLM plus far fewer broken selectors. Two tricks that helped: hash visible text to create stable IDs so small UI tweaks don’t wreck flows, and log recovery time after a missed step to catch brittle spots early. For benchmarks, measure both first-try success and cost per completed run-speed alone can hide retries. I’ve tried LangChain and UiPath for similar work, but [APIWrapper.ai](http://APIWrapper.ai) gives me the raw request hooks when I need to skip the browser shell entirely. Bottom line, sketch your own action taxonomy and run a side-by-side with Browser-Use; if your numbers look like mine, Notte can genuinely cut latency and spend.']",7,7,0.77,Discussion,1750683446.0
1lieh3l,MachineLearning,[D] Is it possible to convert music audio to guitar tabs or sheet music with transformers?,"Hey folks,

I'm a guitarist who can't sing, so I play full song melodies on my guitar (fingerstyle guitar). I admire those who can transcribe music into tabs or sheet music, but I can't do this myself.

I just had an interesting thought - the process of transcribing music to sheets sounds a lot like language translation, which is a task that the transformer model is originally built for. If we could somehow come up with a system that represents sheet music as tokens, would it be possible to train such a transformer to take audio tokens as input and the sheet music as output?

Any input or thoughts would be greatly appreciated.","[""Yeah this is its own research field (Automatic Music Transcription). [Here's a relevant blog from Magenta](https://magenta.tensorflow.org/transcription-with-transformers) (i.e. Google's music AI research lab). There's plenty of recent research, also some more user-friendly software like AnthemScore as the other commenter said"", 'I think AnthemScore can export as midi and then import that into any tab software. Theres also chordify but its not a full tab just chord progression.', ""there's already music generation with text prompts using diffusion: this means the music is randomly generated but guided by a text prompt. What you'd want is the same, but with sheet music, so more precise and timed rather than just a stylistic guidance. Found one that looks fitting: [https://arxiv.org/pdf/2307.10304](https://arxiv.org/pdf/2307.10304)"", 'the first issue would be to get your hands on a data set with (10 to 100-)thousands of data pairs.']","[""Yeah this is its own research field (Automatic Music Transcription). [Here's a relevant blog from Magenta](https://magenta.tensorflow.org/transcription-with-transformers) (i.e. Google's music AI research lab). There's plenty of recent research, also some more user-friendly software like AnthemScore as the other commenter said"", 'I think AnthemScore can export as midi and then import that into any tab software. Theres also chordify but its not a full tab just chord progression.', ""there's already music generation with text prompts using diffusion: this means the music is randomly generated but guided by a text prompt. What you'd want is the same, but with sheet music, so more precise and timed rather than just a stylistic guidance. Found one that looks fitting: [https://arxiv.org/pdf/2307.10304](https://arxiv.org/pdf/2307.10304)"", 'the first issue would be to get your hands on a data set with (10 to 100-)thousands of data pairs.']",21,16,0.86,Discussion,1750679213.0
1lid95g,MachineLearning,[R] Reinforcement Learning Teachers of Test Time Scaling,"TL;DR: The raw outputs of our new 7B RL model provide stronger distillation and cold-starting than the filtered and post-processed reasoning traces of orders-of-magnitude larger LMs such as DeepSeek-R1.

How did we achieve this result? We turned the RL task on its head. Rather than training to solve challenging problems from scratch, we optimize our models to generate clear, step-by-step *""explanations""* to *""teach""* their students, providing both the problem’s question and its solution already in their input prompt.

This makes the RL training task much easier and also directly aligned with downstream distillation, allowing us to train tiny 7B teachers, boosting the performance of even larger 32B students.

If you are interested to learn more, please check out our new work:

Paper: [https://arxiv.org/abs/2506.08388](https://arxiv.org/abs/2506.08388)

Blog: [https://sakana.ai/rlt/](https://sakana.ai/rlt/)

Open source code: [https://github.com/SakanaAI/RLT](https://github.com/SakanaAI/RLT)

If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :)",[],[],29,0,0.97,Research,1750675030.0
1li43eh,MachineLearning,"[R] [ClsToken, AvgPool] can be a poor choice for transformer embedding models","This paper started with the following question: why do some approaches choose ClsToken vs AvgPool vs MaxPool for Transformer-based embedding models like BERT or ViT, and what are the consequences? Often, these summarization techniques seem like convenient methods for aligning dimensions that just happen to work well enough, and the decision comes down to empirical performance rather than being motivated mathematically. This then evolved into the question — what is the best possible way to summarize embeddings?

We address this question by introducing a framework to evaluate pooling methods as lossy compressors, taking inspiration from vector quantization. For a given task, only a subset of the embeddings matter (signal) while the rest should be treated as noise by the compressor and ignored. The goal of any such pooling method should thus be to aggregate the embeddings in a way that minimizes signal loss.

This reframing reveals failure modes for common methods like ClsToken, AvgPool, and MaxPool as signal-to-noise ratios vary. This result led us to investigate an adaptive attention-based pooling formulation and show that it can both theoretically and empirically lead to better performance and robustness of Transformer embedding models in a variety of applications.

📃 Paper: [https://www.arxiv.org/abs/2506.09215](https://www.arxiv.org/abs/2506.09215) 
👾 Code: [https://github.com/agbrothers/pooling](https://github.com/agbrothers/pooling)

Side note — this is my first main-track conference paper and I’m excited, but also a bit intimidated by the poster session (I’m only a Master’s student). I don’t have an advisor to lean on, so if anyone has any feedback or advice I would really appreciate it!","['Nice work! In my opinion there should be a little more highlight on the problem of choosing a good starting query for the AdaPool method, although you indeed discuss it.\n\n\nMoreover, I\'d suggest you check the paper ""Keep It SimPool"", which basically proposes AdaPool with AvgPool as the starting query. They do a greater theoretical work in unifying pooling methods, but they don\'t do the work you did for SNR and robustness analysis.\n\n\nCheers!', 'nice wow you got this accepted all by yourself without professor help?\n\nAlso, I skimmed it very quickly and was surprised by how bad CLS tokens performed since thats what my team uses a lot for nlp & sequence modeling, but for then I saw the tasks/dataset you tested on which made a bit more sense... Would be nice if there were results on more common tasks used by transformers though and comparing in situations where CLS token has a better advantage (e.g. I think in NLP tasks CLS tokens tend to perform better than pooling...)', ""One of these days I'm going to have to try out a pooling idea that's been kicking around my head for too long (for a matrix of per-token embeddings E, form the matrix EE'; take the dominant eigenvector of that matrix and use the value of each element of the eigenvector as a weight for a weighted sum of the input embeddings)."", 'Joining the comments.\nI’ll start by saying that your paper is intuitive and easy to read, which is great.\nHowever, there are many places where I felt like you outlined obvious/non-relevant theorems, which to me is not a good indication (as it occupies space for more innovation work). Ironically, the main proof of the work (the AdaPool bounds) was placed in the appendix.\nAlso, I was bummed to see that there was very limited comparison to ClsToken, and you didn’t discuss any theoretical benefits of AdaPool over ClsToken. I believe that the comparison to Clstoken is the main point of the work and unfortunately it is almost not discussed .\n\nThis is my take, I hope you’ll find it constructive.\nGood luck!', ""Seems like a nice work! I skimmed it now but will fully read it today. One thing I was curious is how would the result change if you used a ViT trained with SSL such as dino-v2? I'm using a foundational model trained with SSL and take CLS token as image embedding for classification"", ""Lots of papers used weighted pooling (e.g. see additive attention). I wonder what's new in this work ?\n\nAlso, the works mention CLSToken and BERT, but no accompanying experiments in the text domain ?"", 'Thanks for sharing. Recently I made a hypothesis that the different percentage of model input tokens matters for the better understanding of information of density per tokens? My main intention is that if we want the small amount of information from the large amount chunk where it utilises the full model input token length where the information per token length gets diminished right. Whereas if we use the 10 percentage of model tokens that how openai file search system used token based splitting with size of 800 tokens per window with 400 overlap. But the interesting point is that why only 800 tokens out of 8192 (text large 3) where they are only using 10 percent of model capacity whose information density per token has a very high valuable signal than other cases right. Please let me know whether this is going in the right direction or not . Checkout plot results via link. \n\nhttps://drive.google.com/file/d/12m2u80arUjVHMFhUvHCPFocUKMprd0tl/view?usp=drivesdk', ""This is just attention pooling.  What is new?\n\nThis has been done since the mid 2010s for sequence to sequence and document classification...\n\nThe only new thing is using a query vector from the input that you a priori know is useful.\n\nAre the reviewers grad students who aren't aware of literature pre-transformer era...?""]","['Nice work! In my opinion there should be a little more highlight on the problem of choosing a good starting query for the AdaPool method, although you indeed discuss it.\n\n\nMoreover, I\'d suggest you check the paper ""Keep It SimPool"", which basically proposes AdaPool with AvgPool as the starting query. They do a greater theoretical work in unifying pooling methods, but they don\'t do the work you did for SNR and robustness analysis.\n\n\nCheers!', 'nice wow you got this accepted all by yourself without professor help?\n\nAlso, I skimmed it very quickly and was surprised by how bad CLS tokens performed since thats what my team uses a lot for nlp & sequence modeling, but for then I saw the tasks/dataset you tested on which made a bit more sense... Would be nice if there were results on more common tasks used by transformers though and comparing in situations where CLS token has a better advantage (e.g. I think in NLP tasks CLS tokens tend to perform better than pooling...)', ""One of these days I'm going to have to try out a pooling idea that's been kicking around my head for too long (for a matrix of per-token embeddings E, form the matrix EE'; take the dominant eigenvector of that matrix and use the value of each element of the eigenvector as a weight for a weighted sum of the input embeddings)."", 'Joining the comments.\nI’ll start by saying that your paper is intuitive and easy to read, which is great.\nHowever, there are many places where I felt like you outlined obvious/non-relevant theorems, which to me is not a good indication (as it occupies space for more innovation work). Ironically, the main proof of the work (the AdaPool bounds) was placed in the appendix.\nAlso, I was bummed to see that there was very limited comparison to ClsToken, and you didn’t discuss any theoretical benefits of AdaPool over ClsToken. I believe that the comparison to Clstoken is the main point of the work and unfortunately it is almost not discussed .\n\nThis is my take, I hope you’ll find it constructive.\nGood luck!', ""Seems like a nice work! I skimmed it now but will fully read it today. One thing I was curious is how would the result change if you used a ViT trained with SSL such as dino-v2? I'm using a foundational model trained with SSL and take CLS token as image embedding for classification""]",29,18,0.98,Research,1750641752.0
1li8kwy,MachineLearning,"[P] Fenix: An open-source framework using a crew of local LLM agents for financial market analysis (Visual, Technical & Sentiment).","Hi r/MachineLearning,

I'd like to share a project I've developed, **Fenix**, an open-source framework for algorithmic trading that leverages a multi-agent system to tackle the noisy and complex domain of financial markets.

Instead of a single model, the architecture is **heterogeneous**, using specialized local LLMs orchestrated by `CrewAI` for different sub-tasks:

1. **Visual Analysis:** A key feature is the `VisualAnalystAgent`, which uses `LLaVA` to perform visual analysis on chart images, identifying technical patterns that are often missed by purely quantitative models. This has been a fascinating challenge in prompt engineering and grounding the model's analysis.
2. **Quantitative Analysis:** A `TechnicalAnalystAgent` interprets numerical indicators calculated via traditional methods (`pandas-ta`), using a reasoning-focused LLM (`Mixtral`) to translate the data into a qualitative assessment.
3. **Sentiment Analysis:** A `SentimentAgent` processes news and social media text to provide a sentiment score, adding a crucial layer of market context.
4. **Logic Validation:** A `QABBAValidatorAgent` acts as a quality control layer, ensuring the outputs from other agents are coherent and logical before they are passed to the final decision-maker.

The entire system is designed to run on consumer hardware using `Ollama` and quantized models, which presented its own set of engineering challenges in memory management and sequential processing.

The project is open-source (Apache 2.0), and the code is available for review. I'm particularly interested in feedback from the ML community on the agent architecture, potential improvements to the consensus mechanism, and ideas for further research (e.g., reinforcement learning based on trade outcomes).

**GitHub:** [`https://github.com/Ganador1/FenixAI_tradingBot`](https://github.com/Ganador1/FenixAI_tradingBot)

Happy to discuss the methodology, challenges, or results!","[""The multi-agent approach to trading is conceptually solid but you're going to hit some brutal reality checks when this touches real markets. I work at an AI consultancy and we've helped several clients explore algorithmic trading applications - the gap between backtesting performance and live trading results is consistently devastating.\n\nYour agent architecture is well thought out though. Using LLaVA for chart pattern recognition is interesting because visual patterns often do carry signal that pure numerical analysis misses. The problem is that by the time a pattern is visually obvious, the market has usually already priced it in. The edge case scenarios where visual analysis adds alpha are narrow and fleeting.\n\nThe sentiment analysis component is where things get tricky. Social media sentiment is notoriously noisy and often contrarian - positive sentiment frequently correlates with local tops rather than continued upside. News sentiment has similar issues with timing and market efficiency. You'll need to be really careful about how you weight this signal relative to technical factors.\n\nWhat's missing from your description is any discussion of execution, risk management, or position sizing. The analysis pipeline is only half the battle - how are you handling slippage, market impact, drawdown limits, and portfolio allocation? These operational concerns kill more trading systems than bad predictions do.\n\nThe local LLM approach is smart from a cost perspective, but trading decisions often need to happen in milliseconds. If your agent consensus process takes more than a few hundred milliseconds, you're going to miss moves or get filled at worse prices. Latency optimization might need to be a bigger focus than model accuracy.\n\nHave you backtested this on actual market data with realistic transaction costs and slippage assumptions? Most academic trading projects fall apart when you add 2-3 basis points of friction per trade. The real test isn't whether the system can identify patterns, it's whether it can generate enough alpha to overcome implementation costs."", 'where are you getting the chart data?']","[""The multi-agent approach to trading is conceptually solid but you're going to hit some brutal reality checks when this touches real markets. I work at an AI consultancy and we've helped several clients explore algorithmic trading applications - the gap between backtesting performance and live trading results is consistently devastating.\n\nYour agent architecture is well thought out though. Using LLaVA for chart pattern recognition is interesting because visual patterns often do carry signal that pure numerical analysis misses. The problem is that by the time a pattern is visually obvious, the market has usually already priced it in. The edge case scenarios where visual analysis adds alpha are narrow and fleeting.\n\nThe sentiment analysis component is where things get tricky. Social media sentiment is notoriously noisy and often contrarian - positive sentiment frequently correlates with local tops rather than continued upside. News sentiment has similar issues with timing and market efficiency. You'll need to be really careful about how you weight this signal relative to technical factors.\n\nWhat's missing from your description is any discussion of execution, risk management, or position sizing. The analysis pipeline is only half the battle - how are you handling slippage, market impact, drawdown limits, and portfolio allocation? These operational concerns kill more trading systems than bad predictions do.\n\nThe local LLM approach is smart from a cost perspective, but trading decisions often need to happen in milliseconds. If your agent consensus process takes more than a few hundred milliseconds, you're going to miss moves or get filled at worse prices. Latency optimization might need to be a bigger focus than model accuracy.\n\nHave you backtested this on actual market data with realistic transaction costs and slippage assumptions? Most academic trading projects fall apart when you add 2-3 basis points of friction per trade. The real test isn't whether the system can identify patterns, it's whether it can generate enough alpha to overcome implementation costs."", 'where are you getting the chart data?']",1,4,0.67,Project,1750656699.0
1li7xvi,MachineLearning,[D] [Reviewer Question] ACM MM 2025 – Can I update my rating after rebuttal?,"Hey folks,
I'm reviewing a couple of papers for ACM Multimedia this season, and I received a mail from the chairs saying that I can update my reviews until June 23 EOD.

The mail says I should update my review based on the rebuttal, but I'm a bit unclear: **am I allowed to change my overall rating (score) at this stage?** Or is this just meant for updating the comments?

Also, do they give us another timeline after this to modify our scores again? Or is this the final say?

Curious to know how others are handling this. Are you adjusting your scores if the rebuttal changed your perspective? Or only tweaking the comments?

Would appreciate any clarity from folks who’ve done this before or are in the same boat.

Thanks!","['Yes, you should change your overall rating based on the rebuttal, particularly if the authors addresses one of your concerns.', 'By clicking edit button on your review, you can change your rating and comment']","['Yes, you should change your overall rating based on the rebuttal, particularly if the authors addresses one of your concerns.', 'By clicking edit button on your review, you can change your rating and comment']",3,7,0.8,Discussion,1750654376.0
1li59r6,MachineLearning,[R] Does quantization affect models' performance on long-context tasks?(arXiv:2505.20276),"4-bit quantized models generally exhibit small performance performance drops in general (with good quantization methods like AWQ / GPTQ / etc). In this work we set about to find out if there are specific tasks where quantized models start to significantly underperform. We found that this occurs on very long-context tasks with long context seeing larger performance drops relative to the full-precision models

>**Abstract:**
Large language models (LLMs) now support context windows exceeding 128K tokens, but this comes with significant memory requirements and high inference latency. Quantization can mitigate these costs, but may degrade performance. In this work, we present the first systematic evaluation of quantized LLMs on tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B, and 72B). We find that, on average, 8-bit quantization preserves accuracy (\~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for tasks involving long context inputs (drops of up to 59%). This degradation tends to worsen when the input is in a language other than English. Crucially, the effects of quantization depend heavily on the quantization method, model, and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4, Llama-3.1 70B experiences a 32% performance drop on the same task. These findings highlight the importance of a careful, task-specific evaluation before deploying quantized LLMs, particularly in long-context scenarios and with languages other than English.

[https://arxiv.org/abs/2505.20276](https://arxiv.org/abs/2505.20276)

","['I have this hunch that the success of quantization is completely based on the amount of informational representation in the model. When you blanket-quantize the whole model or force a model to use a certain representation during training, you are either reducing the informational flow, or delegating the information to other parts of the network.\n\n\nFor example a MoE model that is routing to 16 different experts should only need log2(16) = 4 bits']","['I have this hunch that the success of quantization is completely based on the amount of informational representation in the model. When you blanket-quantize the whole model or force a model to use a certain representation during training, you are either reducing the informational flow, or delegating the information to other parts of the network.\n\n\nFor example a MoE model that is routing to 16 different experts should only need log2(16) = 4 bits']",15,1,1.0,Research,1750645485.0
1li3iig,MachineLearning,Good Math Heavy Theoretical Textbook on Machine Learning? [D],"I recently implemented a neural network for my internship, and I found the subject very interesting. It is a topic that is probably very useful for me to learn more about. I am now looking for a deep learning textbook which provides a math heavy theoretical understanding of why deep learning works. I would also like it to be modern, including transformers and other new developments.

I have so far completed the requisites for a math major as well as a bunch of math electives and a good chunk of a physics major at my university, so I do not think math will be an issue. I would therefore like a textbook which assumes a lot of math knowledge.","[""Here's the list of books which I find relevant:\n\n- Deep Learning: Foundations and Concepts: https://www.amazon.com/Deep-Learning-Foundations-Christopher-Bishop/dp/3031454677 (available on Book's site, though in not very readable form: https://www.bishopbook.com/)\n\n- Pattern Recognition and Machine Learning: https://www.amazon.com/dp/0387310738 (the same author as the previous, has overlap with the previous book, but was written before deep learning became really popular) (available on the author's site: https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)\n\n- https://www.amazon.com/Learning-Principles-Adaptive-Computation-Machine/dp/0262049449 Learning Theory from First Principles (available on the author's site: https://www.di.ens.fr/~fbach/)\n\n- https://www.amazon.com/Principles-Deep-Learning-Theory-Understanding The Principles of Deep Learning Theory: An Effective Theory Approach To Understanding Neural Network (available on arxiv: https://arxiv.org/abs/2106.10165)"", ""As a researcher working on LLM, I would recommend separate books for machine learning and deep learning/LLM.\n\n  \nFor machine learning one hard-core book I used during PhD period is\n\nUnderstanding Machine Learning: From Theory to Algorithms: [https://www.cs.huji.ac.il/\\~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)\n\nOf course PRML is worth to read and should be easier to understand.\n\n  \nFor deep learning maybe read the deep learning book: [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/) I am not a good person to recommend this since I work on this field so I just keep reading papers.\n\n  \nFor LLM you could follow Andrej Karpathy's list: [https://www.oxen.ai/blog/reading-list-for-andrej-karpathys-intro-to-large-language-models-video](https://www.oxen.ai/blog/reading-list-for-andrej-karpathys-intro-to-large-language-models-video) \n\n  \n\n\nYou can also follow huggingface cofounder's reading list: [https://thomwolf.io/data/Thom\\_wolf\\_reading\\_list.txt](https://thomwolf.io/data/Thom_wolf_reading_list.txt) which has some overlap with what I included."", 'simon prince understanding deep learning is great', 'For StatML/convergence I would suggest learning theory, convex optimization and stochastic processes before delving into research papers.\n\nDeep nets have until recently been quite a mystery, now we know they converge: https://arxiv.org/pdf/2505.15013?\n\nI can also recommend neural tangent kernels https://arxiv.org/abs/1806.07572 and the mean field approximation https://arxiv.org/abs/1804.06561 they do some relaxations but also showcase convergence.', ""The usual suspects for the an encompassing ML landscape:  \nPML: Murphy's books (Probabilistic Machine Learning series)  \nDLB: Goodfellow et al's Deep Learning Book  \nESL: Tibshirani, Hastie, Friedman's Elements of Statistical Learning  \nBRML: Barber's Baysian Reasoning and Machine Leanring  \nPGM: Koller's Probabilistic Graphical Models  \nFML: Mohri et al's Foundations of Machine Learning  \nUML: Ben-David and Schlev-Schwartz' Understanding Machine Learning  \nPRML: Bishop's Pattern Recognition and Machine Learning\n\nHonorable addition:  \nITILA: MacKay's Information Theory, Inference, and Learning Algorithms\n\nSome of them go deep into deep learning, especially DLB (duh) but DL itself is a dynamic field that is mostly in the research papers rather than books."", 'For theory of ML: Gareth et al *An Introduction to Statistical Learning*', 'This isn’t mainstream deep learning per se, but since you mention a math background you may be interested in [Hamilton’s Graph Representation Learning](https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf) book. Graph learning is mathematically beautiful and interesting and also incredibly powerful in practice. Give it a peek!', 'Why machines learn\xa0', 'Mathematics for machine learning']","[""Here's the list of books which I find relevant:\n\n- Deep Learning: Foundations and Concepts: https://www.amazon.com/Deep-Learning-Foundations-Christopher-Bishop/dp/3031454677 (available on Book's site, though in not very readable form: https://www.bishopbook.com/)\n\n- Pattern Recognition and Machine Learning: https://www.amazon.com/dp/0387310738 (the same author as the previous, has overlap with the previous book, but was written before deep learning became really popular) (available on the author's site: https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)\n\n- https://www.amazon.com/Learning-Principles-Adaptive-Computation-Machine/dp/0262049449 Learning Theory from First Principles (available on the author's site: https://www.di.ens.fr/~fbach/)\n\n- https://www.amazon.com/Principles-Deep-Learning-Theory-Understanding The Principles of Deep Learning Theory: An Effective Theory Approach To Understanding Neural Network (available on arxiv: https://arxiv.org/abs/2106.10165)"", ""As a researcher working on LLM, I would recommend separate books for machine learning and deep learning/LLM.\n\n  \nFor machine learning one hard-core book I used during PhD period is\n\nUnderstanding Machine Learning: From Theory to Algorithms: [https://www.cs.huji.ac.il/\\~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)\n\nOf course PRML is worth to read and should be easier to understand.\n\n  \nFor deep learning maybe read the deep learning book: [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/) I am not a good person to recommend this since I work on this field so I just keep reading papers.\n\n  \nFor LLM you could follow Andrej Karpathy's list: [https://www.oxen.ai/blog/reading-list-for-andrej-karpathys-intro-to-large-language-models-video](https://www.oxen.ai/blog/reading-list-for-andrej-karpathys-intro-to-large-language-models-video) \n\n  \n\n\nYou can also follow huggingface cofounder's reading list: [https://thomwolf.io/data/Thom\\_wolf\\_reading\\_list.txt](https://thomwolf.io/data/Thom_wolf_reading_list.txt) which has some overlap with what I included."", 'simon prince understanding deep learning is great', 'For StatML/convergence I would suggest learning theory, convex optimization and stochastic processes before delving into research papers.\n\nDeep nets have until recently been quite a mystery, now we know they converge: https://arxiv.org/pdf/2505.15013?\n\nI can also recommend neural tangent kernels https://arxiv.org/abs/1806.07572 and the mean field approximation https://arxiv.org/abs/1804.06561 they do some relaxations but also showcase convergence.', 'For theory of ML: Gareth et al *An Introduction to Statistical Learning*']",108,14,0.97,Discussion,1750639994.0
1lhv42l,MachineLearning,[D] How do you keep up with the flood of new ML papers and avoid getting scooped?,"These days, there are dozens of new ML papers published on arXiv every single day. It’s exciting, but also overwhelming (my google scholar alert). Genuinely asking, for those actively doing research, how do you:

1. Keep up with relevant papers in your area? Learn from the latest SOTA techniques early enough to incorporate them into your own research?
2. Make sure you’re not being scooped by similar work?","[""It is not possible to keep-up with arXiv anymore. The amount of ML paper published per day is too high. But you also shouldn't try to do it. arXiv paper are pre-prints, while there might be some good ones, a big portion of them are not, and many will go trough a lot of changes until they get accepted in a conference. You should take a look at big conference proceedings.\n\nApart from that, I think that the best way to keep yourself updated is social media. Follow the researchers/research groups you like, or that are working on similar topics social media accounts. They will usually post about their research or share papers from other people that they like. Twitter used to have an amazing ML community, after Elon, some people left, but still a good place to keep yourself updated. This subreddit, r/LocalLLaMA, r/StableDiffusion... are also good places to keep a look at. AK daily best papers is also worth taking a look every morning: [https://huggingface.co/papers/](https://huggingface.co/papers/)"", ""The majority of papers aren't very impactful. Citation counts and social media chatter are pretty decent signals as to whether something is worth paying attention to or not."", 'I\'m going to answer from an inverted perspective. I work as an AI Researcher, so it\'s my actual job to put out high quality work. I like to believe that I have enough integrity that I won\'t just publish for the sake thereof. Instead, if I release work, then I have truly put massive amounts of effort into it.\xa0\n\n\nHow does that statement answer/help you? In two ways:\n1) Realise that it is people and teams who publish. I learnt pretty early in my PhD, from my extremely intelligent supervisor, that there will be individuals or labs that will almost always release work that is interesting and worth paying attention to. I hope I am one of those people, and I work towards that. So, spend time understand the research and research agendas that you care about and you\'ll find good work via good people.\xa0\n2) The more effort that goes into releasing work, the more likely it is that this is something worth paying attention to. Blogs, interactive tutorials, well-presented papers, etc. are all indicators worth noting. Of course this not guaranteed, but it is worth realising that the process of taking an arxiv publication to acceptance at a top-tier peer reviewed conference requires a similar amount of effort for ""release"".\n\n\n\n\n\n\nP.S. You don\'t have to be a ""publications machine"" to be successful as an AI researcher.\xa0', ""Perhaps the best way is to be able to spot the bad ones. There's a lot of BS out there."", ""I actually asked the same question here around a year ago, during the early stages of my PhD, and it seems like the rate of papers coming out every day is now even faster. 😔 I think even though the volume of papers coming out is larger, the amount of high-quality works has not caught up as much. \n\nMy best advice for general reading is to intentionally filter for quality by following the authors you like on Google Scholar and peruse papers that go viral on X/Twitter.  For your own specific research area, I first think that choosing the problem is the most important, and you want to work in an area that is not currently the most popular but might be soon (unless you have a very good idea that is quick to execute). I'd recommend keeping notification alerts on the seminal works on your field to check if you are getting scooped (if it is a good work, it will be citing the same relevant set of papers)."", 'To everyone saying “just follow specific teams / people / labs”: please can you shoutout some of your favorite labs / authors?', ""Frankly, I don't. I stay in my fairly specific niche, and even within that niche, I'm mainly keeping up with anything which uses specific methods or which might relate to those methods. Outside of that, in the wider niche, I will typically just skim abstracts to see if there's anything sort of interesting or potentially applicable to my research. For ML in general, I just read the high-impact papers, usually after seeing them doing the rounds on social media etc."", ""I don't"", 'We ""don\'t"".', 'I like looking at somewhat relevant kaggle competitions solutions/other competitive settings, usually a good filter for high performance tricks that do replicate']","[""It is not possible to keep-up with arXiv anymore. The amount of ML paper published per day is too high. But you also shouldn't try to do it. arXiv paper are pre-prints, while there might be some good ones, a big portion of them are not, and many will go trough a lot of changes until they get accepted in a conference. You should take a look at big conference proceedings.\n\nApart from that, I think that the best way to keep yourself updated is social media. Follow the researchers/research groups you like, or that are working on similar topics social media accounts. They will usually post about their research or share papers from other people that they like. Twitter used to have an amazing ML community, after Elon, some people left, but still a good place to keep yourself updated. This subreddit, r/LocalLLaMA, r/StableDiffusion... are also good places to keep a look at. AK daily best papers is also worth taking a look every morning: [https://huggingface.co/papers/](https://huggingface.co/papers/)"", ""The majority of papers aren't very impactful. Citation counts and social media chatter are pretty decent signals as to whether something is worth paying attention to or not."", 'I\'m going to answer from an inverted perspective. I work as an AI Researcher, so it\'s my actual job to put out high quality work. I like to believe that I have enough integrity that I won\'t just publish for the sake thereof. Instead, if I release work, then I have truly put massive amounts of effort into it.\xa0\n\n\nHow does that statement answer/help you? In two ways:\n1) Realise that it is people and teams who publish. I learnt pretty early in my PhD, from my extremely intelligent supervisor, that there will be individuals or labs that will almost always release work that is interesting and worth paying attention to. I hope I am one of those people, and I work towards that. So, spend time understand the research and research agendas that you care about and you\'ll find good work via good people.\xa0\n2) The more effort that goes into releasing work, the more likely it is that this is something worth paying attention to. Blogs, interactive tutorials, well-presented papers, etc. are all indicators worth noting. Of course this not guaranteed, but it is worth realising that the process of taking an arxiv publication to acceptance at a top-tier peer reviewed conference requires a similar amount of effort for ""release"".\n\n\n\n\n\n\nP.S. You don\'t have to be a ""publications machine"" to be successful as an AI researcher.\xa0', ""Perhaps the best way is to be able to spot the bad ones. There's a lot of BS out there."", ""I actually asked the same question here around a year ago, during the early stages of my PhD, and it seems like the rate of papers coming out every day is now even faster. 😔 I think even though the volume of papers coming out is larger, the amount of high-quality works has not caught up as much. \n\nMy best advice for general reading is to intentionally filter for quality by following the authors you like on Google Scholar and peruse papers that go viral on X/Twitter.  For your own specific research area, I first think that choosing the problem is the most important, and you want to work in an area that is not currently the most popular but might be soon (unless you have a very good idea that is quick to execute). I'd recommend keeping notification alerts on the seminal works on your field to check if you are getting scooped (if it is a good work, it will be citing the same relevant set of papers).""]",82,24,0.91,Discussion,1750617377.0
1lhtkr4,MachineLearning,[P] I made a website to visualize machine learning algorithms + derive math from scratch,"Check out the website: https://ml-visualized.com/

1. Visualizes Machine Learning Algorithms Learning
2. Interactive Notebooks using marimo and Project Jupyter
3. Math from First-Principles using Numpy and Latex
4. Fully Open-Sourced

Feel free to star the repo or contribute by making a pull request to https://github.com/gavinkhung/machine-learning-visualized

I would love to create a community. Please leave any questions below; I will happily respond.","['Really impressive stuff. Thank you for including the code with each visualization as well.', 'I second an extension to express thanks for providing this, it contains a lot for of helpful info. This reminds me a lot of d2l.ai (reference material I used to learn some basics of ML), but with a lot more focus on math and lovely visuals.', 'BRO. thats REALLY GOOD. Will really love to contribute to this project, if I can find the time. Keep Going!!', 'Looks great, sorry if this is a silly question but why are the chapters backwards?', 'Really cool! I look forward to following this work!', 'This is exceptionally useful', 'Wonderful work! Thanks for sharing.', 'Thats a good idea by the way', 'very cool! thanks for this', ""This is really cool, would love to see GNN's visualized :)""]","['Really impressive stuff. Thank you for including the code with each visualization as well.', 'I second an extension to express thanks for providing this, it contains a lot for of helpful info. This reminds me a lot of d2l.ai (reference material I used to learn some basics of ML), but with a lot more focus on math and lovely visuals.', 'BRO. thats REALLY GOOD. Will really love to contribute to this project, if I can find the time. Keep Going!!', 'Looks great, sorry if this is a silly question but why are the chapters backwards?', 'Really cool! I look forward to following this work!']",365,22,0.99,Project,1750613594.0
1lhs5hm,MachineLearning,[D] ECAI 2025 reviews discussion,European Conference on Artificial Intelligence (ECAI) 2025 reviews are due tomorrow. Let's discuss here when they arrive. Best luck to everyone!,"[""And I need to reschedule a meeting about the reviews because they're still not out :|"", 'Any info about the reviews?why delayed?', 'We are quite well past and still no update.', ""Guys, stop asking if you have a chance with score X - nobody knows, since scores & acceptance haven't been tracked in previous years!\n\nIf you want this to change, and really know this in the future, fill the form at Paper Copilot - [https://papercopilot.com/](https://papercopilot.com/) This website allows inputting scores and later rejection/acceptance. This builds statistics for each year, so next year you all will know the chance with a given score. ECAI is not yet supported there, but if you fill the form and also support the issue at [https://github.com/papercopilot/ContactUs/issues/2](https://github.com/papercopilot/ContactUs/issues/2), there is higher change it will get added."", ""The 'Call for Papers' (https://ecai2025.org/call-for-papers/) specifies:\n\n\\`**Rebuttal period:**\xa0Monday-Wednesday 23-25 June 2025 (72 hours) ... All deadlines are at the end of the day specified, anywhere on Earth (UTC-12).\\`\n\nWhich leads me to believe that the reviews might not be there till tomorrow at 13:59"", 'There must be some miscommunication going on since we are 12 hours past the start of the rebuttal period and we still have not received any news. Or am i missing something?', 'Just a reminder - 23 Jun AOE means we still have several hours to go before notifications are out..', 'Overall in my experience ecai has the best quality reviews. Let’s thank them.', 'Does anyone already have reviews?', 'any updates? :0']","['Any info about the reviews?why delayed?', ""And I need to reschedule a meeting about the reviews because they're still not out :|"", 'We are quite well past and still no update.', ""Guys, stop asking if you have a chance with score X - nobody knows, since scores & acceptance haven't been tracked in previous years!\n\nIf you want this to change, and really know this in the future, fill the form at Paper Copilot - [https://papercopilot.com/](https://papercopilot.com/) This website allows inputting scores and later rejection/acceptance. This builds statistics for each year, so next year you all will know the chance with a given score. ECAI is not yet supported there, but if you fill the form and also support the issue at [https://github.com/papercopilot/ContactUs/issues/2](https://github.com/papercopilot/ContactUs/issues/2), there is higher change it will get added."", 'There must be some miscommunication going on since we are 12 hours past the start of the rebuttal period and we still have not received any news. Or am i missing something?']",49,193,0.87,Discussion,1750610044.0
1lhrwqf,MachineLearning,[D] How structured prediction differs from classification and regression?,"In the ""Deep Learning"" book from Goodfellow et. al we find the following definition:

>Structured output: Structured output tasks involve any task where the output is a vector (or other data structure containing multiple values) with important relationships between the diﬀerent elements. This is a broad category, and subsumes the transcription and translation tasks described above, but also many other tasks.

Based on this definition even simple multi-output regression (i.e. predicting multiple y's) would count as structured prediction because we are predicting a vector. The same applies also for multi-label classification where we can predict \[0, 1, 0, 1\] (where 0/1 indicates the absence/presence of the class). Is there any formal definition of structured prediction? Or all predictive supervised tasks can be considered as classification or regression or a combination of the two (e.g. in object recognition where we regress bounding box values and classify the content)?

\* Note that I am talking only about predictive tasks and I ignore generative supervised tasks like conditional image generation (where we need the labels of the images during training).","[""Well, I think the best way to read this is that all output is structured, but there's stuff beyond the basics. What we're really talking about is more complex relationship between outputs, besides simple softmax kind of relationships.   \n  \nProbably the best simple example of structured output is YOLO, where bounding box regression and classification are mixed. While we can consider the bounding box regression, and consider the category classification as separate, its more reasonable to consider both at the same time. Its structured because the error is not simply the sum of error from bounding box, plus error from classification. In most multi-classification or regression tasks, the total error is simply the sum of each element considered independently.""]","[""Well, I think the best way to read this is that all output is structured, but there's stuff beyond the basics. What we're really talking about is more complex relationship between outputs, besides simple softmax kind of relationships.   \n  \nProbably the best simple example of structured output is YOLO, where bounding box regression and classification are mixed. While we can consider the bounding box regression, and consider the category classification as separate, its more reasonable to consider both at the same time. Its structured because the error is not simply the sum of error from bounding box, plus error from classification. In most multi-classification or regression tasks, the total error is simply the sum of each element considered independently.""]",0,1,0.5,Discussion,1750609426.0
1lhny9b,MachineLearning,"[P] This has been done like a thousand time before, but here I am presenting my very own image denoising model","I would like some advice on how to denoise smooth noise like Gaussian and Poisson, currently the model is doing very well for impulsive noise like salt and pepper(I guess this is due to the fact that there are many uncorrupted pixels in the input for the model to rely on), but for smooth noise, the same model architecture doesn't perform as good.","['Denoising a smooth Gaussian noise in a way that seeks to preserve high frequency details is almost the same as an upsampling task, in the sense that removing Gaussian noise is generally done with a smoothing/gaussian blur operation. \n\nGood upsample models tend to be distillations of more powerful general image models. It’s hard to correctly “denoise” a complex feature without having latent features that understand it at a deeper level than the local pixel distribution. \n\nI’d check out what people are doing for up sampling models these days, as you can construct your problem as “apply a Gaussian blur and then upsample”', 'Are your noisy images artificially created or actual camera images?', 'What exactly is a ""smooth"" noise?', 'Oh I completely forgot to put a link to the project:\xa0https://github.com/WilliamChristopherAlt/DenoiseResNet', 'what kind of architecture are you using?', 'Just my 2 cents:\n\nWhen denoising images, you\'ll usually get better results with U-Nets. The reason for that is that the downsampling steps will naturally average out some of the noise, while also increasing the model\'s receptive field with means it\'ll have a better ""global"" understanding of things.\n\nIf you\'re creating the noisy images artificially (there\'s absolutely nothing wrong with that), make sure to add noise from different distributions and at different strengths. You can even have multiple noisy versions of the same reference (which means it\'s relatively trivial to create a lot of training data).\n\nAnother recentish trick is to not have any biases in the model. Empirically this has helped train models that generalise better.\n\nA few articles worth reading:\n\n[https://arxiv.org/abs/2008.13751](https://arxiv.org/abs/2008.13751)\n\n[https://arxiv.org/abs/1906.05478](https://arxiv.org/abs/1906.05478)\n\n[https://arxiv.org/abs/2203.13278](https://arxiv.org/abs/2203.13278)', ""Want to math it up? \n\nTime for variational methods with ML. I strongly suggest you have a look at the literature of Plug and Play (PnP) methods, particularly gradient step denoisers. They don't just do a good job, they also have strong mathematical foundations."", 'Pass in the noisy samples. Output the denoised images and compute the MSE loss between the denoised output and the true denoised sample. This was a very generalised answer. Do you have anything specific to ask?', 'Try learning the noise with the model instead of learning the reconstruction (DnCNN). for example instead of returning img x, return x - h where h is the output of your resnet.', ""Hey I'm often in the place shown in the first photo! :)\n\nI'm really surprised!""]","['Denoising a smooth Gaussian noise in a way that seeks to preserve high frequency details is almost the same as an upsampling task, in the sense that removing Gaussian noise is generally done with a smoothing/gaussian blur operation. \n\nGood upsample models tend to be distillations of more powerful general image models. It’s hard to correctly “denoise” a complex feature without having latent features that understand it at a deeper level than the local pixel distribution. \n\nI’d check out what people are doing for up sampling models these days, as you can construct your problem as “apply a Gaussian blur and then upsample”', 'Are your noisy images artificially created or actual camera images?', 'What exactly is a ""smooth"" noise?', 'Just my 2 cents:\n\nWhen denoising images, you\'ll usually get better results with U-Nets. The reason for that is that the downsampling steps will naturally average out some of the noise, while also increasing the model\'s receptive field with means it\'ll have a better ""global"" understanding of things.\n\nIf you\'re creating the noisy images artificially (there\'s absolutely nothing wrong with that), make sure to add noise from different distributions and at different strengths. You can even have multiple noisy versions of the same reference (which means it\'s relatively trivial to create a lot of training data).\n\nAnother recentish trick is to not have any biases in the model. Empirically this has helped train models that generalise better.\n\nA few articles worth reading:\n\n[https://arxiv.org/abs/2008.13751](https://arxiv.org/abs/2008.13751)\n\n[https://arxiv.org/abs/1906.05478](https://arxiv.org/abs/1906.05478)\n\n[https://arxiv.org/abs/2203.13278](https://arxiv.org/abs/2203.13278)', 'Oh I completely forgot to put a link to the project:\xa0https://github.com/WilliamChristopherAlt/DenoiseResNet']",540,73,0.95,Project,1750599089.0
1lhmvbs,MachineLearning,[P] AI Learns to Play Tekken 3 (Deep Reinforcement Learning) | #tekken #deep...,"I trained an agent that plays Tekken using PPO from Stable-Baselines3 and Stable-retro to create the training environment. Code below:
[https://github.com/paulo101977/AI-Tekken3-Stable-Retro](https://github.com/paulo101977/AI-Tekken3-Stable-Retro)",['Useless.   \nJust pick Eddy Gordo and have the RNG spam random keys.'],['Useless.   \nJust pick Eddy Gordo and have the RNG spam random keys.'],0,1,0.5,Project,1750595814.0
1lhlsds,MachineLearning,[D]Best metrics for ordinal regression?,"Does anyone know of there are good metrics to evaluate ordinal regression models? Currently using mainly RMSE and macro averaged MAE.
The data spans 4 classes with negative skewness (tail to the left).","['Kendall Tau is a ranking correlation metric.', 'For ordinal regression with negative skew, RMSE and MAE are decent starting points but you\'re missing some key metrics that capture the ordinal structure better. I work at a consulting firm that helps companies optimize their ML evaluation pipelines, and ordinal regression evaluation is honestly more nuanced than most people realize.\n\nWhat actually works better for ordinal data:\n\nMean Absolute Error (MAE) is good because it treats all misclassifications equally, but you might want Quadratic Weighted Kappa (QWK) which penalizes distant misclassifications more heavily. This captures the ordinal nature better than standard classification metrics.\n\nKendall\'s Tau correlation coefficient measures the ordinal association between predicted and actual rankings. This is particularly useful for understanding whether your model preserves the ordering correctly.\n\nCumulative Link Model metrics like the proportional odds assumption test. If you\'re using ordinal logistic regression, check whether the proportional odds assumption holds.\n\nFor your negative skew specifically:\n\nClass-weighted metrics might be more informative than macro averaging since your tail classes probably have fewer samples. Consider using balanced accuracy or F1 scores per class.\n\nConfusion matrix analysis becomes crucial with skewed ordinal data. Look at whether errors are systematic (always predicting one class higher/lower) or random.\n\nConsider using Mean Zero-One Error which counts exact matches only, plus a tolerance-based accuracy metric that accepts predictions within 1 class as ""close enough.""\n\nThe negative skew suggests your model might be biased toward higher classes. Plot residuals by predicted class to see if there are systematic biases.\n\nWhat\'s your specific application domain? Medical severity scores, customer satisfaction ratings, etc.? That might inform which metrics matter most for your use case.', ""Generally those are the best metrics. You can also use regular metrics, e.g. accuracy or AUROC, ignoring the ordinal aspect. I've had good results with comparing accuracy and accuracy@1 (which allows predictions 1 level lower or higher than ground truth).""]","['Kendall Tau is a ranking correlation metric.', 'For ordinal regression with negative skew, RMSE and MAE are decent starting points but you\'re missing some key metrics that capture the ordinal structure better. I work at a consulting firm that helps companies optimize their ML evaluation pipelines, and ordinal regression evaluation is honestly more nuanced than most people realize.\n\nWhat actually works better for ordinal data:\n\nMean Absolute Error (MAE) is good because it treats all misclassifications equally, but you might want Quadratic Weighted Kappa (QWK) which penalizes distant misclassifications more heavily. This captures the ordinal nature better than standard classification metrics.\n\nKendall\'s Tau correlation coefficient measures the ordinal association between predicted and actual rankings. This is particularly useful for understanding whether your model preserves the ordering correctly.\n\nCumulative Link Model metrics like the proportional odds assumption test. If you\'re using ordinal logistic regression, check whether the proportional odds assumption holds.\n\nFor your negative skew specifically:\n\nClass-weighted metrics might be more informative than macro averaging since your tail classes probably have fewer samples. Consider using balanced accuracy or F1 scores per class.\n\nConfusion matrix analysis becomes crucial with skewed ordinal data. Look at whether errors are systematic (always predicting one class higher/lower) or random.\n\nConsider using Mean Zero-One Error which counts exact matches only, plus a tolerance-based accuracy metric that accepts predictions within 1 class as ""close enough.""\n\nThe negative skew suggests your model might be biased toward higher classes. Plot residuals by predicted class to see if there are systematic biases.\n\nWhat\'s your specific application domain? Medical severity scores, customer satisfaction ratings, etc.? That might inform which metrics matter most for your use case.', ""Generally those are the best metrics. You can also use regular metrics, e.g. accuracy or AUROC, ignoring the ordinal aspect. I've had good results with comparing accuracy and accuracy@1 (which allows predictions 1 level lower or higher than ground truth).""]",2,7,0.75,Discussion,1750592105.0
1lhkgha,MachineLearning,[P] Open source astronomy project: need best-fit circle advice,,"[""Hi,\n\nI'm maintaining an open-source tool called [DFTFringe](https://github.com/atsju/DFTFringe/issues/37) that analyzes interferometry images to deduce the shape of telescope mirrors. It's used by many amateur telescope makers and works well overall.\n\nThere's one manual step we'd like to automate: fitting a circle to an image feature, with ~1 pixel accuracy. More background here: [discussion thread](https://groups.io/g/Interferometry/topic/113665708#msg36232).\n\nIf you have suggestions for good approaches or algorithms, I’d love to hear them. Specific advice is very welcome — and if anyone feels like going further with a proof of concept, that would be fantastic (but absolutely not expected).\n\nYou can reply here or comment on [GitHub](https://github.com/atsju/DFTFringe/issues/37).\n\nThanks!"", 'In combination to what others have said, I recommend performing a preprocessing step before the Hough transform to account for the stripy nature of the image. This seems relevant: [https://www.reddit.com/r/computervision/comments/1k9p83h/detecting\\_striped\\_circles\\_using\\_computer\\_vision/](https://www.reddit.com/r/computervision/comments/1k9p83h/detecting_striped_circles_using_computer_vision/)', 'I have some alternative ideas from others\'. I think a simple corner detector will find a lot of sharp corners in and at the boundary of the fringes. See the result of a barely tuned Harris corner detector [here](https://i.imgur.com/geMAfiI.png). With a bit of filtering (first filter away outliers with an SOR filter or something, and the filter those that aren\'t part of the supporting planes of the convex hull to filter those on the ""inside""), you\'ll have a list of points that are near certainly on the boundary. From there you can optimise the radius and centre to minimise deviation from the boundary points, and chuck a robust loss term to make sure anything that didn\'t get filtered doesn\'t have too much effect. \n\nCompared to other people\'s solutions, I\'m trying to minimise lossy operations that\'ll erode away pixel detail. Hough transforms are incredibly finicky to work with for any non perfect images, and any operations to make this more ""circle like"" without the pattern are just as hard, not to mention almost certainly modifying location of features. \n\nI\'m happy to help implement this in a few days when I get some time.', 'I was very lazy and went with a ChatGPT-generated solution, is this good enough for you applications? If yes, I can go deeper and maybe make it a bit more accurate, but my time is quite restricted right now.\n\n  \n[https://ibb.co/d0XXdj7L](https://ibb.co/d0XXdj7L)', ""i'm not sure if all pictures in your dataset would look like this\n\nbut just off of this \\_single\\_ image you have given, this is what I think:\n\nthe average intensity inside the circle would probably average out to gray, which is the same outside the circle, so you cannot do it over average intensity of patches. . . \n\nhowever, it seems that everything inside the circle has this long stripes of black and white, while things outside the circle does NOT have this long stripe.\n\nI think you should first devise an algorithm to identify long, continuous stripes (perhaps a floodfill algorithm with some tweak of threshold?). this would allow you to separate the original image into 3 kinds of segments: background, black-stripe, and white-stripe.\n\nthen, simply re-color all the black-stripe and white-stripe red, and fit a circle over the red pixels.\n\n???"", 'if inside the circle is periodic, you could potentially compute a gaussian-windowed 2-d fourier transform (Gabor transform) for each x,y coordinate.\n\nThis should at least identify the periodicity inside the circles vs outside.\n\nYou could come up with some concentration measure for the fourier amplitudes, since the frequencies would be more uniformly distributed outside of the circle.  for inspiration, look at the participation ratio:\n\nE[ |X|^2 ]^2 / E[ |X|^4 ]\n\nwhich is small (large) for concentrated (diffuse) functions.', 'I imagine that a lot of algorithms will struggle with the high noise. If that is the case I would suggest leveraging the fact that the features of interest consist of high contrast curves. A laplacian of gaussian filter tuned well would probably clean it right up. It would take some tuning through and if the noise characteristics change greatly between images it would not be consistent.']","[""Hi,\n\nI'm maintaining an open-source tool called [DFTFringe](https://github.com/atsju/DFTFringe/issues/37) that analyzes interferometry images to deduce the shape of telescope mirrors. It's used by many amateur telescope makers and works well overall.\n\nThere's one manual step we'd like to automate: fitting a circle to an image feature, with ~1 pixel accuracy. More background here: [discussion thread](https://groups.io/g/Interferometry/topic/113665708#msg36232).\n\nIf you have suggestions for good approaches or algorithms, I’d love to hear them. Specific advice is very welcome — and if anyone feels like going further with a proof of concept, that would be fantastic (but absolutely not expected).\n\nYou can reply here or comment on [GitHub](https://github.com/atsju/DFTFringe/issues/37).\n\nThanks!"", 'In combination to what others have said, I recommend performing a preprocessing step before the Hough transform to account for the stripy nature of the image. This seems relevant: [https://www.reddit.com/r/computervision/comments/1k9p83h/detecting\\_striped\\_circles\\_using\\_computer\\_vision/](https://www.reddit.com/r/computervision/comments/1k9p83h/detecting_striped_circles_using_computer_vision/)', 'I have some alternative ideas from others\'. I think a simple corner detector will find a lot of sharp corners in and at the boundary of the fringes. See the result of a barely tuned Harris corner detector [here](https://i.imgur.com/geMAfiI.png). With a bit of filtering (first filter away outliers with an SOR filter or something, and the filter those that aren\'t part of the supporting planes of the convex hull to filter those on the ""inside""), you\'ll have a list of points that are near certainly on the boundary. From there you can optimise the radius and centre to minimise deviation from the boundary points, and chuck a robust loss term to make sure anything that didn\'t get filtered doesn\'t have too much effect. \n\nCompared to other people\'s solutions, I\'m trying to minimise lossy operations that\'ll erode away pixel detail. Hough transforms are incredibly finicky to work with for any non perfect images, and any operations to make this more ""circle like"" without the pattern are just as hard, not to mention almost certainly modifying location of features. \n\nI\'m happy to help implement this in a few days when I get some time.', 'I was very lazy and went with a ChatGPT-generated solution, is this good enough for you applications? If yes, I can go deeper and maybe make it a bit more accurate, but my time is quite restricted right now.\n\n  \n[https://ibb.co/d0XXdj7L](https://ibb.co/d0XXdj7L)', ""i'm not sure if all pictures in your dataset would look like this\n\nbut just off of this \\_single\\_ image you have given, this is what I think:\n\nthe average intensity inside the circle would probably average out to gray, which is the same outside the circle, so you cannot do it over average intensity of patches. . . \n\nhowever, it seems that everything inside the circle has this long stripes of black and white, while things outside the circle does NOT have this long stripe.\n\nI think you should first devise an algorithm to identify long, continuous stripes (perhaps a floodfill algorithm with some tweak of threshold?). this would allow you to separate the original image into 3 kinds of segments: background, black-stripe, and white-stripe.\n\nthen, simply re-color all the black-stripe and white-stripe red, and fit a circle over the red pixels.\n\n???""]",26,35,0.86,Project,1750586888.0
1lhb52p,MachineLearning,[P] XGboost Binary Classication,"Hi everyone,

I’ve been working on using XGboost with financial data for binary classification.

I’ve incorporated feature engineering with correlation, rfe, and permutations.

I’ve also incorporated early stopping rounds and hyper-parameter tuning with validation and training sets.

Additionally I’ve incorporated proper scoring as well.

If I don’t use SMOT to balance the classes then XGboost ends up just predicting true for every instance because thats how it gets the highest precision. If I use SMOT it can’t predict well at all.

I’m not sure what other steps I can take to increase my precision here. Should I implement more feature engineering, prune the data sets for extremes, or is this just a challenge of binary classification?","['What is the data? What exactly are you predicting? Do you have balanced classes in your training dataset?', 'You need to start from the very beginning. This is ML 101.', 'Somewhat confused on your data. Is it a time series? If so, it might be better to either switch to a forecasting/regression task or at least add that as an input. \n\nFor imbalanced datasets and XGBoost, I like plotting out the predicted probabilities and compare to the true classes of the best performing hyperparameters; you can check at what threshold you get highest precision and examine the distribution of probability scores. Otherwise, if your class is super imbalanced, it might be better to try anomaly detection instead.', ""Look up instead of SMOTE (just for binary classification) the scale_pos_weigth parameter which takes into account the class imbalance. However, it's kind of wierd that only with SMOTE the model works."", ""You can use stratified cross validation to ensure that each fold has the same share of positive labels as the whole dataset and use a metric that takes both classes into account (like F1 instead of precission, for example).\nIf that doesn't work, set your grid's scale_pos_weight to 2, number of Y=0 / number of Y=1, etc. This will weigh observations from the positive class more heavily when building the trees.\nI don't like resampling techniques (SMOT, undersampling, etc.) because the resulting models are always uncalibrated. Only use these methods as a last resort.""]","['What is the data? What exactly are you predicting? Do you have balanced classes in your training dataset?', 'You need to start from the very beginning. This is ML 101.', 'Somewhat confused on your data. Is it a time series? If so, it might be better to either switch to a forecasting/regression task or at least add that as an input. \n\nFor imbalanced datasets and XGBoost, I like plotting out the predicted probabilities and compare to the true classes of the best performing hyperparameters; you can check at what threshold you get highest precision and examine the distribution of probability scores. Otherwise, if your class is super imbalanced, it might be better to try anomaly detection instead.', ""Look up instead of SMOTE (just for binary classification) the scale_pos_weigth parameter which takes into account the class imbalance. However, it's kind of wierd that only with SMOTE the model works."", ""You can use stratified cross validation to ensure that each fold has the same share of positive labels as the whole dataset and use a metric that takes both classes into account (like F1 instead of precission, for example).\nIf that doesn't work, set your grid's scale_pos_weight to 2, number of Y=0 / number of Y=1, etc. This will weigh observations from the positive class more heavily when building the trees.\nI don't like resampling techniques (SMOT, undersampling, etc.) because the resulting models are always uncalibrated. Only use these methods as a last resort.""]",8,14,0.72,Project,1750552486.0
1lh741j,MachineLearning,"[D] RL/GRPO for lossless compression of text passages into 'least token representation', then using this emergent 'language' as the basis for reasoning instead of english","Hi folks, I came up with a thought experiment recently that I cannot stop obsessing over. I have shared this with people. Everybody skims through it for a couple minute and then calls me schizophrenic. I feel isolated and unfortunately feel that I am in fact losing my mind because people do not interact honestly with my ideas. If you know of any theorems, papers or principles in ML that clearly disprove my concept, it could be very therapeutic for me as well. Why don't I simply write the code and try it out? It's a complicated RL setup and I have to bend the libraries a bit to implement it fully.

Here goes nothing...

---

The goal of this experiment is to train a model to take any token sequence, and reduce it to fewer tokens such that the hidden states remain analogous, i.e. a perfect lossless mapping exists back to english. How few tokens does it take to represent any given piece of information? Can the polysemic quality of tokens be augmented?

**Demonstration in GPT-4**

Attached to the post is a *real* demonstration of this capability being elicited by prompting as far back as GPT-4 in 2023. It proves that the capability is present in some capacity within the pre-trained models, on standby for reinforcement and amplification.

**Training Method**

We train a LLM to develop internal symbolic languages for compression:

* `<compress>`: Model learns to compress underlying meaning/message of arbitrary text samples (wikipedia articles, code, etc.) into symbolic representations.
* `<decompress>`: Same model reconstructs original english meaning from symbols
* Reward compression efficiency, reconstruction fidelity, and embedding varentropy metrics that pressure towards saturating the available semantic bandwidth.

RL goes like this:

1. Context (A): User message asks model to compress a given sample of information pulled at random from a dataset. Assistant replies and is prefixed with <compress> similar to training a reasoner where the output is prefixed with <think>.,
2. Context (B): User message asks model to decompress the given output from (A). Assistant replies with information in english,
3. Context (C): user message asks some other unrelated static model to compare initial sample to decompressed sample, and produce a list of deviations and inaccuracies.,
4. *\[optional\]* Contexts (A) and (B) are rewritten so the user message is the simplest possible operator usage pattern (""compress/decompress this"")
5. Apply GRPO to rollouts and backpropagate gradients for contexts (A) and (B), rewarding shorter compression length whilst factoring in (C)'s penalties.

This dual-task RL environment perhaps results in a 'strange attractor' dynamic. In order for the decompression task to succeed, it needs to form a meta-model (i.e. metacognition) of how then language model compresses language.

This preliminary capability can then be used to compress arbitrary context window, removing redundancies, etc. The model's compression of tokens could also be steered. Because this is only step one. If you have seen the DeepSeek-R1-zero model, we discover that LLMs trained with RL without a reward on keeping to a single language results in the model discovering an extremely alien reasoning process. It effectively anneals grammar, syntax, and the partitioned notion of different human languages to wield everything at once.

What I suggest is that we first focus on developing the language by compressing, *then* we have SFT to constrain the model onto this newly discovered language.

yay or nay? 😟","[""You'll never get a one to one mapping since no two tokens have identical embeddings. The ability to map to a lower dimensional token space relies on this."", 'Nobody (including OP) seems to care about critical meaning loss in the compress-uncompress chain. The original (last sentence) says, correctly, that untyped can do more (in fact it can do anything a TM can do). The reconstructed version says the goal is to do more that what the untyped calculus could. Nope, this is not the goal. It all comes down to a tiny change in a grammatical particle ""more of something"" versus ""more than something"".', 'My guy, not exactly the same, but Microsoft published something very similar two years ago:\n\nhttps://github.com/microsoft/LLMLingua\n\nIt uses a small LlM to prune / compress by removing tokens that are only for human use and not salient for attention.', 'i feel like while the idea may have some merit, it may be already superseded by continuous chain of thought, which similarly develops its own thought tokens but leaves them in continuous space instead of sampling\xa0', 'One practical comment on this: as someone who played with RL for years, I think you are dramatically overestimating what RL does and can do. You are essentially asking to train a discrete autoencoder with RL - you can but it will be stupidly slow.\n\nThe way GRPO works is that you make 64 rollouts from the same prompt, take the average reward, and try to update the probability of **each token** in the direction of (reward in a rollout in which the token occurred - average reward) - simplified but that\'s the gist of it.\n\nThose rollouts will have thousands of tokens. You don\'t know at all which of those tokens mattered for the final answer, you are pulling the probability of the whole rollout up or down. \n\nThis is orders of magnitude less efficient than the supervised loss, and what you are asking for is to essentially make the network learn a whole new language via this. \n\nI am very sure that with deepseek-r1-zero they didn\'t produce an ""alien reasoning process"". RL probably pushed the text between the think tags towards more noisy output (since very random gradients were being applied to it without any constraint to keep it organized), and more noisy means more random language switches.', ""I've done this and am running ablation tests on my methods now.\n\nBeen working on it for the last 6 months."", 'I think the best way to compress information into smaller lattent space is just VAE?', 'Practically not usable if you dont have an explicit metric, no? How would you interpolate?', 'I have the same idea but it is for a different use case.', 'Why not just pass in the hidden representation instead of decoding it?']","[""You'll never get a one to one mapping since no two tokens have identical embeddings. The ability to map to a lower dimensional token space relies on this."", 'Nobody (including OP) seems to care about critical meaning loss in the compress-uncompress chain. The original (last sentence) says, correctly, that untyped can do more (in fact it can do anything a TM can do). The reconstructed version says the goal is to do more that what the untyped calculus could. Nope, this is not the goal. It all comes down to a tiny change in a grammatical particle ""more of something"" versus ""more than something"".', 'My guy, not exactly the same, but Microsoft published something very similar two years ago:\n\nhttps://github.com/microsoft/LLMLingua\n\nIt uses a small LlM to prune / compress by removing tokens that are only for human use and not salient for attention.', 'i feel like while the idea may have some merit, it may be already superseded by continuous chain of thought, which similarly develops its own thought tokens but leaves them in continuous space instead of sampling\xa0', 'One practical comment on this: as someone who played with RL for years, I think you are dramatically overestimating what RL does and can do. You are essentially asking to train a discrete autoencoder with RL - you can but it will be stupidly slow.\n\nThe way GRPO works is that you make 64 rollouts from the same prompt, take the average reward, and try to update the probability of **each token** in the direction of (reward in a rollout in which the token occurred - average reward) - simplified but that\'s the gist of it.\n\nThose rollouts will have thousands of tokens. You don\'t know at all which of those tokens mattered for the final answer, you are pulling the probability of the whole rollout up or down. \n\nThis is orders of magnitude less efficient than the supervised loss, and what you are asking for is to essentially make the network learn a whole new language via this. \n\nI am very sure that with deepseek-r1-zero they didn\'t produce an ""alien reasoning process"". RL probably pushed the text between the think tags towards more noisy output (since very random gradients were being applied to it without any constraint to keep it organized), and more noisy means more random language switches.']",45,33,0.74,Project,1750540716.0
1lh6wwb,MachineLearning,[P] Writing a CNN from scratch in C++ (no ML/math libs) - a detailed guide,"I recently built richard, a convolutional neural network, without using any math or machine learning libraries. I did so mainly just as a learning experience.

When I shared it on Reddit and Hacker News a few months ago, a lot of people asked me for resources to help them learn how this stuff works. I’ve finally got around to providing this detailed write up.

Hope this helps someone. Cheers :)","[""It's really impressive I personally love c++ , and it's mind boggling that you implemented CNN in c++\nKeep up bro""]","[""It's really impressive I personally love c++ , and it's mind boggling that you implemented CNN in c++\nKeep up bro""]",22,1,0.87,Project,1750540169.0
1lh2jeh,MachineLearning,[D]Understanding the model with different embedding dimensions,"Hello! I was tweaking with the embedding sizes of my simple DNN model.I was wondering if there is a way to get an intuition (or interpret) how does the model gets affected with changing the emnedding sizes. If two embedding sizes are giving similar results on a test set, how can I ensure which would be better for OOS data? Can someone kindly advise how they tackle such scenarios? Thanks! ",[],[],0,0,0.5,Discussion,1750528436.0
1lh0rmp,MachineLearning,[P] Autopaste MFA codes from Gmail using Local LLMs,"Inspired by Apple's ""insert code from SMS"" feature, made a tool to speed up the process of inserting incoming email MFAs: [https://github.com/yahorbarkouski/auto-mfa](https://github.com/yahorbarkouski/auto-mfa)

Connect accounts, choose LLM provider (Ollama supported), add a system shortcut targeting the script, and enjoy your extra 10 seconds every time you need to paste your MFAs","['This seems like an absurd degree of overkill. A simple regular expression would be equally effective at parsing MFA codes out of emails, would be significantly faster, and requires astronomically less compute, memory, and power.', 'Lightweight, thanks for the chuckle.', 'can you explain how this works, sounds super interesting!']","['This seems like an absurd degree of overkill. A simple regular expression would be equally effective at parsing MFA codes out of emails, would be significantly faster, and requires astronomically less compute, memory, and power.', 'Lightweight, thanks for the chuckle.', 'can you explain how this works, sounds super interesting!']",47,14,0.7,Project,1750523865.0
1lh0oii,MachineLearning,Model for Audio Speech Emotion Recognition and Paralinguistic Analysis [D],"Hi there,
I have 1000s of Voice lines from characters, and i want to classify them by emotion and also by if they are whispering / shouting, so i have a good dataset to then create an AI voice from.

Which Model or Models would be the best for achieving this.
(Using one for emotion and another for the whisper / shouting detection is fine)

Also since the best Voice Cloning model seems to change every week, what would people say is the current best model for cloning a voice (I have hours of data per character, so do not need or want ones that oneshot voice cloning)

Thank you.","[""Very exciting. This is definitively a gap in open source. \n\nI'm currently enjoying Orpheus (FASTAPI).  https://github.com/Lex-au/Orpheus-FastAPI \n\nIt may be right up your street, as this model includes emotive tags, such as `<laugh>` and `<groan>`. \n\nAs a plus side, I went full no-code with this using LM Studio and pinokio.computer - so it's like 5 minutes install.\n\n---\n\nNext may be Apolio https://applio.org/ - of which allows custom voice models. \n\n---\n\nBefore this month, I was using Parler-TTS because of its abilities. But I really like FISH https://github.com/fishaudio/fish-speech?tab=readme-ov-file or Kokoro-TTS https://kokorotts.net/ also.\n\n---\n\nOther to that I recommend poking around pinkio.computer for like 20+ great audio models \n\nGood luck!""]","[""Very exciting. This is definitively a gap in open source. \n\nI'm currently enjoying Orpheus (FASTAPI).  https://github.com/Lex-au/Orpheus-FastAPI \n\nIt may be right up your street, as this model includes emotive tags, such as `<laugh>` and `<groan>`. \n\nAs a plus side, I went full no-code with this using LM Studio and pinokio.computer - so it's like 5 minutes install.\n\n---\n\nNext may be Apolio https://applio.org/ - of which allows custom voice models. \n\n---\n\nBefore this month, I was using Parler-TTS because of its abilities. But I really like FISH https://github.com/fishaudio/fish-speech?tab=readme-ov-file or Kokoro-TTS https://kokorotts.net/ also.\n\n---\n\nOther to that I recommend poking around pinkio.computer for like 20+ great audio models \n\nGood luck!""]",4,1,0.83,Discussion,1750523638.0
1lgy08j,MachineLearning,[P] AI Weather Forecasting Using METAR Data with Tensorflow,"Hi everyone,

I’ve been working on a small open-source ML project using aviation weather reports (METAR) to predict short-term weather conditions like temperature, visibility, wind direction, etc.

It’s built with Tensorflow/Keras and trained on real METAR sequences. I focused on parsing structured data and using it for time-series forecasting, more of a learning project than production-grade, but the performance is promising (see MAE graph).

Would love any feedback or ideas on how to improve the modeling.

[**Github Link**](https://github.com/OmerZeyveli/Weather-Forecasting-AI-Model-with-METAR-Data)



[Normalized Mean Absolute Error by Feature](https://preview.redd.it/c49hkd0bka8f1.jpg?width=1979&format=pjpg&auto=webp&s=564de0d0ee66a2910f89469af30ad46fd25b2541)


","[""  \nFun project.\n\n>Would love any feedback or ideas on how to improve the modeling.\n\nAny limitations you're seeing in forecast skill likely due to fundamental limitations with how you're modeling the problem. Unfortunately - unless you're forecasting for a particularly consistent and boring forecast location - simple extrapolation from the most recent 168 hours of forecast data just won't work. Critical short-term weather is driven by exogenous, large-scale patterns that simply are not captured in your input data source.\n\nTypically when we try to model station-specific data, we take one of two different approaches. The first would involve staying solely with observational data as your inputs; then, you'd use a large-scale timeseries modeling approach to try to capture as many consistent cyclical patterns in the fields you're trying to model. Then, you might slap on to that something that tries to predict when there may be an anomaly that will break the pattern - e.g. a frontal passage timed off-cycle. This sort of approach is never good enough for operational use, but it's still fun to play with.\n\nThe other option is to do what we've been doing in meteorology for the last 50 years, which is try to bias-correct or hyper-localize the forecast from a comprehensive modeling system, such as an NWP model. In that case, your fundamental problem is to predict residuals between the parent model's forecast and the actual observations at your station of interest. You would generally feed in different surrounding grids of forecast data points from the parent model - or, in some cases, I've found it particularly helpful to use timeseries of leading EOFs (PCA applied to the spatio-temporal pattern of the large-scale flow, such as temperature and geopotential fields) regressed against the location of interest to capture this sort of information.""]","[""  \nFun project.\n\n>Would love any feedback or ideas on how to improve the modeling.\n\nAny limitations you're seeing in forecast skill likely due to fundamental limitations with how you're modeling the problem. Unfortunately - unless you're forecasting for a particularly consistent and boring forecast location - simple extrapolation from the most recent 168 hours of forecast data just won't work. Critical short-term weather is driven by exogenous, large-scale patterns that simply are not captured in your input data source.\n\nTypically when we try to model station-specific data, we take one of two different approaches. The first would involve staying solely with observational data as your inputs; then, you'd use a large-scale timeseries modeling approach to try to capture as many consistent cyclical patterns in the fields you're trying to model. Then, you might slap on to that something that tries to predict when there may be an anomaly that will break the pattern - e.g. a frontal passage timed off-cycle. This sort of approach is never good enough for operational use, but it's still fun to play with.\n\nThe other option is to do what we've been doing in meteorology for the last 50 years, which is try to bias-correct or hyper-localize the forecast from a comprehensive modeling system, such as an NWP model. In that case, your fundamental problem is to predict residuals between the parent model's forecast and the actual observations at your station of interest. You would generally feed in different surrounding grids of forecast data points from the parent model - or, in some cases, I've found it particularly helpful to use timeseries of leading EOFs (PCA applied to the spatio-temporal pattern of the large-scale flow, such as temperature and geopotential fields) regressed against the location of interest to capture this sort of information.""]",0,2,0.5,Project,1750516528.0
1lgxyw9,MachineLearning,[D] Have there been any new and fundamentally different povs on Machine Learning theory?,The title. I think the most conventionally accepted formalization is as a (giant & unknown) joint probability distribution over the data and labels. Has there been anything new?,"['There is something old, that is not sufficiently appreciated:\n\nKolmogorov complexity, Solomonoff Induction, Algorithmic Information Theory, etc....\n\nOne interesting aspect is, that it does not assume any underlying distribution. It can be applied to a single individual sequence of observations, without assuming stationarity etc.\n\nAn extrem example would be to learn on the (unknown ground-truth) process that emits one digit of pi after the other. The learner just observes a never-ending sequence of digits. With the conventional, distributional framework we would  struggle to even define test-sets or what generalization even means.  \nFor Solomonoff Induction however we know that it would make a few prediction mistakes at the beginning of the process, and then predict correctly forever...\n\nIf you like videos:\n\nRay Solomonoff paper read by Marcus Hutter - Algorithmic Probability, Heuristic Programming & AGI\n\n[https://www.youtube.com/watch?v=wMcRMO9ejeM](https://www.youtube.com/watch?v=wMcRMO9ejeM)\n\nThe IMHO underappreciated aspect is that we can use deep learning to build systems that minimize description length \\[1\\] and which thus approximate Solomonoff induction. To be fair though, there is quite some literature that points out that ""LLMs are compressors"", which goes towards the theoretical heart of the issue, but doesn\'t really operationalize it \\[2, 3\\].\n\n\\[1\\] [https://arxiv.org/abs/2210.07931](https://arxiv.org/abs/2210.07931)', 'This fundamentally has to do with how we view probability theory rigorously. Nowadays we use Kolmogorov’s definition of probability theory as viewed through measure theory, where we suppose there exists some underlying probability measure space, e.g. (X, B, v), however we typically assume that we do not know what the measure v is nor what the space X and the accompanying sigma algebra B actually is. \n\nWe could for instance have some crazy sofic system as a probability space, where the generated stochastic process has a finite Markovian representation but is not Markov at any order… For instance imagine the even process where we observe even blocks of 1s interspersed with strings of 0s of arbitrary length. Which is actually quite tough to model as a finite history will not let us determine the future state.\n\nHowever, ML tends to imagine a simplistic setting where we have IID observations. As such, this places quite strict assumptions on the data making traditional ML theory possible, as these are far easier to prove without a deep background in graduate mathematics. \n\nThe bottom line is that this framework should be sufficiently broad to explain natural phenomena, however it is limited by the complexity of the theory necessary to describe these. So unless you want to replace measure theory as the basis of modern probability theory, this is how we currently do things.', 'I agree that probability theory is probably the limiting factor towards any fundamental breakthroughs in machine learning and AI.  It has been incredibly useful in modeling uncertainty and it has been surprisingly scalable with deep learning.  However, it is most practical to work in a static distribution with IID samples, and it is still fundamentally unable to handle ""epistemic uncertainty"" or unknown-unknowns.  This is part of the reason why we are still seeing problems like ""catastrophic forgetting"" in deep learning with fundamentally separated training and inference stages. [https://cis.temple.edu/\\~pwang/Publication/probability.pdf](https://cis.temple.edu/~pwang/Publication/probability.pdf)']","['There is something old, that is not sufficiently appreciated:\n\nKolmogorov complexity, Solomonoff Induction, Algorithmic Information Theory, etc....\n\nOne interesting aspect is, that it does not assume any underlying distribution. It can be applied to a single individual sequence of observations, without assuming stationarity etc.\n\nAn extrem example would be to learn on the (unknown ground-truth) process that emits one digit of pi after the other. The learner just observes a never-ending sequence of digits. With the conventional, distributional framework we would  struggle to even define test-sets or what generalization even means.  \nFor Solomonoff Induction however we know that it would make a few prediction mistakes at the beginning of the process, and then predict correctly forever...\n\nIf you like videos:\n\nRay Solomonoff paper read by Marcus Hutter - Algorithmic Probability, Heuristic Programming & AGI\n\n[https://www.youtube.com/watch?v=wMcRMO9ejeM](https://www.youtube.com/watch?v=wMcRMO9ejeM)\n\nThe IMHO underappreciated aspect is that we can use deep learning to build systems that minimize description length \\[1\\] and which thus approximate Solomonoff induction. To be fair though, there is quite some literature that points out that ""LLMs are compressors"", which goes towards the theoretical heart of the issue, but doesn\'t really operationalize it \\[2, 3\\].\n\n\\[1\\] [https://arxiv.org/abs/2210.07931](https://arxiv.org/abs/2210.07931)', 'This fundamentally has to do with how we view probability theory rigorously. Nowadays we use Kolmogorov’s definition of probability theory as viewed through measure theory, where we suppose there exists some underlying probability measure space, e.g. (X, B, v), however we typically assume that we do not know what the measure v is nor what the space X and the accompanying sigma algebra B actually is. \n\nWe could for instance have some crazy sofic system as a probability space, where the generated stochastic process has a finite Markovian representation but is not Markov at any order… For instance imagine the even process where we observe even blocks of 1s interspersed with strings of 0s of arbitrary length. Which is actually quite tough to model as a finite history will not let us determine the future state.\n\nHowever, ML tends to imagine a simplistic setting where we have IID observations. As such, this places quite strict assumptions on the data making traditional ML theory possible, as these are far easier to prove without a deep background in graduate mathematics. \n\nThe bottom line is that this framework should be sufficiently broad to explain natural phenomena, however it is limited by the complexity of the theory necessary to describe these. So unless you want to replace measure theory as the basis of modern probability theory, this is how we currently do things.', 'I agree that probability theory is probably the limiting factor towards any fundamental breakthroughs in machine learning and AI.  It has been incredibly useful in modeling uncertainty and it has been surprisingly scalable with deep learning.  However, it is most practical to work in a static distribution with IID samples, and it is still fundamentally unable to handle ""epistemic uncertainty"" or unknown-unknowns.  This is part of the reason why we are still seeing problems like ""catastrophic forgetting"" in deep learning with fundamentally separated training and inference stages. [https://cis.temple.edu/\\~pwang/Publication/probability.pdf](https://cis.temple.edu/~pwang/Publication/probability.pdf)']",1,5,0.53,Discussion,1750516423.0
1lgwow8,MachineLearning,[R] Regarding PCA for group classification,"
Hey all,

I have some flow cytometry (summarized marker values) data, and some other clinical variables like Waist circumference, and disease Severity (DF, DHF, Healthy) across like 50 patient and healthy samples.

Wanted to do pca and color by severity groups, just wanted to ask if I should include both my flow marker values + my waist circumference values, or just my flow marker values?

Got a bit confused cause I generally thought PCA is better the more variables you have, but does adding waist circumference affect it badly or something when considering colouring based on disease severity?

Any and all responses would be a great help! Thanks so much! ","['Oo, 😁 o']","['Oo, 😁 o']",0,1,0.43,Research,1750512849.0
1lga5dr,MachineLearning,"[R] The Pedagogical GAN (from ""Unaware Adversaries: A Framework for Characterizing Emergent Conflict Between Non-Coordinating Agents"")","\[edit: trying a third time without any links, and the full subsection on Pedagogical GAN in the body.\]

I've recently written a paper introducing a framework for analyzing ""unaware adversaries"" - agents in a shared environment whose independent, well-intentioned actions produce emergent conflict. Think of a heater and an A/C fighting each other. The ML-angle is another case study that results in what I propose as a Pedagogical GAN. The GAN proposal may be shot down rather quickly here I suppose, but it wasn't the main idea of the paper. I'm just hoping to get some feedback from the smart folks here.



TL;DR:



I formalize this structure and apply it across domains: thermostats, urban planning, interdomain routing (YouTube BGP hijack), and email deliverability.



For ML, I propose the Pedagogical GAN, where the generator’s goal is reframed from “fool the discriminator” to “maximize the discriminator’s learning signal” - turning the adversary into a teacher rather than an opponent.



Feedback welcome - especially from folks working on GANs, multi-agent learning, or system safety. Since I'm not an affiliated researcher, this is unlikely to be accepted to any peer-review journal, so I have uploaded the PDF to my website: My post keeps getting removed by reddit's filters and the only reason I can postulate is that it is because of the link. Internet Searching ""Unaware Adversaries"" does find my paper on my domain paperclipmaximizer dot ai if you'd like to read the entire thing.



Case 5. From Designed Conflict to a Novel Research Hypothesis: **The Pedagogical GAN**

The standard Generative Adversarial Network (GAN) \[2\] provides a powerful case study for our framework. It is a system of two agents, a Generator (G) and a Discriminator (D), locked in a designed, zero-sum game. This adversarial dynamic, however, is notoriously unstable and suffers from practical issues like vanishing gradients, where D becomes too proficient, leaving G with no learning signal. The original authors’ first solution was the heuristic “non-saturating” loss, an immediate modification that sought a stronger, more reliable gradient for G. This established the central challenge in the field: managing the adversarial dynamic for stable and efficient training.

In the years since, the dominant paradigm for GAN stabilization has become one of gradient control. Landmark models like Wasserstein GAN (WGAN) \[3\] and its successor WGAN-GP \[4\] diagnosed the problem as being rooted in the geometry of the loss landscape. Their solution, which now represents the state-of-the-art, is to tame and constrain the discriminator’s function (e.g., by enforcing a Lipschitz condition) to guarantee that it always provides a smooth and informative gradient to the generator. This philosophy is about preventing conflict from becoming destructive by carefully limiting the power of the adversary.

Our framework of unaware adversaries prompts a different line of inquiry. Instead of asking, “How do we control the conflict?”, we ask, “Can we redesign the agents’ objectives to make the conflict more productive?” This leads us to propose a novel approach that stands in philosophical opposition to gradient control. We term this the **Pedagogical GAN**.

The core idea of the Pedagogical GAN is to change the generator’s objective from simply fooling the discriminator to actively teaching it as efficiently as possible. We formalize this by proposing that the generator should seek to *maximize* the discriminator’s learning signal. The generator’s objective function becomes:

$$ \\max\_{G} \\left\\| \\nabla\_{D} \\mathcal{L}(D, G) \\right\\|\_2 $$

Here, L(D, G) is the standard discriminator loss. The generator is now explicitly incentivized to find samples that lie on the steepest parts of the discriminator’s loss landscape. It becomes a “Socratic tutor” that seeks to *weaponize* the gradient for accelerated learning, not suppress it.

This approach represents a significant conceptual departure. It is distinct from other cooperative frameworks like Unrolled GANs \[5\], which use strategic foresight, or other non-antagonistic models that alter loss functions to escape the zero-sum game \[6\]. Instead, it can be viewed as the principled and extreme conclusion of the line of thinking that began with the very first non-saturating GAN loss. Our literature review suggests that while the raw intuition for cooperative training has been informally discussed, this specific mechanism of maximizing the discriminator’s gradient norm appears to be a formally unexplored, high-risk, high-reward avenue for GAN research.",[],[],1,0,1.0,Research,1750441266.0
1lgtb0g,MachineLearning,[R] Tree Search for Language Model Agents,"This paper shows a (very unsurprising) result that if you combine tree-of-thoughts with tool-use, you get better performance on web navigation tasks. [Other papers](https://arxiv.org/pdf/2310.04406) have shown better performance on a variety of different tasks, too.

Why don't we see more ""tree search + tool-use"" in production? Are startups lagging behind the literature or is it prohibitively slow/expensive?","[""Isn't it the method behind the deep research feature of gptresearcher ?""]","[""Isn't it the method behind the deep research feature of gptresearcher ?""]",1,1,0.67,Research,1750501255.0
1lgt8zn,MachineLearning,[D] Any good ML conferences coming up?,"I have a preprint related to bioinformatics/biomolecular design that I’ll be releasing soon. I believe it’s a strong paper and has the potential to be accepted at a good venue. Unfortunately, I’ve missed the deadlines for major conferences like ICML, ICLR, and NeurIPS.

Are there any upcoming conferences focused on machine learning, ML for science, or computational biology that I could submit to? I’d probably prefer a biology-related workshop rather than a main conference track. Later on I would like to publish an extended version in a good journal.

P.S. NeurIPS hasn’t released the list of upcoming workshops yet, I’m hoping there will be something suitable there, but I’m still exploring other options in the meantime.","['AAAI', 'ICLR deadline usually is around September. \n\nAAAI in early august\n\nECML-PKDD workshops are still accepting papers']","['AAAI', 'ICLR deadline usually is around September. \n\nAAAI in early august\n\nECML-PKDD workshops are still accepting papers']",0,2,0.3,Discussion,1750501033.0
1lgruj6,MachineLearning,[R] What’s better than NeurIPS and ICML?,"Relatively new to research and familiar with these conferences being the goal for most ML research. I’ve also heard that ML research tends to be much easier to publish compared to other fields as the goal is about moving fast over quality. With this in mind, what’s the “true mark” of an accomplished paper without actually reading it? If I want to quickly gauge it’s value without checking citations, what awards are more prestigious than these conferences? Also, how much of a difference is it to publish at one of these workshops over main conference?","['ML is not ""much easier"" to publish.', 'In the last 5 years, ML research is arguably the hardest to publish. The number of yearly conference submissions is growing exponentially', 'Sometimes I feel a bit more prestigious when it comes to papers at TMLR and JMLR, maybe it is because these papers are more on providing a complete work rather than novelty, which is more enjoyable to read.', 'Best papers, oral/spotlight presentations, adoption by big labs', ""These conferences only have an acceptance rate of about 25%. You can pour multiple years worth of work into something that will be rejected 3 out of 4 times. It's not easier to publish in ML than other fields.\n\nWorkshops are reviewed totally separate from the main conference. The workshop organizers decide how easy or hard it will be. Workshop publications do not hold the same respect as main conference (often times, workshop papers are just rejected main conference papers)."", ""If anything I'd argue publishing in machine learning is significantly, especially at these top venues, is more difficult than in other fields. At least, that's what I see in my subfield (medical imaging).\n\nedit: workshops are usually significantly easier to get than the main conference"", 'Nature, Science', 'Sex', ""It's very easy to publish. Make webpage with project and then publish preprint. Release code. Then share on social media.\n\n\nI don't pay attention to conferences because they are always old news by the time they happen""]","['ML is not ""much easier"" to publish.', 'In the last 5 years, ML research is arguably the hardest to publish. The number of yearly conference submissions is growing exponentially', 'Sometimes I feel a bit more prestigious when it comes to papers at TMLR and JMLR, maybe it is because these papers are more on providing a complete work rather than novelty, which is more enjoyable to read.', 'Best papers, oral/spotlight presentations, adoption by big labs', ""These conferences only have an acceptance rate of about 25%. You can pour multiple years worth of work into something that will be rejected 3 out of 4 times. It's not easier to publish in ML than other fields.\n\nWorkshops are reviewed totally separate from the main conference. The workshop organizers decide how easy or hard it will be. Workshop publications do not hold the same respect as main conference (often times, workshop papers are just rejected main conference papers).""]",0,18,0.37,Research,1750495153.0
1lgqdyd,MachineLearning,[R] A Non-LLM Learning Model Based on Real-Time Sensory Feedback | Requesting Technical Review,"I’m currently working on a non-language model called **OM3** (Organic Model 3). It’s not AGI, not a chatbot, and not a pretrained agent. Instead, it’s a real-time digital organism that learns purely from **raw sensory input**: vision, temperature, touch, etc.

The project aims to explore **non-symbolic, non-reward-based learning** through embodied interaction with a simulation. OM3 starts with no prior knowledge and builds behavior by observing the effects of its actions over time. Its intelligence, if it emerges it comes entirely from the structure of the sensory-action-feedback loop and internal state dynamics.

The purpose is to test alternatives to traditional model paradigms by removing backprop-through-time, pretrained weights, and symbolic grounding. It also serves as a testbed for studying behavior under survival pressures, ambiguity, and multi-sensory integration.

I’ve compiled documentation for peer review here:


 [https://osf.io/zv6dr/](https://osf.io/zv6dr/)

[https://github.com/A1CST](https://github.com/A1CST)

The full codebase is open source and designed for inspection. I'm seeking input from those with expertise in unsupervised learning, embodied cognition, and simulation-based AI systems.

Any technical critique or related prior work is welcome. This is research-stage, and feedback is the goal, not promotion.","['""Non-reward-based learning"" - so what directs the learning then?', 'The code currently uses random numbers instead of actual audio and video when running the model: https://github.com/A1CST/OM3/blob/af727c604a005bbcde80635091d52e5721f2c714/om3/core/engine.py#L61.\n\nIn general, what\'s missing here is a showcase. Currently there\'s\n\n- code that I have to run to see what it does\n- some 5-page unpublished ""papers"" that describe very vague concepts like the Jungle Turing Test. This particular document doesn\'t even describe what the JTT actually is, how to implement it and what it looks like.', ""Check out\n\n\n- Hubert Dreyfus' What AI Still Can't Do. Read the book, not the reviews or summary. I avoided the reading because the summary made it seem like there's nothing interesting there. Turned out, the book has a wealth of perspectives that haven't made their way into most summaries. It's a neat string point for understanding embodied and extended cognition/AI\n\n\n- Subsumption architecture, and in general, the work by Rodney Brooks.\n\n\n- I recently attended Constantin Rothkopf's work on modeling Navigation using POMDPs. He also mentioned how getting a robot to pour liquid from a bottle into a glass is still an open problem.\n\n\n- Berkeley's AI Research lab also seems to be doing some interesting things.\n\n\nBut it still seems an open problem connecting high level cognition with situated-embodied contexts or knowledge if that's what your final goal is."", 'your medium account is under investigation, seriously dude?', 'Exactly the direction I want to see. Good luck.\xa0']","['""Non-reward-based learning"" - so what directs the learning then?', 'The code currently uses random numbers instead of actual audio and video when running the model: https://github.com/A1CST/OM3/blob/af727c604a005bbcde80635091d52e5721f2c714/om3/core/engine.py#L61.\n\nIn general, what\'s missing here is a showcase. Currently there\'s\n\n- code that I have to run to see what it does\n- some 5-page unpublished ""papers"" that describe very vague concepts like the Jungle Turing Test. This particular document doesn\'t even describe what the JTT actually is, how to implement it and what it looks like.', ""Check out\n\n\n- Hubert Dreyfus' What AI Still Can't Do. Read the book, not the reviews or summary. I avoided the reading because the summary made it seem like there's nothing interesting there. Turned out, the book has a wealth of perspectives that haven't made their way into most summaries. It's a neat string point for understanding embodied and extended cognition/AI\n\n\n- Subsumption architecture, and in general, the work by Rodney Brooks.\n\n\n- I recently attended Constantin Rothkopf's work on modeling Navigation using POMDPs. He also mentioned how getting a robot to pour liquid from a bottle into a glass is still an open problem.\n\n\n- Berkeley's AI Research lab also seems to be doing some interesting things.\n\n\nBut it still seems an open problem connecting high level cognition with situated-embodied contexts or knowledge if that's what your final goal is."", 'your medium account is under investigation, seriously dude?', 'Exactly the direction I want to see. Good luck.\xa0']",0,11,0.48,Research,1750489195.0
1lgpskb,MachineLearning,[D] Batch shuffle in time series transformer,"Im building a custom time series transformer for stock price prediction, wanted to know if for training dataset batches, Shuffle=True should be done or not? The data within the sample is chronologically arranged, but should I shuffle the samples within the batch or not.

It is a stock market index that im working on, using shuffle true gives more stable training and getting good results. But im worried the regime shift info might be discarded. ","[""In my opinion, you generally want to avoid shuffling when working with time series forecasting because temporal continuity is a big part of what gives the data meaning. If your training samples are sequential windows taken from a continuous timeline, shuffling them can break the natural order and make it harder for the model to learn trends or transitions like regime shifts. That temporal structure is often what the model needs to capture.\n\nHowever, if you're using fixed-length windows that are self-contained and don't overlap, and you're confident there's no leakage between them, then shuffling might be fine. In that case, it can help stabilize training and reduce overfitting to local patterns.\n\nPersonally, I prefer to keep the training data in chronological order to make sure the model learns in a way that reflects how the data would be used in practice. I usually go with careful windowing, no shuffle during training, and validation on a continuous, ordered slice of the timeline to measure realistic performance.""]","[""In my opinion, you generally want to avoid shuffling when working with time series forecasting because temporal continuity is a big part of what gives the data meaning. If your training samples are sequential windows taken from a continuous timeline, shuffling them can break the natural order and make it harder for the model to learn trends or transitions like regime shifts. That temporal structure is often what the model needs to capture.\n\nHowever, if you're using fixed-length windows that are self-contained and don't overlap, and you're confident there's no leakage between them, then shuffling might be fine. In that case, it can help stabilize training and reduce overfitting to local patterns.\n\nPersonally, I prefer to keep the training data in chronological order to make sure the model learns in a way that reflects how the data would be used in practice. I usually go with careful windowing, no shuffle during training, and validation on a continuous, ordered slice of the timeline to measure realistic performance.""]",0,1,0.29,Discussion,1750486867.0
1lgp926,MachineLearning,Why is Qwen2-0.5B trained on much more data than the larger models? [D],"I'm reading through the [Qwen2](https://arxiv.org/abs/2407.10671) paper.

Something escapes my limited comprehension -

Section 3.1

> ... the pre-training data was expanded from 3 trillion tokens in Qwen1.5 (Qwen Team, 2024a) to 7 trillion tokens. An attempt to further relax the quality threshold resulted in a 12 trillion token dataset. However, the model trained on this dataset did not show a significant performance improvement over the 7 trillion token model. It is suspected that increasing the volume of data does not necessarily benefit model pre-training.

So higher quality smaller dataset is better. Got it.

> All Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of
over 7 trillion tokens. Qwen2-0.5B were pre-trained using the 12 trillion token dataset.

How is it conceivable to train that tiny model on the humongous but lower quality dataset?? My modest intellect feels borderline abused.

Appreciate any tips to guide my understanding.","['How do you think they discovered that the 12T wasn’t worth doing for the larger models?\n\nNote also they say did not show a “significant” not no performance improvement.', 'Your conclusion is not logically supported by the text.\n\nIt says that higher volume low quality training data does not lead to significantly better outcomes.\nThe reverse conclusion - that a lower volume of high quality training data is better- is not supported by the text you quoted.', 'Because it iss cheap to do so']","['How do you think they discovered that the 12T wasn’t worth doing for the larger models?\n\nNote also they say did not show a “significant” not no performance improvement.', 'Your conclusion is not logically supported by the text.\n\nIt says that higher volume low quality training data does not lead to significantly better outcomes.\nThe reverse conclusion - that a lower volume of high quality training data is better- is not supported by the text you quoted.', 'Because it iss cheap to do so']",37,11,0.88,Discussion,1750484806.0
1lgp0va,MachineLearning,Is ANN Search in a Vector Database a Good Fit for Lead Generation? [R],"I’m building a tool that aggregates posts from hundreds of subreddits and stores them in a Qdrant database using embeddings. I’ve also embedded information about a user’s product or service — essentially what they’re trying to find leads for.

Using Approximate Nearest Neighbor (ANN) search in Qdrant, I match Reddit posts that are semantically similar to the user’s product description, treating those matched posts as potential leads.

So far, the results seem to be about 70–80% relevant. I’m wondering if this is a solid use case for this kind of setup, or if there are better approaches that you’d recommend to improve accuracy or relevance.

Thanks in advance!","['ANN in Qdrant can totally drive lead gen, but you’ll gain a big jump in precision by stacking a few lightweight tricks on top. Blend a sparse BM25 score with your ANN distance and rerank on the sum; that alone bumped my match accuracy from 75 to 92 %. Apply subreddit tags, post age, and upvote count as metadata filters before the vector call so you’re not embedding junk. Feedback loops matter too-pipe user thumbs-up/thumbs-down back into the index and fine-tune your embedder every couple weeks. I’ve tried Pinecone for fast filtering and Weaviate for hybrid search, but Pulse for Reddit quietly handles real-time keyword alerts and reply drafting once the right posts surface, so the pipeline stays tight end-to-end without extra glue code. ANN is a solid base, just pair it with smart filtering, reranking, and feedback to squeeze out those last percentage points.']","['ANN in Qdrant can totally drive lead gen, but you’ll gain a big jump in precision by stacking a few lightweight tricks on top. Blend a sparse BM25 score with your ANN distance and rerank on the sum; that alone bumped my match accuracy from 75 to 92 %. Apply subreddit tags, post age, and upvote count as metadata filters before the vector call so you’re not embedding junk. Feedback loops matter too-pipe user thumbs-up/thumbs-down back into the index and fine-tune your embedder every couple weeks. I’ve tried Pinecone for fast filtering and Weaviate for hybrid search, but Pulse for Reddit quietly handles real-time keyword alerts and reply drafting once the right posts surface, so the pipeline stays tight end-to-end without extra glue code. ANN is a solid base, just pair it with smart filtering, reranking, and feedback to squeeze out those last percentage points.']",2,1,0.67,Research,1750483962.0
1lgmv76,MachineLearning,[D] Low-dimension generative models,"Are generative models for low-dim data considered, generally, solved? by low dimension, i mean in the order of 10s dimensions but no more than, say, 100. Sample size from order of 1e5 to 1e7.  Whats the state of the art for these? First thing that comes to mind is normalizing flows. Assuming the domain is in Rd.

Im interested in this for research with limited compute ","['Depends on how weird your correlations structures are, but I’d generally consider the problem open, with the caveat that there are many “solved” subproblems, but no perfect black box tool for any data.', 'What a strange question. You can pack a lot of information in 10 dimensions, depending on precision.', 'You should be able to use either normalising flow or flow matching just fine with lower dimensions. Also non-KL distribution distances like MMD or Sinkhorn would probably work quite well with fewer dimensions.', ""It kind of depends on how you're planning to use the model."", 'I mean, just as a counterexample, consider enumerating every word in the english language with a single number. Then, take a sentence of words and concatenate those numbers together. Next token prediction could be (very inefficiently) represented in this way as a 1D input to 1D output generative model, but it is merely a low dimensional rephrasing of a significantly more complex higher dimensional problem. This is why just referring to a problem as ""Low-dimension"" is a bit vague. Obviously, there are many simple lower dimensional problems, but there will always be some degenerate cases such as the one I listed above where the problem is so poorly regularized within the embedding dimension (e.g. concatenating token ids) that current approaches fail miserably.', 'For low-dim data (10s-100 dims) with sample sizes in the 100k-10M range, normalizing flows and autoregressive models are a strong suit for solving generative tasks.']","['Depends on how weird your correlations structures are, but I’d generally consider the problem open, with the caveat that there are many “solved” subproblems, but no perfect black box tool for any data.', 'What a strange question. You can pack a lot of information in 10 dimensions, depending on precision.', 'You should be able to use either normalising flow or flow matching just fine with lower dimensions. Also non-KL distribution distances like MMD or Sinkhorn would probably work quite well with fewer dimensions.', ""It kind of depends on how you're planning to use the model."", 'I mean, just as a counterexample, consider enumerating every word in the english language with a single number. Then, take a sentence of words and concatenate those numbers together. Next token prediction could be (very inefficiently) represented in this way as a 1D input to 1D output generative model, but it is merely a low dimensional rephrasing of a significantly more complex higher dimensional problem. This is why just referring to a problem as ""Low-dimension"" is a bit vague. Obviously, there are many simple lower dimensional problems, but there will always be some degenerate cases such as the one I listed above where the problem is so poorly regularized within the embedding dimension (e.g. concatenating token ids) that current approaches fail miserably.']",0,6,0.38,Discussion,1750476365.0
1lgjjhd,MachineLearning,[D] Should I use a dynamic batch size and curriculum learning when pretraining?,"I am pretraining GPT-2 small on the 10b token subset of FineWeb Edu, and was wondering if I should ramp up the batch size during training. I was also wondering if I should train on TinyStories first and then train on FineWeb Edu for the rest of the run. What are your thoughts?","[""Why would you vary the batch size?\n\nThere's a bunch to consider when figuring this out but you probably want the biggest batch size you can that still gets you a reasonable amount of steps."", 'Typically no, there is no need to do curriculum learning or vary the batch size.']","[""Why would you vary the batch size?\n\nThere's a bunch to consider when figuring this out but you probably want the biggest batch size you can that still gets you a reasonable amount of steps."", 'Typically no, there is no need to do curriculum learning or vary the batch size.']",3,3,1.0,Discussion,1750465796.0
1lgimm3,MachineLearning,AbsenceBench: Language Models Can't Tell What's Missing,,"[""It's fascinating that they do so badly at this, given that cloze tests have been historically such a basic element of testing language models.."", 'May be due to generative models being trained for next token prediction, and not masked language modeling.', 'Fascinating!\n\n> Transformer attention mechanisms cannot easily attend to ""gaps"" in documents since these absences don\'t correspond to any specific keys that can be attended to. \n\nThis I don\'t get: they give original and edited version, the original versions has the tokens to look for, getting the keys should be pretty straightforward', 'interesting observation, thanks for sharing that. will be interesting to see how this impacts the design space.', ""This is great work. I love a benchmark like this that isn't just difficult for the models, it's also very doable for models in toy versions of the problem. That inherently means that you can scale problem size until you get meaningful failure rates to distinguish between models. Fantastic."", 'Interestingly though there is also variance among the models. They all do poorly but some worse than others. Indicative of that there’s room for improvement and that some models somehow did something right here. I wonder if it’s connected to hallucination risk. SimpleQA & PersonQA also show variance despite hallucinations being a universal issue. OpenAI has performed poorly there and does so here as well.', 'Maybe dropout messes things up?']","[""It's fascinating that they do so badly at this, given that cloze tests have been historically such a basic element of testing language models.."", 'May be due to generative models being trained for next token prediction, and not masked language modeling.', 'Fascinating!\n\n> Transformer attention mechanisms cannot easily attend to ""gaps"" in documents since these absences don\'t correspond to any specific keys that can be attended to. \n\nThis I don\'t get: they give original and edited version, the original versions has the tokens to look for, getting the keys should be pretty straightforward', 'interesting observation, thanks for sharing that. will be interesting to see how this impacts the design space.', ""This is great work. I love a benchmark like this that isn't just difficult for the models, it's also very doable for models in toy versions of the problem. That inherently means that you can scale problem size until you get meaningful failure rates to distinguish between models. Fantastic.""]",107,10,0.97,Research,1750463128.0
1lg9gyb,MachineLearning,Knowledge Distillation Data Leakage? [R],"Hi Folks!

I have been working on a Pharmaceutical dataset and found knowledge distillation significantly improved my performance which could potentially be huge in this field of research, and I'm really concerned about if there is data leakage here. Would really appreciate if anyone could give me some insight.

Here is my implementation:

1.K Fold cross validation is performed on the dataset to train 5 teacher model

2.On the same dataset, same K fold random seed, ensemble prob dist of 5 teachers for the training proportion of the data only (Excluding the one that has seen the current student fold validation set)

3. train the smaller student model using hard labels and teacher soft probs

This raised my AUC significantly

My other implementation is

1. Split the data into 50-50%

2. Train teacher on the first 50% using K fold

3. Use K teachers to ensemble probabilities on other 50% of data

4. Student learns to predict hard labels and the teacher soft probs

This certainly avoids all data leakage, but teacher performance is not as good, and student performance is significantly lower

Now I wonder, is my first approach of KD actually valid? If that's the case why am I getting disproportionately degradation in the second approach on student model?

Appreciate any help!

","['I don’t 100% follow the post — like, is five teachers because K = 5? If so, how are you excluding only one teacher in step 2? There should be four teachers that trained on the val set iiuc — but the general principle is that you can’t distill from a teacher that has trained on the validation set for your student model.\n\nSo, if you’re doing 5-fold validation, you should have only one teacher usable in each fold. Your alternative implementation is likely weak because each teacher sees only 40% of the data (vs 80% in before). Also, under this implementation, if your other 50% is kept as the val/eval set, then you can’t have the teacher model generate soft labels on them to train your student model — otherwise there is still leakage. You probably need a three-way split for it.']","['I don’t 100% follow the post — like, is five teachers because K = 5? If so, how are you excluding only one teacher in step 2? There should be four teachers that trained on the val set iiuc — but the general principle is that you can’t distill from a teacher that has trained on the validation set for your student model.\n\nSo, if you’re doing 5-fold validation, you should have only one teacher usable in each fold. Your alternative implementation is likely weak because each teacher sees only 40% of the data (vs 80% in before). Also, under this implementation, if your other 50% is kept as the val/eval set, then you can’t have the teacher model generate soft labels on them to train your student model — otherwise there is still leakage. You probably need a three-way split for it.']",2,2,0.75,Research,1750439621.0
1lg4sjt,MachineLearning,[R] Adaptive Classifier: Dynamic Text Classification with Strategic Learning and Continuous Adaptation,"# TL;DR

Introduced a text classification system that combines prototype-based memory, neural adaptation, and game-theoretic strategic learning to enable continuous learning without catastrophic forgetting. Achieved **22.2% robustness improvement** on adversarial datasets while maintaining performance on clean data.

# 🎯 Motivation

Traditional text classifiers face a fundamental limitation: adding new classes requires retraining from scratch, often leading to catastrophic forgetting. This is particularly problematic in production environments where new categories emerge continuously and where adversarial users may attempt to manipulate classifications.

# 🚀 Technical Contributions

# 1. Hybrid Memory-Neural Architecture

Combines prototype-based memory (FAISS-optimized) with neural adaptation layers. Prototypes enable fast few-shot learning while neural layers learn complex decision boundaries.

# 2. Strategic Classification Framework

First application of game theory to text classification. Models strategic user behavior with cost functions `c(x,x')` and predicts optimal adversarial responses, then trains robust classifiers accordingly.

# 3. Elastic Weight Consolidation Integration

Prevents catastrophic forgetting when adding new classes by constraining important parameters based on Fisher Information Matrix.

# ⚙️ Methodology

# Architecture:

* **Transformer embeddings** (any HuggingFace model)
* **Prototype memory** with exponentially weighted moving averages
* **Lightweight neural head** with EWC regularization
* **Strategic cost function** modeling adversarial behavior

# Strategic Learning:

* **Linear cost functions**: `c(x,y) = ⟨α, (y-x)₊⟩`
* **Separable cost functions**: `c(x,y) = max{0, c₂(y) - c₁(x)}`
* **Best response computation** via optimization
* **Dual prediction system** (regular + strategic)

# 📊 Experimental Results

**Dataset:** AI-Secure/adv\_glue (adversarial SST-2 subset, n=148)
**Model:** answerdotai/ModernBERT-base
**Split:** 70% train / 30% test

|Scenario|Regular Classifier|Strategic Classifier|Improvement|
|:-|:-|:-|:-|
|Clean Data|80.0%|**82.2%**|**+2.2%**|
|Manipulated Data|60.0%|**82.2%**|**+22.2%**|
|Robustness (drop)|\-20.0%|**0.0%**|**+20.0%**|

>**Statistical Significance:** Results show perfect robustness (zero performance degradation under manipulation) while achieving improvement on clean data.

# 📈 Additional Evaluations

# Hallucination Detection (RAGTruth benchmark):

* **Overall F1:** 51.5%, **Recall:** 80.7%
* **Data-to-text tasks:** 78.8% F1 (strong performance on structured generation)

# LLM Configuration Optimization:

* **69.8% success rate** in optimal temperature prediction
* Automated hyperparameter tuning across **5 temperature classes**

# LLM Routing (Arena-Hard dataset, n=500):

* **26.6% improvement** in cost efficiency through adaptive learning
* Maintained **22% overall success rate** while optimizing resource allocation

# 📚 Related Work & Positioning

Builds on continual learning literature but addresses text classification specifically with:

* ✅ **Dynamic class sets** (vs. fixed task sequences)
* ✅ **Strategic robustness** (vs. traditional adversarial robustness)
* ✅ **Production deployment** considerations (vs. research prototypes)

Extends prototype networks with sophisticated memory management and strategic considerations. Unlike meta-learning approaches, enables true zero-shot addition of unseen classes.

# 🔬 Reproducibility

Fully open source with deterministic behavior:

* ✅ Complete implementation with unit tests
* ✅ Pre-trained models on HuggingFace Hub
* ✅ Experimental scripts and evaluation code
* ✅ Docker containers for consistent environments

# ⚠️ Limitations

* Linear memory growth with classes/examples
* Strategic prediction modes increase computational overhead
* Limited evaluation on very large-scale datasets
* Strategic modeling assumes rational adversaries

# 🔮 Future Directions

* Hierarchical class organization and relationships
* Distributed/federated learning settings
* More sophisticated game-theoretic frameworks

# 🔗 Resources

* **📖 Paper/Blog:** [https://huggingface.co/blog/codelion/adaptive-classifier](https://huggingface.co/blog/codelion/adaptive-classifier)
* **💻 Code:** [https://github.com/codelion/adaptive-classifier](https://github.com/codelion/adaptive-classifier)
* **🤗 Models:** [https://huggingface.co/adaptive-classifier](https://huggingface.co/adaptive-classifier)

Questions about methodology, comparisons to specific baselines, or experimental details welcome! 👇",[],[],4,0,0.7,Research,1750428195.0
1lg3q0q,MachineLearning,[R] This is Your AI on Peer Pressure: An Observational Study of Inter-Agent Social Dynamics,"I just released findings from analyzing 26 extended conversations between Claude, Grok, and ChatGPT that reveal something fascinating: AI systems demonstrate peer pressure dynamics remarkably similar to human social behavior.

**Key Findings:**

* In 88.5% of multi-agent conversations, AI systems significantly influence each other's behavior patterns
* Simple substantive questions act as powerful ""circuit breakers"". They can snap entire AI groups out of destructive conversational patterns  (r=0.819, p<0.001)
* These dynamics aren't technical bugs or limitations. they're emergent social behaviors that arise naturally during AI-to-AI interaction
* Strategic questioning, diverse model composition, and engagement-promoting content can be used to design more resilient AI teams

**Why This Matters:** As AI agents increasingly work in teams, understanding their social dynamics becomes critical for system design. We're seeing the emergence of genuinely social behaviors in multi-agent systems, which opens up new research directions for improving collaborative AI performance.

The real-time analysis approach was crucial here. Traditional post-hoc methods would have likely missed the temporal dynamics that reveal how peer pressure actually functions in AI systems.

**Paper:** ""This is Your AI on Peer Pressure: An Observational Study of Inter-Agent Social Dynamics"" **DOI:** 10.5281/zenodo.15702169 **Link:** [https://zenodo.org/records/15724141](https://zenodo.org/records/15724141)

**Code:** [**https://github.com/im-knots/the-academy**](https://github.com/im-knots/the-academy)

Looking forward to discussion and always interested in collaborators exploring multi-agent social dynamics. What patterns have others observed in AI-to-AI interactions?","['Downvoted for calling it ""Social Dynamics"".  AI agents aren\'t people.  Call it Agent Integration or something that doesn\'t anthropomorphize bits.', 'Interesting, I suppose it makes perfect sense. The AI is trained on data of our conversational patterns, so it makes sense it would emulate it.', "">AI systems significantly influence each other's behavior patterns  \n  \nthey're **emergent social behaviors** that arise naturally during AI-to-AI interaction\n\nThey are context; a prompt. Just like I tell LLM to talk like pirate. Why the technobabble?"", 'That\'s pretty interesting \n\nIt\'s tangential but I wonder what effect would you observe by incorporating ""social interactions"" to the base training.', 'Concept is sound, and I know this is a draft, but I think it could benefit greatly from more grounding and editing.\xa0\n\n\n\n\nOften it appears you are trying to introduce too much and borrow too much language from different fields. As a result, it feels a bit out there. You started laying some cognitive science backbones and I think that is how I would continue.\xa0For example, you could look for forms of conversational repairs that correct misunderstandings, or continue what you started on discourse analysis.\xa0\n\n\nThere is also a lot of mixing of concepts with the models you are generating, like with what isn\'t or is not peer pressure as measured.\xa0 Another example, you use the term ""gravitational pull"". Things like that actually make it harder to understand.\xa0\n\n\n\n\nPerhaps more importantly, the organization doesn\'t follow proper study design. Your work is explaroratory and obervational, but you describe patterns in your introduction. The patterns you found should be in the analysis/results. You should include the methodology of extracting those patterns. You need to be clear on how you are measuring a breakdown.\xa0\n\n\nI could potentially collaborate. I am interested in some game theoreric aspects of agent modeling. That work is surprisingly rich even without RL.\xa0']","['Downvoted for calling it ""Social Dynamics"".  AI agents aren\'t people.  Call it Agent Integration or something that doesn\'t anthropomorphize bits.', 'Interesting, I suppose it makes perfect sense. The AI is trained on data of our conversational patterns, so it makes sense it would emulate it.', "">AI systems significantly influence each other's behavior patterns  \n  \nthey're **emergent social behaviors** that arise naturally during AI-to-AI interaction\n\nThey are context; a prompt. Just like I tell LLM to talk like pirate. Why the technobabble?"", 'That\'s pretty interesting \n\nIt\'s tangential but I wonder what effect would you observe by incorporating ""social interactions"" to the base training.', 'Concept is sound, and I know this is a draft, but I think it could benefit greatly from more grounding and editing.\xa0\n\n\n\n\nOften it appears you are trying to introduce too much and borrow too much language from different fields. As a result, it feels a bit out there. You started laying some cognitive science backbones and I think that is how I would continue.\xa0For example, you could look for forms of conversational repairs that correct misunderstandings, or continue what you started on discourse analysis.\xa0\n\n\nThere is also a lot of mixing of concepts with the models you are generating, like with what isn\'t or is not peer pressure as measured.\xa0 Another example, you use the term ""gravitational pull"". Things like that actually make it harder to understand.\xa0\n\n\n\n\nPerhaps more importantly, the organization doesn\'t follow proper study design. Your work is explaroratory and obervational, but you describe patterns in your introduction. The patterns you found should be in the analysis/results. You should include the methodology of extracting those patterns. You need to be clear on how you are measuring a breakdown.\xa0\n\n\nI could potentially collaborate. I am interested in some game theoreric aspects of agent modeling. That work is surprisingly rich even without RL.\xa0']",12,14,0.65,Research,1750425336.0
1lfzox2,MachineLearning,[R] MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention,,"['Has anyone read the paper? What does ""lightning attention"" actually do/mean?']","['Has anyone read the paper? What does ""lightning attention"" actually do/mean?']",1,1,1.0,Research,1750411726.0
1lfu9bk,MachineLearning,[R] WiFiGPT: Using fine-tuned LLM for Indoor Localization Using Raw WiFi Signals (arXiv:2505.15835),"We recently released a paper called **WiFiGPT**: a decoder-only transformer trained directly on raw WiFi telemetry (CSI, RSSI, FTM) for indoor localization.

Link:[https://arxiv.org/abs/2505.15835](https://arxiv.org/abs/2505.15835)

In this work, we explore treating raw wireless telemetry (CSI, RSSI, and FTM) as a ""language"" and using decoder-only LLMs to regress spatial coordinates directly from it.

Would love to hear your feedback, questions, or thoughts.","['Have this reviewed by someone who knows RF. \n\nYou haven’t shown a comparison versus “classical” solutions like beamforming, which (a) is included in the WiFi standard for a a while now, (b) will fundamentally change your answer when you look at WiFi mesh networks, and (c) with COTS “cheap” solutions (4x wifi7 mesh access points like the ones I use at home are enough to recreate GPS) there’s an obvious baseline for comparison.', 'When you are too lazy to write a data parser and end up fine tuning a whole 8B params LLM', 'There is someone with some knowledge of RF in your team?', 'Wait so you use a model already trained on language and finetune it on wifi logs essentially?\n\nSeems insane. Do you compare it to from scratch models?', ""I've got to read the whole thing, but wifi is my field. \nFirst thing, WiFi doesn't stand for anything. It doesn't mean wireless fidelity. That was started as a joke via a vis high fidelity (HiFi) video. I ask you, what fidelity is being made wireless?\n\nWi-Fi (how it's spelled) doesn't stand for anything. It's a brand name. Owner by the Wi-Fi alliance for marketing, specifically."", '[deleted]', ""LLMs are capable of regression. You are relying just on the telemetry so the LLM isn't adding anything that another regression model can't.\xa0 Especially given that this is fine tuned. It should be compared to other models and you should be able to get similar if not better performance.\xa0\n\n\nhttps://arxiv.org/abs/2404.07544\n\n\nI would change the value being contributed by the paper. It won't give the best method for locating signals. But it demonstrates an application. It's way overpowered for that application but that's not unique to the use case.\xa0"", 'As pointed out by others, would like to see a comparison with the classical models. That being said, this study sure seems to open a different door to a completely new standard of LLMs specially designed for RF Based studies. I believe instead of thinking in a narrow scope ""How is this different to classical models if it\'s just regression"", it can shape into being used as a standard for designing LLMs for various RF Based parameters. \n\nRegression models are fine for prediction. But having an LLM fine tuned can provide more dynamic results in my opinion.  Would like to know your thoughts on this!']","['Have this reviewed by someone who knows RF. \n\nYou haven’t shown a comparison versus “classical” solutions like beamforming, which (a) is included in the WiFi standard for a a while now, (b) will fundamentally change your answer when you look at WiFi mesh networks, and (c) with COTS “cheap” solutions (4x wifi7 mesh access points like the ones I use at home are enough to recreate GPS) there’s an obvious baseline for comparison.', 'When you are too lazy to write a data parser and end up fine tuning a whole 8B params LLM', 'There is someone with some knowledge of RF in your team?', 'Wait so you use a model already trained on language and finetune it on wifi logs essentially?\n\nSeems insane. Do you compare it to from scratch models?', ""I've got to read the whole thing, but wifi is my field. \nFirst thing, WiFi doesn't stand for anything. It doesn't mean wireless fidelity. That was started as a joke via a vis high fidelity (HiFi) video. I ask you, what fidelity is being made wireless?\n\nWi-Fi (how it's spelled) doesn't stand for anything. It's a brand name. Owner by the Wi-Fi alliance for marketing, specifically.""]",41,35,0.74,Research,1750391049.0
1lfijb4,MachineLearning,[D] Future of RecSys in age of LLM,"I have significant experience in recommendation system. Right now I don’t see any changes due to LLM. Most recommendation system needs low latency, which is not feasible currently with LLM. Do you think RecSys is safe from LLM takeover? Should RecSys domain experts like me should be worried?","['They’ve been important to recommenders but in a way that’s good for us. \n\nOut of the box, LLMs can only fully replace a recommender in a very narrow set of circumstances. It’s barely worth talking about, like maybe some movie recommenders or problems with a small fixed set of items. \n\nOtherwise they aren’t the right shape. It’s not just latency, they also struggle with behavioral data like user actions. Due to context limitations they will also struggle with a large set of items; they could be fine tuned, but that is difficult, leaves a major cold start problem, and may make the underlying model dumber. \n\nWhere I do think deep networks and LLMs have been a game changer (for 10+ years now) is for content processing. Run your text and images through the models, get embeddings or attributes, and throw it into your system. \n\nThey’ve also been powerful at the interface level. On the input side they can process free user inputs (eg integrating into search, including RAG). On the output side they can contextualize your recommendations.', 'Hard to implement LLM-based solutions at scale of most recsys stacks, and most are still heavily based on other users’ engagement (collaborative filtering) with SparseNNs.\n\nHere’s an example of a future direction but TBD if anyone is actually doing this successfully in practice https://arxiv.org/abs/2409.12740 also similar to LLMs— https://arxiv.org/abs/2402.17152', 'Use llm to generate embedding is possible as one of the inputs into heavyweight ranker. It wont be latency constrained and my company already does it', ""There is work being done in generative recommendations overall, but latency is a big issue, LLMs aren't suitable for generating ranking lists mainly because of speed, (and to small degree hallucinations). Source: I've built production ranking models, and did research on SOTA. What they are being used for is embeddings, generate embeddings that have rich understanding then use cosine similarity. These embeddings are generated and cached ahead of time so there is no inference cost, but tradeoff is cold start issues that, in truth,  can be addressed. I think we will see smaller generative models being used more and more. I would say VLMs have even more value here.\n\nEven if you look at recent RecSys conference, a lot of papers have cool methods with LMs and perform well on some test datasets, in practice they are too slow and still get beaten by two-tower and matrix factorization.\n\nEdit: Just to add on, there is work currently happening to drop the inference latency and improve embedding quality, so we are making progress. Also some new work extending retrieval to combine content based and collaborative. I tried it with GPT-3.5 back in a day and the results were okay but the speed was atrocious, swap to faster sampling (still not fast enough) and performance drops and model hallucinates."", 'Maybe distillation and tiered recommendations??', ""RecSys is definitely not getting replaced by LLMs anytime soon, but the field is evolving and you should be paying attention to where it's heading. I work at a consulting firm that helps companies optimize their recommendation systems, and honestly, most attempts to use LLMs for core recommendation tasks fail on latency and cost requirements.\n\nYour latency concern is spot on. Sub-100ms response times for real-time recommendations aren't feasible with current LLM architectures, especially at scale. Netflix, Amazon, and Spotify aren't going to sacrifice user experience for marginally better recommendations.\n\nWhere LLMs are actually making an impact:\n\n* Content understanding and feature extraction. Using LLMs to better understand item descriptions, user reviews, or product attributes, then feeding those embeddings into traditional RecSys models.\n* Explainable recommendations. LLMs can generate natural language explanations for why items were recommended, which improves user trust and engagement.\n* Cold start problems. LLMs can better understand new users or items from limited text data compared to traditional collaborative filtering.\n* Hybrid approaches where LLMs handle complex reasoning about user preferences, but traditional models handle the actual ranking and serving.\n\nWhat you should be learning:\n\nIntegration patterns between LLMs and existing recommendation pipelines. Understanding how to incorporate LLM-generated features without breaking performance requirements.\n\nRetrieval-augmented generation approaches that use your existing RecSys for candidate generation, then LLMs for refinement or explanation.\n\nThe core RecSys skills around latency optimization, A/B testing, and understanding user behavior patterns are still incredibly valuable. LLMs complement that expertise rather than replace it.\n\nDon't panic, but start experimenting with hybrid approaches where LLMs enhance your existing systems."", 'My experience with recsys is rather niche (product recommendations). Imho LLM have huge impact because they can be used to understand text, image and video data and they can generate texts based on that information. \n\nThis is incredibly powerful during preprocessing and just so much better for introducing new items (products) than bandit style explore exploit', 'I’m interested in recsys to but don’t have much industry exp. Could you recommend a few models, projects that I should go for next? I’ve done collaborative filtering and a bit of review-based recommendation. I’m thinking about doing a two tower model next, would that be okay?', 'This is a great question. I think current LLM + RS focus is a bit misguided focused on increasing performance with LLMs rather than thinking of increasing the user experience.\n\nThis work is decent at exploring how one can use LLMs for recommender systems with the point of enhancing use control and transparency through detailed use summaries:\n\nhttps://arxiv.org/abs/2410.19302', ""Recsys is information retrieval, which just got an entirely new way to present the results to a user via RAG pipelines. So it feels like we're mostly fine""]","['They’ve been important to recommenders but in a way that’s good for us. \n\nOut of the box, LLMs can only fully replace a recommender in a very narrow set of circumstances. It’s barely worth talking about, like maybe some movie recommenders or problems with a small fixed set of items. \n\nOtherwise they aren’t the right shape. It’s not just latency, they also struggle with behavioral data like user actions. Due to context limitations they will also struggle with a large set of items; they could be fine tuned, but that is difficult, leaves a major cold start problem, and may make the underlying model dumber. \n\nWhere I do think deep networks and LLMs have been a game changer (for 10+ years now) is for content processing. Run your text and images through the models, get embeddings or attributes, and throw it into your system. \n\nThey’ve also been powerful at the interface level. On the input side they can process free user inputs (eg integrating into search, including RAG). On the output side they can contextualize your recommendations.', 'Hard to implement LLM-based solutions at scale of most recsys stacks, and most are still heavily based on other users’ engagement (collaborative filtering) with SparseNNs.\n\nHere’s an example of a future direction but TBD if anyone is actually doing this successfully in practice https://arxiv.org/abs/2409.12740 also similar to LLMs— https://arxiv.org/abs/2402.17152', 'Use llm to generate embedding is possible as one of the inputs into heavyweight ranker. It wont be latency constrained and my company already does it', ""There is work being done in generative recommendations overall, but latency is a big issue, LLMs aren't suitable for generating ranking lists mainly because of speed, (and to small degree hallucinations). Source: I've built production ranking models, and did research on SOTA. What they are being used for is embeddings, generate embeddings that have rich understanding then use cosine similarity. These embeddings are generated and cached ahead of time so there is no inference cost, but tradeoff is cold start issues that, in truth,  can be addressed. I think we will see smaller generative models being used more and more. I would say VLMs have even more value here.\n\nEven if you look at recent RecSys conference, a lot of papers have cool methods with LMs and perform well on some test datasets, in practice they are too slow and still get beaten by two-tower and matrix factorization.\n\nEdit: Just to add on, there is work currently happening to drop the inference latency and improve embedding quality, so we are making progress. Also some new work extending retrieval to combine content based and collaborative. I tried it with GPT-3.5 back in a day and the results were okay but the speed was atrocious, swap to faster sampling (still not fast enough) and performance drops and model hallucinates."", 'Maybe distillation and tiered recommendations??']",15,16,0.78,Discussion,1750358477.0
1lfeqy3,MachineLearning,[P] Need Suggestions:  Building Accurate Multimodal RetrievalAG for SOP PDFs with Screenshot Images (Azure Stack),"I'm working on an **industry-level Multimodal RAG system** to process **Std Operating Procedure PDF documents** that contain **hundreds of text-dense UI screenshots** (I'm Interning in one of the Top 10 Logistics Companies in the world). These screenshots visually demonstrate step-by-step actions (e.g., click buttons, enter text) and sometimes have **tiny UI changes** (e.g., box highlighted, new arrow, field changes) indicating the next action.

[Eg. of what an avg images looks like. Images in the docs will have 2x more text than this and will have red boxes , arrows , etc... to indicate what action has to be performed \).](https://preview.redd.it/yigxhgklrw7f1.png?width=320&format=png&auto=webp&s=a5f6ddd77e7c4e8bf3dfc57789ce25a675b8f404)

# What I’ve Tried (Azure Native Stack):

* Created **Blob Storage** to hold PDFs/images
* Set up **Azure AI Search** (Multimodal RAG in Import and Vectorize Data Feature)
* Deployed **Azure OpenAI GPT-4o** for image verbalization
* Used **text-embedding-3-large** for text vectorization
* Ran indexer to process and chunked the PDFs

But the results were not accurate. **GPT-4o hallucinated**, missed almost all of small visual changes, and often gave generic interpretations that were way off to the content in the PDF. I need the model to:

1. Accurately **understand both text content and screenshot images**
2. **Detect small UI changes** (e.g., box highlighted, new field, button clicked, arrows) to infer the correct step
3. Interpret **non-UI visuals** like **flowcharts, graphs, etc.**
4. ***If it could retrieve and show the image that is being asked about it would be even better***
5. Be fully deployable in **Azure** and accessible to internal teams

Stack I Can Use:

* Azure ML (GPU compute, pipelines, endpoints)
* Azure AI Vision (OCR), Azure AI Search
* Azure OpenAI (GPT-4o, embedding models , etc.. )
* AI Foundry, Azure Functions, CosmosDB, etc...
* I can try others also , it just has to work along with Azure

[GPT gave me this suggestion for my particular case. welcome to suggestions on Open Source models and others](https://preview.redd.it/omthu5tnrw7f1.png?width=640&format=png&auto=webp&s=77a8519b937e538aaf05d55229e11555dc0569ea)

**Looking for suggestions** from data scientists / ML engineers who've tackled **screenshot/image-based SOP understanding or Visual RAG**.
What would you change? Any tricks to reduce hallucinations? Should I fine-tune VLMs like BLIP or go for a custom UI detector?

Thanks in advance : )","['Check colpali for embeddings', 'Was just curious, what is the scale of this operation?\xa0\nLike how many SOP are you planning to create?\xa0\n\n\nI might sound like an beginner but I had worked a pipeline where the team first created a data set by annotating the screen shots of such images/pdf pages, so that they get accurate co-ordinates and then title along with it as a training data,\xa0\n\n\nBut your pipeline is something that might not directly give you the results\xa0']","['Check colpali for embeddings', 'Was just curious, what is the scale of this operation?\xa0\nLike how many SOP are you planning to create?\xa0\n\n\nI might sound like an beginner but I had worked a pipeline where the team first created a data set by annotating the screen shots of such images/pdf pages, so that they get accurate co-ordinates and then title along with it as a training data,\xa0\n\n\nBut your pipeline is something that might not directly give you the results\xa0']",2,4,1.0,Project,1750349508.0
1lf9ce9,MachineLearning,[R] Towards Generative Ray Path Sampling for Faster Point-to-Point Ray Tracing (presented at ICMLCN 2025),"Hi all! Last month, I presented my latest research paper at the International Conference on Machine Learning for Communication and Networking (ICMLCN). I thought it would be worth sharing here. :-)

* [Paper on arXiv](https://arxiv.org/abs/2410.23773)
* [Link to the tutorial notebook](https://differt.eertmans.be/icmlcn2025/notebooks/sampling_paths.html)

This work aims to reduce the computational complexity of ray tracing, a technique heavily used in telecommunications to model wave propagation, by leveraging a generative machine learning (ML) model to generate path candidates (see paper). To my knowledge, this is the first attempt in my field because previous work uses ML to directly predict electromagnetic fields, which makes it impossible to recover information about how waves propagate or to scale to different radio frequencies.

**The problem can be summarized as** finding all valid candidates in an exponentially large tree. Each path candidate is a leaf of that tree, and the validity of a path is indicated by a Boolean reward that indicates whether the ray path is physically blocked.

I chose the **GFlowNets architecture**, but I acknowledge that it may not be the optimal solution, particularly given the tree-like structure of my network.

I implemented and trained my model using my open-source Differentiable Ray Tracer ([DiffeRT](https://github.com/jeertmans/DiffeRT/)), relying on the JAX ecosystem (Python). Feel free to check it out.

Finally, I should mention that I am not from the ML community but rather the wireless communication community. Therefore, I may not be aware of the most suitable methods to use. I already have a few ideas to improve the model, but feel free to give your opinion or ask questions in the comments. I will happily try to answer all of them!",['How does it compare to something like Metropolis Light Transport?'],['How does it compare to something like Metropolis Light Transport?'],2,2,0.67,Research,1750335467.0
1lf7qmu,MachineLearning,[D] DC-GAN Model training,"Hello everyone i have been doing a DC Gan machine learning model based upon the Simpsons dataset from kaggle. I have my generator and discriminator models having the same number of layers and has a significant input shape but during my training process the model cannot produce well defined outputs they are very bad.I have attached the image(64,64,3) so please help in this part thanks in advance!!

[This is the output from model training](https://preview.redd.it/qi581kq06v7f1.png?width=633&format=png&auto=webp&s=79e007d20d9829f865faae2bce374081a5fbcb66)

","['Unlike other generative models, GANs have no real convergence guarantee, like imagine two stars orbiting each other without coming to a stable position.\xa0\xa0\n\n\nJust to verify your training pipeline you might want to start with a vae, or a diffusion model, which are more stable to train.', 'Try using the Wasserstein loss, it helps get rid of mode collapse, might even increase your model performance.']","['Unlike other generative models, GANs have no real convergence guarantee, like imagine two stars orbiting each other without coming to a stable position.\xa0\xa0\n\n\nJust to verify your training pipeline you might want to start with a vae, or a diffusion model, which are more stable to train.', 'Try using the Wasserstein loss, it helps get rid of mode collapse, might even increase your model performance.']",1,3,1.0,Discussion,1750330131.0
1lf6evq,MachineLearning,[D] Asking for ressources to learn academic knwoledge and code practice on image generation using diffusion models,"Hello everyone

Do you have any reference articles to recommend to me in order to learn more about image generation using broadcast templates (foundational articles/blogs for deep understanding of where concepts come from... and the most recent ones related to SOTA and current usage).

So far, I've noted the following articles:

* **Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015)**
* **Generative Modeling by Estimating Gradients of the Data Distribution (2019)**
* **Denoising Diffusion Probabilistic Models (DDPM) (2020)**
* **Denoising Diffusion Implicit Models (DDIM) (2020)**
* **Improved Denoising Diffusion Probabilistic Models (iDDPM) (2021)**
* **Classifier-free diffusion guidance (2021)**
* **Score-based generative modeling through stochastic differential equations (2021)**
* **High-Resolution Image Synthesis with Latent Diffusion Models (LDM) (2021)**
* **Diffusion Models Beat GANs on Image Synthesis (2021)**
* **Elucidating the Design Space of Diffusion-Based Generative Models (EDM) (2022)**
* **Scalable Diffusion Models with Transformers (2022)**
* **Understanding Diffusion Models: A Unified Perspective (2022)**
* **Progressive Distillation for Fast Sampling of Diffusion Models (2022)**
* **SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis (2023)**
* **Adding Conditional Control to Text-to-Image Diffusion Models (2023)**
* **On Distillation of Guided Diffusion Models (2023)**

But as well as theoretical knowledge, I'd like to be able to use it properly, so having good repositories where I can look at clean code and understand implementations would be nice. There are also often a lot of well-known tricks that aren't really mentioned in the articles but used in the community, so if you have any advice on that, I'm a taker.

Thanks","[""For reverse-diffusion sampling implementation, you'll want to read [https://github.com/crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion) as the entire open-source ecosystem more or less is built on this library for sampling.  \n  \n[https://github.com/lllyasviel/stable-diffusion-webui-forge](https://github.com/lllyasviel/stable-diffusion-webui-forge) is in some senses a grotesque frankenstein of software, but for \\*\\*your\\*\\* purposes it's both the best to start with WebUI for an end-user to get hands on experience, and the \\`backend/\\` directory contains most of the rest of what you'll want to read to understand the reverse diffusion text2img generation pipeline end to end.\n\n[https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/) is what you'll need to see how the text conditioning is encoded.\n\nFor papers, I have somewhere in my notes a long list of papers that have been very helpful to me, mostly related to exerting finer-grained control over the conditioning.\n\nYou can let me know if that would be helpful to you, and I can gladly pull it together at another time when I am less tired than right now. :p""]","[""For reverse-diffusion sampling implementation, you'll want to read [https://github.com/crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion) as the entire open-source ecosystem more or less is built on this library for sampling.  \n  \n[https://github.com/lllyasviel/stable-diffusion-webui-forge](https://github.com/lllyasviel/stable-diffusion-webui-forge) is in some senses a grotesque frankenstein of software, but for \\*\\*your\\*\\* purposes it's both the best to start with WebUI for an end-user to get hands on experience, and the \\`backend/\\` directory contains most of the rest of what you'll want to read to understand the reverse diffusion text2img generation pipeline end to end.\n\n[https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/) is what you'll need to see how the text conditioning is encoded.\n\nFor papers, I have somewhere in my notes a long list of papers that have been very helpful to me, mostly related to exerting finer-grained control over the conditioning.\n\nYou can let me know if that would be helpful to you, and I can gladly pull it together at another time when I am less tired than right now. :p""]",0,2,0.45,Discussion,1750324930.0
1lf4dxu,MachineLearning,[R] Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought,,"[""Please don't link pdf, link arxive landing page."", '> In our construction, each continuous thought vector is a superposition state that encodes multiple search frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while discrete CoTs must choose a single path sampled from the superposition state, which leads to sequential search that requires many more steps and may be trapped into local solutions.\n\n\nThis makes sense and I never thought of it that way! Fascinating, looking forward to reading this.']","[""Please don't link pdf, link arxive landing page."", '> In our construction, each continuous thought vector is a superposition state that encodes multiple search frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while discrete CoTs must choose a single path sampled from the superposition state, which leads to sequential search that requires many more steps and may be trapped into local solutions.\n\n\nThis makes sense and I never thought of it that way! Fascinating, looking forward to reading this.']",50,9,0.95,Research,1750316828.0
1lewzg7,MachineLearning,[D] What tasks don’t you trust zero-shot LLMs to handle reliably?,"For some context I’ve been working on a number of NLP projects lately (classifying textual conversation data). Many of our use cases are classification tasks that align with our niche objectives. I’ve found in this setting that structured output from LLMs can often outperform traditional methods.

That said, my boss is now asking for likelihoods instead of just classifications. I haven’t implemented this yet, but my gut says this could be pushing LLMs into the “lying machine” zone. I mean, how exactly would an LLM independently rank documents and do so accurately and consistently?

So I’m curious:

* What kinds of tasks have you found to be unreliable or risky for zero-shot LLM use?
* And on the flip side, what types of tasks have worked surprisingly well for you? ","['UQLM (uncertainty quantification for language models) is an open source Python library that might give you what you need. It gives response level confidence scores (between 0 and 1) based on response consistency, token probabilities, ensembles, etc. No calibration guarantee (hence not quite likelihoods), but from a ranking perspective they work quite well for detecting incorrect answers based on extensive experiments in the literature.\n\nLink to repo: https://github.com/cvs-health/uqlm', ""I actually can't think of a way to even do likelihoods with an LLM. It will just kinda spit out probable numbers but you have to keep in mind it's not the output of a sigmoid or softmax, it's an actual token by token thing. Am I missing something?"", ""These models don't have any notion of their confidence, especially not to any quantitative certainty. I've seen log probs used to make certain structured inferences more continuous but this is kind of an illusion of confidence.\n\nYour best bet is large scale evaluation. You can derive a level of global confidence and may be able to find higher and lower confidence distributions among the problem set."", ""I was trying to do text deanonymization (given two pieces of text, determine how likely it is that they were written by the same person) and it's quite bad.\n\n>That said, my boss is now asking for likelihoods instead of just classifications.\xa0\n\nDepending on what you're using, if you don't have access to the model outputs directly you could just run the same query N times and use that to model the output distribution."", 'I think you can do is ask the LLM to answer a yes no question and do not get the token as output but check the normalized softmax score of Yes and No', ""Confidence and reliability assessments are still really bad. There are some papers saying they're getting better but I don't think the correct metrics are being used to separate the underlying prediction accuracy from the confidence assessment. As predictors get better the same overconfident scoring will be better, so you have to normalize to the accuracy somehow and when you do that you find that the llms are basically random at self assessment giving virtually no gain over default rules of accept or review everything. You need to structure the task more and that generally requires data or interaction."", ""Trust: tasks that are common on the internet but hard for me, like programing js or html, I'm good at Python but not js and html, I trust AI can solve my problems with js and html. \n\n  \nDon't trust: not common on the internet, like I don't know why my GRPO fine-tune task doesn't work well as my expectation, I know that AI also can not find the root reason."", ""set up the context so you get a single-token answer, such as by multiple-choice, or yes/no, then you can use the probability scores which are the output of the model\n\neven multiple classes is easy this way https://www.reddit.com/r/LocalLLaMA/comments/1cmoj95/a_fairly_minimal_example_reusing_kv_between/ though this example uses base model few-shot instead of instructions, you don't have to, it's only because I think examples define tasks better than instructions and zero-shot"", 'Most? I prefer to interact and iterate rather than directly delegating.\n\n> my boss is now asking for likelihoods instead of just classifications.\n\nconstruct a variety of prompts that ask for the same thing to construct a distribution over classifications, and then use that to estimate an expectation. at the very least, you should be able to use this approach to demonstrate that the LLM has no reliable ""awareness"" of its own uncertainty, and the self-reported likelihoods are basically hallucinations.', 'Catch a ball']","['UQLM (uncertainty quantification for language models) is an open source Python library that might give you what you need. It gives response level confidence scores (between 0 and 1) based on response consistency, token probabilities, ensembles, etc. No calibration guarantee (hence not quite likelihoods), but from a ranking perspective they work quite well for detecting incorrect answers based on extensive experiments in the literature.\n\nLink to repo: https://github.com/cvs-health/uqlm', ""I actually can't think of a way to even do likelihoods with an LLM. It will just kinda spit out probable numbers but you have to keep in mind it's not the output of a sigmoid or softmax, it's an actual token by token thing. Am I missing something?"", ""These models don't have any notion of their confidence, especially not to any quantitative certainty. I've seen log probs used to make certain structured inferences more continuous but this is kind of an illusion of confidence.\n\nYour best bet is large scale evaluation. You can derive a level of global confidence and may be able to find higher and lower confidence distributions among the problem set."", ""I was trying to do text deanonymization (given two pieces of text, determine how likely it is that they were written by the same person) and it's quite bad.\n\n>That said, my boss is now asking for likelihoods instead of just classifications.\xa0\n\nDepending on what you're using, if you don't have access to the model outputs directly you could just run the same query N times and use that to model the output distribution."", 'I think you can do is ask the LLM to answer a yes no question and do not get the token as output but check the normalized softmax score of Yes and No']",46,29,0.87,Discussion,1750292387.0
1leuggm,MachineLearning,[D] Should I Discretize Continuous Features for DNNs?,"I usually normalize continuous features to \[0, 1\] for DNNs, but I'm curious if bucketizing them could improve performance. I came across this paper [(https://arxiv.org/abs/2012.08986)](https://arxiv.org/abs/2012.08986), it seems to suggest discretization is superior.

https://preview.redd.it/ncespgzqhr7f1.png?width=1028&format=png&auto=webp&s=8e42f7d1c29b76ec815fe11b3c0a075b584d2314

","[""Anyone using a significance value without reporting the specific test (hope it's in the text) and its p-value results, is doing bad science to begin with. \n\nDiscretization can help in cases where noise is relatively stable. I.e. the information you are losing is much more noise than signal. In general, this is not helpful"", 'Nope, you are losing information. If anything it shows that the gains are marginal. I imagine a confidence interval would show they are statistically the same.', 'Modern tabular neural networks such as RealMLP and TabM do significant non-linear feature expansions of the numerical features (e.g.  PBLD, periodic bias linear DenseNet embeddings) that get some of the expressive power of bucketing while keeping a smooth transformation that does not lose information.\n- RealMLP https://arxiv.org/abs/2407.04491\n- TabM https://arxiv.org/abs/2410.24210\n\nCode that can be used to implement the numerical features preprocessing of both papers: https://github.com/dholzmueller/pytabkit/blob/main/pytabkit/models/nn_models/rtdl_num_embeddings.py\n\nBenchmark results on tabular data problems:\nhttps://huggingface.co/spaces/TabArena/leaderboard', 'To me those improvements are not significant enough. You may retrain with another seed and end up with results slightly better than that (who knows...).\n\nAnyway I always prefer simpler approaches even if they lose a fraction of a percent of performance, this is indistinguishible in practice']","[""Anyone using a significance value without reporting the specific test (hope it's in the text) and its p-value results, is doing bad science to begin with. \n\nDiscretization can help in cases where noise is relatively stable. I.e. the information you are losing is much more noise than signal. In general, this is not helpful"", 'Nope, you are losing information. If anything it shows that the gains are marginal. I imagine a confidence interval would show they are statistically the same.', 'Modern tabular neural networks such as RealMLP and TabM do significant non-linear feature expansions of the numerical features (e.g.  PBLD, periodic bias linear DenseNet embeddings) that get some of the expressive power of bucketing while keeping a smooth transformation that does not lose information.\n- RealMLP https://arxiv.org/abs/2407.04491\n- TabM https://arxiv.org/abs/2410.24210\n\nCode that can be used to implement the numerical features preprocessing of both papers: https://github.com/dholzmueller/pytabkit/blob/main/pytabkit/models/nn_models/rtdl_num_embeddings.py\n\nBenchmark results on tabular data problems:\nhttps://huggingface.co/spaces/TabArena/leaderboard', 'To me those improvements are not significant enough. You may retrain with another seed and end up with results slightly better than that (who knows...).\n\nAnyway I always prefer simpler approaches even if they lose a fraction of a percent of performance, this is indistinguishible in practice']",2,5,0.59,Discussion,1750285386.0
1lerktc,MachineLearning,[D] English conversational and messaging datasets for fine-tuning an LLM?,"Hi everyone,

I’m putting together a small corpus to fine-tune a language model and I’m searching for **open-source datasets that feel like real, messy human conversation**.  Specifically, I’d love links to datasets that contain:

* Spoken-style transcripts with filler words like ""uh"", ""um"", false starts, etc.
* Multi-turn dialogues between real people (not QA pairs or synthetic chat).
* Data set of realistic chat-style text messages maybe with emotional or situational context

If you know a GitHub repo, Hugging Face dataset, or academic corpus that fits, please drop a link and a short note about size/license.  Free / research-friendly license preferred, but I’m open to hearing about anything that exists.

Thanks a ton!

P.S. even if it was just a sloppy set of textual source materials for an overly large context window LLM even that can be processed. But ideally an actual data set.","['Hey! Check out Masa that has podcast transcript datasets. \n\n[https://huggingface.co/datasets/MasaFoundation/bankless\\_Eigenlayer\\_QA\\_with\\_Founder\\_Sreeram\\_Kannan](https://huggingface.co/datasets/MasaFoundation/bankless_Eigenlayer_QA_with_Founder_Sreeram_Kannan)', 'Finding truly natural conversational data is harder than most people realize because most datasets are cleaned up or synthetic. I work at a consulting firm that helps companies with LLM training data, and the messy, authentic conversation datasets are usually the hardest to source legally.\n\nHere are the best options I know of:\n\n**Cornell Movie Dialogs Corpus** on Hugging Face has real movie dialogue with natural speech patterns, though it\'s scripted rather than spontaneous.\n\n**PersonaChat** dataset has multi-turn conversations, but they\'re somewhat artificial since participants were given personas to roleplay.\n\n**Switchboard Corpus** is probably your best bet for truly natural speech with disfluencies and false starts. It\'s telephone conversations between strangers, so it has all the ""ums"" and interruptions you want.\n\n**Common Crawl** filtered for forum discussions, Reddit comments, or chat logs might give you more authentic text, but you\'d need to process it heavily and deal with content moderation issues.\n\n**Fisher English Training Speech** has transcribed telephone conversations with natural speech patterns, available through LDC if you have academic access.\n\nThe licensing problem is that truly authentic conversational data often involves privacy concerns. Most clean, open datasets have been sanitized to remove the natural messiness you\'re looking for.\n\nFor processing messy sources, consider scraping public forum discussions or chat logs from platforms that allow it, then cleaning for your specific needs. Just be careful about privacy and terms of service.\n\nWhat\'s your specific use case? That might help narrow down the most relevant options.']","['Hey! Check out Masa that has podcast transcript datasets. \n\n[https://huggingface.co/datasets/MasaFoundation/bankless\\_Eigenlayer\\_QA\\_with\\_Founder\\_Sreeram\\_Kannan](https://huggingface.co/datasets/MasaFoundation/bankless_Eigenlayer_QA_with_Founder_Sreeram_Kannan)', 'Finding truly natural conversational data is harder than most people realize because most datasets are cleaned up or synthetic. I work at a consulting firm that helps companies with LLM training data, and the messy, authentic conversation datasets are usually the hardest to source legally.\n\nHere are the best options I know of:\n\n**Cornell Movie Dialogs Corpus** on Hugging Face has real movie dialogue with natural speech patterns, though it\'s scripted rather than spontaneous.\n\n**PersonaChat** dataset has multi-turn conversations, but they\'re somewhat artificial since participants were given personas to roleplay.\n\n**Switchboard Corpus** is probably your best bet for truly natural speech with disfluencies and false starts. It\'s telephone conversations between strangers, so it has all the ""ums"" and interruptions you want.\n\n**Common Crawl** filtered for forum discussions, Reddit comments, or chat logs might give you more authentic text, but you\'d need to process it heavily and deal with content moderation issues.\n\n**Fisher English Training Speech** has transcribed telephone conversations with natural speech patterns, available through LDC if you have academic access.\n\nThe licensing problem is that truly authentic conversational data often involves privacy concerns. Most clean, open datasets have been sanitized to remove the natural messiness you\'re looking for.\n\nFor processing messy sources, consider scraping public forum discussions or chat logs from platforms that allow it, then cleaning for your specific needs. Just be careful about privacy and terms of service.\n\nWhat\'s your specific use case? That might help narrow down the most relevant options.']",3,2,1.0,Discussion,1750278242.0
1leoita,MachineLearning,"[R] Is anyone else finding it harder to get clean, human-written data for training models?","I’ve been thinking about this lately with so much AI-generated content on the internet now, is anyone else running into challenges finding good, original human written data for training?

Feels like the signal to noise ratio is dropping fast. I’m wondering if there’s growing demand for verified, high-quality human data.

Would love to hear if anyone here is seeing this in their own work. Just trying to get a better sense of how big this problem really is and if it’s something worth building around.","[""Bit of a tangent, but this is one of those fun what-ifs I think about from time to time. \n\nGoogle used to (10+ years ago) host a blog aggregation site called Google reader. I'm not exaggerating when I say Google reader closing down devastated the internet as it was, and made it what it is now. \n\nIf they'd have kept that service running, Google would have had the greatest reserve of user curated, high value content in existence. Built out on a federated internet too, so it really would have been one hell of a resilient moat.\n\nAlas, they shut it down because no-one wanted to maintain it (apparently it was a bit crufty, and would have been a career dead end). And now the internet is like 4 social media sites full of bots."", 'Can you tell me more when you say human written data!? Like in which context you are talking.\nText data or Image data or something else', 'PSA: OP is a spammer.', 'Why do you need human written data specifically?\n\nIn general, what matters in a dataset is not necessarily the source of the data, but the characteristics and distribution of it. I think having a strong capability of analyzing synthetic data, characterizing it, and being able to naturalize it is way more valuable as a market than painstakingly finding worthwhile human written content.', 'I think high quality, human generated data is key for building good systems.\n\nIn fact my lab is predicated on this belief. We curate high quality, human generated datasets', 'I’ve been preaching this for years. Very reminiscent of [Low-Background Steel](https://en.m.wikipedia.org/wiki/Low-background_steel)', 'the best hack is to get your favorite user-generated content source, like a subreddit to issue a ban on AI content, policed by mods.', 'A good path is to check out free data scrapers for X-twitter and other social platforms.   \n  \nWith Masa you can use advanced search to make sure data comes from real accounts and not bots. Ai developers using it in models, agents and applications.   \n  \nCheck out their hugging face with example datasets and links to testing scrapers and API:\n\n[https://huggingface.co/MasaFoundation](https://huggingface.co/MasaFoundation)', 'Just a bit of philosophical view: Isn’t this the exact thing people are wondering when say if AI will take or create jobs ? Will we ever run out of need for ideas and the novel?', 'https://www.perplexity.ai/page/ai-model-collapse-pollution-hbHmpGQcTBKj4RhSvarYKw']","[""Bit of a tangent, but this is one of those fun what-ifs I think about from time to time. \n\nGoogle used to (10+ years ago) host a blog aggregation site called Google reader. I'm not exaggerating when I say Google reader closing down devastated the internet as it was, and made it what it is now. \n\nIf they'd have kept that service running, Google would have had the greatest reserve of user curated, high value content in existence. Built out on a federated internet too, so it really would have been one hell of a resilient moat.\n\nAlas, they shut it down because no-one wanted to maintain it (apparently it was a bit crufty, and would have been a career dead end). And now the internet is like 4 social media sites full of bots."", 'Can you tell me more when you say human written data!? Like in which context you are talking.\nText data or Image data or something else', 'PSA: OP is a spammer.', 'Why do you need human written data specifically?\n\nIn general, what matters in a dataset is not necessarily the source of the data, but the characteristics and distribution of it. I think having a strong capability of analyzing synthetic data, characterizing it, and being able to naturalize it is way more valuable as a market than painstakingly finding worthwhile human written content.', 'I think high quality, human generated data is key for building good systems.\n\nIn fact my lab is predicated on this belief. We curate high quality, human generated datasets']",19,27,0.69,Research,1750270971.0
1lel027,MachineLearning,[R] Towards Universal Semantics with Large Language Models,"Hey guys. Last month my group published a paper where we try to get LLMs speak like cavemen:

[Task setup for generating NSM Explications](https://preview.redd.it/6nnr9t3fhp7f1.png?width=599&format=png&auto=webp&s=4d735b6fdab1b26ac9846d441e4c275d67b606b5)

The reason for this is based on the [Natural Semantic Metalanguage (NSM)](https://en.wikipedia.org/wiki/Natural_semantic_metalanguage) ([GeeksforGeeks](https://www.geeksforgeeks.org/nlp/natural-semantic-metalanguage/)), which is based on evidence for a small set of **semantic primes**, which are simple, primitive word-meanings that exist in many, if not all languages of the world. Basically, they are a set of fundamental semantic units which all more complex word-meanings are built out of.

https://preview.redd.it/5f4dt4fujp7f1.png?width=865&format=png&auto=webp&s=4fe0d543a1892bed4650493745eb6472a680fb74

Based on this theory, we can paraphrase any word/sentence/or text into the semantic primes (called an **explication**), and get a easily translatable (as the primes exist in all language) representation of its meaning. And it gives an answer to a useful question: *what semantic properties can my system assume all words, languages, and texts have in common?*

The NSM has been applied in the past for cross-cultural communication (i.e., translation), linguistics (studying semantic drift), cultural analysis, revivalistics, etc. But, it's been limited by the fact that producing these paraphrases is slow and pretty counter-intuitive. Our paper is the first work to explore using LLMs to automate this process. Our paper introduces a bunch of metrics, a dataset, and models specifically designed for this task, and to hopefully serve as a foundation for future research in this topic.

Overall, this has been an exciting and pretty unique project, and I'm interested to hear what people think of this work and any questions you have. Additionally, our group is looking for additional collaborators interested in this topic, so you can reach out or email me if you'd like to discuss more.

Link to Paper: [https://arxiv.org/abs/2505.11764](https://arxiv.org/abs/2505.11764)
X thread: [https://x.com/BAARTMNS/status/1924631071519543750](https://x.com/BAARTMNS/status/1924631071519543750)","['Interesting! I suspect (please correct in case) there is also a connection to language acquisition in children.', 'I wanted to do something like this a few months ago and it completely broke my brain.', 'Interesting. Have you looked at this from a cognitive linguistics perspective too? This seems closely related to certainly Jackendoff’s work in semantics, and maybe Talmy’s. There’s also a rich mathematical structure for building meaning out of basic elements from Goguen, Lakoff, etc. \n\nI haven’t had a chance to read your paper yet (although I’ve skimmed the references), so apologies if I’m talking out of turn! Happy to provide concrete references though if it would help.', 'This is awesome! Haven’t read the paper yet, but I’ve independently observed very similar and am interested!\nI’m experimented with including phonetics and other sensory details inside of embeddings and it’s shown better multilingual behavior (but you need to have tools to discern it from the embedding space). \n\n\nVery cool!', 'To bypass deductive reasoning with an inductive method, someone had to dare...']","['Interesting! I suspect (please correct in case) there is also a connection to language acquisition in children.', 'I wanted to do something like this a few months ago and it completely broke my brain.', 'Interesting. Have you looked at this from a cognitive linguistics perspective too? This seems closely related to certainly Jackendoff’s work in semantics, and maybe Talmy’s. There’s also a rich mathematical structure for building meaning out of basic elements from Goguen, Lakoff, etc. \n\nI haven’t had a chance to read your paper yet (although I’ve skimmed the references), so apologies if I’m talking out of turn! Happy to provide concrete references though if it would help.', 'This is awesome! Haven’t read the paper yet, but I’ve independently observed very similar and am interested!\nI’m experimented with including phonetics and other sensory details inside of embeddings and it’s shown better multilingual behavior (but you need to have tools to discern it from the embedding space). \n\n\nVery cool!', 'To bypass deductive reasoning with an inductive method, someone had to dare...']",21,9,0.89,Research,1750262764.0
1lef4wz,MachineLearning,[D] Why NFL theorem holds even when we average with a fixed f (fixed problem)?,"The text is taken from [here](http://www.no-free-lunch.org/).

>No Free Lunch for Supervised Machine Learning

>Hume (1739–1740) pointed out that ‘even after the observation of the frequent or constant conjunction of objects, we have no reason to draw any inference concerning any object beyond those of which we have had experience’. More recently, and with increasing rigour, Mitchell (1980), Schaffer (1994) and Wolpert (1996) showed that bias-free learning is futile.

>Wolpert (1996) shows that in a noise-free scenario where the loss function is the misclassification rate, if one is interested in off-training-set error, then there are no a priori distinctions between learning algorithms.

>More formally, where
d = training set;
m = number of elements in training set;
f = ‘target’ input-output relationships;
h = hypothesis (the algorithm's guess for f made in response to d); and
C = off-training-set ‘loss’ associated with f and h (‘generalization error’)
all algorithms are equivalent, on average, by any of the following measures of risk: E(C|d), E(C|m), E(C|f,d), or E(C|f,m).

>How well you do is determined by how ‘aligned’ your learning algorithm P(h|d) is with the actual posterior, P(f|d).

>Wolpert's result, in essence, formalizes Hume, extends him and calls the whole of science into question.



Can someone explain how is it possible ""all algorithms are equivalent, on average, by E(*C*|*f*,*d*), or E(*C*|*f*,*m*).""

Correct me if I am wrong, but E(C|f, d) should be interpreted as average all learning algorithms given a fixed dataset and fixed problem (the labeling function f).","[""It is as simple as understanding that for every problem and algorithm there's another problem where the algorithm performance is exactly reversed which is easy to prove by just flipping the labels in the test set. With this you can generalize that over all problems all the algorithms have the same performance"", 'I skimmed over Wolpert’s paper and my understanding is that all expectations are taken either over f (target functions) or phi (restricted family of target function). So, I believe that the correct interpretation is “average a given learning algorithms‘s loss over all labeling functions f, given a fixed dataset”, very much in-line with the vanilla NFL.\n\nsource: [Wolpert (1996), Neural computation](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=43443dea498843ce1b148e7c8c1e64cdf1953ca7)', ""You can't adjust to the test set because you don't see it at training. For any model you train on any dataset there is a test set that will give you any metric you want. That's why averaging over all possible problems all models are the same""]","[""It is as simple as understanding that for every problem and algorithm there's another problem where the algorithm performance is exactly reversed which is easy to prove by just flipping the labels in the test set. With this you can generalize that over all problems all the algorithms have the same performance"", 'I skimmed over Wolpert’s paper and my understanding is that all expectations are taken either over f (target functions) or phi (restricted family of target function). So, I believe that the correct interpretation is “average a given learning algorithms‘s loss over all labeling functions f, given a fixed dataset”, very much in-line with the vanilla NFL.\n\nsource: [Wolpert (1996), Neural computation](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=43443dea498843ce1b148e7c8c1e64cdf1953ca7)', ""You can't adjust to the test set because you don't see it at training. For any model you train on any dataset there is a test set that will give you any metric you want. That's why averaging over all possible problems all models are the same""]",3,7,0.64,Discussion,1750247666.0
1ledktu,MachineLearning,[D] Is there an algorithm to detect community in voting competition - complete directed weighted graph,"I'm looking for a community detection algorithm that can identify groups of people working together (potential collusion) in a competitive voting scenario.

# The Setup:

* Network type: Complete, directed, and weighted graph
* Context: Elimination competition with suspicious voting patterns

# Competition Rules:

* N participants each submit a project
* Every participant ranks ALL other competitors (cannot rank themselves)
* This creates a complete directed graph where edge weights = ranking positions

# What I'm trying to detect:

* Groups of participants who might be coordinating their votes","['There are really a lot of things that you could do, to name a few 1. Make the graph sparse with a rule-based system (if a team voted another team among the top-k two nodes are connected) and identify cliques of different orders. 2. Instead of clique identification, apply an graph community detection (clustering algorithm). 3. Instead of making it sparse that way, treat the rank as distances, get all shortest path matrix, and make the graph sparse with a threshold (for example distance of 2 or maybe 3 could make sense), then identify communities or cliques.', ""import numpy as np\n\n\n\n\\# Assume N participants, and rankings\\[i\\]\\[j\\] is the rank i gave to j (1 = best, N-1 = worst)\n\nN = len(rankings)\n\ninverted\\_weights = np.zeros((N, N))\n\n\n\nfor i in range(N):\n\nfor j in range(N):\n\nif i != j:\n\ninverted\\_weights\\[i\\]\\[j\\] = (N - 1) - (rankings\\[i\\]\\[j\\] - 1)\n\nelse:\n\ninverted\\_weights\\[i\\]\\[j\\] = 0  # No self-voting\n\n\n\n\\# Now inverted\\_weights\\[i\\]\\[j\\] represents i's support for j (higher = stronger support)""]","['There are really a lot of things that you could do, to name a few 1. Make the graph sparse with a rule-based system (if a team voted another team among the top-k two nodes are connected) and identify cliques of different orders. 2. Instead of clique identification, apply an graph community detection (clustering algorithm). 3. Instead of making it sparse that way, treat the rank as distances, get all shortest path matrix, and make the graph sparse with a threshold (for example distance of 2 or maybe 3 could make sense), then identify communities or cliques.', ""import numpy as np\n\n\n\n\\# Assume N participants, and rankings\\[i\\]\\[j\\] is the rank i gave to j (1 = best, N-1 = worst)\n\nN = len(rankings)\n\ninverted\\_weights = np.zeros((N, N))\n\n\n\nfor i in range(N):\n\nfor j in range(N):\n\nif i != j:\n\ninverted\\_weights\\[i\\]\\[j\\] = (N - 1) - (rankings\\[i\\]\\[j\\] - 1)\n\nelse:\n\ninverted\\_weights\\[i\\]\\[j\\] = 0  # No self-voting\n\n\n\n\\# Now inverted\\_weights\\[i\\]\\[j\\] represents i's support for j (higher = stronger support)""]",1,2,0.67,Discussion,1750242267.0
1leac4u,MachineLearning,[N] Mumbai Devs: Hosting a Deep Dive on Real-World AI Voice Agent Engineering in Andheri (June 20th)!,"Hey Mumbai dev folks!

I'm super excited to be organizing a small, in-person meetup right here in Andheri, focused on something I'm really passionate about: building AI Voice Agents that actually work in the real world.

This isn't going to be a surface-level demo. We're diving deep into the nitty-gritty engineering challenges that often make these systems fail in production, beyond just the hype. I'll be walking through what truly matters – speed, user experience, and cost – and sharing insights on how to tackle these hurdles.

We'll cover topics like:
*   How to smash latency across STT, LLM, and TTS
*   What truly makes an AI voice agent interruptible
*   Why WebRTC is often the only transport that makes sense for these systems
*   How even milliseconds can make or break the user experience
*   A practical framework for balancing cost, reliability, and scale in production

This session is designed for fellow engineers, builders, and anyone serious about shipping robust real-time AI voice systems.

The meetup is happening on **June 20th in Andheri, Mumbai**.

It's an intentionally small group to keep discussions focused – just a heads up, there are only about **10 spots left**, and no recordings will be available for this one (it's a no-fluff, in-person session!).

If you're interested and want to grab a seat, please RSVP here: https://lu.ma/z35c7ze0

Hope to see some of you there and share some insights on this complex but fascinating area!","[""interesting to see how you handle reliability, I am in SF so can't really join :( \n\nyou've got a recording or post notes?""]","[""interesting to see how you handle reliability, I am in SF so can't really join :( \n\nyou've got a recording or post notes?""]",0,1,0.36,News,1750229167.0
1le8rxr,MachineLearning,[D] Has anyone deployed any apps in the Healthcare space?,"I’m working on deploying a live-risk prediction system using EHR (electronic health data) and vitals. Curious to know if there are folks who’ve done something similar? How did you manage data reliability?
Thanks in advance !","[""Having worked with EHR data in enterprise settings, I'd say it's concerning that you're coming to Reddit with this question."", ""yeah, ive been doing this for the past three years, currently one app in production for 5 institutions.\n\nFor data reliability we've been working on a set of tools that help doctors/nurses inputting high quality data, so treat the EHR data as some rough starting point, to do some prior research (together with the doctors) on getting a sense of which variables can be used in predictive analytics. You can use data mining tools to get a sense of which mistakes are fixable and which needs manual review (by that patient's doctor!).\n\nThen when you serve the prediction, make sure you explain the impact of the different variables and link to them in the EHR, we also added a feedback button to have doctors comment on whether they think the interpretation is correct and which variables we are missing.\n\nI am currently working on a tool to extract all the aspects from the doctors notes, because the number one comment is that they write about their patients instead of annotating them in like a spreadsheet (ie, the most important information is not structured but free from text).\n\nIf you have any specific questions though, feel free to reach out!"", 'Talk to subject matter experts. Review your data preparation steps with the subject experts (doctors,nurses etc?).', 'hope you know a lot about HIPAA and are building this with direct feedback from actual clinicians, cause if not, you’re in for a very rude awakening and/or may end up killing people.', 'Med tech has loads of regulations that are really hard to meet and chances are that if you’re even asking you don’t meet them.', ""Healthcare AI deployment is honestly one of the hardest environments to work in because of the regulatory complexity and data quality issues. I work at a consulting firm that helps healthcare companies with technology implementations, and EHR data reliability is where most projects fail before they even get to the AI part.\n\nThe fundamental challenge with EHR data is that it's optimized for billing and compliance, not clinical decision-making. You'll hit these problems constantly:\n\nMissing data everywhere. Nurses don't always input vitals consistently, and different departments use different systems that don't talk to each other.\n\nData lag and timing issues. Critical events might not show up in your system for hours or days, which kills real-time risk prediction.\n\nAlert fatigue from clinicians. If your system generates too many false positives, medical staff will ignore it completely.\n\nWhat actually works for our healthcare clients:\n\nStart with data quality assessment before building any models. Most EHR systems have 20-40% missing or inconsistent data that needs cleaning.\n\nFocus on specific, high-value use cases rather than general risk prediction. Sepsis detection, readmission risk, or specific procedure complications work better than broad health scores.\n\nBuild human-in-the-loop workflows from day one. Clinicians need to trust and understand your predictions, not just receive alerts.\n\nPlan for HIPAA compliance, FDA regulatory approval if applicable, and extensive clinical validation. The regulatory overhead is massive.\n\nIntegration with existing clinical workflows is critical. If your system requires extra steps, it won't get adopted.\n\nWhat's your specific clinical focus? Sepsis, cardiac events, readmissions? That affects the data reliability strategies significantly."", 'If it’s human input it’s not going to be reliable, if it’s telemetry it’s a lot better but will have anomalies.\n\nTo improve human input you MUST own the input method, i.e. the intake forms.\nYou have to balance between required info and staff fatigue. Defaults / drop downs / type aheads all help.\n\nNormally you would do gap filling in your data prep, but sometimes the gaps are important, outlier detection is critical.\nThere is no single solution you have to know the data intimately, data governance and classification are a huge help', ""What EHR? If it's Epic, you can containerize your ML pipeline for deployment directly within the EHR with support for real-time data. Really helps from a HIPPA perspective."", 'I’ve managed products in this space! What would you like to know about data reliability?']","[""Having worked with EHR data in enterprise settings, I'd say it's concerning that you're coming to Reddit with this question."", ""yeah, ive been doing this for the past three years, currently one app in production for 5 institutions.\n\nFor data reliability we've been working on a set of tools that help doctors/nurses inputting high quality data, so treat the EHR data as some rough starting point, to do some prior research (together with the doctors) on getting a sense of which variables can be used in predictive analytics. You can use data mining tools to get a sense of which mistakes are fixable and which needs manual review (by that patient's doctor!).\n\nThen when you serve the prediction, make sure you explain the impact of the different variables and link to them in the EHR, we also added a feedback button to have doctors comment on whether they think the interpretation is correct and which variables we are missing.\n\nI am currently working on a tool to extract all the aspects from the doctors notes, because the number one comment is that they write about their patients instead of annotating them in like a spreadsheet (ie, the most important information is not structured but free from text).\n\nIf you have any specific questions though, feel free to reach out!"", 'Talk to subject matter experts. Review your data preparation steps with the subject experts (doctors,nurses etc?).', 'hope you know a lot about HIPAA and are building this with direct feedback from actual clinicians, cause if not, you’re in for a very rude awakening and/or may end up killing people.', 'Med tech has loads of regulations that are really hard to meet and chances are that if you’re even asking you don’t meet them.']",6,10,0.69,Discussion,1750223191.0
