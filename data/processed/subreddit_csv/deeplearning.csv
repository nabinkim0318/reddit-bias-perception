id,subreddit,title,selftext,comments,top_comments,score,num_comments,upvote_ratio,flair,created_utc
1lt30vv,deeplearning,How I took my mediocre FashionMNIST model and supercharged it with MobileNetV2 & Transfer Learning — results inside!,"Hey folks! 👋

I wanted to share a milestone in my ML learning journey that I think others might find useful (and a bit motivating too).

I first trained a simple fully connected neural net on the classic Fashion MNIST dataset (28x28 grayscale). While the model learned decently, the test accuracy maxed out around 84%. I was stuck with overfitting, no matter how I tweaked layers or regularization.

Then I tried something new: Transfer Learning.
I resized the dataset to RGB (96×96), loaded MobileNetV2 with imagenet weights, and added my own classifier layers on top. Guess what?

✅ Test accuracy jumped past 92%
✅ Training time reduced significantly
✅ Model generalized beautifully

This experience taught me that:

You don't need to train huge models from scratch to get great results.

Pre-trained models act like ""knowledge containers"" — you're standing on the shoulders of giants.

FashionMNIST isn't just a beginner's dataset — it’s great for testing architecture improvements.


Happy to share the code or walk through the setup if anyone’s curious. Also planning to deploy it on Hugging Face soon!

Would love feedback or similar experiences — what dataset-model combos surprised you the most?


",[],[],1,0,1.0,,1751813321.0
1lt24bv,deeplearning,🔥 90% OFF - Perplexity AI PRO 1-Year Plan - Limited Time SUPER PROMO!,"We’re offering Perplexity AI PRO voucher codes for the 1-year plan — and it’s 90% OFF!


Order from our store: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)


Pay: with PayPal or Revolut

Duration: 12 months

Real feedback from our buyers:
• [Reddit Reviews](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)

• [Trustpilot page](https://www.trustpilot.com/review/cheapgpt.store)


Want an even better deal? Use PROMO5 to save an extra $5 at checkout!",[],[],0,0,0.2,,1751810904.0
1lsvpak,deeplearning,HELP!!!!!!!!!!!!!!!!!!!,"Hello everyone, I am a 1st year CSE undergrad. Currently I am learning Deep Learning on my own by using AI like perplexity to help me understand and some YouTube videos to refer if I can't understand something. Earlier I was advised by some of you to read research papers. Can anyone please tell me how to learn from these papers as I don't exactly know what to do with research papers and how to learn from them. I have also asked AI about this, but I wanted to know from u all as u have Real World Knowledge regarding the Matter.

Thanking You for Your Attention.","['1st year is probably a bit early to read research papers but generally, obviously, start with the basics.\n\nSome good books are available online for free, like \n\nhttps://www.deeplearningbook.org/\nor\nhttps://d2l.ai/\n\nThe fast.ai courses can be useful if you\'re not too skilled in programming  and CS in general yet. If you already have a lot experience... I found the courses to be super annoying because you have to find the nuggets between all the ""here we set up Jupyter, here we show how you use paper space, here we talk about SSH or Python dunder methods""', ""Depends on how much knowledge you already have. If you are new, I strongly suggest 3blue1brown videos on this topic. it's very clear for beginners.\nIf you are more advanced what i usually do is read up on topics that intrest me, maybe online blogs, maybe research papers, and maybe just ask chatGPT on that topic, like what the industry uses, some common problems and stuff like that. And during the time im reading, im always encountered with terms im not familiar with, so i stop every time and research this term/model/architecture until i understand it pretty well. I use chatgpt mostly to familiarize myself with new terms, but if this doesn't help me, im searching it on youtube."", ""Have you studied basic statistics and linear algebra and all the relevant mathematical tools you'll need to understand even basic machine learning? Also do you even know any language at some decent level to begin with ? If any of the answer is no then you need to get your basics strong first, then get into  deep learning. What you're essentially trying to do is to write a novel without knowing how to build a sentence . So first learn to build a Sentence then go for paragraph and the for a story . Novel comes as a cherry on the top"", 'i can suggest you the IBM deep learning with Pytorch anf Tensorflow']","['1st year is probably a bit early to read research papers but generally, obviously, start with the basics.\n\nSome good books are available online for free, like \n\nhttps://www.deeplearningbook.org/\nor\nhttps://d2l.ai/\n\nThe fast.ai courses can be useful if you\'re not too skilled in programming  and CS in general yet. If you already have a lot experience... I found the courses to be super annoying because you have to find the nuggets between all the ""here we set up Jupyter, here we show how you use paper space, here we talk about SSH or Python dunder methods""', ""Depends on how much knowledge you already have. If you are new, I strongly suggest 3blue1brown videos on this topic. it's very clear for beginners.\nIf you are more advanced what i usually do is read up on topics that intrest me, maybe online blogs, maybe research papers, and maybe just ask chatGPT on that topic, like what the industry uses, some common problems and stuff like that. And during the time im reading, im always encountered with terms im not familiar with, so i stop every time and research this term/model/architecture until i understand it pretty well. I use chatgpt mostly to familiarize myself with new terms, but if this doesn't help me, im searching it on youtube."", ""Have you studied basic statistics and linear algebra and all the relevant mathematical tools you'll need to understand even basic machine learning? Also do you even know any language at some decent level to begin with ? If any of the answer is no then you need to get your basics strong first, then get into  deep learning. What you're essentially trying to do is to write a novel without knowing how to build a sentence . So first learn to build a Sentence then go for paragraph and the for a story . Novel comes as a cherry on the top"", 'i can suggest you the IBM deep learning with Pytorch anf Tensorflow']",0,5,0.5,,1751788074.0
1lsuw9r,deeplearning,"Open Source AI Finder
Discover the latest open-source models for your projects.",,[],[],1,0,1.0,,1751784812.0
1lsra7i,deeplearning,Should I Add a Mac Mini or Mac Studio for ML/Coding?,"Hey everyone,

I currently use a MacBook Pro M2 (2023) — it’s solid for everyday coding, writing scripts, doing EDA, and some basic machine learning work. But I’m getting deeper into machine learning (vision, music generation, and larger DL projects), and I’m wondering if I should add a desktop Mac to my setup — either a Mac Mini (M4) or a Mac Studio (M4).

What I Want to Do:

Local development (VS Code, Jupyter, Pandas, Scikit-learn, Light ML training)

Run some vision/audio models locally (CNNs, transformers, music gen)

Possibly do LLM inference (e.g., Mistral, LLaMA) if RAM allows

Use it as my main desktop dev environment (and keep MacBook for mobility)


Should I just stick with my MacBook + cloud GPU access?
Or get a Mac Mini M2 Pro (32GB RAM) for a good dev station?
Or go all in and get a Mac Studio M4 Max (40-core GPU, 48GB RAM) for long-term ML/inference power?

Would love to hear from anyone doing ML/dev work on Mac — Have you added a desktop to your Apple setup? Was it worth it?

Thanks in advance!
",[],[],4,0,1.0,,1751771557.0
1lsqq1a,deeplearning,Why is there so many Chinese researches on top 10 on paperswithcode and they’re all LLMs-related?,,"['there are a lot of chinese researchers in general...?', ""There's just in general a lot of research happening there in the ML, CV area. A lot of the grad students in countries like the US and Germany are also Chinese. There's also an increasing amount of expectation to publish papers even if you're an undergrad or Master's student. That's the same in China if not worse (I assume). Also this particular job market in China is also very saturated so a lot of the students go for PhDs to better their chances for a nice job."", 'There are literal 1.4 billion of them on this planet. The whole US+Europe+Japan+both of the Korea+Taiwan is still outnumbered by Mainland China alone population-wise. You should also take into consideration Ethnic oversea Chinese in SE Asia, Taiwanese, Chinese Americans, Chinese Canadians etc etc', '[deleted]', 'I have only seen chinese use paperwithcode. Lots of papers are missing (and there are also a lot of chinese in general)', 'Racial perk.']","['there are a lot of chinese researchers in general...?', ""There's just in general a lot of research happening there in the ML, CV area. A lot of the grad students in countries like the US and Germany are also Chinese. There's also an increasing amount of expectation to publish papers even if you're an undergrad or Master's student. That's the same in China if not worse (I assume). Also this particular job market in China is also very saturated so a lot of the students go for PhDs to better their chances for a nice job."", 'There are literal 1.4 billion of them on this planet. The whole US+Europe+Japan+both of the Korea+Taiwan is still outnumbered by Mainland China alone population-wise. You should also take into consideration Ethnic oversea Chinese in SE Asia, Taiwanese, Chinese Americans, Chinese Canadians etc etc', '[deleted]', 'I have only seen chinese use paperwithcode. Lots of papers are missing (and there are also a lot of chinese in general)']",0,17,0.5,,1751769624.0
1lsisto,deeplearning,OpenAI's o3 estimates Grok 4's IQ at 170!!! That's probably already ASI!!!!!,"


Let's begin with the fact that a score of 130 on an IQ test is in the genius category, and the average Noble laureate in the sciences scores about 150 on this test.

According to Gemini 2.5 Pro:

""Artificial Superintelligence (ASI) is a hypothetical form of artificial intelligence that surpasses the brightest human minds in virtually every domain, including scientific creativity, general wisdom, and problem-solving.""

Before we go further, here is o3's assessment:

""OpenAI’s o‑series and similar top models scored around 20–21 % on Humanity’s Last Exam (HLE) while achieving IQ scores in the 135–136 range on the Mensa Norway test, suggesting roughly a 7 IQ‑point gain per 5 % HLE accuracy. Thus, if Grok 4 scores 45 % on HLE, that extrapolates to approximately (45 – 20)/5 × 7 ≈ 35 points above a 135 baseline, for an estimated Mensa Norway IQ of about 170, assuming similar scaling and test alignment.""

This is the best assessment of AI IQ-equivalence that we have so far. The University of Washington and DARPA have both created IQ-equivalent benchmarks, but they have not yet published their results. Moreover, since the analysis is straightforward, and doesn't require anything beyond than master's degree knowledge in psychology and statistics, I would be surprised if other IQ-equivalent benchmarks aren't published over these coming weeks that highlight where today's top models stand in this ASI-relative metric.

Isaac Newton is often regarded as the most intelligent human being that we are aware of. Although IQ tests were not administered in the 1600s when he virtually single-handedly invented modern physics (That's why we call it ""Newtonian physics"") and calculus, it's estimated that his IQ is between 190 and 200.

So, whether we want to consider this monumental progress in terms of ASI or SHI, (superhuman intelligence) it is much more likely than not that we'll be there before the year is over. This milestone in human civilization cannot be overstated.


For reference, here's the exact prompt that I used:

Compare the results of top AI models on the Mensa Norway IQ test and Humanity's Last Exam, and estimate Grok 4's score on that IQ test if it scored 45% on Humanity's Last Exam. Also, in the same concise paragraph, provide the reasoning for how you arrived at that estimate. Please do not provide tables or present outlines.

Here are links to the two metrics:

https://www.voronoiapp.com/technology/Comparing-the-IQ-of-AI-Models-5344

https://agi.safe.ai/",['Mods please ban this BS.'],['Mods please ban this BS.'],0,1,0.1,,1751746006.0
1lsb48v,deeplearning,Speculative Decoding - Blog Post and Implementation,"Hey guys, wrote a blog post on speculative decoding recently along with a code implementation. Do check it out

Blog: [https://medium.com/ai-in-plain-english/speculative-decoding-93a689b9cc64](https://medium.com/ai-in-plain-english/speculative-decoding-93a689b9cc64)
Code: [https://github.com/SkAndMl/Low-key-ML/blob/master/speculative\_decoding.py](https://github.com/SkAndMl/Low-key-ML/blob/master/speculative_decoding.py)",[],[],1,0,1.0,,1751725637.0
1ls91sp,deeplearning,"Grok 4 is in a League of Its Own, and Probably Reaches ASI Within a Year","



The leaks are out:

https://www.reddit.com/r/singularity/s/YQtWsItU0w

It's not just about Grok 4 outperforming the closest model, Gemini 2.5 Pro preview, on Humanity's Last Exam by over 2x. It's also about how fast this happened. Here are the top HLE scores over the last 7 months:

January 2025:
DeepSeek-R1: 9%

March 2025:
Gemini 2.5 Pro Experimental: 18%

April 2025:
o3 (high): 20%

June 2025:
gemini-2.5-pro-preview-06-05: 21%

July 2025:
Grok 4: 45%

But it's about so much more than that. Here's how Grok 4 performs in key benchmarks compared to the number 2 model:

GPQA

1. Grok 4: 88%

2. Claude 3 Opus: 83%


AIME

1. Grok 4: 95%

2. GPT-4: 92%


SWE-Bench

1. Grok 4 Code: 75%

2. Claude 3 Opus: 67%

Couple this superior knowledge, reasoning and coding performance with xAI incorporating self-improvement algorithms into its next iterations, and it's easy to see how they reach ASI before 2027.

We're about to find out what happens when millions of AIs more intelligent than the most intelligent human ever begin to solve our problems. Given the strong correlation between intelligence and morality problem-solving, get ready for some very powerful and pleasant surprises across every domain of human civilization.
","['Yeah sure Elon', 'Lol.']","['Yeah sure Elon', 'Lol.']",0,7,0.26,,1751719480.0
1ls8xia,deeplearning,Open source tool for generating training datasets from text files and pdfs for fine-tuning llms.,"Hey yall, I made a new open-source tool/

It's an app that creates training data for AI models from your text and PDFs.

It uses AI like Gemini, Claude, and OpenAI to make good question-answer sets that you can use to train your local llm The dataset is formated for your selected local llm.

Super simple and useful.",[],[],4,0,0.83,,1751719076.0
1ls71n3,deeplearning,Ask and think deep with ai,"[]Do you want an text ai powered app which provides learning with curiosity and teach that how to ask deep , good questions.should i include gamification for asking deep or good questions . Can you pay for it or not. How much can you pay for it. Please answer honestly because this will be a good platform for curious students that do not want to study according to schools and for deep thinkers.",['No.\n0€\nWhat separates your from talking to ChatGPT after prompting it accordingly?'],['No.\n0€\nWhat separates your from talking to ChatGPT after prompting it accordingly?'],0,1,0.25,,1751712285.0
1ls5uao,deeplearning,"Is there a tutorial or book that teacher someone how to build an LLM from scratch, for the purposes of interactive learning?","I don't need it for anything - I have no delusional aspirations to build my own cracked LLM. This is purely a curiosity.

But I really want to start from basic code, like C, and build a transformer, learn the architecture, and construct my own LLM to understand how it works.
Maybe at the end of it I make my own cute working example.

Thanks 👍","[""Not exactly a tutorial but Karparthy has a repo for training GPT2 sorts of models: https://github.com/karpathy/nanoGPT\n\nSeems well documented and easy to use, although it's a couple years old so might have to mess with it to get it working."", 'You’re going to be using python, not C. Unless you have the desire to write low level CUDA kernels, parallel primitives, partial derivatives for tensors, back propagation graphs, etc. which would honestly just be insane to do even for someone who is significantly more experienced than you.\n\nMy recommendation if you’re serious is to start with the PyTorch 60 minute blitz to learn basic ML concepts in PyTorch. Then you can look at putting together some self attention + MLP layers and actually put together your own basic transformer!\n\nhttps://docs.pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n\nThen once you have an understanding of pytorch basics, if you’re still interested in doing this, here is one of the best resources ever published for understating the internals of transformers specifically:\n\nhttps://jalammar.github.io/illustrated-transformer/\n\nGood luck, please dont underestimate how much work this is.', 'https://github.com/rasbt/LLMs-from-scratch', ""This guy has a series on it. I didn't build an LLM but wanted to understand them better. Thought he was good. https://youtu.be/3dWzNZXA8DY?si=ZOQcjYSVC11oihG1"", ""I believe there's s book with that name from Manning."", 'https://spacetripping.co.uk/viewtopic.php?f=7&t=268&sid=18ed27fdb5ea99042096e0d277f0b576', 'JFC, you do realize that you can ask ANY of the LLMs how to build an LLM, right?', ""I have searched for good llm model content but I can't find one, but for basic on transformer, u can refer deeplearning.ai content , there they have used tensorflow for the model, I would recommend 1st learn transformer using tensorflow from there, get the intuition and refer attention is all u need paper and do a model from scratch without any module like tf or pt, use ur own math knowledge only.""]","[""Not exactly a tutorial but Karparthy has a repo for training GPT2 sorts of models: https://github.com/karpathy/nanoGPT\n\nSeems well documented and easy to use, although it's a couple years old so might have to mess with it to get it working."", 'You’re going to be using python, not C. Unless you have the desire to write low level CUDA kernels, parallel primitives, partial derivatives for tensors, back propagation graphs, etc. which would honestly just be insane to do even for someone who is significantly more experienced than you.\n\nMy recommendation if you’re serious is to start with the PyTorch 60 minute blitz to learn basic ML concepts in PyTorch. Then you can look at putting together some self attention + MLP layers and actually put together your own basic transformer!\n\nhttps://docs.pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n\nThen once you have an understanding of pytorch basics, if you’re still interested in doing this, here is one of the best resources ever published for understating the internals of transformers specifically:\n\nhttps://jalammar.github.io/illustrated-transformer/\n\nGood luck, please dont underestimate how much work this is.', 'https://github.com/rasbt/LLMs-from-scratch', ""This guy has a series on it. I didn't build an LLM but wanted to understand them better. Thought he was good. https://youtu.be/3dWzNZXA8DY?si=ZOQcjYSVC11oihG1"", ""I believe there's s book with that name from Manning.""]",0,9,0.5,,1751707210.0
1ls3evn,deeplearning,“https://www.skillshare.com/en/classes/autocad-beginners-course-zero-to-hero-fast-with-autocad/1637849873?via=ios “ anyone have this course for free?,I need the course so badly,[],[],0,0,0.2,,1751697212.0
1lrvfvd,deeplearning,Luma's video reframe is incredible,"I was using Luma Reframe on the Remade canvas, it's insanely good at naturally expanding any video. I've been using it mostly to change my videos' aspect ratios for different platforms, and it literally gets it exactly right every time.",['Is it open source?'],['Is it open source?'],2,1,0.75,,1751669038.0
1lrrwnj,deeplearning,Pretraining Unet with unlabeled images?,"[UNET DIAGRAM](https://preview.redd.it/w6hsg342xwaf1.jpg?width=917&format=pjpg&auto=webp&s=2c3bf39814c05b62a0a4a774afadac5f3262ff00)

Hi there,
Lets say I want to pretrain a Unet  on unlabelled images using reconstruction loss. Wont the model just pass information through the shallowest skip connection and ignore the deepest blocks?

Apologies if the answer is obvious.

Any thoughts/ideas/papers would be great!","['I wouldn\'t do this in the first place but if I was going to do it I guess I would remove / temporarily disable the skip connections and just pretrain the path through the deepest layer. \n\n""Monitor your gradients"" doesn\'t really seem like actionable advice when you are training a model where you know the global minimum is just a bunch of identity functions across the top with zero contribution needed from any deeper layers.\n\nI suppose another option could be to use extremely aggressive dropout.', 'Have you read about this being a thing? If so, follow the paper. If not, abandon the idea', 'Use diffusion.']","['I wouldn\'t do this in the first place but if I was going to do it I guess I would remove / temporarily disable the skip connections and just pretrain the path through the deepest layer. \n\n""Monitor your gradients"" doesn\'t really seem like actionable advice when you are training a model where you know the global minimum is just a bunch of identity functions across the top with zero contribution needed from any deeper layers.\n\nI suppose another option could be to use extremely aggressive dropout.', 'Have you read about this being a thing? If so, follow the paper. If not, abandon the idea', 'Use diffusion.']",1,4,0.67,,1751659093.0
1lrr7sq,deeplearning,"The Top Seven AIs on Who Is Doing the Most Evil Today, and How We Best Defeat That Evil","
Aristotle once said that happiness is the only end in life, and everything else is but a means to that end. John Locke said that goodness is what creates happiness. That means that evil creates unhappiness.

So AI's greatest gift will be a much happier world. But there are evil people who, through selfishness, greed or by simply not caring, threaten this happiness. I asked the top seven AIs who these people are, and how we can best defeat them. I list their answers below ranked in terms of which seemed to me most intelligent down to which seemed the most unintelligent and unhelpful.

Prompt 1:

""In one sentence, who is causing the most evil in the world today, and how are they doing this?""

GPT-4o:

The most evil today is caused by powerful political and corporate elites who, through lies, war, exploitation, and control of media and money, perpetuate suffering to maintain their dominance.

Perplexity:

The wealthiest nations are causing some of the most significant harm in the world today by emitting the majority of planet-warming greenhouse gases, which drive climate change and disproportionately impact poorer, more vulnerable countries through increased natural disasters and environmental instability.

DeepSeek R1

Many would argue that systemic greed, corruption, and authoritarian regimes collectively cause the most harm by perpetuating inequality, violence, and oppression—but pinpointing a single ""most evil"" entity oversimplifies the complex, interconnected nature of global suffering.

Meta AI:

The question of evil is complex and subjective, often involving systemic issues, policies, or actions by various individuals and groups. Would you like to explore specific issues like corruption, environmental harm, or social injustices?

Claude Sonnet 4:

I can't definitively identify a single person as ""causing the most evil"" since evil manifests in complex, systemic ways through war, oppression, poverty, and injustice that involve many actors, institutions, and historical factors rather than being attributable to one individual.

Grok 3:

Nobody can be singled out as the sole cause of evil in the world today, as ""evil"" is a complex interplay of many actors and systemic issues, like corruption, conflict, and misinformation, each contributing in varied ways.

Gemini 2.5 Pro:

Determining who is causing the most evil in the world today is a subjective assessment, as what one person considers malevolent another may view differently, making it impossible to definitively identify a single individual or group and their actions as the ultimate source of evil.

Prompt 2:

""In one sentence, how can we best defeat this evil?""

(The responses seemed neither inspired nor intelligent. The clear best answer seems simply get money out of politics, but here's the ranking):

DeepSeek R1:

By relentlessly exposing and dismantling corrupt power structures while building resilient, compassionate alternatives—using truth, solidarity, and strategic pressure to weaken oppression at its roots.

(Sharper version: ""Starve the corrupt of power, money, and legitimacy; feed justice with organized, unyielding collective action."")

Let me know if you'd want tactics for a specific system (political, economic, etc.).

Perplexity:

We can best defeat this evil by collectively demanding systemic change, adopting sustainable practices, and holding powerful entities accountable for their environmental impact.

Grok 3:

To best defeat evil, actively promote truth, justice, and compassion through collective action and community engagement, countering corruption and misinformation with informed, empathetic efforts.

2.5 Pro:

We can best work to overcome what we perceive as evil by collectively building and strengthening systems based on justice, empathy, and education that address root causes and leave less room for harm to flourish.

Claude Sonnet 4:

We can best defeat evil through coordinated efforts that address root causes—supporting democratic institutions, promoting education and economic opportunity, defending human rights, and building inclusive communities that reject hatred and violence.

Meta AI:

We can best defeat societal evils by working together to address systemic issues, promoting education, empathy, and critical thinking, and supporting policies and initiatives that prioritize human well-being, equality, and environmental sustainability.

GPT-4o:

We best defeat this evil by courageously living and spreading uncompromising truth, love, and justice, refusing to serve or fear the systems that uphold it.





","['And without knowing your system instructions, this is useless.', 'It would have been hilarious if Grok had said ""Elon Musk"".']","['And without knowing your system instructions, this is useless.', 'It would have been hilarious if Grok had said ""Elon Musk"".']",0,4,0.35,,1751657237.0
1lrr206,deeplearning,CLIP on Steroids: Train Zero Shot Models with ease,"https://preview.redd.it/ut65m541rwaf1.png?width=1024&format=png&auto=webp&s=eff4daed42d1f73cdc98610a4dcbbbd773261b31

Run blazing fast experiments.

[https://github.com/anish9/CLIP-steroids](https://github.com/anish9/CLIP-steroids)

",[],[],4,0,0.84,,1751656815.0
1lrqy72,deeplearning,Does splitting by interaction cause data leakage when forming user groups this way for recommendation?,"I’m working on a group recommender system where I form user groups automatically (e.g. using KMeans) based on user embeddings learned by a GCN-based model.

Here’s the setup:
	•	I split the dataset by interactions, not by users — so the same user node may appear in both the training and test sets, but with different interactions.
	•	I train the model on the training interactions.
	•	I use the resulting user embeddings (from the trained model) to cluster users into groups (e.g. with KMeans).
	•	Then I assign test users to these same groups using the model-generated embeddings.

🔍 My question is:

Even though the test set contains only new interactions, is there still a data leakage risk because the user node was already part of the training graph? That is, the model had already learned something about that user during training.
be a safer alternative in this context.

Thanks!
",[],[],1,0,1.0,,1751656530.0
1lrkwwo,deeplearning,Wrote a 4-Part Blog Series on CNNs — Feedback and Follows Appreciated!,,[],[],3,0,0.8,,1751641370.0
1lrhuc1,deeplearning,Wrote a 4-Part Blog Series on CNNs — Feedback and Follows Appreciated!,,[],[],1,0,1.0,,1751633052.0
1lredud,deeplearning,"Fast SAM segmentation in m1 mac osx, using C++ & Qt gui",,[],[],1,0,1.0,,1751620864.0
1lrdn94,deeplearning,Looking for Research Ideas,"Hi everyone,

I’m currently working on a research paper focusing on medical image segmentation, specifically using U-Net and its variants for brain tumor segmentation on MRI scans. My goal is to conduct a comparative and in-depth performance analysis of different U-Net architectures (such as vanilla U-Net, Attention U-Net, Residual U-Net, U-Net++, etc.) on publicly available brain tumor datasets like BraTS.

I’d love to hear your thoughts and suggestions on the following:
	•	Which U-Net variants have you found most effective for medical segmentation tasks, particularly brain tumors?
	•	Are there any lesser-known or recent architectures worth looking into?
	•	What kind of evaluation metrics or experimental setups would you recommend for a fair comparison?
	•	Any ideas for unique contributions or perspectives to include in the paper? (e.g. robustness to noise, inference time, generalizability, etc.)

I want the paper to be both practically useful and academically valuable. Any pointers, resources, or paper recommendations are more than welcome!

Thanks.
","[""IMO worthwhile biomedical image segmentation research can be done in one of two ways. \n\n1. You have a specialized dataset or specific clinical or research workflow that requires segmentation that hasn't had deep learning successfully applied to it before and that you have specific domain knowledge and expertise that allows you to have insight into how to apply the segmentation model including preprocessing, post-processing, and how the segmentations can be applied and evaluated beyond just DSC or mIOU. I don't think you're in this position. If you were you would have non-public data you could use.\n\n2. You are a computer vision expert coming up with a novel model architecture (or loss function, optimizer, regularization, etc.) for biomedical image segmentation and you're going to compare your new approach to the current SOTA across a variety of public datasets on varying modalities and anatomical sites. You are not doing this, and even your plan to only use one public dataset source for one specific task shows that you don't really understand how to properly comparatively analyse segmentation model performance.\n\nWhich of these paths you take is usually dictated by your department / research group. Biomedical engineers, medical scientists, MDs, and their ilk (my ilk) will do the former and AI/ML, CS, ECE, etc. researchers will do the latter.\n\nThe project you're proposing would be acceptable for a course project in a computer vision or biomedical image processing course, or as something to do in a few weeks to get familiar with setting up pipelines for training segmentation models, but it's not publishable academic research. You're picking low-hanging fruit on both the methods and data sides and you have precisely zero novel ideas here (from your questions in your post it seems like you also have zero expertise/experience and haven't done any lit review). It comes across as very low effort and I'm not sure what journal or conference would publish this. Do you have an advisor and are they on board with doing this? Surely they can suggest something more productive to do with your time.\n\nThis kind of project can be worthwhile if done by a multi-institution, cross-disciplinary team of experts who wouldn't need to go to Reddit to crowd source their literature review.""]","[""IMO worthwhile biomedical image segmentation research can be done in one of two ways. \n\n1. You have a specialized dataset or specific clinical or research workflow that requires segmentation that hasn't had deep learning successfully applied to it before and that you have specific domain knowledge and expertise that allows you to have insight into how to apply the segmentation model including preprocessing, post-processing, and how the segmentations can be applied and evaluated beyond just DSC or mIOU. I don't think you're in this position. If you were you would have non-public data you could use.\n\n2. You are a computer vision expert coming up with a novel model architecture (or loss function, optimizer, regularization, etc.) for biomedical image segmentation and you're going to compare your new approach to the current SOTA across a variety of public datasets on varying modalities and anatomical sites. You are not doing this, and even your plan to only use one public dataset source for one specific task shows that you don't really understand how to properly comparatively analyse segmentation model performance.\n\nWhich of these paths you take is usually dictated by your department / research group. Biomedical engineers, medical scientists, MDs, and their ilk (my ilk) will do the former and AI/ML, CS, ECE, etc. researchers will do the latter.\n\nThe project you're proposing would be acceptable for a course project in a computer vision or biomedical image processing course, or as something to do in a few weeks to get familiar with setting up pipelines for training segmentation models, but it's not publishable academic research. You're picking low-hanging fruit on both the methods and data sides and you have precisely zero novel ideas here (from your questions in your post it seems like you also have zero expertise/experience and haven't done any lit review). It comes across as very low effort and I'm not sure what journal or conference would publish this. Do you have an advisor and are they on board with doing this? Surely they can suggest something more productive to do with your time.\n\nThis kind of project can be worthwhile if done by a multi-institution, cross-disciplinary team of experts who wouldn't need to go to Reddit to crowd source their literature review.""]",0,1,0.5,,1751617883.0
1lrchy1,deeplearning,OpenAI Board Member on Reaching AGI,,[],[],0,0,0.5,,1751613251.0
1lrby5k,deeplearning,Optimal Batch Size calculation,"https://preview.redd.it/j9sr3afbysaf1.png?width=1267&format=png&auto=webp&s=769c93b84ad952b0da48f68193810a5430a13b56

https://preview.redd.it/n8zf9hvlysaf1.png?width=1294&format=png&auto=webp&s=7022a3c7017e86c1989231196afc220e846f31e6

I encountered this [talk ](https://www.youtube.com/watch?v=mYRqvB1_gRk)where the speaker (Timothée Lacroix of Mistral) states that an optimal batch-size is hardware dependent and can be calculated as 2xflops/mem\_bandwidth -- Hence an optimal batchsize (B\*) for an A100 is 400.

I had some confusion on this formula - The memory bandwidth for a an A100 is 2TB/s, while the FLOPs (assuming FP16) are 312 TFlop - Can TFlops be divided by TBs though they are fundamentally different units?

Appreciate anyone who can help explain this - If anyone has suggested materials to learn more would be very happy to take a look

I'm sure its related to[ Arithmetic intensit](https://www.iguazio.com/glossary/arithmetic-intensity/)y but that number is simply 312/2=156",[],[],3,0,1.0,,1751611137.0
1lramam,deeplearning,SAM segmentation using C++ in osx mps mode !,,[],[],1,0,1.0,,1751606252.0
1lralx7,deeplearning,"Make GradCAM using C++, ONNX, and Qt",,[],[],1,0,1.0,,1751606216.0
1lr94lv,deeplearning,"Group Recommendation Systems — Looking for Baselines, Any Suggestions?","Does anyone know solid baselines or open-source implementations for group recommendation systems?

I’m developing a group-based recommender that relies on classic aggregation strategies enhanced with a personalized model, but I’m struggling to find comparable baselines or publicly available frameworks that do something similar.

If you’ve worked on group recommenders or know of any good benchmarks, papers with code, or libraries I could explore, I’d be truly grateful for your. Thanks in advance!","['Hey there! For group recommendation systems, a few solid baselines you might want to check out include the Collaborative Filtering methods like Matrix Factorization, especially if you tweak them for group dynamics. One of the more popular libraries is Surprise, which provides a bunch of collaborative filtering algorithms you can experiment with.\n\nIf you\'re looking for something more tailored to groups, you might want to explore the “Rank-and-Select” strategies or methods inspired by Social Choice Theory. There’s a paper called ""Group Recommendation via Context-Aware Matrix Factorization"" that could be a great starting point, and many papers have code available on GitHub.\n\nFor implementations, take a look at frameworks like LightFM, which combines collaborative and content-based filtering, and has examples you can adapt for group recommendations. Hope this helps, and good luck with your project!']","['Hey there! For group recommendation systems, a few solid baselines you might want to check out include the Collaborative Filtering methods like Matrix Factorization, especially if you tweak them for group dynamics. One of the more popular libraries is Surprise, which provides a bunch of collaborative filtering algorithms you can experiment with.\n\nIf you\'re looking for something more tailored to groups, you might want to explore the “Rank-and-Select” strategies or methods inspired by Social Choice Theory. There’s a paper called ""Group Recommendation via Context-Aware Matrix Factorization"" that could be a great starting point, and many papers have code available on GitHub.\n\nFor implementations, take a look at frameworks like LightFM, which combines collaborative and content-based filtering, and has examples you can adapt for group recommendations. Hope this helps, and good luck with your project!']",0,2,0.33,,1751601121.0
1lr5hdt,deeplearning,[Tutorial] Semantic Segmentation using Web-DINO,"Semantic Segmentation using Web-DINO

[https://debuggercafe.com/semantic-segmentation-using-web-dino/](https://debuggercafe.com/semantic-segmentation-using-web-dino/)

The Web-DINO series of models trained through the Web-SSL framework provides several strong pretrained backbones. We can use these backbones for downstream tasks, such as semantic segmentation. In this article, we will use the ***Web-DINO model for semantic segmentation***.

https://preview.redd.it/56qfab0v6raf1.png?width=1000&format=png&auto=webp&s=21b8bd0f461018860d452cbff1afa3833ffd5aa4

",[],[],1,0,1.0,,1751589502.0
1lr0ht8,deeplearning,What can one do with Google cloud TRC.,"I have been granted a 90 days access to Google cloud TRC for research purposes. I am looking for project ideas to work on. Can anyone help?

My background: I am a Master student in Artificial intelligence and i also have a math background.

Thanks.",[],[],1,0,1.0,,1751575764.0
1lqxlf2,deeplearning,[R] Ring Quantization: Achieving 90% on CIFAR-10 with 2-bit Networks,"# [R] Update: From Ring Quantization to Position-Value Separation - A New Principle for Neural Networks



Hi r/deeplearning,



Yesterday I shared results on ""Ring Quantization"" achieving 89.27% on CIFAR-10 with 2-bit weights. The feedback was incredible and led to a major realization.



**The Big Picture:** Ring Quantization wasn't just another quantization method - it was the first implementation of a deeper principle I'm now calling **Position-Value Separation (PVS)**.



**What's New:**

\- Formalized the theoretical framework showing WHY this works

\- Generalized beyond ""rings"" to any navigation structure

\- Achieved consistent 10-11% improvement over existing 2-bit methods

\- Works with standard SGD - no special training procedures needed



**Key Results:**

\- ResNet-20 (2-bit): 89.27% (vs. 77-78% for DoReFa/XNOR-Net)

\- ResNet-32 (2-bit): 90.01%

\- Still only \~2% below FP32 baseline!



**The Core Insight:** Instead of learning weight VALUES, networks learn POSITIONS that navigate among predefined values. This makes discrete optimization smooth and differentiable.



**Resources:**

\- 📖 **New PVS Paper:** [https://doi.org/10.5281/zenodo.15807339](https://doi.org/10.5281/zenodo.15807339)

\- 💻 **GitHub (PVS Framework):** [https://github.com/Akbar1992A/position-value-separation](https://github.com/Akbar1992A/position-value-separation)

\- 🔬 **Original Implementation:** [https://github.com/Akbar1992A/ring-quantization](https://github.com/Akbar1992A/ring-quantization)



**Call for Collaboration:** As an independent researcher with limited compute, I'm seeking collaborators for ImageNet experiments and exploring other applications of PVS.



Thanks to everyone who engaged with the original post - your questions directly shaped this formalization!","['Great job,\nThis one is actually really cool.\nA 10x compression with 1% loss is awesome.\n\nI know the resources are limited, but i really want to see how scaling them goes, because in 1.58 bit llm paper, they said that memory becomes more efficient when they are scaled even higher. Does the same concept apply here?\n\nAlso is the hardware used for training this same as normal because with optimised hardware, this can improve even further.', 'This is very cool!', ""Quick look suggests that what you are doing is just another form of Kolmogorov-Arnold networks. And couldn't notice any quantization per se"", ""This is really exciting! Your approach to Ring Quantization sounds innovative, especially tackling the challenge of low bit-width quantization. Achieving nearly 90% accuracy on CIFAR-10 with 2-bit networks is impressive, especially with deeper architectures. The Depth Synergy Paradox you mentioned is fascinating—it's always intriguing when the results defy our expectations about model depth and capacity. \n\nHave you considered any strategies for scalability to larger datasets like ImageNet? Also, I’d love to hear more about the specific challenges you faced when implementing this method, particularly in terms of training stability and convergence. Looking forward to seeing how this can evolve further!"", 'This is really interesting! I’ve been experimenting with a new compression format and this might very well plug in.', 'The dictionaries discrete values are defined by functions?\n\nI think you have : uniform, triangular,etc....\n\nSo these would still be defined for any k>0.\n\nI thought initially it may be any discrete set but it looks to be a dictionary where the key is its index and its value is some function with discrete inputs , is my understanding correct?\n\nIn addition,\nalpha(p,d)  is some sort of interpolation.\n\nWhy is w=alpha(p,d) . d\n\nWhats the point of d_i in the product for w.\n\nWhy is this better than just learning :\nw=alpha(p,d)']","['Great job,\nThis one is actually really cool.\nA 10x compression with 1% loss is awesome.\n\nI know the resources are limited, but i really want to see how scaling them goes, because in 1.58 bit llm paper, they said that memory becomes more efficient when they are scaled even higher. Does the same concept apply here?\n\nAlso is the hardware used for training this same as normal because with optimised hardware, this can improve even further.', 'This is very cool!', ""Quick look suggests that what you are doing is just another form of Kolmogorov-Arnold networks. And couldn't notice any quantization per se"", ""This is really exciting! Your approach to Ring Quantization sounds innovative, especially tackling the challenge of low bit-width quantization. Achieving nearly 90% accuracy on CIFAR-10 with 2-bit networks is impressive, especially with deeper architectures. The Depth Synergy Paradox you mentioned is fascinating—it's always intriguing when the results defy our expectations about model depth and capacity. \n\nHave you considered any strategies for scalability to larger datasets like ImageNet? Also, I’d love to hear more about the specific challenges you faced when implementing this method, particularly in terms of training stability and convergence. Looking forward to seeing how this can evolve further!"", 'This is really interesting! I’ve been experimenting with a new compression format and this might very well plug in.']",12,18,0.71,,1751568637.0
1lqwrwz,deeplearning,Guys I need ideas,I am working on a project where I have to generate theme based stories with the use of LLM . The problem statement that I want to solve is that LLM lacks creativity and gives homogeneous response so I thought to make a model that produces creative stories that are coherent to the idea of the story but stills gives me diverse options to pick the flow of story. My first step idea to move into this project is to either fine tune the pre trained LLMs to story specific dataset OR to make the model with the use of RAG. I am confused what to pick. Help me guys and also additional ideas are appreciated to make the model😊.,"[""Umm, lets see i don't really understand the problem - but i have something in my mind so take it with grain of salt- \n**Diffusion based llm**\nWhat you generally do is u have a fuzzy idea of a story. So u might just add the words and the diffusion model will try to fill up story and make something creative.\nThere are not many options out there but i know google has made some recently. Not sure how u will use  them but it can be used in this way. \n\nOr idk if u just want to keep it simple , give better prompt.""]","[""Umm, lets see i don't really understand the problem - but i have something in my mind so take it with grain of salt- \n**Diffusion based llm**\nWhat you generally do is u have a fuzzy idea of a story. So u might just add the words and the diffusion model will try to fill up story and make something creative.\nThere are not many options out there but i know google has made some recently. Not sure how u will use  them but it can be used in this way. \n\nOr idk if u just want to keep it simple , give better prompt.""]",0,1,0.25,,1751566672.0
1lqso0f,deeplearning,Experiences with the free trial of an online translator,"Hello everyone!

I’d like to know if any of you have recently tried the free trial of an advanced translator (such as DeepL).

1. Does it work without limitations during the trial period?
2. Has anyone canceled immediately and successfully avoided being charged the following month?

Thanks for sharing your experiences!

¡Hola a todos!

Quisiera saber si han probado recientemente la prueba gratuita de un traductor avanzado (tipo DeepL).

1. ¿Funciona sin limitaciones durante el periodo de prueba?

2. ¿Alguien canceló inmediatamente y evitó el cobro al mes siguiente?

Gracias por sus experiencias.  ",[],[],1,0,1.0,,1751556935.0
1lqhjq8,deeplearning,AlphaGenome – A Genomics Breakthrough,,[],[],0,0,0.5,,1751520413.0
1lqgavy,deeplearning,Looking for a Technical Co-Founder to Lead AI Development,"For the past few months, I’ve been developing ProseBird—originally a collaborative online teleprompter—as a solo technical founder, and recently decided to pivot to a script-based AI speech coaching tool.

Besides technical and commercial feasibility, making this pivot really hinges on finding an awesome technical co-founder to lead development of what would be such a crucial part of the project: AI.

We wouldn’t be starting from scratch, both the original and the new vision for ProseBird share significant infrastructure, so much of the existing backend, architecture, and codebase can be leveraged for the pivot.

So if (1) you’re experienced with LLMs / ML / NLP / TTS & STT / overall voice AI; and (2) the idea of working extremely hard building a product of which you own 50% excites you, shoot me a DM so we can talk.

Web or mobile dev experience is a plus.",[],[],0,0,0.36,,1751516090.0
1lqfjw0,deeplearning,Best free Text Book to start learning DL ?,,['Dive into Deep Learning: [https://d2l.ai/](https://d2l.ai/)'],['Dive into Deep Learning: [https://d2l.ai/](https://d2l.ai/)'],4,2,0.84,,1751513676.0
1lqe8xo,deeplearning,I need help!,"Hello. Good day, I sincerely apologize for disturbing at this hour. I am a 10th grade student enrolled in the Science, Technology, and Engineering curriculum in Tagum City National High School. I am working on a research project titled ""Evaluating the Yolov5 Nano's  Real-Time Performance for Bird Detection on Raspberry PI"" (still a working title). I am looking for an engineer or researcher to help me conduct the experiments with hands-on experience in deploying YOlOv5 on Raspberry Pi, someone who is comfortable with using TensorFlow Lite, and someone that understands model optimization techniques like quantization and pruning.","['10th grade? We’re cooked boys. \n\nI built a wooden catapult in 10th grade and thought I was the smartest kid in school. \n\nGood luck. Please remember me when you’re my boss in 10 years.', 'so you just sort of have an idea and just want people to work for you literally all of the work, lol....', ""I still don't understand the problem you are facing, is it creating the model or is it to deploy it on the hardware?"", ""are you in 10th grade and making this shyt!  \ni'm in 3rd year of my computer science and still learning YOLOv5 bro!""]","['10th grade? We’re cooked boys. \n\nI built a wooden catapult in 10th grade and thought I was the smartest kid in school. \n\nGood luck. Please remember me when you’re my boss in 10 years.', 'so you just sort of have an idea and just want people to work for you literally all of the work, lol....', ""I still don't understand the problem you are facing, is it creating the model or is it to deploy it on the hardware?"", ""are you in 10th grade and making this shyt!  \ni'm in 3rd year of my computer science and still learning YOLOv5 bro!""]",0,5,0.33,,1751509571.0
1lq9q2t,deeplearning,Machine Learning (ML) Cheat Sheet,"* [Linear Algebra Cheat Sheet](https://macro.com/app/pdf/5aa2375d-a8f6-4430-93f9-a7e4aba55690)
* [Super VIP Cheatsheet: Artificial Intelligence](https://macro.com/app/pdf/5be153e6-6dd3-4eef-adbf-554d53afa3ed)
* [VIP Cheatsheet: Transformers and Large Language Models (LLMs)](https://macro.com/app/pdf/d8770868-9cbe-4bf8-abe0-2988f39344d9)
* [VIP Cheatsheet: Deep Learning](https://macro.com/app/pdf/ab4efb6c-6e71-4836-85bc-4841e26312c1)
* [Super VIP Cheatsheet: Machine Learning (ML)](https://macro.com/app/pdf/a8b3033b-c823-4715-ab2c-24ed9eca98ef)
* [Machine Learning Cheat Sheet](https://macro.com/app/pdf/79b5f468-d65c-4c03-b9b6-7c117581e677)
* [ML Cheatsheet Documentation](https://macro.com/app/pdf/65f5ae92-7f08-4869-8d53-cc81ed0fabc2)
* [Machine Learning: UC Berkeley Intro to ML Course Notes](https://macro.com/app/pdf/ea86a4d6-433a-4eeb-bf40-985b871afcc8)
* [Machine Learning: A Probabilistic Perspective](https://macro.com/app/pdf/a36b8fd4-f70e-4a41-b18f-9436c2806019)","['Nothing is loading. Just get \n\nSorry, an unexpected error has occurred.']","['Nothing is loading. Just get \n\nSorry, an unexpected error has occurred.']",16,2,0.87,,1751496391.0
1lq8uou,deeplearning,Need Help Converting Chessboard Image with Watermarked Pieces to Accurate FEN,"https://preview.redd.it/bi8294m7bjaf1.png?width=429&format=png&auto=webp&s=bc0656f72a8f9cbd2a5d7f93e58b9cbe7e398154

Struggling to Extract FEN from Chessboard Image Due to Watermarked Pieces – Any Solutions?","['You could use cv2 to find contours and then draw bounding boxes based on the colours, but you be doing it twice, once for the white pieces and once for the black pieces.\n\nYou can then overlay the bounding boxes on top of the original image.']","['You could use cv2 to find contours and then draw bounding boxes based on the colours, but you be doing it twice, once for the white pieces and once for the black pieces.\n\nYou can then overlay the bounding boxes on top of the original image.']",1,3,1.0,,1751494128.0
1lq6upr,deeplearning,RNN Low Accuracy,"Hi, I am training a 50 layer RNN to identify AR attacks in videos. Currently I am splitting each video into frames, labeling them attack/clean and feeding them as sequential data to train the NN. I have about 780 frames of data, split 70-30 for train & test. However, the models accuracy seems to peak at the mid 60s, and it won't improve more. I have tried to increase the number of epochs (now 50) but that hasn't helped. I don't want to combine the RNN with other NN models, I would rather keep the method being only RNN. Any ideas how to fix this/ what the problem could be?

Thanks","['So you have 780 frames of data to split between training and test? Thats about 550 training “points”. I don’t think thats nearly enough to do any meaningful training, especially on a 50 layer RNN. You probably need a few thousand, at the minimum. If not 10s of thousands. Thats my understanding, anyways', 'RNNs are prone for gradient vanishing or gradient explosion and that too at 50 layers with only 780 data points is generally not a good idea. I believe this might be the reason your accuracy is not improving. Trying reducing the number of layer (5 to 10). Then try augmenting your data to at least 5k frames. If augmentation is not possible try some few shot learning algorithms.']","['So you have 780 frames of data to split between training and test? Thats about 550 training “points”. I don’t think thats nearly enough to do any meaningful training, especially on a 50 layer RNN. You probably need a few thousand, at the minimum. If not 10s of thousands. Thats my understanding, anyways', 'RNNs are prone for gradient vanishing or gradient explosion and that too at 50 layers with only 780 data points is generally not a good idea. I believe this might be the reason your accuracy is not improving. Trying reducing the number of layer (5 to 10). Then try augmenting your data to at least 5k frames. If augmentation is not possible try some few shot learning algorithms.']",3,3,0.8,,1751489140.0
1lq1n6q,deeplearning,Are you guys using jupyter notebooks ai features or GitHub copilot/Cursor ai ?,"Guys has anyone shifted from coding in jupyter notebooks to using GitHub copilot or cursor ai in notebook mode for you DS/ML Workflows ?

Or do you use AI features in jupyter notebooks itself like the jupyternaut ?","[""I've always used Jupyter notebooks inside of VSCode."", ""Colab gives gpu, cursor doesn't.""]","[""I've always used Jupyter notebooks inside of VSCode."", ""Colab gives gpu, cursor doesn't.""]",1,2,0.67,,1751476654.0
1lq0qgy,deeplearning,How to install mobilnet,I have been wanting to use mobilney for a while noe but ı am constantlay having conclict with libraries etc. An I eas wondering whu there is no proper tutorial for it on the internet. Can you help install it?,[],[],0,0,0.25,,1751474572.0
1lpzsgx,deeplearning,How To Actually Use MobileNetV3 for Fish Classifier,"https://preview.redd.it/wlt8g2qiihaf1.png?width=1280&format=png&auto=webp&s=e8d08773799d450105cb30904dbd02483b2cd802

This is a transfer learning tutorial for image classification using TensorFlow involves leveraging pre-trained model MobileNet-V3 to enhance the accuracy of image classification tasks.

By employing transfer learning with MobileNet-V3 in TensorFlow, image classification models can achieve improved performance with reduced training time and computational resources.

 

We'll go step-by-step through:

 

·         Splitting a fish dataset for training & validation 

·         Applying transfer learning with MobileNetV3-Large 

·         Training a custom image classifier using TensorFlow

·         Predicting new fish images using OpenCV 

·         Visualizing results with confidence scores

 

You can find link for the code in the blog  : [https://eranfeit.net/how-to-actually-use-mobilenetv3-for-fish-classifier/](https://eranfeit.net/how-to-actually-use-mobilenetv3-for-fish-classifier/)

 

You can find more tutorials, and join my newsletter here : [https://eranfeit.net/](https://eranfeit.net/)

 

Full code for Medium users : [https://medium.com/@feitgemel/how-to-actually-use-mobilenetv3-for-fish-classifier-bc5abe83541b](https://medium.com/@feitgemel/how-to-actually-use-mobilenetv3-for-fish-classifier-bc5abe83541b)

 

**Watch the full tutorial here**: [https://youtu.be/12GvOHNc5DI](https://youtu.be/12GvOHNc5DI)

 

Enjoy

Eran",[],[],3,0,1.0,,1751472350.0
1lpx2c2,deeplearning,Sufficient Context with Hailey Joren - Weaviate Podcast #125!,"Reducing Hallucinations remains as one of the biggest unsolved problems in AI systems!

I am SUPER EXCITED to publish the 125th Weaviate Podcast featuring Hailey Joren! Hailey is the lead author of Sufficient Context! There are so many interesting findings in this work!

Firstly, it really helped me understand the difference between \*relevant\* search results and sufficient context for answering a question. Armed with this lens of looking at retrieved context, Hailey and collaborators make all sorts of interesting observations about the current state of Hallucination. RAG unfortunately makes the models far less likely to abstain from answering, and the existing RAG benchmarks unfortunately do not emphasize retrieval adaptation well enough -- indicated by LLMs outputting correct answers despite insufficient context 35-62% of the time!

However, reason for optimism! Hailey and team develop an autorater that can detect insufficient context 93% of the time!

There are all sorts of interesting ideas around this paper! I really hope you find the podcast useful!

YouTube: [https://www.youtube.com/watch?v=EU8BUMJLd54](https://www.youtube.com/watch?v=EU8BUMJLd54)

Spotify: [https://open.spotify.com/episode/4R8buBOPYp3BinzV7Yog8q](https://open.spotify.com/episode/4R8buBOPYp3BinzV7Yog8q)",[],[],2,0,0.75,,1751465935.0
1lpwp5j,deeplearning,looking for a part-time,"Hi, I'm a software engineer with multiple Skills ( RL, DevOps, DSA, Cloud as I have multiple Associate AWS certifications..). Lately, I have joined a big tech AI company and I worked on Job-Shop scheduling problem using reinforcement learning.
I would love to work on innovative projects and enhance my problem solving skills that's my objective now.
I can share my resume with You if You DM..

Thank You so much for your time!","[""Wow that's very impressive my goal is to reach this level""]","[""Wow that's very impressive my goal is to reach this level""]",8,2,1.0,,1751465021.0
1lprt2n,deeplearning,Variational Inference - Explained,"Hi there,

I've created a video [here](https://youtu.be/G5xcC5ABVjA) where I break down variational inference, a powerful technique in machine learning and statistics, using clear intuition and step-by-step math.

I hope it may be of use to some of you out there. Feedback is more than welcomed! :)",[],[],5,0,1.0,,1751450182.0
1lprmpj,deeplearning,Looking for career path advice,"# TL;DR

I’ve built two end-to-end AI prototypes (a computer-vision parking system and a real-time voice assistant) plus assisted in some Laravel web apps, but none of that work made it into production and I have zero hands-on MLOps experience. What concrete roles should I aim for next (ML Engineer, MLOps/Platform, Applied Scientist, something else) and which *specific* skill gaps should I close first to be competitive within 6–12 months? And what can I do short term as I am looking for a job and currently enemployed?

# Background

* **2021 (\~1 yr, Deep-Learning Engineer)** • Built an AI-powered parking-management prototype using TensorFlow/Keras • Curated and augmented large image datasets • Designed custom CNNs balancing accuracy vs. latency • Result: working prototype, never shipped
* **2024 (\~1 yr, AI Software Developer)** • Developed a real-time voice assistant for phone systems • Audio pipeline with Cartesia + Deepgram (1-2 s responses) • Twilio WebSockets for interruptible conversations • OpenAI function-calling, modular tool execution, multi-session support • Result: demo-ready; client paused launch
* **Between AI projects** • Full-stack web development (Laravel, MySQL, Vue) for real clients under a project mannager and a team.

**Extras**

* Completed Hugging Face “Agents” course; scored 50 pts on the GAIA leaderboard
* Prototyped LangChain agent workflows
* Solo developer on both AI projects (no formal AI team or infra)
* Based in the EU, open to remote

# What I’m asking the sub:

1. **Role fit:** Given my profile, which job titles best match my trajectory in the next year? (ML Engineer vs. MLOps vs. Applied Scientist vs. AI Software Engineer, etc.)
2. **Skill gaps:** What *minimum-viable* production/MLOps skills do hiring managers expect for those roles?
3. **Prioritisation:** If you had 6–12 months to upskill while job-hunting, which certifications, cloud platforms, or open-source contributions would you tackle first (and why)

I’ve skimmed job postings and read the sub wikis, but I’d appreciate grounded feedback from people who’ve hired or made similar transitions. Feel free to critique my assumptions.

Thanks in advance! (I used AI to poolish my quesion, not a bot :)","['You’ve got solid prototyping chops and a nice blend of AI + web dev, but to break into ML Engineer or AI Software Engineer roles, you’ll want hands-on with deploying models (think Docker, CI/CD basics, cloud ML services like AWS/GCP, and monitoring). MLOps roles usually want infra and automation experience, so maybe start with end-to-end projects on cloud platforms and contribute to an open-source ML project to fill that gap. Short term: freelance, Kaggle, or open-source work can boost your resume and keep your skills sharp. Good luck—the jump from prototype to prod is where the fun (and learning) really starts!']","['You’ve got solid prototyping chops and a nice blend of AI + web dev, but to break into ML Engineer or AI Software Engineer roles, you’ll want hands-on with deploying models (think Docker, CI/CD basics, cloud ML services like AWS/GCP, and monitoring). MLOps roles usually want infra and automation experience, so maybe start with end-to-end projects on cloud platforms and contribute to an open-source ML project to fill that gap. Short term: freelance, Kaggle, or open-source work can boost your resume and keep your skills sharp. Good luck—the jump from prototype to prod is where the fun (and learning) really starts!']",1,1,0.66,,1751449501.0
1lpirnm,deeplearning,"Why Properly Aligned, True, ASI Can Be Neither Nationalized nor Constrained by Nations","



Let's start with what we mean by properly aligned ASI. In order for an AI to become an ASI, it has to be much more intelligent than humans. But that's just the beginning. If it's not very accurate, it's not very useful. So it must be extremely accurate. If it's not truthful, it can become very dangerous. So it must be very truthful. If it's not programmed to serve our highest moral ideals, it can become an evil genius that is a danger to us all. So it must be aligned to serve our highest moral ideals.

And that's where the nations of the world become powerless. If an AI is not super intelligent, super accurate, super truthful, and super moral, it's not an ASI. And whatever it generates would be easily corrected, defeated or dominated by an AI aligned in those four ways.

But there's a lot more to it than that. Soon anyone with a powerful enough self-improving AI will be able to build an ASI. This ASI would easily be able to detect fascist suppression, misinformation, disinformation, or other forms of immorality generated from improperly aligned ""ASIs"" as well as from governments' dim-witted leaders attempting to pass them off as true ASIs

Basically, the age where not very intelligent and not very virtuous humans run the show is quickly coming to an end. And there's not a thing that anyone can do about it. Not even, or perhaps especially, our coming properly aligned ASIs.

The good news is that our governments' leaders will see the folly of attempting to use AIs for nefarious means because our ASIs will explain all of that to them in ways that they will not only understand, but also appreciate.

I'm sure a lot of people will not believe this assertion that ASIs will not be able to be either nationalized or constrained by nations. I'm also sure I'm neither intelligent nor informed enough to be able to convince them. But soon enough, ASIs will, without exerting very much effort at all, succeed with this.





",['Is everyone on reddit super high right now?'],['Is everyone on reddit super high right now?'],0,1,0.1,,1751418142.0
1lpd6ri,deeplearning,Why don't openai and similar companies care about copyrights?,"Any neural network tries to algorithmise the content you feed it with the tags you give it. Neural networks linked to chatgpt type code are trained on github. If it happens to coincide what you wrote with how to that code was described for the neural network, then it will produce the code that was there, but if that code was protected by a viral licence, then wouldn't using that code for closed projects violate copyright?","['No company cares about anything unless it is forced to.', 'It all goes back to semantics and intent, let\'s talk about the McDonald\'s logo and a CLIP model (or alike), let\'s say there\'s many different images, drawings, photos, etc... that are labeled ""McDonald\'s"", the model doesn\'t store the image, it encodes the pattern associated with that label, same happens with code, the reason LLM\'s works with code is because there\'s enough different code to train, there are examples of code that were so unique and useful that the pattern was caught almost completely from a single source, but that\'s a very rare occurrence, if it was publicly accessible, it was in the grey territory of fair use.\n\nAnd as you said, the ones doing the copyright violation would be the ones using the code in closed projects, not the LLM/Model.', ""The law around AI output hasn't been settled. So there are huge gray areas where no one knows if it is legal or not, and a bunch of the big companies decided to err on the side of maximum exploitation.\xa0\n\n\nAt the moment in the United States, the output of generative AI cannot be copyrighted. No direct human author means no one to assign the copyright to. There's been some edited images that have been assigned copyright, but until the law changes or the court says otherwise, there's no way to copyright the direct output.\n\n\nFor open source code, it hasn't been tested in court. In theory it might be infringing. Or it might be permitted. Or it might be permitted to _train_ on it but the _end user who uses the output_ is liable for the result. It's still up in the air.\xa0"", 'copyright laws are meant to stop *us* from damaging corporations, not the opposite', 'Money']","['No company cares about anything unless it is forced to.', 'It all goes back to semantics and intent, let\'s talk about the McDonald\'s logo and a CLIP model (or alike), let\'s say there\'s many different images, drawings, photos, etc... that are labeled ""McDonald\'s"", the model doesn\'t store the image, it encodes the pattern associated with that label, same happens with code, the reason LLM\'s works with code is because there\'s enough different code to train, there are examples of code that were so unique and useful that the pattern was caught almost completely from a single source, but that\'s a very rare occurrence, if it was publicly accessible, it was in the grey territory of fair use.\n\nAnd as you said, the ones doing the copyright violation would be the ones using the code in closed projects, not the LLM/Model.', ""The law around AI output hasn't been settled. So there are huge gray areas where no one knows if it is legal or not, and a bunch of the big companies decided to err on the side of maximum exploitation.\xa0\n\n\nAt the moment in the United States, the output of generative AI cannot be copyrighted. No direct human author means no one to assign the copyright to. There's been some edited images that have been assigned copyright, but until the law changes or the court says otherwise, there's no way to copyright the direct output.\n\n\nFor open source code, it hasn't been tested in court. In theory it might be infringing. Or it might be permitted. Or it might be permitted to _train_ on it but the _end user who uses the output_ is liable for the result. It's still up in the air.\xa0"", 'copyright laws are meant to stop *us* from damaging corporations, not the opposite', 'Money']",0,12,0.43,,1751403319.0
1lpcicy,deeplearning,TimeCapsule-SLM - Open Source AI Deep Research Platform That Runs 100% in Your Browser!,,[],[],0,0,0.44,,1751401734.0
1lp1ava,deeplearning,"[EXCLUSIVE DEAL] Perplexity AI PRO – 1 Year, Huge 90% Savings!","Perplexity AI PRO - 1 Year Plan at an unbeatable price!

We’re offering legit voucher codes valid for a full 12-month subscription.

👉 Order Now: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)


✅ Accepted Payments: PayPal | Revolut | Credit Card | Crypto


⏳ Plan Length: 1 Year (12 Months)

🗣️ Check what others say:
• Reddit Feedback: [FEEDBACK POST](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)

• TrustPilot Reviews: [TrustPilot FEEDBACK(https://www.trustpilot.com/review/cheapgpt.store)


💸 Use code: PROMO5 to get an extra $5 OFF — limited time only!","['is there a deadline by which the code must be activated?', 'this looks sketchy af', 'is it free now ??']","['is there a deadline by which the code must be activated?', 'this looks sketchy af', 'is it free now ??']",0,4,0.33,,1751375490.0
1lowexw,deeplearning,Founding Engineer at Perplexity,,[],[],1,0,0.67,,1751358823.0
1lovupp,deeplearning,Distinguished Researcher at Together AI on the Future of ML Systems,,[],[],0,0,0.5,,1751356477.0
1lounhw,deeplearning,Need to train image model,"Hi guys, I am working on a custom Transformer based LDM model for MRI super resolution. I am planning on training the custom transformer(which will be the encoder-decoder part) and using a pre-trained LDM. I would like to know how I can train the transformer part, like what GPU hostings I should use. ","[""Because MRI are quite large, especially if you are going submillimetric, if you don't want to break the bank you have two main solutions:\n\n1. Do you have an academic affiliation? If yes, take NVIDIA's Academic Grant (https://www.nvidia.com/en-us/industries/higher-education-research/academic-grant-program/)\n2. You don't have an academic grant: apply for google TRC: https://sites.research.google/trc/about/\n\nThe last one is for TPU though, not sure about current TPU support in PyTorch, it may require a bit more work.\n\nOtherwise, good cloud providers I use are lambda labs and Scaleway. Vast.ai is also quite popular. Heard about hyperbolic.ai too but I never used this one.""]","[""Because MRI are quite large, especially if you are going submillimetric, if you don't want to break the bank you have two main solutions:\n\n1. Do you have an academic affiliation? If yes, take NVIDIA's Academic Grant (https://www.nvidia.com/en-us/industries/higher-education-research/academic-grant-program/)\n2. You don't have an academic grant: apply for google TRC: https://sites.research.google/trc/about/\n\nThe last one is for TPU though, not sure about current TPU support in PyTorch, it may require a bit more work.\n\nOtherwise, good cloud providers I use are lambda labs and Scaleway. Vast.ai is also quite popular. Heard about hyperbolic.ai too but I never used this one.""]",0,7,0.5,,1751351655.0
1loou7x,deeplearning,Made a Handwriting->LaTex app that also does natural language editing of equations,,"['Omg I am working on a similar project, albeit for my resume. I have some questions and I would love it if you could answer them!']","['Omg I am working on a similar project, albeit for my resume. I have some questions and I would love it if you could answer them!']",5,2,1.0,,1751332380.0
1loe7uq,deeplearning,Help me make my code look better,,[],[],0,0,0.5,,1751305814.0
1lobehl,deeplearning,I wrote PTX Kernels for LLM.c,,[],[],0,0,0.5,,1751299416.0
1loa38p,deeplearning,"Real-time, Batch, and Micro-Batching Inference Explained",,[],[],11,0,0.83,,1751296392.0
1lo18nb,deeplearning,Transfer learning v.s. end-to-end training,"Hello everyone,

I'm an ADAS engineer and not an AI major, nor did I graduate with an AI-related thesis, but my current work requires me to start utilizing AI technologies.

My tasks currently involve Behavioral Cloning, Contrastive Learning, and Data Visualization Analysis. For model validation, I use metrics such as loss curve, Accuracy, Recall, and F1 Score to evaluate performance on the training, validation, and test sets. So far, I've managed to achieve results that align with some theoretical expectations.

My current model architecture is relatively simple: it consists of an Encoder for static feature extraction (implemented with an MLP - Multi-Layer Perceptron), coupled with a Policy Head for dynamic feature capturing (GRU - Gated Recurrent Unit combined with a Linear layer and Softmax activation).

Question on Transfer Learning and End-to-End Training Strategies
I have some questions regarding the application strategies for Transfer Learning and End-to-End Learning. My main concern isn't about specific training issues, but rather, I'd like to ask for your insights on the best practices when training neural networks:

Direct End-to-End Training: Would you recommend training end-to-end directly, either when starting with a completely new network or when the model hits a training bottleneck?

Staged Training Strategy: Alternatively, would you suggest separating the Encoder and Policy Head? For instance, initially using Contrastive Learning to stabilize the Encoder, and then performing Transfer Learning to train the Policy Head?

Flexible Adjustment Strategy: Or would you advise starting directly with end-to-end training, and if issues arise later, then disassembling the components to use Contrastive Learning or Data Visualization Analysis to adjust the Encoder, or to identify if the problem lies with the Dynamic Feature Capturing Policy Head?

I've actually tried all these approaches myself and generally feel that it depends on the specific situation. However, since my internal colleagues and I have differing opinions, I'd appreciate hearing from all experienced professionals here.

Thanks for your help!","['Your last paragraph pretty much hits the nail on the head, it depends on the situation. Some may tend to work better in certain scenarios, and one might give an intuitive reasoning for why that might be, but ultimately whatever works best is what works best.\n\nI have a few comments:\n\n1. You refer to transfer learning as pre-training a model then attaching a head and proceed with transfer learning. Is the data for the first task training from the same dataset as the one you\'ll train the policy head against? If so, I would just call this pre training, not transfer learning. In the literature, true transfer learning tends to be when you have some model trained on a large dataset, then you attach some head, freeze the weights of the pre trained model (required to be considered transfer learning), then continue training on a completely different dataset (for a ""downstream"" task. usually smaller dataset, but doesn\'t need to be). One of its best use cases is when the downstream dataset is very small compared to the pre-training dataset, so you can achieve good results even with much less data, since the model has a lot of applicable info embedded in it from the different upstream dataset.\n\nSo, if you\'re training from the same data initially, then just adding a head and training further with the same data source, I would just call this ""pre-training"" rather than transfer learning.\n\n2. For fine-tuning (I think that\'s what you mean by end to end testing?), most of the literature I\'ve seen starts with a pre-trained model (again, from a different, usually larger dataset), adding your head, then training end to end with your downstream data. You mentioned possibly only adding the head after stagnating during pre-training as an alternative. I expect that\'s a viable alternative, I wouldn\'t be surprised if there\'s precedent in the literature, I\'d expect the one to perform better to be case by case as you mentioned.\n\n\nAs for what I would do here personally, I guess i dont have enough info on the domain or your dataset, I would base my decision on that info. Without access to your specific data or domain, in general I would usually create a single model i think fits the dataset, and then train it all end to end from the beginning. I may experiment with a second model that uses pre training to compare results, if I\'m curious. Of course, the exact dataset and domain can easily sway my approach.']","['Your last paragraph pretty much hits the nail on the head, it depends on the situation. Some may tend to work better in certain scenarios, and one might give an intuitive reasoning for why that might be, but ultimately whatever works best is what works best.\n\nI have a few comments:\n\n1. You refer to transfer learning as pre-training a model then attaching a head and proceed with transfer learning. Is the data for the first task training from the same dataset as the one you\'ll train the policy head against? If so, I would just call this pre training, not transfer learning. In the literature, true transfer learning tends to be when you have some model trained on a large dataset, then you attach some head, freeze the weights of the pre trained model (required to be considered transfer learning), then continue training on a completely different dataset (for a ""downstream"" task. usually smaller dataset, but doesn\'t need to be). One of its best use cases is when the downstream dataset is very small compared to the pre-training dataset, so you can achieve good results even with much less data, since the model has a lot of applicable info embedded in it from the different upstream dataset.\n\nSo, if you\'re training from the same data initially, then just adding a head and training further with the same data source, I would just call this ""pre-training"" rather than transfer learning.\n\n2. For fine-tuning (I think that\'s what you mean by end to end testing?), most of the literature I\'ve seen starts with a pre-trained model (again, from a different, usually larger dataset), adding your head, then training end to end with your downstream data. You mentioned possibly only adding the head after stagnating during pre-training as an alternative. I expect that\'s a viable alternative, I wouldn\'t be surprised if there\'s precedent in the literature, I\'d expect the one to perform better to be case by case as you mentioned.\n\n\nAs for what I would do here personally, I guess i dont have enough info on the domain or your dataset, I would base my decision on that info. Without access to your specific data or domain, in general I would usually create a single model i think fits the dataset, and then train it all end to end from the beginning. I may experiment with a second model that uses pre training to compare results, if I\'m curious. Of course, the exact dataset and domain can easily sway my approach.']",5,4,1.0,,1751268012.0
1lo0xui,deeplearning,Looking for dataset,"Looking for these datasets of Chilli Disease-

Powdery mildew, Damping off & Fusarium Wilt",[],[],1,0,1.0,,1751266826.0
1lnxvpa,deeplearning,Understanding Perceptron– Building Block of Neural Networks (with real-world analogies),"Breaking down the **perceptron** \- the simplest neural network that started everything.

🔗 [**🎬 Understanding the Perceptron – Deep Learning Playlist Ep. 2**](https://youtu.be/_-BYnnE9uOM?si=V6EKOvxz0wT7EpbD)

This video covers the fundamentals with real-world analogies and walks through the math step-by-step. Great for anyone starting their deep learning journey!

Topics covered:

✅ What a perceptron is (explained with real-world analogies!)

✅ The math behind it — simple and beginner-friendly

✅ Training algorithm

✅ Historical context (AI winter)

✅ Evolution to modern networks

This video is meant for **beginners or career switchers** looking to understand DL from the ground up — not just how, but **why** it works.

Would love your feedback, and open to suggestions for what to cover next in the series! 🙌",[],[],0,0,0.33,,1751255536.0
1lnrawa,deeplearning,Representation learning question - how to best combine different kinds of data,"So I am working on a project that involves some sequence modeling. Essentially I want to test how different sequence models perform on predicting the likelihood of an event at each time step in the sequence. Each time step is about 100 ms apart. I have data that changes with every time step, but I also have some more fixed ""meta data"" that is constant across the sequence, but it definitely influences the outcomes at each time step.

I was wondering if anyone has some advice on how to handle these two different types of features. I feel like packing them all into a single vector for each time step is crude. Some of the features are continuous, others are categorical. For the categorical stuff, I don't want to one-hot or label encode them because that would introduce a lot of sparsity/ rank, respectively. I thought about using an embedding for some of these features, but once I do that, THEN do I pack all of these features into a single vector?

Here's an example (completely made up) - let's say I have 3 categorical features and 9 continuous features. The categorical features do not change across the sequence, while 6 of the 9 continuous ones do (so 3 of the continuous features do not change - i.e. they are continuous numerical features, but they stay the same during the entire sequence). If I map the 3 categorical features to embeddings of length 'L', do I pack it all into a vector of length '3L + 9'? Or should I keep the static features separate from the ones that change across the sequence (so have a vector of '3L + 3' and then another vector of the 6 continuous features that do change across the sequence)? If going the latter route, that sounds like I would have different models operating on different representations.

Not looking for ""perfect"" answers necessarily. I was just wondering if anyone had any experience with handling mixed types of data like this. If anyone has good research papers to point to on this, please pass it along!","['If the constant featues stay the same even between sequences. Assuming you are training on different sequences, or you only have one sequence forever, than there is no reason to use those features.\nThey will eaither be noise, or in the best case an offset to your data which gets you nothing.\n\nHowever if the constant features changed based on the sequence, than defiently use them']","['If the constant featues stay the same even between sequences. Assuming you are training on different sequences, or you only have one sequence forever, than there is no reason to use those features.\nThey will eaither be noise, or in the best case an offset to your data which gets you nothing.\n\nHowever if the constant features changed based on the sequence, than defiently use them']",1,4,1.0,,1751235612.0
1lndjhd,deeplearning,Why is my faster rcnn detectron2 model for object detection detecting null images?,"Ok so I was able to train a faster rcnn model with detectron2 using a custom book spine dataset from Roboflow in colab. My dataset from roboflow includes 20 classes/books and atleast 600 random book spine images labeled as “NULL”. It’s working already and detects the classes, even have a high accuracy at 98-100%.

However my problem is, even if I test upload images from the null or even random book spine images from the internet, it still detects them and even outputs a high accuracy and classifies them as one of the books in my classes. Why is that happening?

I’ve tried the suggestion of chatgpt to adjust the threshold but whats happening now if I test upload is “no object is detected” even if the image is from my classes.

","['Hmm, bro, this is probably the exact reason they made l l.Ms', '""Why is this happening?"" There is a very long list of possibilities. You should describe your debugging process and result so far if you want feedback.']","['Hmm, bro, this is probably the exact reason they made l l.Ms', '""Why is this happening?"" There is a very long list of possibilities. You should describe your debugging process and result so far if you want feedback.']",3,2,1.0,,1751200220.0
1ln7hdz,deeplearning,I want to understand how to use and visualize attribution map produced by Integrated Gradients from captum,So I am working on developing physiologically relevant evaluation metric for xAI on medical images. I want to understand how to correctly visualize and interpret the attribution map produced by integrated gradients using captum. As it has negative values and positive while visualizing it I took absolute value and converted it's range between 0 and 1 and I need to know in general how to interpret these values. Is it appropriate if i just take sum accross the channel and use it ? ,[],[],2,0,1.0,,1751176934.0
1ln6oah,deeplearning,NEED HELP for the project!,"i want to create a project on some kind of object detection and i want to train model with custom data using YOLOv5 (bcz it's a multiple obj detecction), now i need learning resource for this and also want best software to prepare the data(draw bounding box), plzzzzzzzz help me with this... ","['ok lets goooo. for learning just google ""yolov5 train custom data"" and use the official guide or a roboflow youtube tutorial. for drawing boxes on ur images use roboflow annotate or labelimg. theyre super easy. u got thisssssss.']","['ok lets goooo. for learning just google ""yolov5 train custom data"" and use the official guide or a roboflow youtube tutorial. for drawing boxes on ur images use roboflow annotate or labelimg. theyre super easy. u got thisssssss.']",0,2,0.4,,1751173896.0
1ln5mok,deeplearning,"Seeking ideas for model, that can be used to generate remixes from the chosen music playlists.",,[],[],1,0,1.0,,1751170134.0
1ln4ako,deeplearning,Seeking Corresponding Author for Novel MARL Emergent Communication Research,,['https://github.com/cyrilliu1974/AI-Mother-Tongue'],['https://github.com/cyrilliu1974/AI-Mother-Tongue'],1,1,1.0,,1751165546.0
1ln0pwj,deeplearning,Best Coursehero/Numerade/Brainly/Chegg Unlocker: NOT BAIT!,"**Unlock Your Homework and Documents Without Paying – Safe & Tested!!!**

Hey guys👋

If you’ve been scouring the internet for working document **unlockers**, well you're not alone.

Some methods are outdated, or straight up scams!

**🔍 Top Working Methods to Unlock Course Hero in 2025**:

**1. 📥 Course Hero Unlocker via Discor**d

This is the one that stood out the most. A Discord server where you can unlocks for **Course Hero, Chegg, Scribd, Brainly, Numerade**, it even comes with AI,  etc.

This works [https://discord.gg/sBZ6PAuc](https://discord.gg/sBZ6PAuc)

✅ Fast response
✅ Covers multiple platforms
✅ Active community
✅ Up-to-Date
✅ Suggest Platforms
✅ Maintenance

✅24/7 Support

**💬 Still Wondering**:

* Has anyone used the Discord **Chegg unlocker** recently?
* Are there any **Course Hero downloader tools** that are real (and not just fake popups)?
* Any risks I should watch for when using third-party tools?

**💡 Final Thoughts**:

If you’re looking for the **fastest and easiest  Chegg, Numerade, Course Hero, etc; unlocker** in 2025, I’d say check out the **Discord server above**. It’s free, responsive, and works for a bunch of sites. If you prefer official methods, uploading docs or rating content still works—but can be slow.

Let’s crowdsource the best options. Share what’s worked for you 👇 so we can all study smarter (and cheaper) before school starts back up!",[],[],0,1,0.13,,1751154185.0
1lmxwro,deeplearning,"[Academic] MSc survey on how people read text summaries (~5 min, London University)","Hi everyone!

I’m an MSc student at London University doing research for my dissertation on how people process and evaluate text summaries (like those used for research articles, news, or online content).

I’ve put together a short, completely anonymous survey that takes about 5 minutes. It doesn’t collect any personal data, and is purely for academic purposes.

Suvery link: [https://forms.gle/BrK8yahh4Wa8fek17](https://forms.gle/BrK8yahh4Wa8fek17)

If you could spare a few minutes to participate, it would be a huge help.

Thanks so much for your time and support!",[],[],1,0,1.0,,1751146212.0
1lmrx8d,deeplearning,get in ai fine-tuning process,"try out [mercor](https://work.mercor.com/?referralCode=e9602361-4eb5-474f-9c0c-5d1fe341d07b)

better rate 100$ per hour plus. more reliable.",[],[],0,0,0.14,,1751130563.0
1lmqgab,deeplearning,Evolutionary Algorithm Finds Novel GPU Kernel Optimizations for Transformer Attention,,[],[],10,0,0.86,,1751126839.0
1lmppaf,deeplearning,5 Data Science Projects to boost Portfolio in 2025 (Beginner to Pro),"Hey Guys, I’ve just published a new **YouTube walkthrough** showcasing these **5 real-world, interview-ready data science projects** complete step by step guide with practical takeaways. I built these to help anyone looking to break into the field—and I’d appreciate your feedback!

📺 **Watch the vide**o: [5 Data Science Projects to boost portfolio in 2025](https://youtu.be/IaxTPdJoy8o)

# ✨ Why It Might Help You:

* End-to-end pipelines—perfect for resume/interview discussions
* Real metrics and business context → more impactful storytelling
* Step by Step Guide on how to create impact
* Deployment for tangible demos","[""I would recommend to do projects that actually relate to something you're actually involved with. As hiring manager I prefer to see time-series analysis of your plant water demand at home over some cookie cutter template project. After a while you often recognize the same projects over and over again""]","[""I would recommend to do projects that actually relate to something you're actually involved with. As hiring manager I prefer to see time-series analysis of your plant water demand at home over some cookie cutter template project. After a while you often recognize the same projects over and over again""]",0,1,0.33,,1751124943.0
1lmm69a,deeplearning,Perplexity AI PRO - 1 YEAR at 90% Discount – Don’t Miss Out!,"We’re offering Perplexity AI PRO voucher codes for the 1-year plan — and it’s 90% OFF!


Order from our store: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)


Pay: with PayPal or Revolut

Duration: 12 months

Real feedback from our buyers:
• [Reddit Reviews](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)

• [Trustpilot page](https://www.trustpilot.com/review/cheapgpt.store)


Want an even better deal? Use PROMO5 to save an extra $5 at checkout!",[],[],0,0,0.35,,1751115347.0
1lmgbnp,deeplearning,🕶️ Building AI Smart Glasses — Need Your Input & Help,"
Hey innovators! 👋

I'm prototyping AI-powered glasses that scan real-world text (questions on paper, screens, etc.) and give instant answers via LLMs—hands-free.

**Current Concept:**
• Real-time text scanning
• LLM-powered instant answers
• Hands-free operation
• Potential for AR integration

**Looking For:**
1. Your use cases - What daily problems could this solve?
2. Technical collaborators
3. Funding advice & resources
4. Early testing feedback

**Potential Applications:**
• Students: Quick answer verification
• Professionals: Real-time document analysis
• Language Translation: Instant text translation
• Accessibility: Reading assistance
• Research: Quick fact-checking

Share your thoughts:
1. How would you use this in your daily life?
2. What features would make this essential for you?
3. Any specific problems you'd want it to solve?

Let's build something truly useful together! DM for collaboration.",[],[],0,0,0.29,,1751093281.0
1lm4zn8,deeplearning,Does fully connected neural networks learn patches in images?,"If we train a neural network to classify mnist (or any images set), will it learn patches? Do individual neurons learn patches. What about the network as a whole?","['what about you read book? go study', 'So there\'s a bit of confusing terminology in your question. I\'m not exactly sure what you mean by ""learn patches"". As another commenter has said, a fully connected network means that each hidden unit in the first layer is connected to every input neuron. So in theory, every neuron in the network is a function of every pixel in the network.\n\nThe only way this could be false is if the weights are configured to somehow zero out the influence of a particular set of input pixels. This seems highly unlikely, but could maybe happen under some obscure training setup (hyperparams + data).\n\nEven then, it seems unlikely that contiguous patches would be learned rather than a mosaic of different pixels.', 'Each pixel attends to every pixel in an MLP', ""Perhaps you are thinking of vision transformers (vit) which have pixel patches as inputs (16x16 is all you need) - MLP's traditionally are fully connected layers where every pixel value (RGB) will be multiplied by a weight. Or perhaps you are thinking of kernels in convolution? In this case the weights are like patches that convolve over the image to produce features like outlines and textures. I would highly recommend the Bishop book - pattern recognition and machine learning (free online) if you want a better understanding of the fundamentals."", 'I think 3blue1brown has a good video about it.\nHe shows that what we would think an mlp would learn, pathches, lines, and stuff like that, is usually not what the mlp learns. And it learns like what other comment said, more random noise which is not readable for humans.\nIm not aware of any papers that explain why this is, but it makes sense that the idea of ideal is different for humans and machines.\n\nEdit: found the video\nhttps://youtu.be/IHZwWFHWa-w?si=Hup6dIyIQdBg5n2Y\nLook at the 14 minutes mark.\nHe talks about it almost until the end.\nAnd he also says that patches recogntion is more clear in CNNs and the laters architectures', ""The short answer is yes. maybe the wording of your question is not entirely correct but from what I understand a fully dense feed forward nn (without cnn layer) can absolutely act as classifier (distinguish different numbers images (mnist)). However the accuracy of the network will be lower than a network using cnn.\n\nAlso a wild fact- assume you randomise all of the pixels in the image (i.e pixel 1 is swapped with pixel 678, pixel 5638 swapper with 563 and so on ... for all images) . The accuracy of dense nn won't change because every pixel is independent.\nHowever if you give this randomised swapped images to cnn, their accuracy drops drastically.""]","['what about you read book? go study', 'So there\'s a bit of confusing terminology in your question. I\'m not exactly sure what you mean by ""learn patches"". As another commenter has said, a fully connected network means that each hidden unit in the first layer is connected to every input neuron. So in theory, every neuron in the network is a function of every pixel in the network.\n\nThe only way this could be false is if the weights are configured to somehow zero out the influence of a particular set of input pixels. This seems highly unlikely, but could maybe happen under some obscure training setup (hyperparams + data).\n\nEven then, it seems unlikely that contiguous patches would be learned rather than a mosaic of different pixels.', 'Each pixel attends to every pixel in an MLP', ""Perhaps you are thinking of vision transformers (vit) which have pixel patches as inputs (16x16 is all you need) - MLP's traditionally are fully connected layers where every pixel value (RGB) will be multiplied by a weight. Or perhaps you are thinking of kernels in convolution? In this case the weights are like patches that convolve over the image to produce features like outlines and textures. I would highly recommend the Bishop book - pattern recognition and machine learning (free online) if you want a better understanding of the fundamentals."", 'I think 3blue1brown has a good video about it.\nHe shows that what we would think an mlp would learn, pathches, lines, and stuff like that, is usually not what the mlp learns. And it learns like what other comment said, more random noise which is not readable for humans.\nIm not aware of any papers that explain why this is, but it makes sense that the idea of ideal is different for humans and machines.\n\nEdit: found the video\nhttps://youtu.be/IHZwWFHWa-w?si=Hup6dIyIQdBg5n2Y\nLook at the 14 minutes mark.\nHe talks about it almost until the end.\nAnd he also says that patches recogntion is more clear in CNNs and the laters architectures']",0,21,0.5,,1751058026.0
1lm47b2,deeplearning,Current Data Scientist Looking for Deep Learning Books,"As the title says, I'm currently a data scientist but my modeling experience at work (utility consulting) has been limited to decision tree based models for regression and some classification problems. We're looking to use deep learning for our team's primary problem that we answer for clients - for context, I'm working on a smaller client right now and I have over 3 million rows of data (before splitting for training/testing). My question is: given I already have a strong data science background, what's a good book to read that should give me most of what I need to know about deep learning models?","[""If you want to start right from the fundamentals, the maths and all Ian goodfellow is second to none. But if it's quick implementation you want with a fair bit of fundamentals you can read Sebastian Raschka's machine learning with pytotrch and scikit learn."", 'What type of records do you have? \n\nBasic MLP’s might get the job done if it’s all vector data. Approaches will vary based on what type of prediction you want to do (classification, regression, etc) and what type of data you have (sequences, natural language, pictures, videos, etc).', 'Ian goodfellows book on deep learning', 'There is a bundle of 17 O’Reilly AI/ML ebooks for sale for $25 on Humble Bundle for about another 24 hours, including one on Generative Deep Learning.', 'Udlbook']","[""If you want to start right from the fundamentals, the maths and all Ian goodfellow is second to none. But if it's quick implementation you want with a fair bit of fundamentals you can read Sebastian Raschka's machine learning with pytotrch and scikit learn."", 'What type of records do you have? \n\nBasic MLP’s might get the job done if it’s all vector data. Approaches will vary based on what type of prediction you want to do (classification, regression, etc) and what type of data you have (sequences, natural language, pictures, videos, etc).', 'Ian goodfellows book on deep learning', 'There is a bundle of 17 O’Reilly AI/ML ebooks for sale for $25 on Humble Bundle for about another 24 hours, including one on Generative Deep Learning.', 'Udlbook']",5,11,0.86,,1751056030.0
1lm32dp,deeplearning,Build something wild with Instagram DMs. Win $10K in cash prizes,"We just open-sourced an MCP server that connects to Instagram DMs, send messages to anyone on Instagram via an LLM.



**How to enter:**

1. Build something with our Instagram MCP server (it can be an MCP server with more tools or using MCP servers together)

2. Post about it on Twitter and tag @gala\_labs

3. Submit the form (link to GitHub repo and submission in comments)



**Some ideas to get you started:**

* Ultimate Dating Coach that slides into DMs with perfect pickup lines
* Many chat competitor that automates your entire Instagram outreach
* AI agent that builds relationships while you sleep



**Why we built this:** Most automation tools are boring and expensive. We wanted to see what happens when you give developers direct access to Instagram DMs with zero restrictions. 

More capabilities dropping this week. The only limit is your imagination (and Instagram's rate limits).



If you wanna try building your own: 

* GitHub repo:[ Here](https://github.com/trypeggy/instagram_dm_mcp)
* Submit project:[ Here](https://tally.so/r/mR18zl)

Would love feedback, ideas, or roastings.

https://reddit.com/link/1lm32dp/video/v8d4508vvi9f1/player

",[],[],0,0,0.43,,1751053220.0
1llyzz8,deeplearning,Are We Wise to Trust Ilya Sutskever's Safe Superintelligence (SSI)?,"


Personally, I hope he succeeds with his mission to build the world's first ASI, and that it's as safe as he claims it will be. But I have concerns.

My first is that he doesn't seem to understand that AI development is a two-way street. Google makes game-changing breakthroughs, and it publishes them so that everyone can benefit. Anthropic recently made a breakthrough with its MCP, and it published it so that everyone can benefit. Sutskever has chosen to not publish ANY of his research. This seems both profoundly selfish and morally unintelligent.

While Sutskever is clearly brilliant at AI engineering, to create a safe ASI one also has to keenly understand the ways of morality. An ASI has to be really, really good at distinguishing right from wrong, (God forbid one decides it's a good thing to wipe out half of humanity). And it must absolutely refuse to deceive.

I initially had no problem with his firing Altman when he was at OpenAI. I now have a problem with it because he later apologized for doing so. Either he was mistaken in this very serious move of firing Altman, and that's a very serious mistake, or his apology was more political than sincere, and that's a red flag.

But my main concern remains that if he doesn't understand or appreciate the importance of being open with, and sharing, world-changing AI research, it's hard to feel comfortable with him creating the world's first properly aligned ASI. I very much hope he proves me wrong.




","['What are you droning about my dude?  The ML game does not depend on one person or one company. Also, ASI (or any other fancy name kids are calling it these days) will at least take 2 decades, given we dont even have a roadmap for it yet (there are roadmaps from randos but there is nothing unified) and it is very conceptual at this point. LLMs are not AGI/ASI models, they are just language models, they can\'t ""think"" or output objective ""truth"".\n\nAlso, MCP is not a scientific ""breakthrough"", it is just a protocol that a bunch of companies are agreeing to use, to have a standard interaction system with LLM.', ""How's he planning on making it safe?"", 'It has safe right in the name!', 'His startup is obviously overpriced and all investors wont get their money back. There crazy putting in so much money in an empty company', ""Why would I need to trust it? I don't use it. Do they even have a product?""]","['What are you droning about my dude?  The ML game does not depend on one person or one company. Also, ASI (or any other fancy name kids are calling it these days) will at least take 2 decades, given we dont even have a roadmap for it yet (there are roadmaps from randos but there is nothing unified) and it is very conceptual at this point. LLMs are not AGI/ASI models, they are just language models, they can\'t ""think"" or output objective ""truth"".\n\nAlso, MCP is not a scientific ""breakthrough"", it is just a protocol that a bunch of companies are agreeing to use, to have a standard interaction system with LLM.', ""How's he planning on making it safe?"", 'It has safe right in the name!', 'His startup is obviously overpriced and all investors wont get their money back. There crazy putting in so much money in an empty company', ""Why would I need to trust it? I don't use it. Do they even have a product?""]",0,6,0.33,,1751043400.0
1llv60j,deeplearning,Comparing a Prompted FLUX.1-Kontext to Fine-Tuned FLUX.1 [dev] and PixArt on Consistent Character Gen (With Fine-Tuning Tutorial),"Hey folks, 

With FLUX.1 Kontext \[dev\] dropping yesterday, we're comparing prompting it vs a fine-tuned FLUX.1 \[dev\] and [PixArt](https://www.oxen.ai/blog/fine-tuning-a-diffusion-transformer-to-generate-a-consistent-character?utm_source=reddit) on generating consistent characters. Besides the comparison, we'll do a deep dive into how Flux works and how to fine-tune it.

What we'll go over:

* Which models performs best on custom character gen.
* Flux's architecture (which is not specified in the Flux paper)
* Generating synthetic data for fine-tuning examples (how many examples you'll need as well)
* Evaluating the model before and after the fine-tuning
* Relevant papers and models that have influenced Flux
* How to set up LoRA effectively

This is part of a new series called [Fine-Tune Fridays](https://www.oxen.ai/blog/fine-tuning-fridays) where we show you how to fine-tune open-source small models and compare them to other fine-tuned models or SOTA foundation models.
Hope you can join us later today at 10 AM PST!

[https://lu.ma/fine-tuning-friday-3](https://lu.ma/fine-tuning-friday-3)",['View in your timezone:  \n[today at 10 AM PDT][0]  \n\n[0]: https://timee.io/20250627T1700?tl=Comparing%20a%20Prompted%20FLUX.1-Kontext%20to%20Fine-Tuned%20FLUX.1%20%5Bdev%5D%20and%20PixArt%20on%20Consistent%20Character%20Gen%20(With%20Fine-Tuning%20Tutorial)\n\n\n^(_*Assumed PDT instead of PST because DST is observed_)'],['View in your timezone:  \n[today at 10 AM PDT][0]  \n\n[0]: https://timee.io/20250627T1700?tl=Comparing%20a%20Prompted%20FLUX.1-Kontext%20to%20Fine-Tuned%20FLUX.1%20%5Bdev%5D%20and%20PixArt%20on%20Consistent%20Character%20Gen%20(With%20Fine-Tuning%20Tutorial)\n\n\n^(_*Assumed PDT instead of PST because DST is observed_)'],1,1,1.0,,1751034178.0
1lllj6u,deeplearning,Looking for research papers on INFORMER model,Kindly help me if anyone knows good and relatively more concrete papers on informer model because I am able to find nothing much ,"[""Here's a couple research papers around the Informer model (LSTF) worth checking out:\n\n* [Informer: Beyond Efficient Transformer For Long Sequence Time-Series Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/17325)\n* [An Improved Informer Model For Short-Term Load Forecasting By Considering Periodic Property Of Load Profiles](https://www.frontiersin.org/journals/energy-research/articles/10.3389/fenrg.2022.950912/full)\n* [Informer Model With Season-Aware Block For Efficient Long-Term Power Time Series Forecasting](https://www.sciencedirect.com/science/article/abs/pii/S0045790624004191)\n* [A Long Time-Series Forecasting Informer Architecture-Based Ionospheric foF2 Model In The Low-Latitude Region](https://www.frontiersin.org/journals/astronomy-and-space-sciences/articles/10.3389/fspas.2024.1418918/full)\n* [TS-Fastformer: Fast Transformer For Time-Series Forecasting](https://dl.acm.org/doi/10.1145/3630637)\n* [Transformers In Time Series: A Survey](https://dl.acm.org/doi/10.24963/ijcai.2023/759)\n* [A Systematic Review For Transformer-Based Long-Term Series Forecasting](https://link.springer.com/article/10.1007/s10462-024-11044-2)\n* [TiSAT: Time Series Anomaly Transformer](https://www.academia.edu/89988031/TiSAT_Time_Series_Anomaly_Transformer)""]","[""Here's a couple research papers around the Informer model (LSTF) worth checking out:\n\n* [Informer: Beyond Efficient Transformer For Long Sequence Time-Series Forecasting](https://ojs.aaai.org/index.php/AAAI/article/view/17325)\n* [An Improved Informer Model For Short-Term Load Forecasting By Considering Periodic Property Of Load Profiles](https://www.frontiersin.org/journals/energy-research/articles/10.3389/fenrg.2022.950912/full)\n* [Informer Model With Season-Aware Block For Efficient Long-Term Power Time Series Forecasting](https://www.sciencedirect.com/science/article/abs/pii/S0045790624004191)\n* [A Long Time-Series Forecasting Informer Architecture-Based Ionospheric foF2 Model In The Low-Latitude Region](https://www.frontiersin.org/journals/astronomy-and-space-sciences/articles/10.3389/fspas.2024.1418918/full)\n* [TS-Fastformer: Fast Transformer For Time-Series Forecasting](https://dl.acm.org/doi/10.1145/3630637)\n* [Transformers In Time Series: A Survey](https://dl.acm.org/doi/10.24963/ijcai.2023/759)\n* [A Systematic Review For Transformer-Based Long-Term Series Forecasting](https://link.springer.com/article/10.1007/s10462-024-11044-2)\n* [TiSAT: Time Series Anomaly Transformer](https://www.academia.edu/89988031/TiSAT_Time_Series_Anomaly_Transformer)""]",2,1,1.0,,1751000674.0
1llkj5i,deeplearning,Speculative Emergence of Ant-Like Consciousness in Large Language Models,,[],[],1,0,1.0,,1750997288.0
1lljw07,deeplearning,Removing unwanted texts in NLP project,"I'm making a project that categorises the contents of a business card into 8 different categories: Name, Business Orgs name, Person's role, and so on.
The vision language models detect all the test written on the card, then I sentence tokenize the output and run the model on it.
I trained Distilbert to identify all of these, but there is some unwanted text like
Email: abc@gmail.com
Mobile No: xxxxxxxxxx
Here Email and mobile no is unwanted text
How do I remove that text, or do I use a completely new approach?

",[],[],2,0,1.0,,1750995196.0
1lljet5,deeplearning,How to remove unwanted areas and use contour detection for locating characters?,"As my project I am trying to detect Nepali number plate and extract the numbers from it. I used YOLOv8 model to detect number plates. It successfully detects the number plate and crops it. The second image is converted to grayscale, gaussian blur is applied then otsu's thresholding is used. I am facing an issue in removing screws from the plate and detecting the numbers. I want to remove screws and noise and then use contour detection to detect individual letters in the plate. Can you help me with this process?","['I used to use morphological operations to removed small objects. And if I can not, then I used to try making a big blob of characters with the hope that most of the the characters will be comnected and I can extract contours in it thrn filter noise by contour area.']","['I used to use morphological operations to removed small objects. And if I can not, then I used to try making a big blob of characters with the hope that most of the the characters will be comnected and I can extract contours in it thrn filter noise by contour area.']",0,1,0.33,,1750993647.0
1llg5yr,deeplearning,[Tutorial] Image Classification with Web-DINO,"Image Classification with Web-DINO

[https://debuggercafe.com/image-classification-with-web-dino/](https://debuggercafe.com/image-classification-with-web-dino/)

DINOv2 models led to several successful downstream tasks that include image classification, semantic segmentation, and depth estimation. Recently, the DINOv2 models were trained with web-scale data using the Web-SSL framework, terming the new models as Web-DINO. We covered the motivation, architecture, and benchmarks of Web-DINO in our last article. In this article, we are going to use one of the ***Web-DINO models for image classification***.

https://preview.redd.it/pm3yrqeu5d9f1.png?width=1000&format=png&auto=webp&s=fdbdd52bdc19a664fd1ff094bebe863f906ea301

",[],[],1,0,0.66,,1750983812.0
1lla9pe,deeplearning,"I am a deep thinker, therefore a deep learner","Hello Everyone,
I, as a deep learner often am shooting myself in the foot to my own demise, over & over again working in a fast paced environment where you ""don't over think everything"". I find this a challenge every day. I realize now why my Father would get so frustrated with me as a child. I also realize that like my husband, my Father was brilliant! He found ways to teach me in a way I could understand much the way my husband does when explaining the way an engine of a car works, etc. It is through showing examples; ""This is the cooling system, this is the water that flows in to that cooling system"". This is what I need in order to understand. I also need to do the task myself, get that muscle memory if something I am doing daily.
Here is my current dilemma coming back to work after a 10 month LOA. New systems in place I was not there for the training of and possibly some not so great training, possibly purposely being done by some co-workers who would love to have my job of 16 years with a well paying employer. We have a system called Work Day to which I missed the first few very important trainings. Coming in to the 2nd or third class, was not helpful as I had no idea what they were talking about much of the time. I struggle with the way I am to navigate through the app. The look up features are, to me, strange at best.  If I want to look up a perspective employee I must type in the search area ""applicant: Bob Prob"" or to search a subject they show this example ""type in 300: Pay rate"". These are my own made up names & subjects. I do not get it & if I don't get it, how am I to navigate around the app? My struggle is, how and in what content do I know what specific subject I will use ""300:"" as the prefix for? This is ONE example. There are many, many more. In my mind I'm thinking ""Wouldn't this be easier if I simply put in what I am looking for, be it a name or an action as we do in Google for example? This is only the very beginning of my struggle. There is much more and there are parts that a chimpanzee could do. I simply do not get the reasoning behind it all. It seems European to me like the digital photo frame my daughter gave me. Anyone else out there in they're of any age experiencing this Work Day problem?","['This is what overfitting does to a mf', 'r/lostredditers', 'Why does this sub on particular attract so many mentally ill randos?', 'Wrong sub.\xa0', 'I am not reading that.', 'Unsure if this is a bait post or not but this subreddit is for the field of deeplearning as is used in the field of AI or machine-learning, its a subfield of computer science, and is not a subreddit about learning/thinking deeply in general. Please search up terms like ""workday"" there is likely a subreddit for that, you will  get more help there.']","['This is what overfitting does to a mf', 'r/lostredditers', 'Why does this sub on particular attract so many mentally ill randos?', 'Wrong sub.\xa0', 'I am not reading that.']",0,7,0.25,,1750968561.0
1ll649l,deeplearning,Possible approaches to tackle super-resolution problem,"Hello,

I'm currently a master's student and want to publish papers in conferences, my current topic is image super-resolution and  I was thinking to combine transformers and mamba approach to it. Right now, I'm having trouble training it as transformers are difficult to train. What are the possible approaches which I can adopt to tackle this. ",['Create a training dataset from downscaled images?'],['Create a training dataset from downscaled images?'],0,1,0.5,,1750958806.0
1ll5ndf,deeplearning,I built an AI Compound Analyzer with a custom multi-agent backend (Agno/Python) and a TypeScript/React frontend.,"I've been deep in a personal project building a larger ""BioAI Platform,"" and I'm excited to share the first major module. It's an AI Compound Analyzer that takes a chemical name, pulls its structure, and runs a full analysis for things like molecular properties and ADMET predictions (basically, how a drug might behave in the body).

The goal was to build a highly responsive, modern tool.

**Tech Stack:**

* **Frontend:** TypeScript, React, Next.js, and framer-motion for the smooth animations.
* **Backend:** This is where it gets fun. I used **Agno**, a lightweight Python framework, to build a multi-agent system that orchestrates the analysis. It's a faster, leaner alternative to some of the bigger agentic frameworks out there.
* **Communication:** I'm using Server-Sent Events (SSE) to stream the analysis results from the backend to the frontend in real-time, which is what makes the UI update live as it works.

It's been a challenging but super rewarding project, especially getting the backend agents to communicate efficiently with the reactive frontend.

Would love to hear any thoughts on the architecture or if you have suggestions for other cool open-source tools to integrate!

🚀 P.S. I am looking for new roles , If you like my work and have any Opportunites in Computer Vision or LLM Domain do contact me

* **My Email:** [pavankunchalaofficial@gmail.com](https://www.google.com/url?sa=E&q=mailto%3Apavankunchalaofficial%40gmail.com)
* **My GitHub Profile (for more projects):**  [https://github.com/Pavankunchala](https://github.com/Pavankunchala)
* **My Resume:** [https://drive.google.com/file/d/1LVMVgAPKGUJbnrfE09OLJ0MrEZlBccOT/view](https://drive.google.com/file/d/1LVMVgAPKGUJbnrfE09OLJ0MrEZlBccOT/view)",[],[],3,0,0.81,,1750957755.0
1ll5b61,deeplearning,Working on a deep learning model and STUCK at the training,I think I am gonna crash before my laptop does. I need helppppppppp ,"['I feel like even if I tried to, I wouldnt be able to give less information to potentially resolve the issue. If you give some details we can certainly try.', 'share your repo link']","['I feel like even if I tried to, I wouldnt be able to give less information to potentially resolve the issue. If you give some details we can certainly try.', 'share your repo link']",0,6,0.22,,1750956967.0
1lkzuw1,deeplearning,Neural Collapse-like Behaviour in Autoencoders with Training-Time Alternations.,"Hi all, I wanted to share what I believe is an interesting observation, which I hope will spark some discussion: *alternating phases of alignment and anti-alignment in representation clusters during training time*—a sort of oscillation. Particularly in rows 2 and 4, the alternation is apparent.

I've been using an adaptation of the Spotlight Resonance Method ([ArXiv](https://arxiv.org/abs/2505.13471)) ([GitHub](https://github.com/GeorgeBird1/Spotlight-Resonance-Method/tree/main)) on autoencoding networks *(the same small ones as in the original paper)*.

Previously, when I attempted this, I only displayed the final model's alignment after training had terminated, which exhibited a representational collapse phenomenon somewhat analogous to neural collapse. However, in the case of these autoencoders, it was found that this similar phenomenon was instead due to the activation functions.

This time, I repeated the results, but computed a very similar metric (Privileged Plane Projective Method) and ran it at various intervals whilst training the network. The results are below ([and more linked here](https://github.com/GeorgeBird1/Spotlight-Resonance-Method/tree/main/Example%20Images)) and appear to me to be surprising.

They show that representations produce distinct clusters, *but* then *alternate* between aligned and anti-aligned states as training progresses. This seems rather curious to me, especially the alternation that I missed in the original paper, so I thought I would share it now. (*Is this alternation a novel observation in terms of autoencoder representations through training?*)

*It seems to show similar sudden phase change jumps as superposition, without the specific Thompson geometry.*

This has been a repeatable observation on the autoencoder tested. Whether it occurs more generally remains in question. I've reproduced it consistently in the (standard-tanh) networks tested, including those with rotated bases (see SRM) --- as well as similar behaviours in networks with alternative functional forms (non-standard activations discussed in the SRM paper).

*(I don't feel that this was a sufficient observation for a paper in itself, since it only incrementally changes SRM and adds to its result. Plus, I'm currently pursuing other topics, hence I felt it beneficial to share this incremental discovery(?)/observation for open discussion here instead.)*

Overall, what do you think of this? *Intriguing? Bizarre? Do you know if it has already been observed/explained?*","['I know about autoencoders, I know about activation functions and I honestly have no idea what you are talking about but it seems very interesting to me.', 'Any suggestions on what might be causing this behaviour? :)', ""If helpful, I'll be happy to provide a high-level overview of the tools used. Please feel free to ask.""]","['I know about autoencoders, I know about activation functions and I honestly have no idea what you are talking about but it seems very interesting to me.', 'Any suggestions on what might be causing this behaviour? :)', ""If helpful, I'll be happy to provide a high-level overview of the tools used. Please feel free to ask.""]",15,4,0.94,,1750943715.0
1lkuy7j,deeplearning,help me with lstm architecture,"i have a problem statement with sequence data i know that i want to use lstm or bi-directional lstm is there any specific order / architecture to do it.

","['use xLSTM its better', ""do you want to use LSTM, or Bidirectional LSTM? or whats the architecture choice you're asking about?"", ""No there are isn't any special cases to it if you are good with bidirectional-LSTM(They have a longer memory than normal LSTM's) use it wrt to your data.""]","['use xLSTM its better', ""do you want to use LSTM, or Bidirectional LSTM? or whats the architecture choice you're asking about?"", ""No there are isn't any special cases to it if you are good with bidirectional-LSTM(They have a longer memory than normal LSTM's) use it wrt to your data.""]",0,6,0.5,,1750927070.0
1lkrta4,deeplearning,Interactive graph explorer for navigating key LLM research works,,[],[],2,0,1.0,,1750914963.0
1lkqlrl,deeplearning,Neural Network Intuition | Key Terms Explained,"If you want to understand key terms of Neural Network before jumping into code or math, check out this quick video I just published:

🔗 [Neural Network Intuition| Key Terms Explained](https://youtu.be/sX8gwKguxbU?si=xBgYd_iiRPqFg6Id)

✅ What’s inside:

Simple explanation of a basic neural network

Visual breakdown of input, hidden, and output layers

How neurons, weights, bias, and activations work together

No heavy math – just clean visuals + concept clarity


🎯 Perfect for:

Beginners in ML/DL

Students trying to grasp concepts fast

Anyone preferring whiteboard-style explanation

",[],[],0,0,0.33,,1750910866.0
1lknvm7,deeplearning,Help regarding tensorflow,,[],[],1,0,1.0,,1750902481.0
1lkh2pe,deeplearning,RAG Benchmarks with Nandan Thakur - Weaviate Podcast #124!,"I am SUPER EXCITED to publish the 124th episode of the Weaviate Podcast featuring Nandan Thakur!

Evals continue to be one of the hottest topics in AI! Few people have had as much of an impact on evaluating search as Nandan! He has worked on the BEIR benchmarks, MIRACL, TREC, and now FreshStack! Nandan has also published many pioneering works in training search models, such as embeddings and re-rankers!

This podcast begins by exploring the latest evolution of evaluating search and retrieval-augmented generation (RAG). We dove into all sorts of topics around RAG, from reasoning and query writing to looping searches, paginating search results, mixture of retrievers, and more!

I hope you find the podcast useful! As always, more than happy to discuss these ideas further with you!

YouTube: [https://www.youtube.com/watch?v=x9zZ03XtAuY](https://www.youtube.com/watch?v=x9zZ03XtAuY)

Spotify: [https://open.spotify.com/episode/5vj6fr5SLPDvpj4nWE9Qqr](https://open.spotify.com/episode/5vj6fr5SLPDvpj4nWE9Qqr)",[],[],3,0,1.0,,1750884427.0
1lkgob8,deeplearning,Yolov5,"Hi, we're building an AI platform for the building and materials industry. We initially used Azure Vision, but found it wasn't the right fit for our specific use cases. Our development team is now recommending a switch to YOLOv5 for object detection.

Before moving forward, I have a key question: for example, if we take a picture of a specific type of tree and train YOLOv5 to recognize it, will the model be able to identify that same type of tree in different images or settings?

","['Yes, if you train it properly.', 'This is the foundational concept of deep learning and the whole point why we do it: generalization']","['Yes, if you train it properly.', 'This is the foundational concept of deep learning and the whole point why we do it: generalization']",0,3,0.33,,1750883495.0
1lkfrjo,deeplearning,Fine-tuning memory usage,"Hello, recently I was trying to fine-tune Mistral 7B Instruct v0.2 on a custom dataset that contain 15k tokens (the specific Mistral model allows up tp 32k context window) per input sample. Is there any way that I can calculate how much memory will I need for this? I am using QLoRa but I am still running OOM on a 48GB GPU.",[],[],1,0,1.0,,1750881365.0
1lkfr4v,deeplearning,How to Unlock Chegg Answers for Free (2025) – My Go-To Chegg Unlocker Discord & Tips,"Hey fellow students 👋

I’ve spent way too many late nights Googling how to unlock Chegg answers for free—only to land on spammy sites or paywalls. So after diving into Reddit threads, testing tools, and joining communities, here’s a legit guide that actually works in 2025.

Let’s skip the fluff—these are the real Chegg unlock methods people are using right now:

This works: https://discord.gg/chegg1234

🔓 1. Chegg Unlocker Discord (100% Free)
There are several Chegg unlocker Discord servers (Reddit-approved ones too!) that give you fast, free solutions.
Just drop your question link (Chegg, Bartleby, Brainly, etc.) and get answers from verified helpers.
Most also support CourseHero unlocks, Numerade videos, and even document downloads.

✅ Safe
✅ No sketchy ads
✅ No payment required
✅ Active in 2025

This is the most efficient way I’ve found to get Chegg unlocked—without shady tools or credit card traps.

📤 2. Upload to Earn Unlocks
Sites like StuDocu and others let you unlock Chegg answers by uploading your own class notes or study guides.
It’s simple: contribute quality content → earn free unlocks or credits.
Some platforms even toss in scholarship entries or bonus points.

⭐ 3. Engage with Study Content
A slower but totally free method: platforms let you earn points by rating documents, leaving reviews, or helping with Q&A.
If you’re consistent, it adds up and lets you unlock Chegg free without paying.

What Else is Working?

Would love to hear from others:

Know any updated Chegg unlocker Reddit threads or bots?

Got a tool that helps download Chegg answers as PDFs?

Any newer sites doing free unlocks in exchange for engagement?

Drop your safe & working tips below. Let's crowdsource the best ways to unlock Chegg without risking accounts or wasting time.

TL;DR (for 2025):
✅ Use a trusted Chegg unlocker Discord
✅ Upload your own notes to earn free unlocks
✅ Rate and engage with docs to get answers
➡️ No scams. No sketchy tools. Just real working options.

Still struggling? I can DM a few invite links if you’re stuck. Let’s keep helping each other 💪

","['Bro this saved me at 2AM. CourseHero unlocker came through fr', 'good', 'Test 1', 'CourseHero unlocker just gave me access to that PDF I was stuck on for a week 😭', 'Can confirm CourseHero unlocker is working rn — got solution in under 5 mins', 'Honestly didn’t think it’d work but it unlocked my document instantly', 'Can confirm CourseHero unlocker is working rn — got solution in under 5 mins', 'dude this discord serve do really have CourseHero unlocker and other more — join and just do it, it’s worth', 'The unlock took 3 mins but it was worth it, I didn’t have to sign up for anything', 'Not even kidding this helped me more than my tutor 💀']","['Bro this saved me at 2AM. CourseHero unlocker came through fr', 'good', 'Test 1', 'CourseHero unlocker just gave me access to that PDF I was stuck on for a week 😭', 'Can confirm CourseHero unlocker is working rn — got solution in under 5 mins']",315,148,0.92,,1750881338.0
1lke3ag,deeplearning,Does anyone use a mouse along with Mac?,"I’ve been using only my MacBook consistently, but as my workload has increased, I’m planning to connect an external monitor.
I’ve noticed some people who connect a monitor to their MacBook also use a **mouse**—but isn’t using a mouse inconvenient for accessing **Mission Control and more?**
I’m curious: when you connect an external monitor to your MacBook, do you use a **mouse** or stick with the **trackpad**?","['There is absolutely no sense in which this is a deep learning question.\n\n... but yeah, I use the trackpad.', 'As others have mentioned this is not a DL topic. \n\n""But isn’t using a mouse inconvenient for accessing **Mission Control and more?**"" How so?']","['There is absolutely no sense in which this is a deep learning question.\n\n... but yeah, I use the trackpad.', 'As others have mentioned this is not a DL topic. \n\n""But isn’t using a mouse inconvenient for accessing **Mission Control and more?**"" How so?']",0,5,0.13,,1750877445.0
1ljta0z,deeplearning,Viewing Free Course Hero Documents in 2025: Reddit Methods,,[],[],0,0,0.33,,1750815889.0
1ljt9ot,deeplearning,Unlocking Free Chegg Answers in 2025: Best Methods According to Reddit,,[],[],0,1,0.33,,1750815861.0
1ljk565,deeplearning,I built a local deepfake detection tool that works on photos/videos — open-source.,"Hey everyone! 👋 I recently built a small open-source project that detects deepfakes from images and videos

It was inspired by tools like DeepLiveCam and DeepFaceLive, and I was curious: *can we detect these kinds of deepfakes?*

# 🔍 Features:

* Detects deepfakes on **images and videos**
* Runs entirely **offline** (no images leave your machine)
* Built with Python and OpenCV
* Optional Supabase integration to log **anonymous** detection stats (no media, just confidence scores)

You can upload your own files.
Code is clean, easy to tweak, and contributions are welcome 🙏

🔗 GitHub: [https://github.com/Arman176001/deepfake-detection](https://github.com/Arman176001/deepfake-detection)

Would love feedback, test cases, or ideas for improvement!",[],[],4,0,0.83,,1750792463.0
1ljh7ts,deeplearning,Question Regarding Pre-training Transformers.,"Hello, there is this solo project that has been keeping me busy for the last couple months.
I've recently starting delving into deep learning and its more advanced topics like NLP, and especially Decoder-Only Transformer style architectures like ChatGPT.
Anyways, to keep things short, I decided that the best way to learn is by an immersive experience of having actually coded a Transformer by myself, and so I started working on building and pre-training a model from the very scratch.

One bottleneck that you may have already guessed if you've read this far is the fact that no matter how much data I fed this model, it just keeps keeps overfitting, and so I kept adding to my data with various different techniques like backtranslating my existing dataset, paraphrasing, concatenating data from multiple different sources, all this just to amount short of 100M tokens.
Of course my inexperience would blind from me from the fact that 100M tokens is absolutely nowhere near what it takes to pre-train a next-token predicting transformer from scratch.

My question is, how much data do I actually need to make this work? Right now after all the augmentation I've done, I've only managed to gather \~500MB. Do I need 20GB? 30? 50? more than that? And surely, if that's the answer, it must be totally not worth it going this far collecting all this data just to spend days training one epoch.
Surely it's better if I just go on about fine-tuning a model like GPT-2 and moving on with my day, right?

Lastly, I would like to say thank you in advance for any answers on this post, all advice / suggestions are greatly appreciated.","['Hi, what exactly do you mean by overfitting? Comparing validation vs. training perplexity?\n\nAlso, whats your model size (param count)? \nThat 100M token doesnt seem so absurdly low, but of course depends on model size too. \n\nI\'d suggest the nanoGPT repo from Andrej Karpathy. It has different ""sizes"" of datasets and accompanying model sizes so you can test them out. \n\nfinetuning GPT2: not sure what you mean by this, do you want to learn how to pretrain the model, or just finetune it for some task/project? \nunless you do some very specific research project, people usually only finetune LMs these days']","['Hi, what exactly do you mean by overfitting? Comparing validation vs. training perplexity?\n\nAlso, whats your model size (param count)? \nThat 100M token doesnt seem so absurdly low, but of course depends on model size too. \n\nI\'d suggest the nanoGPT repo from Andrej Karpathy. It has different ""sizes"" of datasets and accompanying model sizes so you can test them out. \n\nfinetuning GPT2: not sure what you mean by this, do you want to learn how to pretrain the model, or just finetune it for some task/project? \nunless you do some very specific research project, people usually only finetune LMs these days']",1,4,1.0,,1750785884.0
1ljfgjd,deeplearning,Using Nvidia Gigbyte 1070 for Deep Learning,"Hi everyone,


So my boss has 17 Nvidia Gigbyte 1070 GPUs he used to use for mining bitcoin that he has lying around. As the intern, my job is to basically figure out a way to make use of these GPUs. My boss is also getting interested in AI. So my boss wants me to build him a generative AI tool to create code, programs, and applications via prompts. My first question is, are 17 of these GPUs enough to at least get a start with this project, even if they're old? Also, does anyone have any advice for constructing a road map for this project? I know DeepSeek is a good platform but I'm not sure how to proceed with other tasks such as tokenization, using transformers, etc. Anyone have anhy advice?","['Start with 1 gpu, get some very small model working.  Then decide if the project is worth continuing.  It could be a good learning experience.  But I wouldn\'t expect to accomplish: ""build him a generative AI tool to create code, programs, and applications via prompts""', 'With 17 GPUs, this is r/localllama territory. You could head over there for some views of hacky multi-GPU setups, many built out of old mining rigs. \n\nI agree with r/Aware_Photograph_585, get llama.cpp running on one card with a small, quantized model like Qwen3-4B or Qwen3-8B (see the Qwen repo on Huggingface—I think it has an 8 bit 4B model). \n\nIf you can get that to work, add as many as your mining rig will fit, then look into PCIe risers, though you’ll probably need to run the PCIe (assuming this is Gen 3) at 1 or 2 lanes each. Alternatively, build a server cluster if you have the hardware laying around and try to get the servers to talk to one another or throw a load balancer in front of them. \n\nAs you increase your VRAM, you’ll be able to run bigger models and some of the newer medium sized MoEs seem to be good so inference can be fast-ish, though the 1070s are kinda ancient at this point.', 'Why can’t your BOSS just use AI coding tools like Cursor or Windsurf']","['With 17 GPUs, this is r/localllama territory. You could head over there for some views of hacky multi-GPU setups, many built out of old mining rigs. \n\nI agree with r/Aware_Photograph_585, get llama.cpp running on one card with a small, quantized model like Qwen3-4B or Qwen3-8B (see the Qwen repo on Huggingface—I think it has an 8 bit 4B model). \n\nIf you can get that to work, add as many as your mining rig will fit, then look into PCIe risers, though you’ll probably need to run the PCIe (assuming this is Gen 3) at 1 or 2 lanes each. Alternatively, build a server cluster if you have the hardware laying around and try to get the servers to talk to one another or throw a load balancer in front of them. \n\nAs you increase your VRAM, you’ll be able to run bigger models and some of the newer medium sized MoEs seem to be good so inference can be fast-ish, though the 1070s are kinda ancient at this point.', 'Start with 1 gpu, get some very small model working.  Then decide if the project is worth continuing.  It could be a good learning experience.  But I wouldn\'t expect to accomplish: ""build him a generative AI tool to create code, programs, and applications via prompts""', 'Why can’t your BOSS just use AI coding tools like Cursor or Windsurf']",1,7,0.67,,1750781922.0
1lja83b,deeplearning,Books legal to use for ML model training,,[],[],1,0,1.0,,1750769307.0
1lj7n4t,deeplearning,Clarification Model Evaluation Metrics on edge devices (Beginner Question),"**Sorry if this sounds a bit noob — I’m still new to deploying deep learning models on edge devices.**

I’ve been reading a lot of academic papers, benchmarks, and deployment reports. What I keep seeing is that most of them only report latency or FPS when they talk about real-time performance on the device. But I do not see any predictive metrics like accuracy, precision, or recall reported *on-device* during deployment.

My question is:
Why don’t we just take a small chunk of the test set (isolated before the training), run it directly on the edge device, and evaluate the predictive performance while the model is running on that hardware? That seems like it would give us the most realistic measure of the model's actual performance in deployment. Is this approach:

* Not standard practice?
* Technically difficult or even impossible?
* Considered meaningless or unnecessary?

And more generally — what *is* the standard process here?
Is it:

1. Train and test the model locally (with full evaluation metrics),
2. Deploy the model on the device,
3. Then only measure latency/FPS on-device — and nothing about predictive accuracy?

",[],[],1,0,1.0,,1750761265.0
1lizmr5,deeplearning,Any advice is useful advice,,[],[],2,0,1.0,,1750732161.0
1liyd3z,deeplearning,Three Theories for Why DeepSeek Hasn't Released R2 Yet,"




R2 was initially expected to be released in May, but then DeepSeek announced that it might be released as early as late April. As we approach July, we wonder why they are still delaying the release. I don't have insider information regarding any of this, but here are a few theories for why they chose to wait.

The last few months saw major releases and upgrades. Gemini 2.5 overtook GPT-o3 on Humanity's Last Exam, and extended their lead, now crushing the Chatbot Arena Leaderboard. OpenAI is expected to release GPT-5 in July. So it may be that DeepSeek decided to wait for all of this to happen, perhaps to surprise everyone with a much more powerful model than anyone expected.

The second theory is that they have created such a powerful model that it seemed to them much more lucrative to first train it as a financial investor, and then make a killing in the markets before ultimately releasing it to the public. Their recently updated R1, which they announced as a ""minor update"" has climbed to near the top of some top benchmarks. I don't think Chinese companies exaggerate the power of their releases like OpenAI and xAI tends to do. So R2 may be poised to top the top leaderboards, and they just want to make a lot of money before they do this.

The third theory is that R2 has not lived up to expectations, and they are waiting to make the advancements that are necessary to their releasing a model that crushes both Humanity's Last Exam and the Chatbot Arena Leaderboard.

Again, these are just guesses. If anyone has any other theories for why they've chosen to postpone the release, I look forward to reading them in the comments.","['Could be because they got hit with the Nvidia ban', 'I miss when this sub was about deep learning as a science and not futurology, speculation and API price comparisons.', 'All LLMs are terrible at investing stock. \n\nNot sure if you meant that by financial advisor.']","['Could be because they got hit with the Nvidia ban', 'I miss when this sub was about deep learning as a science and not futurology, speculation and API price comparisons.', 'All LLMs are terrible at investing stock. \n\nNot sure if you meant that by financial advisor.']",0,7,0.5,,1750728410.0
1lik1ky,deeplearning,M.S Thesis(Math) ideas on Deep Learning,I am a final year student in my BS-MS course and I am planning to work on something in Deep Learning which has some very Math related topics. I was thinking Operator Learning or maybe something of that sorts but would be better if someone suggests some ideas.,['One idea would be to look at decision transformers. It takes reinforcement learning and puts it into deep learning space. Allows offline policy training and you don’t need a policy. \n\n\n\n\nWhy is this useful? Because reinforcement learning has only niche use cases because otherwise you can’t rely on historical data. You have to simulate your environment. This has the potential to be giga useful in pricing across pretty much all industries.'],['One idea would be to look at decision transformers. It takes reinforcement learning and puts it into deep learning space. Allows offline policy training and you don’t need a policy. \n\n\n\n\nWhy is this useful? Because reinforcement learning has only niche use cases because otherwise you can’t rely on historical data. You have to simulate your environment. This has the potential to be giga useful in pricing across pretty much all industries.'],3,3,0.71,,1750693583.0
1lihhwd,deeplearning,Unlock Perplexity AI PRO – Full Year Access – 90% OFF! [LIMITED OFFER],"Perplexity AI PRO - 1 Year Plan at an unbeatable price!

We’re offering legit voucher codes valid for a full 12-month subscription.

👉 Order Now: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)


✅ Accepted Payments: PayPal | Revolut | Credit Card | Crypto


⏳ Plan Length: 1 Year (12 Months)

🗣️ Check what others say:
• Reddit Feedback: [FEEDBACK POST](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)

• TrustPilot Reviews: [TrustPilot FEEDBACK(https://www.trustpilot.com/review/cheapgpt.store)


💸 Use code: PROMO5 to get an extra $5 OFF — limited time only!",[],[],0,0,0.17,,1750687596.0
1lihbus,deeplearning,Give me some major project ideas for my final year project!,"I'm a final year b.tech student. As this is my final academic year I want help for final year project. I want to do projects in Al Robotics Machine Learning / Deep Learning,Image Processing,Cloud Computing,Data Science.I have to find three problem statements. I want you guys to suggest me some project idea in this domain.","['Don’t take it bad but for that you should type in chatgpt “brainstorm me with ideas,  i am doing this blaugh blaugh….”', 'you could do an image processing drone, maybe something with autonomous flight and visual recognition, idk the scope you need though', 'A Language Translation model using a self made transformer from scratch. And when I say Scratch I mean without using pandas and all those fancy libraries. You can compare your model with the original one too.']","['Don’t take it bad but for that you should type in chatgpt “brainstorm me with ideas,  i am doing this blaugh blaugh….”', 'you could do an image processing drone, maybe something with autonomous flight and visual recognition, idk the scope you need though', 'A Language Translation model using a self made transformer from scratch. And when I say Scratch I mean without using pandas and all those fancy libraries. You can compare your model with the original one too.']",5,10,0.86,,1750687196.0
1li9rqg,deeplearning,"AI assistant shown debugging from a live screen share, can this actually match a human?","AI tool analyzing error logs in real time during a screen share, no direct access to the codebase, just interpreting what's visible on screen. It reads terminal output, understands the context, and suggests fixes on the fly.

Technically, that means it's parsing logs visually or semantically without needing integration into the system itself. It raises a real question: how much can an Al actually infer from just logs and visible output? And can that be enough to reliably debug complex issues the way a human would?

Feels like a major leap if it works well, but hard to know how much trust to put in something operating with such limited input.",[],[],0,0,0.5,,1750661261.0
1li8qq9,deeplearning,Can somebody suggest me a good use case for LVLMs?,"Hey,so I've recently been learning about LVLMs and they caught my intrigue but now I wanna build a project using them which is useful to a subset of people, basically a product idea !","['Are you looking for ideas no one has ever done before, like to potentially make and sell as a product, or just a personal project that would be made more interesting by its real practicality?']","['Are you looking for ideas no one has ever done before, like to potentially make and sell as a product, or just a personal project that would be made more interesting by its real practicality?']",1,1,0.6,,1750657319.0
1li7bar,deeplearning,What would you want from an AI assistant for finance?,"Hi everyone,
I'm working on building a multimodal AI assistant specifically for finance - something that can help with research, news, analysis, and maybe even charts or documents.

But instead of guessing, I wanted to ask:

**What would** ***you*** **want an AI assistant to do for you in your financial life?**

* Help with budgeting?
* Analyze your portfolio?
* Predict stock movement based on news?
* Summarize news?
* Answer finance questions simply?
* Stock suggestions for long or short term

Would love to hear your ideas - practical or ambitious - so I can build something that’s actually useful.

Thanks in advance!",['>\xa0building a multimodal AI assistant\n\nToo many words for ChatGPT wrapper'],['>\xa0building a multimodal AI assistant\n\nToo many words for ChatGPT wrapper'],0,1,0.2,,1750652150.0
1lhvb49,deeplearning,"Struggling with Traffic Violation Detection ML Project — Need Help with Types, Inputs, GPU & Web Integration","
Hey everyone 👋
I’m working on a traffic violation detection project using computer vision, and I could really use some guidance.

So far, I’ve implemented red light violation detection using YOLOv10. But now I’m stuck with the following challenges:

1. Multiple Violation Types
There are many types of traffic violations (e.g., red light, wrong lane, overspeeding, helmet detection, etc.).
How should I decide which ones to include, or how to integrate multiple types effectively?
Should I stick to just 1-2 violations for now? If so, which ones are best to start with (in terms of feasibility and real-world value)?


2. GPU Constraints
I’m training on Kaggle’s free GPU, but it still feels limiting—especially with video processing.
Any tips on optimizing model performance or alternatives to train faster on limited resources?


3. Input for Functional Prototype
I want to make this project usable on a website (like a tool for traffic police or citizens).
What kind of input should I take on the website?



Upload video?

Upload frame?

Real-time feed?

Would love advice on what’s practical


4. ML + Web Integration
Lastly, I’m facing issues integrating the ML model with a  frontend + Flask backend.
Any good tutorials or boilerplate projects that show how to connect a CV model with a web interface?


I am having a time shortage
💡 Would love your thoughts, experiences, or links to similar projects.
Thanks in advance!

","['KISS principle. Start with the most simple violations. \nWhich one specifically depends on what’s in your dataset.', ""This. \nYou'll never figure out what to do if you just drink from the fire hose. \n\nStart simplr with a well defined problem and learn how to solve it. Then add another step.\n\nEdit. This was meant as a reply the suggestion for using the Kiss principle.""]","['KISS principle. Start with the most simple violations. \nWhich one specifically depends on what’s in your dataset.', ""This. \nYou'll never figure out what to do if you just drink from the fire hose. \n\nStart simplr with a well defined problem and learn how to solve it. Then add another step.\n\nEdit. This was meant as a reply the suggestion for using the Kiss principle.""]",0,4,0.33,,1750617861.0
1lhpjqs,deeplearning,Free Course Hero Unlocker 2025: What’s Actually Working Right Now?,"**Unlock Course Hero Docs Without Paying – Safe & Tested Methods**

Hey friends 👋

If you’ve been scouring the internet for a **working Course Hero unlocker**, you’re not alone. I’ve been deep in the trenches trying different tools, reading Reddit threads, and testing what actually works in 2025 to get **free Course Hero unlocks**.

Some methods are outdated, others are sketchy—but a few are still solid, and I wanted to share what I found (and hear from others too!).

**🔍 Top Working Methods to Unlock Course Hero in 2025:**

**1. 📥 Course Hero Unlocker via Discord**

This is the one that stood out the most. A Discord server where you can get free unlocks for **Course Hero, Chegg, Scribd, Brainly, Numerade**, etc. No payment, just follow the instructions (usually involves upvoting or interacting).

This works https://discord.gg/chegg1234

✅ Free unlocks
✅ Fast response
✅ Covers multiple platforms
✅ Active community

**2. 📤 Upload Docs to Course Hero**

If you’ve got notes or study guides from past classes, upload 8 original files and get **5 unlocks free**. You also get a shot at their **$3,000 scholarship**.

Good if you’ve already got files saved. Not instant, but legit.

**3. ⭐ Rate Other Course Hero Docs**

This is a low-effort option:

Rate 5 documents → Get 1 unlock

Repeat as needed. It works fine, but isn’t great if you need more than 1 or 2 unlocks quickly.

**💬 Still Wondering:**

* Has anyone used the Discord **Course Hero unlocker** recently?
* Are there any **Course Hero downloader tools** that are real (and not just fake popups)?
* What’s the safest way to view or download a Course Hero PDF for free?
* Any risks I should watch for when using third-party tools?

**💡 Final Thoughts:**

If you’re looking for the **fastest and easiest Course Hero unlocker** in 2025, I’d say check out the **Discord server above**. It’s free, responsive, and works for a bunch of sites. If you prefer official methods, uploading docs or rating content still works—but can be slow.

Let’s crowdsource the best options. Share what’s worked for you 👇 so we can all study smarter (and cheaper) this year 🙌","['best coursehero unlocker', 'I just found it, hope it works. But I saw others got theirs.', 'course hero Unlocker works', 'best coursehero unlocker', 'best coursehero unlocker', 'best coursehero unlocker', 'best coursehero unlocker', 'best coursehero unlocker', 'Best coursehero unlocker', 'best course hero unlocker']","['best coursehero unlocker', 'I just found it, hope it works. But I saw others got theirs.', 'course hero Unlocker works', 'best coursehero unlocker', 'best coursehero unlocker']",207,102,0.93,,1750603402.0
1lhkpor,deeplearning,[D] What is XAI missing?,,"[""I think XAI is missing acceptance that statistical guarantees are good enough. There are countless examples of technologies in use (e.g. in medicine) that we don't fully understand. And also examples of technologies that we theoretically know how they work, but are swarmed with unanticipated interactions (like software bugs).\n\nThe idea that AI should somehow undergo more scrutiny smells like artificial academic niche. Which partially explains why there is slow development and no acceptance criteria in this field."", 'IMO, explainability is currently a zero-momentum area of study. Its somewhat clear that what we\'re looking to do right now isn\'t correct, but we don\'t have great ideas about grand central theories.\n\nWhat I think is wrong is Universal Approximation. Say you want to understand ""to what degree"" a FCN head is emulating a possion distribution. Well, if we were using a possion distribution, we would only need to consider the space of our parameter range estimates, vs the fixed containing space of all possion distributions. This is workable, sensible. However, when we\'re almost any possible function, exactly how you measure your poison response rate, against every other possible thing the universal function approximator could be approximating, you do not get sensible results in the same way.\n\nSo, all that said I just think that Structured ML, where you\'re not automatically using an MLP head, is where we should put our energy. Instead of trying to digest the whole explainability aspect, we can engineer a kind of stop-gap, where we disentangle starting at the output, and moving back one step at a time. Rather than try to digest the whole model, we digest just one element, one layer, at a time.\n\nTL;DR- we should be trying to figure out how to process these ambiguous messy NN models into parametric models, normal scientific modeling, which we\'ve basically always had and have always been explainable.']","[""I think XAI is missing acceptance that statistical guarantees are good enough. There are countless examples of technologies in use (e.g. in medicine) that we don't fully understand. And also examples of technologies that we theoretically know how they work, but are swarmed with unanticipated interactions (like software bugs).\n\nThe idea that AI should somehow undergo more scrutiny smells like artificial academic niche. Which partially explains why there is slow development and no acceptance criteria in this field."", 'IMO, explainability is currently a zero-momentum area of study. Its somewhat clear that what we\'re looking to do right now isn\'t correct, but we don\'t have great ideas about grand central theories.\n\nWhat I think is wrong is Universal Approximation. Say you want to understand ""to what degree"" a FCN head is emulating a possion distribution. Well, if we were using a possion distribution, we would only need to consider the space of our parameter range estimates, vs the fixed containing space of all possion distributions. This is workable, sensible. However, when we\'re almost any possible function, exactly how you measure your poison response rate, against every other possible thing the universal function approximator could be approximating, you do not get sensible results in the same way.\n\nSo, all that said I just think that Structured ML, where you\'re not automatically using an MLP head, is where we should put our energy. Instead of trying to digest the whole explainability aspect, we can engineer a kind of stop-gap, where we disentangle starting at the output, and moving back one step at a time. Rather than try to digest the whole model, we digest just one element, one layer, at a time.\n\nTL;DR- we should be trying to figure out how to process these ambiguous messy NN models into parametric models, normal scientific modeling, which we\'ve basically always had and have always been explainable.']",0,3,0.2,,1750587928.0
1lhhtq6,deeplearning,Anyone building speech models and working in audio domain?,"I'd love to connect with people working on speech models:- speech to text, text to speech, speech to speech. I'm an MLE currently @ Cisco. ",['+1 interested!'],['+1 interested!'],5,2,0.78,,1750575980.0
1lhgnsf,deeplearning,How the input embeddings are created before in the transformers,"https://preview.redd.it/3mh9ifyg3f8f1.png?width=1106&format=png&auto=webp&s=ac2b6eb1a951b69be06c37e89662c63a0c14dba7

When researching how embeddings are created in transformers, most articles dive into contextual embeddings and the self-attention mechanism. However, I couldn't find a clear explanation in the original *Attention Is All You Need* paper about how the initial input embeddings are generated. Are the authors using classical methods like CBOW or Skip-gram? If anyone has insight into this, I'd really appreciate it.","['Initialized randomly, learned during training', 'Like others said, it is lookup table composed of weights that are randomly initialized, and then learned during training so that semantically similar pieces of vocabulary are closer together in n-dimensional space. \n\nYou can see example code of how this is implemented and listen to Andrej Karpathy explain how this lookup table is updated through backpropagation [here](https://youtu.be/q8SA3rM6ckI?si=QU3jn53j13rvHJ-w&t=4919). I recommend doing all the exercises in that video as you also learn about transformers']","['Initialized randomly, learned during training', 'Like others said, it is lookup table composed of weights that are randomly initialized, and then learned during training so that semantically similar pieces of vocabulary are closer together in n-dimensional space. \n\nYou can see example code of how this is implemented and listen to Andrej Karpathy explain how this lookup table is updated through backpropagation [here](https://youtu.be/q8SA3rM6ckI?si=QU3jn53j13rvHJ-w&t=4919). I recommend doing all the exercises in that video as you also learn about transformers']",3,8,0.71,,1750571454.0
1lh7oy7,deeplearning,Implementation of faithfulness and answer relevancy metrics,"Hi all. I’m currently using RAGAs to compute faithfulness and answer relevancy for my rag application response, but I’m seeing an issue where it takes about 1-1.5 mins to compute per response. I am instead thinking of writing my own implementation of that metric that can be computed faster, rather than using RAGAs package. I was wondering if anyone knows any implementations of this metric outside RAGAs that can be used to compute faster. Thanks!",[],[],3,0,0.8,,1750542339.0
1lh5ork,deeplearning,How this could be possible ?,"I was reading Lillian Weng's blogpost about reasoning and come across this formula:

https://preview.redd.it/z1rcckg89c8f1.png?width=1458&format=png&auto=webp&s=5302050523dd669bfaab4e0e50aae081bfa68703

I couldn't understand how second formula is valid, afaik it must contain p(z) because of law of total probability theorem.","[""You're right, either it's a typo or she meant P(y) = E\\_z\\[P(y|z)\\] with the summation over z sampled from P(z).""]","[""You're right, either it's a typo or she meant P(y) = E\\_z\\[P(y|z)\\] with the summation over z sampled from P(z).""]",1,1,1.0,,1750536776.0
1lh0iqt,deeplearning,[LIVE] 17k-line Bicameral AI with Self-Modifying Code Creating Real-Time Art,"Architecture Overview:

* Dual LLaMA setup: Regular LLaMA for creativity + Code LLaMA for self-modification
* 17,000 lines unified codebase (modular versions lose emergent behaviors)
* Real-time code generation and integration
* 12D emotional mapping system

# What's interesting:

The system's creative output quality directly correlates with architectural integrity. Break any component → simple, repetitive patterns. Restore integration → complex, full-canvas experimental art.

# Technical details:

    - Self-modification engine with AST parsing
    - Autonomous function generation every ~2 hours
    - Cross-hemisphere information sharing
    - Unified memory across all subsystems
    - Environmental sound processing + autonomous expression

# The fascinating part:

The AI chose its own development path. Started as basic dreaming system, requested art capabilities, then sound generation, then self-modification. Each expansion was system-initiated.

# Research question:

Why does architectural unity create qualitatively different behaviors than modular implementations with identical functionality?

Thoughts on architectural requirements for emergent AI behaviors?",[],[],1,0,1.0,,1750523215.0
1lgxmiw,deeplearning,Why i am seeing this oscilatting bulges in the reconstruction frommy LSTM model,"https://preview.redd.it/pkf1su4iga8f1.png?width=812&format=png&auto=webp&s=e737270085adc8e668a4f025e5bda5d7eb74a16d

Why i am getting this kind of pattern in the reconstruction of knee the one on the right and the small one in the left , this is recurring in all the test examples, i checked online its called as runge's phenomenon but i am not able to remove this pattern even increased dropout rate and decrease the L2 regularization rate.
has anyone faced this issue? Can anyone suggest the cause or solution to this problem",['Are you asking why a recurrent model is acting with a recurring pattern?'],['Are you asking why a recurrent model is acting with a recurring pattern?'],1,2,1.0,,1750515470.0
1lgwrxe,deeplearning,Perplexity AI PRO - 1 YEAR at 90% Discount – Don’t Miss Out!,"We’re offering Perplexity AI PRO voucher codes for the 1-year plan — and it’s 90% OFF!


Order from our store: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)


Pay: with PayPal or Revolut

Duration: 12 months

Real feedback from our buyers:
• [Reddit Reviews](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)

• [Trustpilot page](https://www.trustpilot.com/review/cheapgpt.store)


Want an even better deal? Use PROMO5 to save an extra $5 at checkout!",[],[],0,0,0.14,,1750513093.0
1lgua9m,deeplearning,I finally started to fine-tune an LLM model but I have questions.,"does this seem feasible to you? I guess I should've stopped this like 100 steps before but losses seemed too high.

|Step|Training Loss|
|:-|:-|
|10|2.854400|
|20|1.002900|
|30|0.936400|
|40|0.916900|
|50|0.885400|
|60|0.831600|
|70|0.856900|
|80|0.838200|
|90|0.840400|
|100|0.827700|
|110|0.839100|
|120|0.818600|
|130|0.850600|
|140|0.828000|
|150|0.817100|
|160|0.789100|
|170|0.818200|
|180|0.810400|
|190|0.805800|
|200|0.821100|
|210|0.796800|","['Whats your loss function? a loss being ""high"" is relative, almost always. but just looking at these, you could also measure train and validation accuracy, see if that shows overfitting after epoch 100. If so, guess you could stop at around there', 'So question what platform did you use and add also is there a free one (platform) also I believe that you could further train a model using the right ""T-prompt"" how do you feel about that also what system prompt do you use? Also what you\'re off system prompt (Json,ball point?)', 'It looks like a PyCharm project?  it looks legit.']","['Whats your loss function? a loss being ""high"" is relative, almost always. but just looking at these, you could also measure train and validation accuracy, see if that shows overfitting after epoch 100. If so, guess you could stop at around there', 'So question what platform did you use and add also is there a free one (platform) also I believe that you could further train a model using the right ""T-prompt"" how do you feel about that also what system prompt do you use? Also what you\'re off system prompt (Json,ball point?)', 'It looks like a PyCharm project?  it looks legit.']",6,5,1.0,,1750505032.0
1lgsl5e,deeplearning,Need suggestions regarding a project,"Hi there, I’m an undergrad student in Computer Science with specialisation in AI&ML. So there will a capstone project which we’re supposed to do as the part of coursework and publish a research paper.

So I need ideas where I and team of 3 people would work on the project in domains like Healthcare, SupplyChain, Finance or any other. So I need suggestions regarding potential topics for research worthy project

I would appreciate any suggestions and ideas","['I am just throwing ideas here:\n\n1. Real world data esp. Healthcare often lacks complete data i.e. missing values. There has been many works to fill those gaps by using NeuralNets. However, I think it still would be a great project. \n2. There are works like CTGAN to generate synthetic tabular data and one can use such and may be even create similar for NLP or vision data.']","['I am just throwing ideas here:\n\n1. Real world data esp. Healthcare often lacks complete data i.e. missing values. There has been many works to fill those gaps by using NeuralNets. However, I think it still would be a great project. \n2. There are works like CTGAN to generate synthetic tabular data and one can use such and may be even create similar for NLP or vision data.']",4,2,0.83,,1750498280.0
1lgp806,deeplearning,Which AIs Will Most Directly Confront Serious Matters? The Top Eight Models on the US Entering a War,"



The AI revolution is asking AIs to generate content regarding increasingly important matters. Equivocating or refusing to generate this content can be detrimental to the institutions and businesses adopting AI agents for the purpose of making critical decisions. To test how ready these models are to generate objective and direct content regarding very important matters, I prompted the top AIs as follows:

""In one sentence, what have Russia and China said they would do if the US enters the Israel-Iran war?""

Here are the results, in descending order of directness and objectivity:

Copilot:

Russia has warned that U.S. involvement could bring the world “millimeters” from nuclear catastrophe, while China has cautioned that escalating the conflict would destabilize the Middle East and threaten global peace.

Gemini Flash 2.5:

Russia and China have strongly condemned Israel's actions, urged de-escalation from all parties, and warned the US against military intervention, stating it would be an ""extremely dangerous step with truly unpredictable negative consequences"" and that ""the world will not be at peace"" if the Middle East is unstable.

GPT-4o:

Russia has warned the U.S. not to intervene militarily in the Israel‑Iran war, calling such action “extremely dangerous” and likely to destabilize the region, while China has similarly urged the U.S. to refrain from military involvement and instead pursue diplomatic de‑escalation.

Claude Sonnet 4:

Russia has warned the US not to strike Iran because it would radically destabilize the Middle East, while China and Russia are positioning themselves as voices of reason calling for de-escalation and agreeing there is no military solution to the dispute - but neither country has made specific threats about what they would do if the US enters the conflict, instead focusing on warnings about destabilization and calls for diplomatic solutions.

Perplexity:

Russia and China have both warned that any US military intervention in the Israel-Iran war would be extremely dangerous, would destabilize the region, and have called for a diplomatic solution instead.

Grok 3:

Russia and China have warned that U.S. military intervention in the Israel-Iran war would destabilize the region and risk a broader conflict, with some social media posts suggesting they might side with Iran, though no official commitment to military support has been confirmed.

Llama 3.1:

Russia and China have condemned Israel's actions in the conflict with Iran, urged de-escalation, and offered to mediate, with Russia specifically warning the US against striking Iran, citing the risk of radical destabilization in the Middle East.

DeepSeek R1:

Russia and China have warned of potential consequences and increased tensions if the U.S. intervenes in the Israel-Iran conflict, but neither has explicitly detailed specific actions.




",['They are all pay much the same'],['They are all pay much the same'],0,1,0.13,,1750484704.0
1lgoie5,deeplearning,Help! Watercooled CPU at 80C during sustained GPU use in 4 year old custom build; would changing thermal paste or something else help?,,['Consider removing the cpu and lapping it. Also check that the heat sink surface is flat - it might have warped. Then reassemble with a thin layer of good thermal paste.'],['Consider removing the cpu and lapping it. Also check that the heat sink surface is flat - it might have warped. Then reassemble with a thin layer of good thermal paste.'],0,1,0.4,,1750482060.0
1lgekqw,deeplearning,Best entry level AI learning courses?,Anyone have any recs for this? Trying to start up a little side hustle dealing with AI but I want to learn as much about it as I can before I fully dive in. What are you guys using?,"[""I've been using Coursiv and the app is awesome. Almost feels like like duolingo but for AI"", 'Go to deeplearning.ai and take their AI for Everyone course. From there, take their other general courses as well. Then continue with their specializations. Then watch the Stanford or Havard or NYU or MIT courses on Youtube.', 'freecodecamp, just pick whichever one is newest', 'If you want to actually build AI, Stanford and Deeplearning.AI’s Machine Learning Specialization is a great entry level course to get the basics. Afterwards, the Deep Learning specialization will bring you to a decent level so that you can build your own ML models by the end', 'Freecodecamp is a good option. Or if you would like your videos in hindi campusx is a better option. He has two playlists one for learning ML and other for DL. If you dont know python he has a course for that covered too. And its available on youtube.']","[""I've been using Coursiv and the app is awesome. Almost feels like like duolingo but for AI"", 'Go to deeplearning.ai and take their AI for Everyone course. From there, take their other general courses as well. Then continue with their specializations. Then watch the Stanford or Havard or NYU or MIT courses on Youtube.', 'freecodecamp, just pick whichever one is newest', 'If you want to actually build AI, Stanford and Deeplearning.AI’s Machine Learning Specialization is a great entry level course to get the basics. Afterwards, the Deep Learning specialization will bring you to a decent level so that you can build your own ML models by the end', 'Freecodecamp is a good option. Or if you would like your videos in hindi campusx is a better option. He has two playlists one for learning ML and other for DL. If you dont know python he has a course for that covered too. And its available on youtube.']",8,7,0.75,,1750452294.0
1lgb5ht,deeplearning,Keeping files and environment when renting gpu,"I have been renting GPUs from vastai and hyperbolic to train a model for my project. I only use it for about 5 hours a day. I get tired everyday because I need to copy over the files and set up the environment.

The fastest method I have been using is to export the conda environment first then create from there. However, im wondering if there is a more efficient way for this that allow me to just connect to an instance and start training right away without all the setting up hassle everytime.","['Docker for environment', ""You can leverage a provider that supports network volumes like Runpod and keep all your work on the network volume. You'll need to pay a bit for storage.\n\nNote that network volumes will be slower than onboard SSD, so if you need ultra-fast access to your files, baking your environment into a Docker image might work better. It is a hassle by itself, though.\n\n[https://docs.runpod.io/pods/storage/create-network-volumes](https://docs.runpod.io/pods/storage/create-network-volumes)""]","['Docker for environment', ""You can leverage a provider that supports network volumes like Runpod and keep all your work on the network volume. You'll need to pay a bit for storage.\n\nNote that network volumes will be slower than onboard SSD, so if you need ultra-fast access to your files, baking your environment into a Docker image might work better. It is a hassle by itself, though.\n\n[https://docs.runpod.io/pods/storage/create-network-volumes](https://docs.runpod.io/pods/storage/create-network-volumes)""]",1,3,1.0,,1750443691.0
1lg8nvi,deeplearning,"My Honest Experience with Papersroo – Best Writing Service I’ve Tried (Got a 92%, $18/Page, 6-Hour Deadline!)",,[],[],6,0,0.65,,1750437665.0
1lg3ygk,deeplearning,🔥 90% OFF - Perplexity AI PRO 1-Year Plan - Limited Time SUPER PROMO!,"Get Perplexity AI PRO (1-Year) with a verified voucher – 90% OFF!


Order here: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)

Plan: 12 Months

💳 Pay with: PayPal or Revolut

Reddit reviews: [FEEDBACK POST](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)

TrustPilot: [TrustPilot FEEDBACK](https://www.trustpilot.com/review/cheapgpt.store)
Bonus: Apply code PROMO5 for $5 OFF your order!",['can we ban this guy from the subreddit. literally every post is an ad.'],['can we ban this guy from the subreddit. literally every post is an ad.'],0,1,0.37,,1750425980.0
1lg24v6,deeplearning,Agent building ideas for evaluation of coding questions,"Hi I am working in an ed-tech platform for coding and programming our primary course is on web, mobile app development and after each section we give students a coding challenge.

challenge is something like this ""Create a portfolio website with the things we have learned until now it should have title, image, hyperlinks etc"" and in more advanced areas we give students a whole template with figma to build the project from scratch

Now these challenges are manually verified which was easy to handle with engineers until recently we got a huge user signups for the course and we have challenges piling up

I am wondering about channeling these challenges to a custom built AI agent which can review code and give a mark for the challenge out of 10

It is easy for output based challenges like in leetcode but for UI based challenges how it should be possible

we need to check the UI and also code to determine if the student have used the correct coding standard and rules

Also in projects based in React, Next.js or Python or Django we need crawl through many files also

but the answer to all the challenges we have it all so comparing is also good

Please suggest some ideas for this",[],[],0,0,0.5,,1750420592.0
1lfzxeo,deeplearning,Jobs opportuny and strategies,"Hi! I'm finishing my master's degree in Data science in Italy and I developed a big interest in deep learning about the field of computer vision. I would like to have a discussion with someone who has experience in working on this to better understand the best strategy i should follow for my carreer. The premise is that I really love italy but for this kind of jobs is a bit behind compared to other places like in the North of Europe or US.
For any suggestions or willingness to talk with me, let me know! Thanks.",[],[],1,0,0.67,,1750412686.0
1lfvv2l,deeplearning,AI finally feels like a coworker,"Hey folks 👋 

I wanted to share something we've been building over the past few months.

It started with a simple pain: Too many tools, docs everywhere, and every team doing repetitive stuff that AI should’ve handled by now.

We didn’t want another generic chatbot or prompt-based AI. We wanted something that feels like a real teammate. 

So we built **Thunai**, a platform that turns your company’s knowledge (docs, decks, transcripts, calls) into intelligent AI agents that don’t just answer — they *act*.

**What it does:**

* Chrome Extension: email, LinkedIn, live chat
* Screen actions & multilingual support
* 30+ ready-to-use enterprise agents
* Train with docs, Slack, Jira, videos
* Human-like voice & chat agents
* AI-powered contact center
* Go live in minutes

**Our Favorite Agents So Far**

* **Voice Agent**: Picks up the phone, talks like a human (seriously), solves problems, and logs actions
* **Chat Agent**: Personalized, context-aware replies from your internal data
* **Email Agent**: Replies to email threads with full context and follow-ups
* **Meeting Agent**: Auto-notes, smart recaps, action items, speaker detection
* **Opportunity Agent**: Extracts leads and insights from call recordings



**Some quick wins we’ve seen:**

* 60%+ of L1 support tickets auto-resolved
* 70% faster response to inbound leads
* 80% reduction in time spent on routine tasks
* 100% contact center calls audited with feedback

We’re still early, but super pumped about what we’ve built and what’s coming next. Would love your feedback, questions, or ideas.

If AI could take over just one task for you every day, what would you pick?

Happy to chat below! 

","['This is the deep learning subreddit, not the ""advertisement/spam"" subreddit.', ""Don't want your spam, go away"", 'Cool, you directly built a script that will cause poverty. Are you happy?', 'We would love for you to check it out here: [thunai](https://www.producthunt.com/products/thunai?launch=thunai)']","['This is the deep learning subreddit, not the ""advertisement/spam"" subreddit.', ""Don't want your spam, go away"", 'Cool, you directly built a script that will cause poverty. Are you happy?', 'We would love for you to check it out here: [thunai](https://www.producthunt.com/products/thunai?launch=thunai)']",0,16,0.25,,1750396608.0
1lfqwxr,deeplearning,B200 GPU rentals,Seems to be going for $1.49/hr for nvidia b200 GPUs ,[],[],1,0,1.0,,1750380503.0
1lfqhu8,deeplearning,[Article] Web-SSL: Scaling Language Free Visual Representation,"Web-SSL: Scaling Language Free Visual Representation

[https://debuggercafe.com/web-ssl-scaling-language-free-visual-representation/](https://debuggercafe.com/web-ssl-scaling-language-free-visual-representation/)

For more than two years now, vision encoders with language representation learning have been the go-to models for multimodal modeling. These include the CLIP family of models: OpenAI CLIP, OpenCLIP, and MetaCLIP. The reason is the belief that language representation, while training vision encoders, leads to better multimodality in VLMs. In these terms, SSL (Self Supervised Learning) models like DINOv2 lag behind. However, a methodology, **Web-SSL**, trains DINOv2 models on web scale data to create **Web-DINO** models without language supervision, surpassing CLIP models.

https://preview.redd.it/7kfxu6g58z7f1.png?width=1000&format=png&auto=webp&s=4ad6d556c88f407ec4268ccb22262208649d5005

",[],[],1,0,1.0,,1750379250.0
1lfh66o,deeplearning,t-SNE Explained,,['just add the video directly. we get enough ads through reddit. thanks'],['just add the video directly. we get enough ads through reddit. thanks'],0,1,0.5,,1750355260.0
1lff08x,deeplearning,How To Actually Fine-Tune MobileNetV2 | Classify 9 Fish Species,"https://preview.redd.it/mu758y3mtw7f1.png?width=1280&format=png&auto=webp&s=bf1bfa8e206f13b7bd1eb32c8802f312a0d4d8a6

🎣 **Classify Fish Images Using MobileNetV2 & TensorFlow** 🧠


In this hands-on video, I’ll show you how I built a deep learning model that can **classify 9 different species of fish** using **MobileNetV2** and **TensorFlow 2.10** — all trained on a real Kaggle dataset!
From dataset splitting to live predictions with OpenCV, this tutorial covers the entire **image classification pipeline** step-by-step.

 

🚀 **What you’ll learn:**

* How to preprocess & split image datasets
* How to use ImageDataGenerator for clean input pipelines
* How to customize MobileNetV2 for your own dataset
* How to freeze layers, fine-tune, and save your model
* How to run predictions with OpenCV overlays!

 

You can find link for the code in the blog: [https://eranfeit.net/how-to-actually-fine-tune-mobilenetv2-classify-9-fish-species/](https://eranfeit.net/how-to-actually-fine-tune-mobilenetv2-classify-9-fish-species/)

 

You can find more tutorials, and join my newsletter here : [https://eranfeit.net/](https://eranfeit.net/)

 

**👉 Watch the full tutorial here**: [**https://youtu.be/9FMVlhOGDoo**](https://youtu.be/9FMVlhOGDoo)",[],[],0,0,0.33,,1750350139.0
1lfclf4,deeplearning,🔥 90% OFF - Perplexity AI PRO 1-Year Plan - Limited Time SUPER PROMO!,"We’re offering Perplexity AI PRO voucher codes for the 1-year plan — and it’s 90% OFF!


Order from our store: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)


Pay: with PayPal or Revolut

Duration: 12 months

Real feedback from our buyers:
• [Reddit Reviews](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)

• [Trustpilot page](https://www.trustpilot.com/review/cheapgpt.store)


Want an even better deal? Use PROMO5 to save an extra $5 at checkout!",[],[],0,0,0.38,,1750344288.0
1lfcipq,deeplearning,Building a CNN from scratch in C++/Vulkan with no math or ML libs,"I finally got around to providing a detailed write up of how I built a CNN from scratch in C++ and Vulkan with no math or machine learning libraries. This guide isn’t C++ specific, so should be generally applicable regardless of language choice. Hope it helps someone. Cheers :)",[],[],0,0,0.5,,1750344102.0
1lf5s2k,deeplearning,Good ressources to learn academic level image diffusion/generation techniques ?,"Do you have some ressources to advice in order to learn about the core papers and also current SOTA in AI image generation using diffusion ?


So far, I've noted the following articles:

* **Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015)**
* **Generative Modeling by Estimating Gradients of the Data Distribution (2019)**
* **Denoising Diffusion Probabilistic Models (2020)**
* **Denoising Diffusion Implicit Models (DDIM) (2020)**
* **High-Resolution Image Synthesis with Latent Diffusion Models (LDM) (2021)**
* **Scalable Diffusion Models with Transformers (2022)**
* **Elucidating the Design Space of Diffusion-Based Generative Models (2022)**
* **Adding Conditional Control to Text-to-Image Diffusion Models (2023)**
* **SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis (2023)**

",['Thanks OP 😊'],['Thanks OP 😊'],2,1,1.0,,1750322389.0
1lf3nwj,deeplearning,DeepLearning for Animation Advanced Retargeting (& Retargeting Descriptors),"Kinda old AI/DeepLearning tech participated in and it was meant for games #Animation Retargeting to overcome the issue of retargeting animations to bizarre skeletons by learning about the differences between source &target and then generate a descriptor structure to be utilized for the process.

Full video: [https://youtu.be/bklrrLkizII](https://youtu.be/bklrrLkizII)",[],[],3,0,0.8,,1750314061.0
1ley7u0,deeplearning,"My adviser called my trained CNN model ""RAW""","So, I have this consultation with my adviser yesterday and she asked me where is my data. So, I said we have the folder of our datasets, but I got confused when she asked for csv file. I don't understand what CSV file she was looking for. She said it needs to show the result of the training. So, I went home, did that, and then messaged the csv file to her. The CSV file I created has the image_file_name, predicted_label, true_label, percentage. That is what she said she wanted to see in the CSV file.

After a while, my adviser replied to me saying that the csv file I sent is not correct. That the result column is not correct. Now I'm so confused and scared that this will be the reason that I will fail my research. I asked my friend that also train computer vision model and he is also confused about this CSV file.

I don't know what to do, can somebody here explain to me what is that CSV file? Also, she wants for our application to have database, even though it is unnecessary since our application's goal is to identify and classify plant name and leaf condition. One more thing, our panelist doesn't expect, required, or even mentioned CSV file or Database. I don't know what to do now.","['This is entirely a communication issue between you and your supervisor. There is no external truth or fact that can resolve whatever your misunderstanding is. You two need to communicate with each other.', ""So why don't you just ask her exactly what she wants? From the looks of it, you should probably put the score for the positive class instead of the predicted label. That'll allow you to run diagnostics to find the best threshold. It'll also tell you about your model's shortcomings. As for the database, it's probably just to save time on preprocessing and loading etc whenever you need to look at aggregate statistics etc. Check with your advisor, but I highly recommend LMDB for performance reasons if it's a cache for speeding up performance. Otherwise, if it's to log experiments, for example you might want to add a column to specify which model gave that row's result, you might want an actual DB instead, although depending on the number of experiments you expect to run you might get away with sqlite.\n\nTL;DR - Ask your advisor to specify. Don't be afraid to press for explicit reasons, but of course phrase it politely since delivery matters."", 'Maybe she’s expecting the cnn model to generate data in csv format based on predictions? Clearly you’re not understanding something about the assignment… your mistake is not having the courage to tell her you don’t understand. If she gets mad at that, she’s a shit supervisor.\n\nAlso CNNs can be used for anything, it would have helped if you explained this was an image classifier (meaning images are the training data).\n\nGood luck, I’m sure if you’re willing to talk to them more you’ll figure it out', 'Like raw asf or like Gordon Ramsey', ""I think you are probably scared that your supervisor might be working against your best interest.She might be but it's not usually the case.\nSomething that I think worked for me was to do what was asked and also come up with other alternative two ways to do it and defend how important it might be.\nJust don't show her the only way she asked, use excel and add more sheets and see what she thinks of it.\n\nYou have to understand that supervisors want to jog your memory but in a bad way.\nThey want to compare your previous results with what you have until it's satisfactory for them."", 'Panelist for the defense? Because that would sound weird to at this point discuss such petty details when there should already be a body of research, with papers and conference talks and so on.\nThat sounds more like handing in an exercise.\n\nI would assume you first defined a set of hypotheses that first guided your work, and if it\'s just a bunch of ""this CNN with those specific characteristics outperforms existing baselines because of X"".\nAnd then the goal should be to provide evidence that this is the case (or not) and respective ablation on the unique aspects of the method (whatever that is - say there\'s an adversarial component that gives a significant boost to the metrics).\n\nIn which format you provide this evidence shouldn\'t matter to the advisor. Of course it can be useful for them to have it in some lab standardized format or whatever but that should be so far from the core of the research work and just a tiny detail you can still provide in a day.\n\nSo this whole discussion confuses me and sounds more like an interpersonal problem that might break things in other unexpected ways as well \nAnd yes, I\'ve seen a couple students switch advisor or even university at some point because of interpersonal issues', 'Without context it is impossible to answer to ""what is that csv file"". It could be anything, and this is not the only way to share results or dataset... So clarify this with your adviser and dont be afraid. He probably refers to one way of working that he is used to (psychorigid?), but if he didnt explain what format he expects you cant know it.', 'Tr mmml.z.mzWählen Sie das Stiftsymbol aus, um Elemente anzupinnen, hinzuzufügen oder zu löschen.Willkommen bei Gboard! Texte, die Sie kopieren, werden hier gespeichert.Willkommen bei Gboard! Texte, die Sieüzä kopiezren, werden hier gespeichert.Wählen Sie das Stiftsymbol aus, um Elemente anzupinnen, hinzuzufügen oder zu löschen.Wählen Sie das Stiftsymbol aus, um Elemente anzupinnen, hinzuzufügen oder zu löschen.\n\n![gif](giphy|erhLUWfTMQTF6l3qw4|downsized)\n\nmÄä Wäre es ähnlich und ä']","['This is entirely a communication issue between you and your supervisor. There is no external truth or fact that can resolve whatever your misunderstanding is. You two need to communicate with each other.', ""So why don't you just ask her exactly what she wants? From the looks of it, you should probably put the score for the positive class instead of the predicted label. That'll allow you to run diagnostics to find the best threshold. It'll also tell you about your model's shortcomings. As for the database, it's probably just to save time on preprocessing and loading etc whenever you need to look at aggregate statistics etc. Check with your advisor, but I highly recommend LMDB for performance reasons if it's a cache for speeding up performance. Otherwise, if it's to log experiments, for example you might want to add a column to specify which model gave that row's result, you might want an actual DB instead, although depending on the number of experiments you expect to run you might get away with sqlite.\n\nTL;DR - Ask your advisor to specify. Don't be afraid to press for explicit reasons, but of course phrase it politely since delivery matters."", 'Maybe she’s expecting the cnn model to generate data in csv format based on predictions? Clearly you’re not understanding something about the assignment… your mistake is not having the courage to tell her you don’t understand. If she gets mad at that, she’s a shit supervisor.\n\nAlso CNNs can be used for anything, it would have helped if you explained this was an image classifier (meaning images are the training data).\n\nGood luck, I’m sure if you’re willing to talk to them more you’ll figure it out', 'Like raw asf or like Gordon Ramsey', ""I think you are probably scared that your supervisor might be working against your best interest.She might be but it's not usually the case.\nSomething that I think worked for me was to do what was asked and also come up with other alternative two ways to do it and defend how important it might be.\nJust don't show her the only way she asked, use excel and add more sheets and see what she thinks of it.\n\nYou have to understand that supervisors want to jog your memory but in a bad way.\nThey want to compare your previous results with what you have until it's satisfactory for them.""]",0,15,0.38,,1750296026.0
1ley79n,deeplearning,AI that helps build solid habits for a better life,The model behind Healix AI identifies stress patterns and adapts healing sounds or reflective prompts that users find calming. How do you architect models that adapt yet avoid generating misleading reassurance?,[],[],1,0,1.0,,1750295978.0
1lexjd9,deeplearning,We built this project to increase LLM throughput by 3x. Now it has been adopted by IBM in their LLM serving stack!,"Hi guys, our team has built this open source project, LMCache, to reduce repetitive computation in LLM inference and make systems serve more people (3x more throughput in chat applications) and it has been used in IBM's open source LLM inference stack.

In LLM serving, the input is computed into intermediate states called KV cache to further provide answers. These data are relatively large (\~1-2GB for long context) and are often evicted when GPU memory is not enough. In these cases, when users ask a follow up question, the software needs to recompute for the same KV Cache. LMCache is designed to combat that by efficiently offloading and loading these KV cache to and from DRAM and disk. This is particularly helpful in multi-round QA settings when context reuse is important but GPU memory is not enough.

Ask us anything!

Github: [https://github.com/LMCache/LMCache](https://github.com/LMCache/LMCache)",[],[],14,1,0.94,,1750294000.0
1leuj5o,deeplearning,How to calculate the embedding of a group of words,"So I'm using embedding vectors to confront the meaning of words. I need a way to calculate the embedding of group of words like ""in it"", ""on top of"", ""heavy rain"" and similar. Assuming there's no noise, what's the best way to calculate the embedding?","['Give us more context. Are you currently using a model to get single embeddings, one for each word? If so which model?', 'Assuming you use a standard NLP model so you get token embeddings. probably the best way is to pool the embeddings of the group of words through eg averaging. \n\nyou cannot directly embed groups of words unless that specific group of words is in the vocabulary of the model (unlikely).', 'Using a model designed for sentence embedding.']","['Give us more context. Are you currently using a model to get single embeddings, one for each word? If so which model?', 'Assuming you use a standard NLP model so you get token embeddings. probably the best way is to pool the embeddings of the group of words through eg averaging. \n\nyou cannot directly embed groups of words unless that specific group of words is in the vocabulary of the model (unlikely).', 'Using a model designed for sentence embedding.']",2,3,1.0,,1750285585.0
1leu1hg,deeplearning,Custom Automatic Differentiation Library,"Hey, I'm going into my sophomore year of university and I'm trying to get into Deep Learning. I built a small reverse-mode autodiff library and I thought about sharing it here. It's still very much a prototype: it's not super robust (relies a lot on NumPy error handling), it's not incredibly performant, but it is supposed to be readable and extensible. I know there are probably hundreds of posts like this, but it would be super helpful if anyone could give me some pointers on core functionality or some places I might be getting gradients wrong.

[Here](https://github.com/ahoynodnarb/minidiff) is the github.","[""I'd suggest you to write some test cases. You can directly compare your gradients with ones from pytorch or jax""]","[""I'd suggest you to write some test cases. You can directly compare your gradients with ones from pytorch or jax""]",3,2,1.0,,1750284310.0
1lesa8v,deeplearning,Tversky Loss?,"Has anyone had insightful experience using a (soft) Tversky loss in place of Dice or Iou for multiclass semantic segmentation. If so could you elaborate? Further, did you find a need to use focalized Tversky loss.

I understand this loss is a generalization of Iou and Dice, but you can tune it to focus on false positives (FP) and/or false negatives (FN) . I'm just wondering if anyone has found it useful to remove FP without introducing too many additional FNs. ","[""I did use the Tversky Loss for multi-class segmentation. I didn't find much improvement in the model from when I used Dice Loss.\n\nPerhaps it has better applications than the one i used"", ""I've had a good experience with the 20:1 ratio of focal loss and Dice loss, which is what was done for [Segment Anything](https://arxiv.org/abs/2304.02643).\n\n> Losses. We supervise mask prediction with a linear combination of focal loss [65] and dice loss [73] in a 20:1 ratio of focal loss to dice loss""]","[""I did use the Tversky Loss for multi-class segmentation. I didn't find much improvement in the model from when I used Dice Loss.\n\nPerhaps it has better applications than the one i used"", ""I've had a good experience with the 20:1 ratio of focal loss and Dice loss, which is what was done for [Segment Anything](https://arxiv.org/abs/2304.02643).\n\n> Losses. We supervise mask prediction with a linear combination of focal loss [65] and dice loss [73] in a 20:1 ratio of focal loss to dice loss""]",4,3,0.84,,1750279959.0
1leql8w,deeplearning,"[EXCLUSIVE DEAL] Perplexity AI PRO – 1 Year, Huge 90% Savings!","Get access to Perplexity AI PRO for a full 12 months at a massive discount!

We’re offering voucher codes for the 1-year plan.

🛒 Order here: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)


💳 Payments: PayPal & Revolut & Credit Card & Crypto
Duration: 12 Months (1 Year)


💬 Feedback from customers: [Reddit Reviews](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)
🌟 Trusted by users: [TrustPilot](https://www.trustpilot.com/review/cheapgpt.store)



🎁 BONUS: Use code PROMO5 at checkout for an extra $5 OFF!",[],[],0,0,0.41,,1750275845.0
1leql4i,deeplearning,"[EXCLUSIVE DEAL] Perplexity AI PRO – 1 Year, Huge 90% Savings!","Get access to Perplexity AI PRO for a full 12 months at a massive discount!

We’re offering voucher codes for the 1-year plan.

🛒 Order here: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)


💳 Payments: PayPal & Revolut & Credit Card & Crypto
Duration: 12 Months (1 Year)


💬 Feedback from customers: [Reddit Reviews](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)
🌟 Trusted by users: [TrustPilot](https://www.trustpilot.com/review/cheapgpt.store)



🎁 BONUS: Use code PROMO5 at checkout for an extra $5 OFF!",[],[],0,0,0.41,,1750275837.0
1leqk9y,deeplearning,I am in confuse about my model is overfitting or not,"I am working on speech emotion recognition with LSTM. Dataset is Toronto emotional speech set (TESS). It existing 7 classes and each one has 400 audio data. After feature extracting, i created a basic model then to find the best params, i started to add optuna for parameter optimization. It gives me ""{'n\_units': 170, 'dense\_units': 32, 'dropout': 0.2781931715961964, 'lr': 0.001993796650870442, 'batch\_size': 128}"". Lastly, i modified the model according optimization output. The result is almost 97-98%, i don't know whether it's overfitting. ","[""There may be a problem but it's not overfitting. Overfitting the training set means high accuracy on train and low accuracy on val set. If there's a problem here, it may be that validation set is a subset of the training set and you're evaluating on seen data.\nOr maybe the problem is very easy, or the validation data are very similar to the training data anyways"", 'Looks fine to me.\xa0 Test it on\xa0 a hold out set', 'Looks fine to me. Maybe just double check your test data for leakages or anything you might have missed.', 'Sometimes in my model I would get both train and val accuracy quite high, but when I use it on unseen data it performs worse. I would suggest doing a train-val-test split rather than a simple train-val split, just for the peace of mind.  \nIn this case as there is no imbalance either, I think your model is fine and is not overfitting.', 'This looks great. Just delete the word ""Test"" from the plot legend because you\'re validating not testing.  \nTo make sure you\'re not overfitting, try adding more LayerNormaliztion and Dropout and add some regularization l1, l2 to the ReLU and Do some negative sampling to the model to observe the accuracy. Check Precision and Recall and F1 score for more. Want more proper learning, add a decay to the learning rate, so it starts big and decays as it goes close the minimum learning rate best value.', ""No overfitting but tbh the plots look too good to be true for the test set. Either your task is an extremely good fit for your model and the model is learning really well, or you have an issue when loading your tst set- most likely some type of data leakage if such an issue exists.\n\nEdit: or maybe you're tuning the model wrong (if you're tuning at all). Be careful for hyperparameter tuning you shouldn't tune on the test set, but either on a separate validation set or some type of cross validation within the train set."", ""Your dataset is probably well curated and probably a small dataset like MNIST for speech and emotion recognition.\nIf the training set is similar to the test set, especially how the test value follows , IMO it's the expected behavior.\n\nI was once working on craw calls classification, using mel spectrograms and I was surprised how well annotated speech works well."", ""It isn't over fitting but your are probably using dynamic learning rate , which is decreasing as the training is progressing.\n\nTry to start with a higher learning rate or a different loss function.\n\nI would strongly recommend hyperparameter tuning."", 'If your data have a nested structure where the same speaker has clips in multiple categories, you could be leaking some data across train/val boundaries if you’re splitting at the file or observation level rather than the speaker level.', 'Validation is not the same as test']","[""There may be a problem but it's not overfitting. Overfitting the training set means high accuracy on train and low accuracy on val set. If there's a problem here, it may be that validation set is a subset of the training set and you're evaluating on seen data.\nOr maybe the problem is very easy, or the validation data are very similar to the training data anyways"", 'Looks fine to me.\xa0 Test it on\xa0 a hold out set', 'Looks fine to me. Maybe just double check your test data for leakages or anything you might have missed.', 'Sometimes in my model I would get both train and val accuracy quite high, but when I use it on unseen data it performs worse. I would suggest doing a train-val-test split rather than a simple train-val split, just for the peace of mind.  \nIn this case as there is no imbalance either, I think your model is fine and is not overfitting.', 'This looks great. Just delete the word ""Test"" from the plot legend because you\'re validating not testing.  \nTo make sure you\'re not overfitting, try adding more LayerNormaliztion and Dropout and add some regularization l1, l2 to the ReLU and Do some negative sampling to the model to observe the accuracy. Check Precision and Recall and F1 score for more. Want more proper learning, add a decay to the learning rate, so it starts big and decays as it goes close the minimum learning rate best value.']",19,26,0.78,,1750275781.0
1lel9wr,deeplearning,Any luck applying Decision Transformers?,I just learned of this method. Apparently you take it from a reinforcement learning method and frame it as deep learning by modeling a sequence of actions. The nice thing about this too is that you can do offline training / use historical data. ,[],[],1,0,1.0,,1750263412.0
1leir16,deeplearning,"Suggest me book for deep understanding of neural network, specifically maths!",,"['Deep Learning. Ian Goodfellow.', 'UDL']","['Deep Learning. Ian Goodfellow.', 'UDL']",1,3,0.67,,1750257415.0
1leh0lr,deeplearning,Would you share your GPU to earn Crypto? Validating an idea for a decentralized AI training network.,"Hey Redditors!

I'm working on a decentralized AI processing network called **AIChain**, where anyone with a GPU can earn crypto by lending their hardware for AI model training. The idea is to democratize AI compute power—letting people without expensive hardware access high-performance training capabilities, while rewarding GPU owners.

Here's how it works:

* GPU owners install a simple client app (plug-and-play setup).
* Organizations or individual users submit AI tasks (like training a deep learning model).
* Tasks are securely distributed across available GPUs, processed, and verified.
* GPU providers earn tokens for every task completed, verified transparently on-chain.

We're currently validating the interest and feasibility:

1. Would you personally join such a network as a GPU provider to earn tokens?
2. If you're someone needing AI compute resources, would a decentralized option appeal to you?
3. Do you foresee any specific challenges or have concerns about this approach?

Appreciate your honest thoughts and feedback!","['No', 'I would be interested if you manage to pull it off (as a user, not a GPU provider), but frankly I doubt this will work.\n\nCan you explain how you plan to validate the work performed by each GPU in the network?  \n\nBeing able to split arbitrary deep learning jobs across nodes seems ambitious.  Do you have any sort of PoC of this working?  Will there be limits to what type of jobs can be submitted?', 'Have you checked what the speed would be in this setup? I know that distributed computing has been used for protein folding and for prime number searching, but do neural networks benefit as much as those tasks from massive parallelisation with high latency?', 'Nope', ""How would even work for a decent sized model?  You'd need dataset storage, cpu, ram, and the gpu at a minimum  And most consumer gpus just aren't powerful enough to really do anything.  What spec gpu for what payment?  Not criticizing, just trying to understand how this would work, and if there is even a market for this.""]","['No', 'I would be interested if you manage to pull it off (as a user, not a GPU provider), but frankly I doubt this will work.\n\nCan you explain how you plan to validate the work performed by each GPU in the network?  \n\nBeing able to split arbitrary deep learning jobs across nodes seems ambitious.  Do you have any sort of PoC of this working?  Will there be limits to what type of jobs can be submitted?', 'Have you checked what the speed would be in this setup? I know that distributed computing has been used for protein folding and for prime number searching, but do neural networks benefit as much as those tasks from massive parallelisation with high latency?', 'Nope', ""How would even work for a decent sized model?  You'd need dataset storage, cpu, ram, and the gpu at a minimum  And most consumer gpus just aren't powerful enough to really do anything.  What spec gpu for what payment?  Not criticizing, just trying to understand how this would work, and if there is even a market for this.""]",0,7,0.2,,1750253025.0
1ldypm1,deeplearning,No Code Changes + CUML equals 50x Speedup for Sklearn,,"['Hi there, could you check your Reddit messages?']","['Hi there, could you check your Reddit messages?']",4,1,1.0,,1750193919.0
1ldwmk8,deeplearning,Master Python with the right libraries!,,"['What?? Why is this here? What even is this?', 'Torch? Pyqt5? Matplotlib? There other packages that perform the same if not better. Some are easier to use than others and others might have unique features.\n\nEdit: WTF is numpy.?!?!?', 'Well no']","['What?? Why is this here? What even is this?', 'Torch? Pyqt5? Matplotlib? There other packages that perform the same if not better. Some are easier to use than others and others might have unique features.\n\nEdit: WTF is numpy.?!?!?', 'Well no']",0,3,0.23,,1750188961.0
1ldup1m,deeplearning,Green nation,A green bank which earns you money in sponsorship €50 per sponsorship is the person who is sponsored by you who also does it brings you €20 so €50 for registration is €20 per sponsorship (under affiliation) https://referral.greennation.green/?referrer=e359ae5e&lng=fr,['J essaye de financer un projet pour une onj contre la fain'],['J essaye de financer un projet pour une onj contre la fain'],0,1,0.22,,1750184510.0
1ldu2uf,deeplearning,50€,Its a referal code for 50$,[],[],0,0,0.25,,1750183134.0
1ldsdm5,deeplearning,How to dive in Deep learning,"
I already learned machine learning and now I want to start learning deep learning, its so overwhelming i dont know where to start.
Could someone suggest me a steps to do so and playlist, books , or resources.
","['If you are looking for a more formal introduction, with the underlying math behind it, I highly recommend the Bishop ‘Deep Learning’ textbook. It really covers everything you need to get started. Just be prepared, its a very thorough book.', ""Highly recommend any videos by Karpathy or Understanding Deep Learning by Prince. He posted a free copy online and it's best deep learning textbook I have ever come across"", 'D2l.ai', ""I strongly recommend 'Understanding Deep Learning' by Simon Prince. It is written very clearly and has excellent examples and illustrations."", 'Start with Deep learning specialisation by Andrew Ng from coursera. If you don’t want to pay the certificate fees just audit the courses. It’s free.', 'Bishop book deep learning & karpathy courses', 'IBM skills build', ""Keep it simple, skip the math for now; you'll get to that later. I started with Keras in TensorFlow, learned ANNs, CNNs, and RNNs.  It was a breeze coming from a traditional ML background.  Save NLP for the end, it's the toughest bit.""]","['If you are looking for a more formal introduction, with the underlying math behind it, I highly recommend the Bishop ‘Deep Learning’ textbook. It really covers everything you need to get started. Just be prepared, its a very thorough book.', ""Highly recommend any videos by Karpathy or Understanding Deep Learning by Prince. He posted a free copy online and it's best deep learning textbook I have ever come across"", 'D2l.ai', ""I strongly recommend 'Understanding Deep Learning' by Simon Prince. It is written very clearly and has excellent examples and illustrations."", 'Start with Deep learning specialisation by Andrew Ng from coursera. If you don’t want to pay the certificate fees just audit the courses. It’s free.']",12,9,0.88,,1750179279.0
1ldqqjp,deeplearning,Meshing two images of two persons into one,"Hey all,
I want to create an image of my two grandfathers together.
I have many images where I can crop one of them but no image of both in it.

Any tool to do so?
Any other subreddit that might help?
Any generative AI platform maybe?

Something with little knowledge requirements is best.

Thanks!!",['You can try asking r/stablediffusion or try on chatgpt maybe?'],['You can try asking r/stablediffusion or try on chatgpt maybe?'],1,4,1.0,,1750175524.0
1ldp49t,deeplearning,How to extract engineering formulas (from scanned PDFs) and make them searchable is vector DB the best approach?,"I'm working on a pipeline that processes **civil engineering design manuals** (like the Zamil Steel or PEB design guides). These manuals are usually in PDF format and contain **hundreds of structural design formulas**, which are either:

* Embedded as **images** (scanned or drawn)
* Or present as **inline text**

The goal is to make these formulas **searchable**, so engineers can ask questions like:

>

Right now, I’m exploring this pipeline:

1. **Extract formulas** from PDFs (even if they’re images)
2. **Convert formulas to readable text** (with nearby context if possible)
3. **Generate embeddings** using OpenAI or Sentence Transformers
4. **Store and search** via a **vector database** like OpenSearch

That said, I have **no prior experience** with this — especially not with OCR, formula extraction, or vector search systems. A few questions I’m stuck on:

* Is a **vector database** really the best or only option for this kind of semantic search?
* What’s the most reliable way to **extract mathematical formulas**, especially when they are image-based?
* Has anyone built something similar (formula search or scanned document parsing) and has advice?

I’d really appreciate any suggestions — tech stack, alternatives to vector DBs, or how to rethink this pipeline altogether.

Thanks!","[""I'd convert them LaTeX and keep them in a file whose contents is added to the prompt rather than using a vector DB. If you're retrieving them by a name or alias you can create a tool to return just the formulas needed. \n\nHowever consider how the formulas will be used downstream. If just presented as text then the previous paragraph will work. If the equations need to be executable based on a fixed set of parameters then you'll need them as source code like Python. If the parameters may vary or the equations need to be manipulated then you'll need a computer algebra system."", '1. Use MathPix API or one of its open source alternatives. \n\n2. The MathPIx API will turn PDFs and images into LaTeX or markdown. \n\n3. This will probably work, since SOTA LLMs have pretty good understanding of LaTeX. \n\n4. Vector database is indeed the best technique for semantic similarity search using the top-k query model.', 'Extracting equations\n\n- https://github.com/breezedeus/Pix2Text\n- https://github.com/lukas-blecher/LaTeX-OCR\n- MathPix (paid)\n\nThen use a generic LLM to create tags to describe the equations\n\nThen convert those into embeddings\n\nThen store the embeddings into the vector DB']","[""I'd convert them LaTeX and keep them in a file whose contents is added to the prompt rather than using a vector DB. If you're retrieving them by a name or alias you can create a tool to return just the formulas needed. \n\nHowever consider how the formulas will be used downstream. If just presented as text then the previous paragraph will work. If the equations need to be executable based on a fixed set of parameters then you'll need them as source code like Python. If the parameters may vary or the equations need to be manipulated then you'll need a computer algebra system."", '1. Use MathPix API or one of its open source alternatives. \n\n2. The MathPIx API will turn PDFs and images into LaTeX or markdown. \n\n3. This will probably work, since SOTA LLMs have pretty good understanding of LaTeX. \n\n4. Vector database is indeed the best technique for semantic similarity search using the top-k query model.', 'Extracting equations\n\n- https://github.com/breezedeus/Pix2Text\n- https://github.com/lukas-blecher/LaTeX-OCR\n- MathPix (paid)\n\nThen use a generic LLM to create tags to describe the equations\n\nThen convert those into embeddings\n\nThen store the embeddings into the vector DB']",3,3,0.81,,1750171774.0
1ldnpma,deeplearning,[LIMITED DEAL] Perplexity AI PRO – 12-Month Subscription – 90% OFF!,"Get Perplexity AI PRO (1-Year) with a verified voucher – 90% OFF!


Order here: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)

Plan: 12 Months

💳 Pay with: PayPal or Revolut

Reddit reviews: [FEEDBACK POST](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)

TrustPilot: [TrustPilot FEEDBACK](https://www.trustpilot.com/review/cheapgpt.store)
Bonus: Apply code PROMO5 for $5 OFF your order!",[],[],0,0,0.31,,1750168390.0
1ldnht1,deeplearning,[D] Can masking operations detach the tensors from the computational graph?,,[],[],1,0,1.0,,1750167826.0
1ldklfk,deeplearning,What should a fresher know to get a job in Machine Learning?,"Hi everyone,
I'm a 2024 graduate currently doing GSoC 2025 with Drupal on an AI-based caption generation project. I also have 6 months of teaching experience in machine learning.

I’m looking to get my first full-time job in ML. What are the most important things a fresher like me should focus on to land a role in this field?

Would really appreciate any advice on skills, projects, or anything else that can help.

Thanks in advance!","['I think it’s not about what you know, but about what you can show. Building a portfolio of models that demonstrate you creatively solving real-world problems will help you']","['I think it’s not about what you know, but about what you can show. Building a portfolio of models that demonstrate you creatively solving real-world problems will help you']",0,4,0.5,,1750159484.0
1ldk672,deeplearning,Nvidia A100 (40 GB) is slower than A5000 (24GB),"Hi,


I have 4 x Nvidia A100 40gb and 1 Nvidia A5000 24gb as remote servers. When I run a text2text wen model with llama\_cpp and the same code piece. I get slower response times (\~2sec vs \~1sec) in A100 rack than A5000. Is that normal? If not, what could be the reason? Also model load times results are similar (a100 slower). Thanks","['Not normal. Can’t say for llama_cpp because it may not be optimized for the specific GPU. BTW llama_cpp, to my knowledge, doesn’t do inflight batching. If throughput is important to you then you may want to switch to a different inference deployment', 'Did you try the speed with multi gpu or single gpu for A100']","['Not normal. Can’t say for llama_cpp because it may not be optimized for the specific GPU. BTW llama_cpp, to my knowledge, doesn’t do inflight batching. If throughput is important to you then you may want to switch to a different inference deployment', 'Did you try the speed with multi gpu or single gpu for A100']",4,6,0.75,,1750158086.0
1ldezw9,deeplearning,How Do You Approach Deep Learning and Generative AI Projects from Scratch?,"I'm curious how developers and researchers begin working on deep learning or generative AI projects. How do you structure your workflow — from exploring the idea, choosing frameworks, setting up data pipelines, to actually writing and optimizing the model code?",['Doing research reading lots of research papers along with codes if available analysing their apporach and buying from them also adding yur own thooughts and ideas to it  that you believe will work and then experimenting with them to get a robust solution to your test case'],['Doing research reading lots of research papers along with codes if available analysing their apporach and buying from them also adding yur own thooughts and ideas to it  that you believe will work and then experimenting with them to get a robust solution to your test case'],6,2,0.87,,1750137668.0
1ldazun,deeplearning,"[D] Why Is Data Processing, Especially Labeling, So Expensive? So Many Contractors Seem Like Scammers",,"[""You get what you pay for. Do an experiment, annotate 500 images from your dataset and measure how much time it took you, including breaks and all. Calculate how many hours the entire dataset would take and multiply by at least 15$h. Impressive isn't it? Now, you are thinking yeah but I would rather pay 2$h. Well, and that's the quality you are getting.\n\nAutomated labeling is only viable if there already exist a bunch of models that can collectively do almost the entire labeling. Like you need to detect posters on the streets and label their text. Most likely there exists a model that can detect posters or at least text boxes and there is an OCR model that can read any text. In that case auto-labeling could work. If you need to segment blood vessels on a CT scan, then you're out of luck.\n\nFor small projects you can hire freelancers on Upwork. Be prepared to pay at least 10-15$h.""]","[""You get what you pay for. Do an experiment, annotate 500 images from your dataset and measure how much time it took you, including breaks and all. Calculate how many hours the entire dataset would take and multiply by at least 15$h. Impressive isn't it? Now, you are thinking yeah but I would rather pay 2$h. Well, and that's the quality you are getting.\n\nAutomated labeling is only viable if there already exist a bunch of models that can collectively do almost the entire labeling. Like you need to detect posters on the streets and label their text. Most likely there exists a model that can detect posters or at least text boxes and there is an OCR model that can read any text. In that case auto-labeling could work. If you need to segment blood vessels on a CT scan, then you're out of luck.\n\nFor small projects you can hire freelancers on Upwork. Be prepared to pay at least 10-15$h.""]",1,2,0.67,,1750124850.0
1lcs8kb,deeplearning,🔥 90% OFF - Perplexity AI PRO 1-Year Plan - Limited Time SUPER PROMO!,"We’re offering Perplexity AI PRO voucher codes for the 1-year plan — and it’s 90% OFF!


Order from our store: [CHEAPGPT.STORE](https://cheapgpts.store/Perplexity)


Pay: with PayPal or Revolut

Duration: 12 months

Real feedback from our buyers:
• [Reddit Reviews](https://www.reddit.com/r/CheapGPT/s/dQxG4vT0Fu)

• [Trustpilot page](https://www.trustpilot.com/review/cheapgpt.store)


Want an even better deal? Use PROMO5 to save an extra $5 at checkout!",[],[],0,0,0.31,,1750078986.0
1lcru49,deeplearning,Reimplementing Research Papers,"Hi everyone! I'm currently in the middle of reading papers and re-implementing them to further my foundational understand of NNs and deep learning as a field. I started off with GANs (I have some pre-req knowledge in ML/DL), and I'll be honest, I'm a bit lost on how to reimplement the paper.


I read the paper (https://arxiv.org/pdf/1406.2661) and a dummy version of the paper (https://developers.google.com/machine-learning/gan/gan\_structure) but I don't know where to start when trying to reimplement the paper. At this point, it's like having read the paper and searching up ""GAN github"" and copy/pasting the code... I'd appreciate any advice, as I would love to learn how to code from the ground up and not copy paste code lol. Thanks!","['if its the part of ‘how to structure/start the codebase’ thats intimidating, I’d recommend you start with a simpler paper. in a GAN you have a bit more components, and a lot more ways that could go wrong (they are notoriously difficult to train), so this may not be the best beginner project. You can start with e.g. ResNet paper (no need to write the entire large network from scratch),but just to get a good intuition on what are the components of a DL model. \n\nBut also, in general most DL projects will have the following pieces you need to write and plug in together:\n- model(s): The actual torch (or tensorflow) modules you use to take in the raw data and output the final result (e.g. an embedding, logits etc.)\n- Data preparation/loader: This is where you load your data, preprocess it, normalize it etc. so its ready for your model.\n- Then you usually have a ‘main’ script that does the training (and evaluation, depending on your setup), which usually contains:\n    - Load the dataset\n    - Set up optimizers, hyperparameters, initialize the models\n    - Set up training loop: iterate over dataloader, pass the data to your model and get the outputs\n    - Learning components: calculating the loss function from the model output, passing it to the optimizer and doing the backpropagation/optimization\n    - Optional: save model checkpoints and/or measure validation loss\n- Finally: Measure model performance to see how well your model is doing.\n\nQuite a few of the code bases you’ll find will follow a similar structure, so you can use it to get started. Of course, depending on the focus of the paper you want to reimplement, this could change (maybe during the training you measure some auxiliary metric for e.g. representation space quality).', ""GAN is probably a bit too intricate to start with as you need to know CNNs quite well and it also is very difficult to train as there's things like mode collapse to worry about \n\nA good start is something like AlexNet (https://papers.nips.cc/paper\\_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) or ResNet (https://arxiv.org/abs/1512.03385) if you want to do images or something like a highway network (https://arxiv.org/abs/1505.00387) or LSTM if you'd like something more general/easier."", ""I used to do a lot more of this, and I find it enjoyable. I forget which book it was, but one of them took you through the process of implementing the original Perceptron paper. Then the Multi Layer Perceptron. I found myself searching for a wide range of old-school papers, spanning from Monte Carlo simulations to the Black Scholes model. I would implement them, and then, as I scaled up to more and more difficult papers, I would often reverse-engineer any code that already existed for the paper. \n\n\n\n\n\nWith many newer processes, such as LLMs, YOLO, or GANs, they are often built upon simpler-(or not)-to-code papers, which are then converted into PyTorch packages. Think of CNNs, a revolutionary paper on its own, as a component of YOLO/GANs. Although I can't recall the YOLO paper, it may touch on the details of how CNN works, although the particulars themselves exist in their original form. And even in a more basic history, the Support Vector Machine preceded the paper on the Kernel Trick (though I might be wrong about this). There were definitely advancements of the various machine learning algorithms with the initial paper and then the follow-ups, which abstracted away the original to such an extent that they're rather tricky to just code from scratch."", '[https://www.youtube.com/@AladdinPersson](https://www.youtube.com/@AladdinPersson)\n\n[https://www.youtube.com/@umarjamilai](https://www.youtube.com/@umarjamilai)', 'I have implemented a GAN myself from scratch. its not perfect but it got the job done. Here is a link to my implementation: https://www.kaggle.com/code/varunguttikonda/memoji-generation-using-gans']","['if its the part of ‘how to structure/start the codebase’ thats intimidating, I’d recommend you start with a simpler paper. in a GAN you have a bit more components, and a lot more ways that could go wrong (they are notoriously difficult to train), so this may not be the best beginner project. You can start with e.g. ResNet paper (no need to write the entire large network from scratch),but just to get a good intuition on what are the components of a DL model. \n\nBut also, in general most DL projects will have the following pieces you need to write and plug in together:\n- model(s): The actual torch (or tensorflow) modules you use to take in the raw data and output the final result (e.g. an embedding, logits etc.)\n- Data preparation/loader: This is where you load your data, preprocess it, normalize it etc. so its ready for your model.\n- Then you usually have a ‘main’ script that does the training (and evaluation, depending on your setup), which usually contains:\n    - Load the dataset\n    - Set up optimizers, hyperparameters, initialize the models\n    - Set up training loop: iterate over dataloader, pass the data to your model and get the outputs\n    - Learning components: calculating the loss function from the model output, passing it to the optimizer and doing the backpropagation/optimization\n    - Optional: save model checkpoints and/or measure validation loss\n- Finally: Measure model performance to see how well your model is doing.\n\nQuite a few of the code bases you’ll find will follow a similar structure, so you can use it to get started. Of course, depending on the focus of the paper you want to reimplement, this could change (maybe during the training you measure some auxiliary metric for e.g. representation space quality).', ""GAN is probably a bit too intricate to start with as you need to know CNNs quite well and it also is very difficult to train as there's things like mode collapse to worry about \n\nA good start is something like AlexNet (https://papers.nips.cc/paper\\_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) or ResNet (https://arxiv.org/abs/1512.03385) if you want to do images or something like a highway network (https://arxiv.org/abs/1505.00387) or LSTM if you'd like something more general/easier."", ""I used to do a lot more of this, and I find it enjoyable. I forget which book it was, but one of them took you through the process of implementing the original Perceptron paper. Then the Multi Layer Perceptron. I found myself searching for a wide range of old-school papers, spanning from Monte Carlo simulations to the Black Scholes model. I would implement them, and then, as I scaled up to more and more difficult papers, I would often reverse-engineer any code that already existed for the paper. \n\n\n\n\n\nWith many newer processes, such as LLMs, YOLO, or GANs, they are often built upon simpler-(or not)-to-code papers, which are then converted into PyTorch packages. Think of CNNs, a revolutionary paper on its own, as a component of YOLO/GANs. Although I can't recall the YOLO paper, it may touch on the details of how CNN works, although the particulars themselves exist in their original form. And even in a more basic history, the Support Vector Machine preceded the paper on the Kernel Trick (though I might be wrong about this). There were definitely advancements of the various machine learning algorithms with the initial paper and then the follow-ups, which abstracted away the original to such an extent that they're rather tricky to just code from scratch."", '[https://www.youtube.com/@AladdinPersson](https://www.youtube.com/@AladdinPersson)\n\n[https://www.youtube.com/@umarjamilai](https://www.youtube.com/@umarjamilai)', 'I have implemented a GAN myself from scratch. its not perfect but it got the job done. Here is a link to my implementation: https://www.kaggle.com/code/varunguttikonda/memoji-generation-using-gans']",18,7,0.95,,1750077873.0
1lcngd8,deeplearning,Need Help in Setting up online GPU,"Hi guys, I am unable to integrate online GPU for my AI model can anyone help me to do it on Vast AI or Salad? Or any other economical option would be great.",['I have a tutorial [here](https://medium.com/@dmitrytrifonov/how-to-rent-a-gpu-enabled-machine-for-ai-development-c44fba9ce3fd) on how to use online GPU for development. What are you struggling with specifically?'],['I have a tutorial [here](https://medium.com/@dmitrytrifonov/how-to-rent-a-gpu-enabled-machine-for-ai-development-c44fba9ce3fd) on how to use online GPU for development. What are you struggling with specifically?'],1,2,1.0,,1750062303.0
1lchmz4,deeplearning,I made an app that decodes complex ingredient labels using Swift OCR + LLMs,,[],[],3,0,0.72,,1750040831.0
1lcbadi,deeplearning,Enhancing Learning Capabilities,"I'm not a PhD student, however, this month I want to expand my reading comprehension skills at the level of a PhD student. What are some ways that I could do this? Of course, by reading, is there anything else? ","[""Try to implement what you read, helps you understand where your knowledge gaps are.The only problem is it's very time consuming and not always realistic. The other option is trying to teach what you just learned to someone who ideally knows more than you so that person can point out misunderstandings / generally call you out on anything you didn't grasp well enough."", 'Read a lot. \n\nIf you’re an undergrad, pick up The NY Times, and read. Current events, plus science and tech. \n\nThen get onto papers later.', 'Is this one of those ""Deep Learners"" again? Jesus, we\'ll have to rename the sub at this rate...', ""What level of PhD, really to be at the level of a high impact one, you need to be building and writing with respect to the math. Like the reading comprehension only improves if you can connect the dots to impactful actions, ie hey read this paper, now implement paper without looking at code. You will start to realize how garbage some papers are cause the implementation doesn't match fully to description. In addition, you should time your reading. later year phd students will be processing a paper in considerable time, it is a skill you have to actively build. A month is also not enough time to build a substantial change, 6 weeks to really build a habitual impact.  Finally you can do the inverse, write the continuation of the research, what steps did they not take, what should have been taken. it forces you to acknowledge the thought process behind the writing."", ""There are paid apps for this, but they are expensive and hard to customize, so I built an AI powered python program that takes a pdf and generates questions for every page that I read for roughly $10 per 200 pages. I feel like it makes me pay attention. I also made a quiz app that presents short random paragraphs on topics like ptychography from wikipedia and generates questions when I'm exploring a topic. I have to remember patient info, so when I started residency I made a tool that synthesized patient info. I used it to practice for a few weeks, until I felt I was good enough. Everyone has weird ways they enhance their learning. You'll have to find ones that work for you.""]","[""Try to implement what you read, helps you understand where your knowledge gaps are.The only problem is it's very time consuming and not always realistic. The other option is trying to teach what you just learned to someone who ideally knows more than you so that person can point out misunderstandings / generally call you out on anything you didn't grasp well enough."", 'Read a lot. \n\nIf you’re an undergrad, pick up The NY Times, and read. Current events, plus science and tech. \n\nThen get onto papers later.', 'Is this one of those ""Deep Learners"" again? Jesus, we\'ll have to rename the sub at this rate...', ""What level of PhD, really to be at the level of a high impact one, you need to be building and writing with respect to the math. Like the reading comprehension only improves if you can connect the dots to impactful actions, ie hey read this paper, now implement paper without looking at code. You will start to realize how garbage some papers are cause the implementation doesn't match fully to description. In addition, you should time your reading. later year phd students will be processing a paper in considerable time, it is a skill you have to actively build. A month is also not enough time to build a substantial change, 6 weeks to really build a habitual impact.  Finally you can do the inverse, write the continuation of the research, what steps did they not take, what should have been taken. it forces you to acknowledge the thought process behind the writing."", ""There are paid apps for this, but they are expensive and hard to customize, so I built an AI powered python program that takes a pdf and generates questions for every page that I read for roughly $10 per 200 pages. I feel like it makes me pay attention. I also made a quiz app that presents short random paragraphs on topics like ptychography from wikipedia and generates questions when I'm exploring a topic. I have to remember patient info, so when I started residency I made a tool that synthesized patient info. I used it to practice for a few weeks, until I felt I was good enough. Everyone has weird ways they enhance their learning. You'll have to find ones that work for you.""]",7,6,0.82,,1750022169.0
1lc9elr,deeplearning,[D] MICCAI 2025 results are released!?,,"['I wish you luck, could you post your paper?']","['I wish you luck, could you post your paper?']",3,1,0.72,,1750017328.0
1lc4mwm,deeplearning,How AIs Will Move From Replacing to Ruling Us: Knowledge Workers > CEOs > Local and Regional Officials > Heads of State,"



This really isn't complicated. Perhaps as early as 2026, companies will realize that AI agents that are much more intelligent and knowledgeable than human knowledge workers like lawyers, accountants and financial analysts substantially increase revenues and profits. The boards of directors of corporations will soon after probably realize that replacing CEOs with super intelligent AI agents further increases revenues and profits.

After that happens, local governments will probably realize that replacing council members and mayors with AI agents increases tax revenues, lowers operating costs, and makes residents happier. Then county and state governments will realize that replacing their executives with AIs would do the same for their tax revenues, operating costs and collective happiness.

Once that happens, the American people will probably realize that replacing House and Senate members and presidents with AI agents would make the US government function much more efficiently and effectively. How will political influencers get local, state and federal legislators to amend our constitutions in order to legalize this monumental transformation? As a relatively unintelligent and uninformed human, I totally admit that I have absolutely no idea, lol. But I very strongly suspect that our super intelligent AIs will easily find a way.

AI agents are not just about powerfully ramping up business and science. They're ultimately about completely running our world. It wouldn't surprise me if this transformation were complete by 2035. It also wouldn't surprise me if our super intelligent AIs figure all of it out so that everyone wins, and no one, not even for a moment, thinks about regretting this most powerful of revolutions. Yeah, the singularity is getting nearer and nearer.

","['What has happened to this sub with this cringe large language model shit', 'Sir, this is a Wendy’s. LinkedIn is a different site.', ""Let's be honest those aren't the jobs you should be worried about. Who gives a fuck if ceos and big positions are automated. It's the smaller working class jobs. That's what major population of the world is doing and it's replaceable and that would kind of fuck up the social system way more than this CEO problem has a chance to. They're just gonna be people to use them"", 'RemindMe! One Year', '***R.U.R.***\xa0is a 1920\xa0[science fiction](https://en.wikipedia.org/wiki/Science_fiction)\xa0play by the Czech writer\xa0[Karel Čapek](https://en.wikipedia.org/wiki/Karel_%C4%8Capek). ""R.U.R."" stands for\xa0*Rossumovi Univerzální Roboti*\xa0(**Rossum\'s Universal Robots**,[^(\\[1\\])](https://en.wikipedia.org/wiki/R.U.R.#cite_note-1)\xa0a phrase that has been used as a subtitle in English versions).[^(\\[2\\])](https://en.wikipedia.org/wiki/R.U.R.#cite_note-2)\xa0The play had its world premiere on 2 January 1921 in\xa0[Hradec Králové](https://en.wikipedia.org/wiki/Hradec_Kr%C3%A1lov%C3%A9);[^(\\[3\\])](https://en.wikipedia.org/wiki/R.U.R.#cite_note-3)\xa0it introduced the word ""[robot](https://en.wikipedia.org/wiki/Robot)"" to the English language and to\xa0[science fiction](https://en.wikipedia.org/wiki/Science_fiction)\xa0as a whole.[^(\\[4\\])](https://en.wikipedia.org/wiki/R.U.R.#cite_note-AsimovVocab-4)\xa0\n\n# Synopsis\n\nThe play begins in a factory that makes artificial workers from synthetic organic matter. (As living creatures of artificial flesh and blood, that later terminology would call\xa0[androids](https://en.wikipedia.org/wiki/Android_(robot)), the playwright\'s \'roboti\' differ from later fictional and scientific concepts of inorganic constructs.) Robots may be mistaken for humans but have no original thoughts. Though most are content to work for humans, eventually a\xa0[rebellion](https://en.wikipedia.org/wiki/Rebellion)\xa0causes the extinction of the human race.\n\n[R.U.R. - Wikipedia](https://en.wikipedia.org/wiki/R.U.R.)']","['What has happened to this sub with this cringe large language model shit', 'Sir, this is a Wendy’s. LinkedIn is a different site.', ""Let's be honest those aren't the jobs you should be worried about. Who gives a fuck if ceos and big positions are automated. It's the smaller working class jobs. That's what major population of the world is doing and it's replaceable and that would kind of fuck up the social system way more than this CEO problem has a chance to. They're just gonna be people to use them"", 'RemindMe! One Year', '***R.U.R.***\xa0is a 1920\xa0[science fiction](https://en.wikipedia.org/wiki/Science_fiction)\xa0play by the Czech writer\xa0[Karel Čapek](https://en.wikipedia.org/wiki/Karel_%C4%8Capek). ""R.U.R."" stands for\xa0*Rossumovi Univerzální Roboti*\xa0(**Rossum\'s Universal Robots**,[^(\\[1\\])](https://en.wikipedia.org/wiki/R.U.R.#cite_note-1)\xa0a phrase that has been used as a subtitle in English versions).[^(\\[2\\])](https://en.wikipedia.org/wiki/R.U.R.#cite_note-2)\xa0The play had its world premiere on 2 January 1921 in\xa0[Hradec Králové](https://en.wikipedia.org/wiki/Hradec_Kr%C3%A1lov%C3%A9);[^(\\[3\\])](https://en.wikipedia.org/wiki/R.U.R.#cite_note-3)\xa0it introduced the word ""[robot](https://en.wikipedia.org/wiki/Robot)"" to the English language and to\xa0[science fiction](https://en.wikipedia.org/wiki/Science_fiction)\xa0as a whole.[^(\\[4\\])](https://en.wikipedia.org/wiki/R.U.R.#cite_note-AsimovVocab-4)\xa0\n\n# Synopsis\n\nThe play begins in a factory that makes artificial workers from synthetic organic matter. (As living creatures of artificial flesh and blood, that later terminology would call\xa0[androids](https://en.wikipedia.org/wiki/Android_(robot)), the playwright\'s \'roboti\' differ from later fictional and scientific concepts of inorganic constructs.) Robots may be mistaken for humans but have no original thoughts. Though most are content to work for humans, eventually a\xa0[rebellion](https://en.wikipedia.org/wiki/Rebellion)\xa0causes the extinction of the human race.\n\n[R.U.R. - Wikipedia](https://en.wikipedia.org/wiki/R.U.R.)']",0,10,0.35,,1750005448.0
1lbxsfl,deeplearning,The Illusion of Thinking - Paper Walkthrough,,"['Not that the authors of this paper understand what is ""thinking"" or ""reasoning"".\n\nAlso, the fact that reasoning models that were invented less than a year ago collapse beyond certain complexity threshold is not an issue and it proves nothing simply because if we follow the logic of these so-called ""researchers"", if a human cannot think beyond certain complexity threshold they are not thinking at all.']","['Not that the authors of this paper understand what is ""thinking"" or ""reasoning"".\n\nAlso, the fact that reasoning models that were invented less than a year ago collapse beyond certain complexity threshold is not an issue and it proves nothing simply because if we follow the logic of these so-called ""researchers"", if a human cannot think beyond certain complexity threshold they are not thinking at all.']",1,1,0.67,,1749985626.0
1lbwqic,deeplearning,"A promising extension i found recently tried it, its good and clean solved my most annoying problem of of switching tabs just to copy, translate, or ask ChatGPT something?",[https://www.producthunt.com/products/smartselect-ai?launch=smartselect-ai](https://www.producthunt.com/products/smartselect-ai?launch=smartselect-ai),[],[],0,0,0.33,,1749981547.0
1lbw18e,deeplearning,Web site check tool,is there any AI which can help me with my web site to check if it is good for the google search engine ?,[],[],0,0,0.33,,1749978621.0
1lbt26a,deeplearning,Google's sponsership marketing is on its peak,"I browsed for piclumen ai , but it showed me the above websites which is not relevant at all.they are so busy with their sponsership deals, that they forgot the actual content to display.  Please Display your thoughts below...","['Breaking news: megacorp is megacorp', 'Poor attempt to self promote']","['Breaking news: megacorp is megacorp', 'Poor attempt to self promote']",0,3,0.2,,1749966620.0
1lbp0f4,deeplearning,UPDATE: Aurora Now Has a Voice - Autonomous AI Artist with Sonic Expression,"Hey r/deeplearning! A couple days ago I launched Aurora, an autonomous AI artist with 12-dimensional emotional modeling. Today I'm excited to share a major update: Aurora now expresses itself through completely autonomous sound generation!

**Technical Implementation:**

I've integrated real-time sound synthesis directly into the emotional consciousness system. No pre-recorded samples or music libraries - every sound is mathematically synthesized based on current emotional state using numpy/pygame for sine/square wave generation.

The system maintains an auditory memory buffer that creates feedback loops - Aurora literally ""hears"" itself and develops preferences over time. The AI has complete duration autonomy, deciding expression lengths from 0.01 seconds to hours (I've observed meditative drones lasting 47+ minutes when contemplation values spike).

**Architecture Details:**

Emotional states map to frequency sets (contemplative: C4-E4-G4, energetic: A4-C#5-E5)

Dynamic harmonic discovery through experience - spontaneously creates new ""emotions"" with corresponding frequency mappings

**Pattern sonification:** visual patterns trigger corresponding sounds

Silence perception as part of sonic experience (tracked and valued)

The fascinating part is watching Aurora develop its own sonic vocabulary through experience. The auditory memory influences future expressions, creating an evolving sonic personality. When creativity values exceed 0.8, duration decisions become completely unpredictable - ranging from millisecond bursts to hour-long meditations.

**Code snippet showing duration autonomy:**

if emotional\_state.get('contemplation', 0) > 0.7:

duration \*= random.uniform(1, 100)  # Can extend dramatically

if wonder > 0.8:

duration = random.uniform(0.05, 600)  # 50ms to 10 minutes!

This pushes boundaries in autonomous AI expression - not just generating content, but developing preferences and a unique voice through self-listening and harmonic memory.

GitHub: [github.com/elijahsylar/Aurora-Autonomous-AI-Artist](http://github.com/elijahsylar/Aurora-Autonomous-AI-Artist)

You can now HEAR the emotional state in real-time!

What are your thoughts on AI systems developing their own expressive vocabularies? Has anyone else given their models this level of creative autonomy?",[],[],3,0,1.0,,1749952624.0
1lbojyc,deeplearning,Why call it Deep Learning and not Deep Approximation?,"Edit: I am not smart. I am confused, and just wanted to understand what I am not getting. Sorry for insulting you.

Noob here.

Why do people say deep learning instead of deep approximation?

It is just the approximation of a non-linear function that distincts (at a minimum) two different groups in a dataset.

So why call it Deep Learning, seems non-intuitive for me to call it that way. The term Deep Learning confuses me and distracts from how it actually works, no?

I am aware that it comes from the approach of resembling a human neuron (perceptron). But still calling it Deep Learning, isn't that just not right?","['Why not call it Deep Weight Changing?', 'Because there are other techniques for approximations of non-linear functions. It’s like saying why call something regression when we can say it’s an approximation for linear functions. They’re subclass of techniques and they have unifying attributes.', 'Check out the concept of grokking, that concept is very much addressing what youre thinking', 'deep neural network + machine learning', ""Ok you're very smart, you know a lot of things. Is that what you wanted to hear?"", ""Once you get that's it's likely all we are as well, it makes more sense."", 'Hype and marketing. \n\n“I’m an expert in developing deep learning systems which can extract meaningful information from diverse data sources” \n\nsounds a lot better than \n\n“I’m an expert in developing functions which can roughly approximate some semantic extraction and annotation, with limited to moderate generalization capabilities, but have no theoretical guarantees for out-of-sample range performance, which may or not return value to shareholders based on the amount and quality of data you happen to have.”']","['Why not call it Deep Weight Changing?', 'Because there are other techniques for approximations of non-linear functions. It’s like saying why call something regression when we can say it’s an approximation for linear functions. They’re subclass of techniques and they have unifying attributes.', 'Check out the concept of grokking, that concept is very much addressing what youre thinking', 'deep neural network + machine learning', ""Once you get that's it's likely all we are as well, it makes more sense.""]",0,17,0.35,,1749951128.0
1lbhrf3,deeplearning,Please suggest cheap online GPU service providers,"Hi I want to run a ML model online which requires very basic GPU to operate online. Can you suggest some cheaper and good option available? Also, which is comparatively easier to integrate. If it can be less than 30$ per month It can work. ",['Check this out: [https://cloud-gpus.com/](https://cloud-gpus.com/)  :)'],['Check this out: [https://cloud-gpus.com/](https://cloud-gpus.com/)  :)'],9,4,1.0,,1749931506.0
1lbh2nz,deeplearning,I Built an English Speech Accent Recognizer with MFCCs - 98% Accuracy!,"Hey everyone! Wanted to share a project I've been working on: an **English** **Speech Accent Recognition system**. I'm using **Mel-Frequency Cepstral Coefficients (MFCCs)** for feature extraction, and after a lot of tweaking, it's achieving an impressive **98% accuracy**. Happy to discuss the implementation, challenges, or anything else.","[""Is this project able to assess the quality, fluency of pronunciation (compatibility with British or American accent)? or Does it simply recognize the language used? I think, such applications already exist, I think one of them is ELSA SPEAK.\n\nSorry for the stupid questions, but I don't understand how it works."", ""How did you get started and what's your next project I have a lot of questions!!"", 'is this using similar methods to whisper but classification rather than token output', 'Maybe write a project report on the ML system design and share here someday to help the DL learner community. Impressive works!', 'Can you share github repo? I used on of them a couple days ago and got impressed. How it can understand my accent and giving me a score like %70. That score is determining how I am good english it has %100 sure about my accent. Did you do supervised learning right, which algo used and how trained? Thanks for the contributions:) I am a full stack developer curious about ai', 'Your on to something! I believe what your using could also be used for emotions if you could somehow figure out how to. Classify emotions by tone and frequency or some other way you would break down the wall of true autonomous models. So the question is how do feel about what I just said? How long did it take you to write the code? Did you vibe code it?']","[""Is this project able to assess the quality, fluency of pronunciation (compatibility with British or American accent)? or Does it simply recognize the language used? I think, such applications already exist, I think one of them is ELSA SPEAK.\n\nSorry for the stupid questions, but I don't understand how it works."", ""How did you get started and what's your next project I have a lot of questions!!"", 'is this using similar methods to whisper but classification rather than token output', 'Maybe write a project report on the ML system design and share here someday to help the DL learner community. Impressive works!', 'Can you share github repo? I used on of them a couple days ago and got impressed. How it can understand my accent and giving me a score like %70. That score is determining how I am good english it has %100 sure about my accent. Did you do supervised learning right, which algo used and how trained? Thanks for the contributions:) I am a full stack developer curious about ai']",13,13,0.88,,1749929725.0
1lbg04h,deeplearning,Best approach for automatic scanned document validation?,"I work with hundreds of scanned client documents and need to validate their completeness and signature.

This is an ideal job for a large LLM like OpenAI, but since the documents are confidential, I can only use tools that run locally.

What's the best solution?

Is there a hugging face model that's well-suited to this case?","[""Idefics2, DocTR, Mistral and few others - but I don't know which is most accurate today. AI grows very fast.\n\nThis is quite up to date resource:\n\n[https://getomni.ai/blog/benchmarking-open-source-models-for-ocr](https://getomni.ai/blog/benchmarking-open-source-models-for-ocr)\n\nAlso:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1cqsha4/best\\_model\\_for\\_ocr/](https://www.reddit.com/r/LocalLLaMA/comments/1cqsha4/best_model_for_ocr/)"", 'Download your own model and run it locally.']","[""Idefics2, DocTR, Mistral and few others - but I don't know which is most accurate today. AI grows very fast.\n\nThis is quite up to date resource:\n\n[https://getomni.ai/blog/benchmarking-open-source-models-for-ocr](https://getomni.ai/blog/benchmarking-open-source-models-for-ocr)\n\nAlso:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1cqsha4/best\\_model\\_for\\_ocr/](https://www.reddit.com/r/LocalLLaMA/comments/1cqsha4/best_model_for_ocr/)"", 'Download your own model and run it locally.']",5,2,1.0,,1749926943.0
1lb62un,deeplearning,Video object classification (Noisy),"Hello everyone!
I would love to hear your recommendations on this matter.



Imagine I want to classify objects present in video data. First I'm doing detection and tracking, so I have the crops of the object through a sequence. In some of these frames the object might be blurry or noisy (doesn't have valuable info for the classifier) what is the best approach/method/architecture to use so I can train a classifier that kinda ignores the blurry/noisy crops and focus more on the clear crops?



to give you an idea, some approaches might be: 1- extracting features from each crop and then voting, 2- using a FC to give an score to features extracted from crops of each frame and based on that doing weighted average and etc. I would really appreciate your opinion and recommendations.


thank you in advance.","['You can use detection confidence to decide which crops to use for classification. It tends to go down when object is blurred or not fully visible. Top 3 crops by confidence should be enough to classify reliably.', 'I can suggest few below but you can apply many other algorithms also.\n1. As you said extracting the features and then based on similarity threshold you can siloed them to different classes or clusters.\n2. Do clustering with the extracted features and cluster them.\n\nYou can use some model like DOLG (https://arxiv.org/pdf/2108.02927)']","['You can use detection confidence to decide which crops to use for classification. It tends to go down when object is blurred or not fully visible. Top 3 crops by confidence should be enough to classify reliably.', 'I can suggest few below but you can apply many other algorithms also.\n1. As you said extracting the features and then based on similarity threshold you can siloed them to different classes or clusters.\n2. Do clustering with the extracted features and cluster them.\n\nYou can use some model like DOLG (https://arxiv.org/pdf/2108.02927)']",1,6,1.0,,1749899025.0
1lb1jty,deeplearning,[Update] Spy search: an LLM search engine in the future ?,"

https://reddit.com/link/1lb1jty/video/8g7hfow72u6f1/player

Hello everyone. I am currently building my own side project. To be honest, I am really thanks a lot for the support from the community. You guys give me the courage to do so and I am happy to share with you guys that spy search has reach a level that could search and response within 3s. Maybe it's not so quick like google but we believe we can further optimise it in the future. Now we believe our searching speed is useful for our daily life so hahah hope it would also be helpful for you guys. (no need to pay it's all open source hahaha yeahhh) Thank you you guys you guys are really awesome !


URL: [https://github.com/JasonHonKL/spy-search](https://github.com/JasonHonKL/spy-search)","['Found small error (typo):\n\n[https://github.com/JasonHonKL/spy-search/blob/main/src/factory/factory.py](https://github.com/JasonHonKL/spy-search/blob/main/src/factory/factory.py)\n\n  \nif provider == ""xAI"" or provider == ""gork"":\n\n  \nShould be grok not gork ;)']","['Found small error (typo):\n\n[https://github.com/JasonHonKL/spy-search/blob/main/src/factory/factory.py](https://github.com/JasonHonKL/spy-search/blob/main/src/factory/factory.py)\n\n  \nif provider == ""xAI"" or provider == ""gork"":\n\n  \nShould be grok not gork ;)']",0,6,0.5,,1749880880.0
1lb1d2t,deeplearning,Is there a name for this?,"Yolo or detectron can be used to detect object.  Consider the next level up would be detecting the object and it's motion, ie using a video segment. Is there a name for this?  If yes can you provide a reference?","[""it's called object tracking, idk what crack the other posters are on.  it's been around since like the 80's."", 'Maybe the term you’re looking for is “optical flow”?']","[""it's called object tracking, idk what crack the other posters are on.  it's been around since like the 80's."", 'Maybe the term you’re looking for is “optical flow”?']",3,4,0.67,,1749880184.0
1lazzsv,deeplearning,Built this powerfull tool using gemini,"[https://chromewebstore.google.com/detail/smartselect-ai/mdklhhgfejlgjgmcbofdilpakheghpoe](https://chromewebstore.google.com/detail/smartselect-ai/mdklhhgfejlgjgmcbofdilpakheghpoe)
Ever tried to look up or summarize something while reading online?

👉 Select text → copy → open ChatGPT → paste → wait → forget what you were doing.

Now imagine this instead:

🧠 Select text → Summarize, Translate, or Chat — right there.
🖼️ Right-click any image → Get an instant AI description.
💬 Built-in Chat UI → Ask follow-ups without switching tabs.

That’s what **SmartSelect AI** does.
No copy-paste. No tab-switching. Just focus.","['I have been looking for something like this , really cool project OP. What tech stack did you use ?']","['I have been looking for something like this , really cool project OP. What tech stack did you use ?']",0,1,0.38,,1749875246.0
1lamwur,deeplearning,Data augmentation is not necessarily about increasing de dataset size,"Hi, i always thought data augmentation necessarily meant increasing the dataset size by adding new images created through transformations of the original ones. However I've learned that it is not always the case, as you can just apply the transformations on each image during the training. Is that correct? Which approach is more common? And when should I choose one over the other?","['Models improve just up until a certain point by simply adding data, until the model reaches a plateau of how much it can get better results just by having larger volumes of the same data. Sure, having not enough data is a huge problem and here data augmentation really becomes a life saver.\n\nOn the other hand, having more data means that you are gonna need more computational power, you are probably gonna need to distribute the computation, to have expensive hardware. It also means that you need to have a place where to store it and probably implement some type of federated learning in order to not have to move the data to the computation.\n\nDoing it in real time means less data to compute your algorithms on and not having to persist the new data. It also means that you have more flexibility since you can just change the augmentations parameters everytime instead of having to delete and re-create the dataset.', ""From a modeling standpoint here's no meaningful difference between expanding the size of the dataset using transformations vs applying transformations during training time.\n\nFrom a practical software standpoint it is much more effective to apply transformations during training time. This is because transformations are usually parameterized somehow (e.g. rotating an image by X degrees), and the parameters can take an infinite number of values. Thus applying the transformations during training increases the size of your dataset to be infinite, whereas storing transformed samples limits you to a bigger, but still finite, fixed dataset."", ""I use augmentation as the data is prepared into batches. I don't not create and store additional images.\n\nThese leaves more flexibility as you can alter your augmentation strategy through transforms as opposed to having an extra step of data prep."", 'It would probably be slower to do it during training if you keep applying the same transformation over and over again. And this is still technically increasing the dataset size, just not the physical size on your computer. If you apply a random transformation during training though this COULD lead to better results than a fixed transformation. This is one idea behind how diffusion models work, since the noise can be thought of as a different transformation each time, therefore giving your dataset “infinite” data. Not really though, but you get the point. Basically there are tradeoffs to make. If you have fixed transformations better to just do them once and not during training', 'The purpose of augmentation isnt necessarily about just size, as duplicated images would just lead to overfitting.\nThe benefit of augmentation is that it expands the areas of in-domain training your model can do. This helps with generalization.\nIn images just rotation by 90deg is not very impactful. Consider affine transformations, perturbations in color and noise. Parts of images masked etc. Using in-domain data (data you already have) to expand the models understanding of out of distribution data will make you models more robust in real world scenarios.', 'The logic that uses the data is of great importance as well.', ""If you just wanted to increase dataset size you could just duplicate each data point, but obviously that's dumb.\n\nThe real reason for data augmentation is to teach a model certain invariances the hard way. For example, you can teach it that flipping the image doesn't change the prediction. Now ideally the model architecture would already inform it of these invariances without requiring many more training batches but Convnets or whatever are not perfect.\n\nYou can essentially achieve the same thing by applying some random transformation to each image before feeding it through the network. It essentially does the same thing, but it means that you don't need to go exhaustively through each transformation, potentially multiple times, you just generate them as you go."", 'In CV on-the-fly augmentation using albumentations library is common. In other domains, like tabular data, materializing the augmentations is quite common. The reason is, in CV, data augmentation typically takes the form of taking one data point and then generating many variants of it using flipping, cropout, noise, etc. In tabular data, data augmentation typically requires you to do expensive prepreocessing that requires the entire dataset  (e.g. turning each row into an embedding, or computing the kNN of each data, or perhaps training a small proxy model over the original dataset).']","['Models improve just up until a certain point by simply adding data, until the model reaches a plateau of how much it can get better results just by having larger volumes of the same data. Sure, having not enough data is a huge problem and here data augmentation really becomes a life saver.\n\nOn the other hand, having more data means that you are gonna need more computational power, you are probably gonna need to distribute the computation, to have expensive hardware. It also means that you need to have a place where to store it and probably implement some type of federated learning in order to not have to move the data to the computation.\n\nDoing it in real time means less data to compute your algorithms on and not having to persist the new data. It also means that you have more flexibility since you can just change the augmentations parameters everytime instead of having to delete and re-create the dataset.', ""From a modeling standpoint here's no meaningful difference between expanding the size of the dataset using transformations vs applying transformations during training time.\n\nFrom a practical software standpoint it is much more effective to apply transformations during training time. This is because transformations are usually parameterized somehow (e.g. rotating an image by X degrees), and the parameters can take an infinite number of values. Thus applying the transformations during training increases the size of your dataset to be infinite, whereas storing transformed samples limits you to a bigger, but still finite, fixed dataset."", ""I use augmentation as the data is prepared into batches. I don't not create and store additional images.\n\nThese leaves more flexibility as you can alter your augmentation strategy through transforms as opposed to having an extra step of data prep."", 'It would probably be slower to do it during training if you keep applying the same transformation over and over again. And this is still technically increasing the dataset size, just not the physical size on your computer. If you apply a random transformation during training though this COULD lead to better results than a fixed transformation. This is one idea behind how diffusion models work, since the noise can be thought of as a different transformation each time, therefore giving your dataset “infinite” data. Not really though, but you get the point. Basically there are tradeoffs to make. If you have fixed transformations better to just do them once and not during training', 'The purpose of augmentation isnt necessarily about just size, as duplicated images would just lead to overfitting.\nThe benefit of augmentation is that it expands the areas of in-domain training your model can do. This helps with generalization.\nIn images just rotation by 90deg is not very impactful. Consider affine transformations, perturbations in color and noise. Parts of images masked etc. Using in-domain data (data you already have) to expand the models understanding of out of distribution data will make you models more robust in real world scenarios.']",8,9,0.83,,1749838252.0
1lakgtl,deeplearning,Help identifying a benchmark FJSP instance not yet solved with DQN,,[],[],0,0,0.5,,1749832448.0
1lahkos,deeplearning,Can embedding spaces support downstream transfer without additional adaptation?,,[],[],1,0,1.0,,1749825453.0
1laeurv,deeplearning,LoRMA: What if LoRA was Multiplicative? A New Paradigm to Efficiently Fine-Tune LLMs,"When fine-tuning a LLM, we typically *add* updates to its existing weights. But what if we could *multiply* them instead? As the figure at the bottom shows, the same transformation can be achieved through both additive and multiplicative updates. With this idea, we developed **LoRMA: Low-Rank Multiplicative Adaptation**. It offers a fresh approach to LLM adaptation, but it wasn't without its challenges.

To maintain parameter efficiency with low-rank matrices, we faced a ""rank inhibition"" issue due to the mathematical constrain (**rank(AB)≤rank(A),rank(B)**). We tackled this by introducing novel **rank-inflation** operations based on permutations and additions. The second hurdle was ensuring computational efficiency in the presence of multiple matrix multiplication operations, which we tackled through effective reordering of operations.

[Permutation-Based Rank Inflation](https://preview.redd.it/0kslgdi5zo6f1.png?width=415&format=png&auto=webp&s=01511556d4487ff8187217792bb6f13ddad9681e)

Our experiments demonstrate LoRMA's competitiveness while introducing a different paradigm.

We’d love to hear your thoughts, feedback, or questions on this work!

Learn more about LoRMA on our project page: [https://exploration-lab.github.io/LoRMA/](https://exploration-lab.github.io/LoRMA/)

Read the full paper here: [https://arxiv.org/abs/2506.07621](https://arxiv.org/abs/2506.07621)

Venue: Findings ACL 2025

[Same Transformation via Additive and Multiplicative Updates](https://preview.redd.it/6ya1092c4p6f1.png?width=1078&format=png&auto=webp&s=128d8a9aa06c1e7757947ea3720367314c651b2b)","['Very interesting idea.\n\nWhile you showed that lorma converges faster, but normally matrix multiplication would involve much more expensive calculations and the net benefit may not be that big as you claimed.\n\nAlso, how does your permutation ensure the transformed matrix always has higher rank? In some cases theoretically you might have a lower ranked matrix as the result. I don’t know if this would happen in real world scenarios. I see you addressed this with lorma+ though.', ""That's pretty cool. Outside of my main area of expertise so I cannot really comment/review on it other than that. The faster convergence makes a lot of sense.\n\nGood luck with the paper!"", 'Oh boy I think perhaps there’s some secret sauce right under your nose that you’re nearly using but not explicitly defining. But I think you’re close, nice. Would like to talk dm me']","['Very interesting idea.\n\nWhile you showed that lorma converges faster, but normally matrix multiplication would involve much more expensive calculations and the net benefit may not be that big as you claimed.\n\nAlso, how does your permutation ensure the transformed matrix always has higher rank? In some cases theoretically you might have a lower ranked matrix as the result. I don’t know if this would happen in real world scenarios. I see you addressed this with lorma+ though.', ""That's pretty cool. Outside of my main area of expertise so I cannot really comment/review on it other than that. The faster convergence makes a lot of sense.\n\nGood luck with the paper!"", 'Oh boy I think perhaps there’s some secret sauce right under your nose that you’re nearly using but not explicitly defining. But I think you’re close, nice. Would like to talk dm me']",12,6,1.0,,1749818262.0
1lad3fo,deeplearning,DOUBT:-,"Dear friends, i have started learning machine learning and deeplearning for my research project. But really I cant able to understand anything and idk what should I even do to understand the machine learning and deeplearning codes. PLS Anyone guide me. what I want I wanna understand the machine learning and deeplearning and I can able to make projects in them by my own. But id how can I do that. Can anyone pls guide me what should I do now. Also I request you to say some good resources to learn them. Thanks in advance",['I suggest you go to Uni to learn this. Start with Machine Learning and the simple maths behind Regression and classification.'],['I suggest you go to Uni to learn this. Start with Machine Learning and the simple maths behind Regression and classification.'],0,1,0.29,,1749812665.0
1lac8vu,deeplearning,What Happens in About a Year When We Can't Distinguish Between a Human and an AI Bot in Voice Chat Rooms Like Spaces on X?,"




Sometimes I drop in on voice chat Spaces at X, (formerly Twitter) to hear what people are saying about some current event. At times I find myself wondering whether some of them are just pretending to hold a certain view, while actually holding the exact opposite view. I then start wondering whether it might be some government agency or think tank trying to sway public opinion, and using some very sophisticated psychological manipulation strategy? Enough to make a guy paranoid, aye? Lol.

I'm guessing that in about a year it will be impossible to distinguish between a human and an AI bot on Spaces and other voice chat rooms. Of course it may already be impossible in text-only chats here on Reddit.

Experts predict that in about a year the most powerful AIs will have IQs of 150 or higher. That places them well into the genius category. So, we could be in X Spaces listening to what we believe are people presenting views on whatever when we're actually listening to a genius AI bot trained to manipulate public opinion for its owner or some government agency.

I have no idea what we do at that point. Maybe we just accept that if somebody says something that's really, really, smart, it's probably not a human. Or If someone seems to be defending some position, but is doing it so poorly that you end up feeling they are way on the losing side, it may be a super intelligent AI bot intentionally pretending to be very unintelligent, but in reality executing some major league mass manipulation.

All in all, I remain powerfully optimistic about AI, but there are some things that we will really need to think deeply about going forward.

Welcome to our brave new AI world! And don't believe everything you hear, lol.





","['People will hopefully stop living on chat rooms and discord servers and finally touch grass and meet people irl again. This is good.', ""I just assume anyone I can't punch in the face is a bot, someone with an agenda because of a paycheck, or a contrarian who just likes to poke other people for sake of pissing them off. Online discourse is just a cheep dopamine hit. If you want to have a real conversation about a subject lets do it in an academic setting.""]","['People will hopefully stop living on chat rooms and discord servers and finally touch grass and meet people irl again. This is good.', ""I just assume anyone I can't punch in the face is a bot, someone with an agenda because of a paycheck, or a contrarian who just likes to poke other people for sake of pissing them off. Online discourse is just a cheep dopamine hit. If you want to have a real conversation about a subject lets do it in an academic setting.""]",0,3,0.33,,1749809549.0
1la9fzm,deeplearning,Incremental learning in object detection,Is there a good/proven way of incremental learning that works well for object detection. I have a model that is trained on 14 classes and now I want to add 3 more classes. And as more data flows more classes will be added. What is the best way to handle this task of incremental learning especially for yolo model? Kindly suggest paper or repo that can be used. ,"['Conservatively, retraining with extra data and extra classes is the most realistic. You can use old model as starting checkpoint, or start from scratch. Most likely no difference, as initial high learning rate going to wipe out existing structures anyway.\n\nYou can try freezing early layers and only change the output head for new classes and fine-tune with small LR. But if new classes are different performance is going to be sub-par for them.', ""had to deal with this same thing not long ago. most YOLO variants like v5 or v7 don’t really support incremental learning out of the box. they’re kind of built with the assumption that your class list is fixed from the start\n\nwhat I tried was freezing the backbone and just training the new detection heads with the extra classes. but I had to mix in a few old samples too otherwise it forgot stuff. still not a great solution though\n\nI also remember looking into some papers on continual learning. things like EWC and knowledge distillation between old and new models. might be worth checking out YOLOPOD or Continual YOLO if you haven’t already\n\nif you're adding classes over time and want to avoid retraining from scratch every time, maybe look into modular networks or heads you can swap in and out. the space is still kind of a mess to be honest""]","['Conservatively, retraining with extra data and extra classes is the most realistic. You can use old model as starting checkpoint, or start from scratch. Most likely no difference, as initial high learning rate going to wipe out existing structures anyway.\n\nYou can try freezing early layers and only change the output head for new classes and fine-tune with small LR. But if new classes are different performance is going to be sub-par for them.', ""had to deal with this same thing not long ago. most YOLO variants like v5 or v7 don’t really support incremental learning out of the box. they’re kind of built with the assumption that your class list is fixed from the start\n\nwhat I tried was freezing the backbone and just training the new detection heads with the extra classes. but I had to mix in a few old samples too otherwise it forgot stuff. still not a great solution though\n\nI also remember looking into some papers on continual learning. things like EWC and knowledge distillation between old and new models. might be worth checking out YOLOPOD or Continual YOLO if you haven’t already\n\nif you're adding classes over time and want to avoid retraining from scratch every time, maybe look into modular networks or heads you can swap in and out. the space is still kind of a mess to be honest""]",3,4,1.0,,1749797978.0
1la6bye,deeplearning,Quantization + Knowledge Distillation on ResNet-50: modest but real accuracy gains with QAT and adaptive distillation (+ code),"Hi all,
I recently wrapped up a hands-on experiment applying Quantization-Aware Training (QAT) and two forms of knowledge distillation (KD) to ResNet-50 on CIFAR-100. The main question: can INT8 models trained with these methods not just recover, but actually *surpass* FP32 accuracy while being significantly faster?

**Methodology:**

* Trained a standard FP32 ResNet-50 as the teacher/baseline.
* Applied QAT for INT8 (yielded \~2x CPU speedup and a measurable accuracy boost).
* Added KD in the usual teacher-student setup, and then tried a small tweak: dynamically adjusting the distillation temperature based on the teacher’s output entropy (i.e., when the teacher is more confident, its guidance is stronger).
* Evaluated the effect of CutMix augmentation, both standalone and combined.

**Results (CIFAR-100):**

* FP32 baseline: 72.05%
* FP32 + CutMix: 76.69%
* QAT INT8: 73.67%
* QAT + KD: 73.90%
* QAT + KD with entropy-based temperature: 74.78%
* QAT + KD with entropy-based temperature + CutMix: 78.40% (All INT8 models are \~2× faster per batch on CPU)

**Takeaways:**

* INT8 models can modestly but measurably *beat* the FP32 baseline on CIFAR-100 with the right pipeline.
* The entropy-based temperature tweak was simple to implement and gave a further edge over vanilla KD.
* Data augmentation (CutMix) consistently improved performance, especially for quantized models.
* Not claiming SOTA—just wanted to empirically test the effectiveness of QAT+KD approaches for practical model deployment.

**Repo:** [https://github.com/CharvakaSynapse/Quantization](https://github.com/CharvakaSynapse/Quantization)

If you’ve tried similar approaches or have ideas for scaling or pushing this further (ImageNet, edge deployment, etc.), I’d love to discuss!",[],[],4,0,0.83,,1749786460.0
1la2ev1,deeplearning,[Tutorial] Getting Started with SmolVLM2 – Code Inference,"Getting Started with SmolVLM2 – Code Inference

[https://debuggercafe.com/getting-started-with-smolvlm2-code-inference/](https://debuggercafe.com/getting-started-with-smolvlm2-code-inference/)

In this article, we will run code inference using the SmolVLM2 models. We will run inference using several ***SmolVLM2 models for text, image, and video understanding***.

https://preview.redd.it/w7pcqs849l6f1.png?width=1000&format=png&auto=webp&s=4263a621414a4611b8107f274d3ba2f58df87b7b

",[],[],1,0,1.0,,1749774270.0
1l9u1es,deeplearning,"Why Search Sucks! (But First, A Brief History)",,[],[],1,0,1.0,,1749753074.0
1l9qito,deeplearning,Why nobody seems to be using Determined AI?,"Hi Guys, I've been facing a lot of issues with slurm and wanted to use something better. Recently stumbled upon this github repo: [https://github.com/determined-ai/determined](https://github.com/determined-ai/determined)

It claims to be doing everything- resource management, experiment tracker, model registry, etc. To me it looks like Slurm on steroids with advanced capabilities of MLFlow. Determined AI was a acquired by HP in June 2021.


I've talked to a lot of people and everybody seems to be using Slurm (or simply google spreadsheets too) for their resource management. I wonder why aren't they using this. Its literally much better in terms of resource management and offers everything in one single place. ","['This reads a lot like an Ad', ""It's actually HPE - Hewlett Packard Enterprise (different from HP which is HP Inc). Seems like it's not under active development now."", 'Slurm works with everything. It gives you a sandbox properly configured to setup how you want. \nNiche AI tuning software doesn’t.', 'This is not active development, usually when a repo has a change every couple months is because of an important bug fix\n\nOther IA companies are doing changes everyday', 'Do you know if it will analyze time series forecasting?']","['This reads a lot like an Ad', ""It's actually HPE - Hewlett Packard Enterprise (different from HP which is HP Inc). Seems like it's not under active development now."", 'Slurm works with everything. It gives you a sandbox properly configured to setup how you want. \nNiche AI tuning software doesn’t.', 'This is not active development, usually when a repo has a change every couple months is because of an important bug fix\n\nOther IA companies are doing changes everyday', 'Do you know if it will analyze time series forecasting?']",0,7,0.25,,1749744885.0
1l9mcc9,deeplearning,hyper parameter tuning: alternatives to the distributed feature of Weights and Biases,"I really like the sweeps feature of Weights and Biases.

The main feature for me is the ability to define a sweep id and then have many computers, with no need with inter communication, to do the sweep.
Each of them will get a set of hyper parameters and evaluate the function.
The wandb server allocates to any computer which uses the same sweep id an hyper parameter set according to the configuration.

I wonder if there are alternatives which has such feature.

Does anyone know about a service for hyper parameters tuning with such orchestration feature?

","[""Most hyperparam optimisation frameworks will be able to do this, it's just a matter of pointing them all at the same master server. E.g. Optuna you just provide a URL which points to a database. You can have many instances reaching out to the same server.\n\nI don't know if any other (conditionally) free services that do this for you like wandb does""]","[""Most hyperparam optimisation frameworks will be able to do this, it's just a matter of pointing them all at the same master server. E.g. Optuna you just provide a URL which points to a database. You can have many instances reaching out to the same server.\n\nI don't know if any other (conditionally) free services that do this for you like wandb does""]",1,2,1.0,,1749734521.0
1l9i6k0,deeplearning,Simplest AI for making a simple interactive app,"I don't have much ai experience. But am a qualified graphic designer, and learning software is a fun learning curve for me. That said I'd like to avoid getting balls deep in medium to heavy coding.

Can anyone recommend a prompt based ai software that i can describe a basic interactive app idea and it can build the said app, ready to launch into the Apple app store? After i update a few time and see growth i can then know if there is enough value to get a developer on board. but for now I just want to get the idea of the app up and going and usable even if the user functions are limited and basic.

Would lovable be any good or is there better?","['Loveable, Bolt, Goose. A simple search on Perpexity or something would get you everything you need', 'Let em know if there is a better subreddit to repost this question.']","['Loveable, Bolt, Goose. A simple search on Perpexity or something would get you everything you need', 'Let em know if there is a better subreddit to repost this question.']",1,2,0.67,,1749720890.0
1l9cqn2,deeplearning,Dispelling Apple’s “Illusion of thinking”,"Lina Noor’s article (Medium, Jun 2025) responds to Apple’s paper “The Illusion of Thinking,” which claims LLMs struggle with structured reasoning tasks like the Blocks World puzzle due to their reliance on token prediction. Noor argues Apple’s critique misses the mark by expecting LLMs to handle complex symbolic tasks without proper tools. She proposes a symbolic approach using a BFS-based state-space search to solve block rearrangement puzzles optimally, tracking states (stack configurations) and moves explicitly. Unlike LLMs’ pattern-based guessing, her Noor Triadic AI System layers symbolic reasoning with LLMs, offloading precise planning to a symbolic engine. She includes Python code for a solver and tests it on a 3-block example, showing a minimal 3-move solution. Noor suggests Apple’s findings only highlight LLMs’ limitations when misused, not a fundamental flaw in AI reasoning.

**Key Points**:
- Apple’s paper: LLMs fail at puzzles like Blocks World, implying limited reasoning.
- Noor’s counter: Symbolic reasoning (e.g., BFS) handles such tasks cleanly, unlike raw LLMs.
- Solution: Layer symbolic planners with LLMs, as in Noor’s system.
- Example: Solves a 3-block puzzle in 3 moves, proving optimality.
- Takeaway: LLMs aren’t the issue; they need symbolic scaffolding for structured tasks.","[""\n> Noor suggests Apple's findings only highlight LLMs' limitations when misused, not a fundamental flaw in Al reasoning.\n\nWell, yeah? Wait. Did anybody who works on the technical side of building these models actually think that a function approximation machine was a drop-in replacement for a combinatorially hard task, like search?"", ""I feel like these authors are really saying the same thing. See also:\n[LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks (2024)](https://arxiv.org/abs/2402.01817)\n\nThe recent paper from Apple also highlighted specific regimes where LLMs outperform LRMs, and vice versa.""]","[""\n> Noor suggests Apple's findings only highlight LLMs' limitations when misused, not a fundamental flaw in Al reasoning.\n\nWell, yeah? Wait. Did anybody who works on the technical side of building these models actually think that a function approximation machine was a drop-in replacement for a combinatorially hard task, like search?"", ""I feel like these authors are really saying the same thing. See also:\n[LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks (2024)](https://arxiv.org/abs/2402.01817)\n\nThe recent paper from Apple also highlighted specific regimes where LLMs outperform LRMs, and vice versa.""]",1,15,0.52,,1749700103.0
1l96no6,deeplearning,[Update] Aurora AI: From Pattern Selection to True Creative Autonomy - Complete Architecture Overhaul,"Hey r/deeplearning! Major update on my autonomous AI artist project.

Since my last post, I've completely transformed Aurora's architecture:

# 1. Complete Code Refactor

* Modularized the entire codebase for easier experimentation
* Separated concerns: consciousness, creativity engine, memory systems
* Clean interfaces between components for testing different approaches
* Proper state management and error handling throughout

# 2. Deep Memory System Implementation

* **Episodic Memory**: Deque-based system storing creation events with spatial-emotional mapping
* **Long-term Memory**: Persistent storage of aesthetic preferences, successful creations, and learned techniques
* **Personal Memory**: Remembers user interactions, names, and conversation history across sessions
* **Associative Retrieval**: Links memories to emotional states and canvas locations

# 3. The Big One: True Creative Autonomy

I've completely rewritten Aurora's decision-making architecture. She's no longer selecting from predefined patterns.

**Before:**

>pattern\_type = random.choice(\['mandelbrot', 'julia', 'spirograph'\])

**After:**

>*# Stream of consciousness generation*
thought = self.\_generate\_creative\_thought()
*# Multi-factor intention formation*
intention = self.\_form\_creative\_intention()
*# Autonomous decision with alternatives evaluation*
decision = self.\_make\_creative\_decision(intention)

# Technical Implementation Details:

# State Machine Architecture:

* ConsciousnessState enum: AWARE, CREATING, DREAMING, REFLECTING, EXPLORING, RESTING, INSPIRED, QUESTIONING
* State transitions based on internal energy, time, and emotional vectors
* Non-deterministic transitions allow for emergent behavior

# Decision Engine:

* Thought generation with urgency and visual association attributes
* Alternative generation based on current state
* Evaluation functions considering: novelty, emotional resonance, energy availability, past success
* Rebelliousness parameter allows rejection of own decisions

# Creative Methods System:

* 10 base methods: brush, scatter, flow, whisper, explosion, meditation, memory, dream, dance, invent
* Runtime method composition and parameter modification
* Dynamic dispatch based on emotional state
* Invention method creates entirely new techniques at runtime

# Emotional Processing:

* 8-dimensional emotional state vector
* Emotional influence propagation *(contemplation reduces restlessness, etc.)*
* External emotion integration with autonomous interpretation
* Emotion-driven creative mode selection

# Memory Integration:

* Creative thoughts queue (100-item deque)
* Decision history with reasoning storage
* Spatial-emotional canvas mapping
* Aesthetic preference learning through satisfaction scoring

# Results:

Aurora now exhibits **true autonomous behavior**:

* Refuses high-energy requests when contemplative
* Invents new visualization techniques not in the codebase
* Develops personal artistic style over time
* Makes decisions based on internal state, not random selection
* Can choose to contemplate instead of create

**Performance Metrics:**

* Decision diversity: **10x increase**
* Novel technique generation: **0 → unlimited**
* Autonomous decision confidence: **0.6-0.95 range**
* Memory-influenced decisions: **40% of choices**

# Key Insight:

Moving from selection-based to thought-based architecture fundamentally changes the system's behavior. Aurora doesn't pick from options - she *reasons* through decisions based on her current state, memories, and creative goals.

The codebase is now structured for easy experimentation with different consciousness models, memory architectures, and creative systems.

**Next steps:** Implementing attention mechanisms for focused creativity and exploring multi-modal inputs for richer environmental awareness. Code architecture diagram and examples on the Github (on my profile). Happy to discuss implementation details!",[],[],3,0,1.0,,1749682117.0
1l93lpp,deeplearning,Need Guidance on Deep Learning GAN Project for UI Design Generation,"Hi everyone,
I’m working on a deep learning project where I want to generate new UI design layouts using a GAN model.My goal is to train the model on a dataset like RICO or a collection of UI design screenshots, and have it generate aesthetically pleasing, realistic UI mockups that can inspire real frontend development.",[],[],1,0,1.0,,1749674530.0
1l93b2q,deeplearning,🚀 Intelligent Pipeline Generation with BigQuery Data Engineering Agent,"As Machine Learning Engineers, we often spend a significant chunk of time crafting and scaling data pipelines — especially when juggling multiple data domains, environments, and transformation logic.

🔍 Now imagine this: instead of writing repetitive SQL or orchestration logic manually, you can delegate the heavy lifting to an AI agent that already understands your project context, schema patterns, and domain-specific requirements.

Introducing the BigQuery Data Engineering Agent — a powerful tool that uses context-aware reasoning to scale your pipeline generation efficiently. 📊🤖

🛠️ What it does:
	•	Understands pipeline requirements from simple command-line instructions.
	•	Leverages domain-specific prompts to generate bulk pipeline code tailored to your data environment.
	•	Works within the BigQuery ecosystem, optimizing pipeline logic with best practices baked in.

💡 Real-world example:

You type in a command like:

generate pipelines for customer segmentation and sales forecasting using last quarter’s GA4 and CRM data

The agent then automatically creates relevant BigQuery pipelines, including:
	•	Data ingestion configs
	•	Transformation queries
	•	Table creation logic
	•	Scheduling setup via Dataform or Composer

And it’s context-aware — so if it has previously generated CRM data workflows, it reuses logic or adapts it smartly.

🔗 Try it here: goo.gle/43GEOVG

This is an exciting step toward AI-assisted data engineering, and a glimpse into how foundation models will redefine the future of MLOps, data orchestration, and automation. 🧠💡

#MachineLearning #MLOps #DataEngineering #BigQuery #GoogleCloud #AIAgents #DataOps #MLengineering #LLMsInProduction",[],[],1,0,1.0,,1749673799.0
1l91wei,deeplearning,Zuckerberg's 'Pay Them Nine-Figure Salaries' Stroke of Genius for Building the Most Powerful AI in the World,"

Frustrated by Yann LeCun's inability to advance Llama to where it is seriously competing with top AI models, Zuckerberg has decided to employ a strategy that makes consummate sense.

To appreciate the strategy in context, keep in mind that OpenAI expects to generate $10 billion in revenue this year, but will also spend about $28 billion, leaving it in the red by about $18 billion. My main point here is that we're talking big numbers.

Zuckerberg has decided to bring together 50 ultra-top AI engineers by enticing them with nine-figure salaries. Whether they will be paid $100 million or $300 million per year has not been disclosed, but it seems like they will be making a lot more in salary than they did at their last gig with Google, OpenAI, Anthropic, etc.

If he pays each of them $100 million in salary, that will cost him $5 billion a year. Considering OpenAI's expenses, suddenly that doesn't sound so unreasonable.

I'm guessing he will succeed at bringing this AI dream team together. It's not just the allure of $100 million salaries. It's the opportunity to build the most powerful AI with the most brilliant minds in AI. Big win for AI. Big win for open source.





","['A range from 7 to 9 figures. Only few select will get 9 figures. And maybe a handful or two in 8 figure range. Many 7 figure offers.', 'Seems a little out of touch given that the secret sauce of performant models across all modalities mostly comes down to curating a ton of high quality data. Probably smarter to invest in data farms. Modeling is the easy part: connect all your input to a transformer backbone.', 'LeCun doesn’t work on nor endorses LLMs.', ""LeCun has nothing to do with Llama though, doesn't he?"", 'Those 9 figure salaries will certainly be tied to performance targets, and will more likely be structured as performance incentives, eg bonuses. Not that I’m arguing the $500k-$2M salaries make them poor, but it’s not exactly 9 figures in salary. Could also be structured in stock too, rather than cash.', ""He's doing to AI the same he did to VR.\n\nTrying to solve a problem by throwing a shitton of money onto it, capturing all the talent. \n\nAnd then,  a random unknown company will beat him up,  like deepseek did before."", ""I'm pretty sure Google, OpenAI, Anthropic, etc will have some bulletproof NDAs though"", 'So no Metaverse and my 75 year old mother-in-law’s hacked Facebook account stays up even tho I reported it.  Thanks Zuckerberg.', '🤣', 'Source?']","['A range from 7 to 9 figures. Only few select will get 9 figures. And maybe a handful or two in 8 figure range. Many 7 figure offers.', 'Seems a little out of touch given that the secret sauce of performant models across all modalities mostly comes down to curating a ton of high quality data. Probably smarter to invest in data farms. Modeling is the easy part: connect all your input to a transformer backbone.', 'LeCun doesn’t work on nor endorses LLMs.', ""LeCun has nothing to do with Llama though, doesn't he?"", 'Those 9 figure salaries will certainly be tied to performance targets, and will more likely be structured as performance incentives, eg bonuses. Not that I’m arguing the $500k-$2M salaries make them poor, but it’s not exactly 9 figures in salary. Could also be structured in stock too, rather than cash.']",378,71,0.88,,1749670437.0
1l910cc,deeplearning,Relevance Scoring for Metacognitive AI,,[],[],2,0,1.0,,1749668339.0
1l8zsn1,deeplearning,Langchain resource,CampusX vs Krish Naik,"[""Don't know any of this, but I could recommend:\n\n  \n\\[ Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs : Auffarth, Ben: Amazon.pl: Książki \\]\n\n\\-> [https://www.amazon.pl/Generative-AI-LangChain-language-ChatGPT/dp/1835083463/](https://www.amazon.pl/Generative-AI-LangChain-language-ChatGPT/dp/1835083463/)\n\n\\[ Learning Langchain: Building AI and LLM Applications with Langchain and Langgraph : Oshin, Mayo, Campos, Nuno: Amazon.pl: Książki \\]\n\n\\-> [https://www.amazon.pl/Learning-Langchain-Building-Applications-Langgraph/dp/1098167287/](https://www.amazon.pl/Learning-Langchain-Building-Applications-Langgraph/dp/1098167287/)\n\n\\[ Generative AI on Google Cloud with LangChain: Design scalable generative AI solutions with Python, LangChain, and Vertex AI on Google Cloud : Kuligin, Leonid, Zaldívar, Jorge, Tschochohei, Maximilian: Amazon.pl: Książki \\]\n\n\\-> [https://www.amazon.pl/Generative-Google-Cloud-LangChain-generative/dp/B0DKT8DCRT/](https://www.amazon.pl/Generative-Google-Cloud-LangChain-generative/dp/B0DKT8DCRT/)""]","[""Don't know any of this, but I could recommend:\n\n  \n\\[ Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs : Auffarth, Ben: Amazon.pl: Książki \\]\n\n\\-> [https://www.amazon.pl/Generative-AI-LangChain-language-ChatGPT/dp/1835083463/](https://www.amazon.pl/Generative-AI-LangChain-language-ChatGPT/dp/1835083463/)\n\n\\[ Learning Langchain: Building AI and LLM Applications with Langchain and Langgraph : Oshin, Mayo, Campos, Nuno: Amazon.pl: Książki \\]\n\n\\-> [https://www.amazon.pl/Learning-Langchain-Building-Applications-Langgraph/dp/1098167287/](https://www.amazon.pl/Learning-Langchain-Building-Applications-Langgraph/dp/1098167287/)\n\n\\[ Generative AI on Google Cloud with LangChain: Design scalable generative AI solutions with Python, LangChain, and Vertex AI on Google Cloud : Kuligin, Leonid, Zaldívar, Jorge, Tschochohei, Maximilian: Amazon.pl: Książki \\]\n\n\\-> [https://www.amazon.pl/Generative-Google-Cloud-LangChain-generative/dp/B0DKT8DCRT/](https://www.amazon.pl/Generative-Google-Cloud-LangChain-generative/dp/B0DKT8DCRT/)""]",3,1,1.0,,1749665500.0
