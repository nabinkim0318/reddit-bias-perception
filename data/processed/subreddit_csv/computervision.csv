id,subreddit,title,selftext,comments,top_comments,score,num_comments,upvote_ratio,flair,created_utc
1lt0x4u,computervision,Help in project,"Hey everyone!

I’m working on a computer vision project focused on face recognition for attendance systems, but I’m approaching it differently than most existing solutions.

My system uses a camera mounted above a doorway. The goal is to detect and recognize faces instantly the moment a face appears, even for a fraction of a second. No waiting, no perfect face alignment just fast, reliable detection as people walk through.

I’ve found it really hard to get existing models to work well in this setup and it always takes a bit like 2-5seconds not quick detection and I’m still new to this field so if anyone has advice, model suggestions, tuning tips, or just general guidance, I’d appreciate it a lot.

Thanks in advance!
","['Object detection for faces + tracking. Then check the top-k best detection score crops for FaceID. You need fast detection model for this to work, or good hardware. FaceID could be asyn and slow.', 'yolo can do it real time (face detection with bounding box), then do recognition\n\nIf it is not commercial, use newer yolo models (8), if it is, older will suffice', '> not even for a fraction of a second\n\n0 microsecond latency isn’t a spec, it’s magic. How fast do you really need it?']","['Object detection for faces + tracking. Then check the top-k best detection score crops for FaceID. You need fast detection model for this to work, or good hardware. FaceID could be asyn and slow.', 'yolo can do it real time (face detection with bounding box), then do recognition\n\nIf it is not commercial, use newer yolo models (8), if it is, older will suffice', '> not even for a fraction of a second\n\n0 microsecond latency isn’t a spec, it’s magic. How fast do you really need it?']",2,3,1.0,Help: Project,1751807567.0
1lssfxa,computervision,Installing detectron2 or mmdetection on HPC is near impossible,"Hi, I am new to using the bigger ML CV packages so I'm not sure what the common practice is. I'm currently trying to do some ML tasks on my university cluster using a custom dataset in my lab.

I was wondering if it was worth the hassle trying to install detectron2 or mmdetection on my cluster account or if it's better to just write the programs from scratch.

I've spent a really long time trying to install these, but it seems impossible to get any compatibility working, especially since I need it to work with another workflow I have. I also don't have any sudo permissions (of course) so I can't really force the necessary packages that they specify.","['ditch mmdetection, it’s a nightmare mess of a terrible codebase', 'I worked  extensively with detectron2. Detectron2 is really easy to install in linux based system. Just pip install. The depency like pytorch and cuda you have to manage  properly. \n\nWith mmdetection it is always been a big problem for installing. Mmdetection is also not maintained actively so  i understand. But for detectron2 you can mention what issue are you facing.']","['ditch mmdetection, it’s a nightmare mess of a terrible codebase', 'I worked  extensively with detectron2. Detectron2 is really easy to install in linux based system. Just pip install. The depency like pytorch and cuda you have to manage  properly. \n\nWith mmdetection it is always been a big problem for installing. Mmdetection is also not maintained actively so  i understand. But for detectron2 you can mention what issue are you facing.']",7,4,1.0,Help: Project,1751775573.0
1lsmn0j,computervision,Making yolo faster,Hi everyone I’m using yolov8 for a project for person detection. I’m just using a webcam on my laptop and trying to run the object detection in real time but it’s super slow and lags quite a bit. I’ve tried using different models and right now I’m using v8 nano but it’s still pretty bad. I was wondering if anyone has any tips to increase the speed? Anything helps thanks so much!,"['Define: laptop, software stack, super slow, realtime, and image.\xa0', ""Reduce model resolution to smth like 320x320. Convert it to OpenVINO. Quantize using NNCF. Should be possible to achieve 30-40 FPS.\n\nAdditionally, you can skip similar frames using motion detection and only perform inference when frame has changed above threshold, or when there hasn't been any movement for, say, 0.5 sec."", ""What native format are u using? If it's is .pt you can convert the model to .onnx or openvino format for CPU inference"", 'Yolov11 is suposed to run faster.\n\nBetter graphic card runs faster. 3060 runs 20x faster than 2060.\n\nLower resolution computes faster', 'Which dataset?']","['Define: laptop, software stack, super slow, realtime, and image.\xa0', ""Reduce model resolution to smth like 320x320. Convert it to OpenVINO. Quantize using NNCF. Should be possible to achieve 30-40 FPS.\n\nAdditionally, you can skip similar frames using motion detection and only perform inference when frame has changed above threshold, or when there hasn't been any movement for, say, 0.5 sec."", ""What native format are u using? If it's is .pt you can convert the model to .onnx or openvino format for CPU inference"", 'Yolov11 is suposed to run faster.\n\nBetter graphic card runs faster. 3060 runs 20x faster than 2060.\n\nLower resolution computes faster', 'Which dataset?']",7,7,0.82,Help: Project,1751756656.0
1lsmm7r,computervision,Making yolo faster,Hi everyone I’m using yolov8 for a project for person detection. I’m just using a webcam on my laptop and trying to run the object detection in real time but it’s super slow and lags quite a bit. I was wondering if anyone has any tips to increase the speed? Anything helps thanks so much!,"[""1. Use an external GPU if you have it \n2. Run a fp16 or int8 variant of the model \n3. Reduce the input image size you are sending to the model\n4. Run a lower variant model like nano or small\n5. Frame skip if you can't process every frame\n\n\nThere are many other things you can do as well to boost the performance or inference speed. It depends upon how much you want it or whatever works best for you.""]","[""1. Use an external GPU if you have it \n2. Run a fp16 or int8 variant of the model \n3. Reduce the input image size you are sending to the model\n4. Run a lower variant model like nano or small\n5. Frame skip if you can't process every frame\n\n\nThere are many other things you can do as well to boost the performance or inference speed. It depends upon how much you want it or whatever works best for you.""]",0,1,0.5,Help: Project,1751756590.0
1lsj2pq,computervision,Classification using multiple inputs?,"Working on image analysis tasks where it may be helpful to feed the network with photos taken from different viewpoints.

Before I spend time building the pipelines I figured I should consult published research, but surprisingly I'm not finding much out there outside of 3D reconstruction and video analysis.

The domain is plywood manufacturing. Closeup photos of plywood need to be classified according to the type of wood (i.e. looking at the grain textures) which would benefit from seeing a photo of the whole sheet (i.e. any stamps or other manmade markings, and large-scale grain features). A defect detection model also needs to run on the whole-sheet image. When inspecting defects it's helpful to look at the sheet from multiple angles (i.e. to ""cancel out"" reflections and glare).

Is anyone familiar with research into what I guess would be called ""multi-view classification and detection""? Or have you worked on this area yourself?",['Just feed the same network with those images and implement a heuristic which aggregates the results and picks a single end result from the multiple ones'],['Just feed the same network with those images and implement a heuristic which aggregates the results and picks a single end result from the multiple ones'],3,3,1.0,Help: Project,1751746756.0
1lshx5i,computervision,"Image description models (Object detection, OCR, Image processing, CNN) make LLMs SOTA in AI agentic benchmarks like Android World and Android Control","Yesterday, I finished evaluating my Android agent model, deki, on two separate benchmarks: Android Control and Android World. For both benchmarks I used a subset of the dataset without fine-tuning. The results show that image description models like deki enables large LLMs (like GPT-4o, GPT-4.1, and Gemini 2.5) to become State-of-the-Art on Android AI agent benchmarks using only vision capabilities, without relying on Accessibility Trees, on both single-step and multi-step tasks.


deki is a model that understands what’s on your screen and creates a description of the UI screenshot with all coordinates/sizes/attributes. All the code is open sourced. ML, Backend, Android, code updates for benchmarks and also evaluation logs.

All the code/information is available on GitHub: [https://github.com/RasulOs/deki](https://github.com/RasulOs/deki)

I have also uploaded the model to Hugging Face:
Space: [orasul/deki](https://huggingface.co/spaces/orasul/deki)
(Check the analyze-and-get-yolo endpoint)

Model: [orasul/deki-yolo](https://huggingface.co/orasul/deki-yolo)","['Nice work, I’ll have to give this a shot!']","['Nice work, I’ll have to give this a shot!']",11,1,1.0,Discussion ,1751743621.0
1lsdb1u,computervision,"Tiger Woods’ Swing — No Motion Capture Suit, Just AI",,"['Surface-level and joint tracking from a video. All in 3D. Completely marker-less and no motion capture suit needed.', 'Mediapipe, I guess?', 'What are you using?  YOLO?', 'Can you post other videos of your model in action, idk like on [this](https://www.youtube.com/shorts/hO7Ro1Da5uM) one.\n\nIf true, this is amazing work! However it looks too good to be true.', 'It’s like the naysayers haven’t ever heard of the bitter lesson.\n\nSure, it’s technically impossible to infer absolute distances from a single photo. Maybe the video was shot in an alternate universe where people are 3 millimeters tall or something. But in practice, with enough training data a model can easily infer something that’s more than close enough. Just like a person born with one eye can very reliably perform tasks that the rest of us would have trouble with if we close one of our eyes. The world is FULl of reliably clues about the size of things, and a model trained in enough data will learn to recognize those clues.\xa0', ""do you have a paper or code to show? i'm interested in your work. the results look impressive. \n\ni haven't seen an approach that tracks surface level poiints, but i guess one could achieve similar results with SMPL or similar methods [https://smpl.is.tue.mpg.de/](https://smpl.is.tue.mpg.de/)"", 'Hey, this looks amazing, would you consider open sourcing?', 'This is only for pre-recorded videos? Or can it work with live video? [SAM2](https://ai.meta.com/sam2/) for example can only segment objects in recorded video because of how they trained the memory mechanism. Wondering if you are using a similar concept - also really cool results regardless open source or a paper or something would be great for all of here to see if you’re willing to share']","['Surface-level and joint tracking from a video. All in 3D. Completely marker-less and no motion capture suit needed.', 'Mediapipe, I guess?', 'What are you using?  YOLO?', 'Can you post other videos of your model in action, idk like on [this](https://www.youtube.com/shorts/hO7Ro1Da5uM) one.\n\nIf true, this is amazing work! However it looks too good to be true.', 'It’s like the naysayers haven’t ever heard of the bitter lesson.\n\nSure, it’s technically impossible to infer absolute distances from a single photo. Maybe the video was shot in an alternate universe where people are 3 millimeters tall or something. But in practice, with enough training data a model can easily infer something that’s more than close enough. Just like a person born with one eye can very reliably perform tasks that the rest of us would have trouble with if we close one of our eyes. The world is FULl of reliably clues about the size of things, and a model trained in enough data will learn to recognize those clues.\xa0']",32,38,0.78,Showcase,1751731529.0
1ls9fox,computervision,How to install mobilnet,,['Just use torchvision. It’s built in. Google will bring up tutorials for using torchvision and you can select any of the built-in models.\xa0'],['Just use torchvision. It’s built in. Google will bring up tutorials for using torchvision and you can select any of the built-in models.\xa0'],1,1,0.67,Help: Project,1751720700.0
1ls5xav,computervision,Advice and Tips for transfer learning and fine tuning Vision models,"Hi everyone,

I'm currently diving into classical computer vision models to deepen my understanding of the field, and I've hit a roadblock with transfer learning. Specifically, I'm struggling to achieve good results. My accuracy is stuck around 60% when trying to transfer learn the Food-101 dataset on models like AlexNet, ResNet, and VGG. The models are either overfitting or underfitting, depending on many layers I freeze or add to the model.

Could anyone recommend some good learning resources on effectively performing transfer learning and correctly setting hyperparameters? Any guidance would be greatly appreciated.","['The standard approach is to freeze the whole pre trained model, and add one or more layers , and do the training. \n\nYou say you observed under and overfitting. Can you share the summary of each scheme ? Especially include the number of layers you add, graphs of epochs vs loss, optimizer, train algo, and your main hyperparams.\n\nI feel like theres nothing specific to transfer learning that would cause under or over fitting . All the standard approaches to fixing each would apply.']","['The standard approach is to freeze the whole pre trained model, and add one or more layers , and do the training. \n\nYou say you observed under and overfitting. Can you share the summary of each scheme ? Especially include the number of layers you add, graphs of epochs vs loss, optimizer, train algo, and your main hyperparams.\n\nI feel like theres nothing specific to transfer learning that would cause under or over fitting . All the standard approaches to fixing each would apply.']",6,2,1.0,Help: Project,1751707570.0
1ls3c5o,computervision,Can I buy pyimagesearch university computer vision course for it's monthly cost of 28 dollars and is it worth it for it's yearly cost of 345 dollars,"They mention a monthly cost as 28 dollars, but there is no option to select 28 dollars on buying page and there is only a yearly cost option as 345 dollars.. at the moment I can't afford the  yearly cost..further need to know is this course worth buying at a price of 345 dollars for a year..","['There’s no reason to pay somebody when you can learn for free.', 'You’ll probably learn something from pyimagesearch if you know absolutely nothing about code or computer vision and require a structured plan, but I’d also recommend trying the free route first. You can get a free account at Anthropic and get Claude to generate a lesson plan for you with coding examples that is the same quality as what pyimagesearch gives you(in fact it will be better because you’ll be forced to debug hallucinations). The reason I say this is because I’ve been watching the guy behind the site for the past 8 years. He is definitely a businessman and not actually experienced in computer vision beyond what he googled to create his courses.', 'I would say no.']","['There’s no reason to pay somebody when you can learn for free.', 'You’ll probably learn something from pyimagesearch if you know absolutely nothing about code or computer vision and require a structured plan, but I’d also recommend trying the free route first. You can get a free account at Anthropic and get Claude to generate a lesson plan for you with coding examples that is the same quality as what pyimagesearch gives you(in fact it will be better because you’ll be forced to debug hallucinations). The reason I say this is because I’ve been watching the guy behind the site for the past 8 years. He is definitely a businessman and not actually experienced in computer vision beyond what he googled to create his courses.', 'I would say no.']",0,7,0.25,Discussion ,1751696913.0
1ls2lwd,computervision,YOLO Darknet Inferencer in C++,"**YOLO-DarkNet-CPP-Inference** is a high-performance C++ implementation for running YOLO object detection models trained using **Darknet**. This project is designed to deliver fast and efficient real-time inference, leveraging the power of OpenCV and modern C++.

It supports detection on both **static images** and **live camera feeds**, with output saved as annotated images or videos/GIFs. Whether you're building robotics, surveillance, or smart vision applications, this project offers a flexible, lightweight, and easy-to-integrate solution.[Github](https://github.com/2Thanh/YOLO-DarkNet-CPP-Inference)

","['This just wraps about the opencv dnn function.', ""Can the model trained this way be exported to ONNX or other formats? Cause it's useless otherwise. Your custom C++ runtime doesn't fit most of the use cases.""]","['This just wraps about the opencv dnn function.', ""Can the model trained this way be exported to ONNX or other formats? Cause it's useless otherwise. Your custom C++ runtime doesn't fit most of the use cases.""]",0,2,0.5,Help: Project,1751694069.0
1ls1eqq,computervision,"So how does movement detection work, when you want to exclude the cameraman's movement?","Seems a bit complicated, but I want to be able to track movement when I am moving but exclude my movement.  I also want it to be done when live. Not on a recording.

I also want this to be flawless. Is it possible to implement this flawlessly?


Edit: I am trying to create a tool for paranormal investigations for a phenomenon where things move behind your back when you're taking a walk in the woods or some other location.

Edit 2:

My idea is a 360-degree system that aids situational awareness.

Perhaps for Bigfoot enthusiasts or some kind of paranormal investigation, it would be a cool hobby.
","['Are you trying to catch ghosts? I’m sorry but they aren’t real so you won’t have any training datasets', "">  Is it possible to implement this flawlessly?\n\nShort answer is no, for example something might be moving at the same relative speed compared to you, so you would see that as a static object on the camera.\n\nYou can estimate the camera movement using Visual SLAM or Visual Odometry for simpler movements. After you have the camera movement you can compare what has moved on the scene with this estimated movement, and that's your foreground movement estimation."", ""Video stabilization and MoG would be pretty easy to implement. I doubt you can get it to be flawless except for large close up objects though\n\nEdit: just saw the line about it being a 360 video. It would make it a little bit more complicated. You'd need to wrap things around and include that in your detection"", 'If whatever film might move (like trees or whatever) then the only option is a gyroscope on the camera with custom written software that uses that info to counter the own movement. Will not be flawless. Will also be very hard. And probably not realtime as well.', 'Does your camera have an Inertial Measurement Unit (IMU), or - do you have access to an (precise) IMU (""gyro"") sensor data during recording? That would allow to detect ""movements"" and then to apply the ""anit-movement"" for ""stabilization"" - not sure whether this is what you mean with ""excluding camera man\'s movement"".', ""Totally solved with industrial solutions in the film VFX industry, but you'll never afford them, and if the makers of them learn why you want the systems they won't talk to you. I used to make them."", 'you could look into visual SLAM with IMU', 'To answer your question technically, optical flow and minus average motion could be pretty well. But idk what you are are trying to do']","['Are you trying to catch ghosts? I’m sorry but they aren’t real so you won’t have any training datasets', "">  Is it possible to implement this flawlessly?\n\nShort answer is no, for example something might be moving at the same relative speed compared to you, so you would see that as a static object on the camera.\n\nYou can estimate the camera movement using Visual SLAM or Visual Odometry for simpler movements. After you have the camera movement you can compare what has moved on the scene with this estimated movement, and that's your foreground movement estimation."", ""Video stabilization and MoG would be pretty easy to implement. I doubt you can get it to be flawless except for large close up objects though\n\nEdit: just saw the line about it being a 360 video. It would make it a little bit more complicated. You'd need to wrap things around and include that in your detection"", 'If whatever film might move (like trees or whatever) then the only option is a gyroscope on the camera with custom written software that uses that info to counter the own movement. Will not be flawless. Will also be very hard. And probably not realtime as well.', 'Does your camera have an Inertial Measurement Unit (IMU), or - do you have access to an (precise) IMU (""gyro"") sensor data during recording? That would allow to detect ""movements"" and then to apply the ""anit-movement"" for ""stabilization"" - not sure whether this is what you mean with ""excluding camera man\'s movement"".']",10,14,0.86,Help: Project,1751689491.0
1lrsnbo,computervision,GitHub - Hugana/p2ascii: Image to ascii converter,"Hey everyone,

I recently built p2ascii, a Python tool that converts images into ASCII art, with optional Sobel-based edge detection for orientation-aware rendering. It was inspired by a great video on ASCII art and edge detection theory, and I wanted to try implementing it myself using OpenCV.

It features:

- Sobel gradient orientation + magnitude for edge-aware ASCII rendering

 - Supports plain and colored ASCII output (image and text)

- Transparency mode for image outputs (no background, just characters)

I'd love feedback or suggestions — especially regarding performance or edge detection tweaks.
",['This is brilliant! Is it possible to scale down to fit certain number of character per row?'],['This is brilliant! Is it possible to scale down to fit certain number of character per row?'],6,2,1.0,Showcase,1751661101.0
1lris4d,computervision,Does algebraic topology in 3D CV give good results? If so what are some novel problems that can be solved using it?,"There are a lot of papers that make use of algebraic topology (AT) especially topics like persistent (co)homology and Hodge theory but do they give desired results? i.e. better results than conventional approaches, or do they solve problems that could otherwise not have been solved? or are they more computationally efficient?

Some of the uses I've read up on are for providing better loss functions by making point clouds more geometry aware, and cases with limited data. Others include creating methods that work on other 3D representations like manifolds and meshes.

[Topology-Aware Latent Diffusion for 3D Shape Generation](https://arxiv.org/html/2401.17603v1) paper uses persistent homology to generate shapes with desired topological properties (no. of holes) by injecting that information in the diffusion process. This is a good application (if I'm correct) as the workaround would be to caption the dataset with the desired property which is tedious and a new property means re-captioning.

But I doubt whether or not the results produced by AT are good because if they were the area would have been more popular but seems very niche today. So is this a good area to focus on? Are there any novel 3d CV problems to be solved using this?",[],[],8,0,0.83,Discussion ,1751635773.0
1lrhuvj,computervision,Wrote a 4-Part Blog Series on CNNs — Feedback and Follows Appreciated!,,[],[],0,0,0.5,Help: Theory ,1751633097.0
1lrbxex,computervision,"How do I replicate, and/or undo, this kind of camera-shot text for my dataset?","https://preview.redd.it/kre1supk0taf1.png?width=2044&format=png&auto=webp&s=f2098b7ebbaa5a6804b67198908d494b2a09f586

This is after denoising by averaging frames. Observations:

1. Weird inconsistent kind of artifact-looking green glow behind text. I notice a very slight glow in real life too.
2. Inconsistent color and shape, the S and U are good examples, some spots are darker than others.
3. Smooth-ish color transitions, notice the dot on the ""i"" only has one pixel of max darkness, with the rest fading around it to make the circle. Every character fades at the edges. Sorta looks like anti aliasing but natural

By undo I mean put it into a consistent form without all these camera photo inconsistencies. Trying to make a good synthetic dataset, maybe with BlenderProc or Unreal Engine or such",[],[],3,0,0.71,Help: Theory ,1751611058.0
1lr5i74,computervision,Semantic Segmentation using Web-DINO,"Semantic Segmentation using Web-DINO

[https://debuggercafe.com/semantic-segmentation-using-web-dino/](https://debuggercafe.com/semantic-segmentation-using-web-dino/)

The Web-DINO series of models trained through the Web-SSL framework provides several strong pretrained backbones. We can use these backbones for downstream tasks, such as semantic segmentation. In this article, we will use the ***Web-DINO model for semantic segmentation***.

https://preview.redd.it/56qfab0v6raf1.png?width=1000&format=png&auto=webp&s=21b8bd0f461018860d452cbff1afa3833ffd5aa4

",[],[],1,0,0.67,Showcase,1751589573.0
1lr23s4,computervision,Adapting YOLO for 1D Bounding Box,"Hi everyone!

This is my first post on this subreddit, but i need some help in regards of adapting YOLO v11 object detection code.

In short, I am using YOLOv11 OD as an image ""segmentator"" - splitting images into slices. In this case the hight parameters such as Y and H are dropped so the output only contains X and W.

Previously I just implemented dummy values within the dataset (setting Y to 0.5 and H to 1.0) and simply ignoring these values in the output, but I would like to try and get 2 parameters for the BBoxes.

As of now I have adapted head.py for the smaller dimensionality and updates all of the functions to handle 2 parameter cases. None the less I cannot manage to get working BBoxes.

Has anyone tried something similar? Any guidance would be much appreciated!","[""I haven't tried this myself, but I'm trying to wrap my head around the problem. How is it different from keypoint estimation?""]","[""I haven't tried this myself, but I'm trying to wrap my head around the problem. How is it different from keypoint estimation?""]",2,2,1.0,Help: Project,1751579897.0
1lqp1oi,computervision,Need dataset suggestions,"I’m looking for datasets specifically labeled with the **human or person or people** class to help my robot reliably detect people from a **low-angle perspective**. Currently, it performs well in identifying full human bodies in new environments, but it occasionally struggles when people wear different types of clothing—especially in close proximity.

For example, the YOLO model failed to detect a person walking nearby in shorts, but correctly identified them once they moved farther away. I need the **highest possible accuracy**, and I’m planning to fine-tune my model again.

I've come across the JRD dataset, but it might take some time to access. I also tried searching on Roboflow, but couldn’t find datasets with the specific low-angle or human-clothing variation tags I need.

If anyone knows a suitable dataset or can help, I’d really appreciate it.","[""I don't think there is one."", 'Is this one helpful? (I really like dataset ninja)  \n[https://datasetninja.com/rgbd-people](https://datasetninja.com/rgbd-people)']","[""I don't think there is one."", 'Is this one helpful? (I really like dataset ninja)  \n[https://datasetninja.com/rgbd-people](https://datasetninja.com/rgbd-people)']",2,4,0.75,Help: Project,1751547789.0
1lqoqey,computervision,Finding Figures in an image,"Hey everyone, I'm trying to solve this issue where I'm looking for figures/illustrations in a given image. The Image has a background figure that can be filling the whole image or parts of it or a collage and on other place a layout (could be transparent) with text on it. I would like to locate the revealed part of the figure (not the parts under the transparent layout) as a bounding box. So far what worked for me best is a fine tuned version of layoutlmv3 but it's quite slow on cpu and I feel like it's an overkill solution. Tried also
Doclayout-yolo https://github.com/opendatalab/DocLayout-YOLO

But generally yolo is not helpful in this case since it cannot generalize well on a different figures compared to finding a limited set of objects (even after fine tuning).

Would appreciate any advice on this thanks ",[],[],1,0,1.0,Help: Project,1751546913.0
1lqlll7,computervision,Open Pose models for pose estimation,"hii! I wanted to checkout the Open Pose models for exploration
I tried following the articles and github repo but the link to the 'pose\_iter\_440000.caffemodel' file seems to be broken both on the official links as well as in repos. Can anyone help me figure this out? Thanks.",['got it'],['got it'],2,1,1.0,Help: Project,1751536435.0
1lql2zj,computervision,Face recognition Accuracy,"I am trying to do a project using face recognition and i need to get high accuracy(above 90%), I can only use Open source and need to have to recognize faces at real time. I have currently used multiple open source models and trained custom datasets but i haven't gotten anything above 85% accuracy. The project is done in python & if anyone know any models that have high accuracy do comment/reply.

I used multiple pre-trained models and used custom datasets to increase the accuracy but the accuracy is not increasing above 80-85%. I have used Facenet, Arcface, Dlib as the models. Is there any other models that could be better ?

","['I achieved accuracy above 99% in humans on Arcface, but I had to clean the data (MS-CELEB-1M).', 'What do you mean by ""90% accuracy""? Ignoring the fact that accuracy isn\'t a good metric in face recognition, it highly depends on the test-set.\nIn general 90% seems very easy to achieve. I think any model will do, you just need enough training data.']","['I achieved accuracy above 99% in humans on Arcface, but I had to clean the data (MS-CELEB-1M).', 'What do you mean by ""90% accuracy""? Ignoring the fact that accuracy isn\'t a good metric in face recognition, it highly depends on the test-set.\nIn general 90% seems very easy to achieve. I think any model will do, you just need enough training data.']",4,5,1.0,Help: Project,1751534351.0
1lqk5dh,computervision,Need to detect colors but the code ends,"I am trying to learn to detect colors with opencv in c++ in the same way i did in python (here is the link to the code https://github.com/Dawsatek22/opencv\_color\_detection/blob/main/color\_tracking/red\_and\_\_blue.py)

but if i try to work in c++ it builds but when i launch the code the loop ends before the webcam opens i post he code below so that people can see what wrong with it. update i found out how to read the frame but i cannot detect colors i show the code now to check to see whatswrong with it:

    #include <iostream>
    #include ""opencv2/objdetect.hpp""
    #include ""opencv2/highgui.hpp""
    #include ""opencv2/imgproc.hpp""
    #include ""opencv2/videoio.hpp""
    #include <string>
    using namespace cv;
    using namespace std;
    char s = 's';
    int min_blue = (110,50,50);
    int  max_blue=  (130,255,255);

    int   min_red = (0,150,127);
    int  max_red = (178,255,255);

    int main(){
    VideoCapture cam(0, CAP_V4L2);
        Mat frame, red_threshold , blue_threshold ;
          Mat hsv_red;
       Mat hsv_blue;
        int camera_device;


    if (! cam.isOpened() ) {

    cout << ""camera is not open""<< '\n';

     {
            if( frame.empty() )
            {
                cout << ""--(!) No captured frame -- Break!\n"";

            }

            //-- 3. Apply the classifier to the frame




         // Convert to HSV  for red and blue

        }


    }
    while ( cam.read(frame) ) {





         cvtColor(frame,hsv_red,COLOR_BGR2GRAY);
       cvtColor(frame,hsv_blue, COLOR_BGR2GRAY);
    // ranges colors
       inRange(hsv_red,Scalar(min_red),Scalar(max_red),red_threshold);
       inRange(hsv_blue,Scalar(min_blue),Scalar(max_blue),blue_threshold);

       std::vector<std::vector<cv::Point>> red_contours;
            findContours(hsv_red, red_contours, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);


            // Draw contours and labels
            for (const auto& red_contour : red_contours) {
                Rect boundingBox_red = boundingRect(red_contour);
                rectangle(frame, boundingBox_red, Scalar(0, 0, 255), 2);
                putText(frame, ""Red"", boundingBox_red.tl(), cv::FONT_HERSHEY_SIMPLEX, 1, Scalar(0, 0, 255), 2);
            }

        std::vector<std::vector<Point>> blue_contours;
            findContours(hsv_red, blue_contours, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);

            // Draw contours and labels
            for (const auto& blue_contours : blue_contours) {
                Rect boundingBox_blue = boundingRect(blue_contours);
                rectangle(frame, boundingBox_blue, cv::Scalar(0, 0, 255), 2);
                putText(frame, ""blue"", boundingBox_blue.tl(), FONT_HERSHEY_SIMPLEX, 1, Scalar(0, 0, 255), 2);
            }

       imshow(""red and blue detection"",frame);
    //imshow(""blue detection"",frame);
    if ( waitKey(10) == (s) ) {

        cam.release();
    }


    }}

","[""You create an instance of \\`VideoCapture cam\\` - but you don't use \\`cam\\` to actually CAPTURE a frame, i.e. read a frame from the object, do you?"", 'i reedited the code but i can not detect blue and red objects any ideas?', 'You need to add delay timer when openig camera, so camera stream initialised first andverify with a safety chek.  \nEddited:  \nBTW on which OS you are trying to run it ?  \nMac, Linux, Windows etc ?']","[""You create an instance of \\`VideoCapture cam\\` - but you don't use \\`cam\\` to actually CAPTURE a frame, i.e. read a frame from the object, do you?"", 'i reedited the code but i can not detect blue and red objects any ideas?', 'You need to add delay timer when openig camera, so camera stream initialised first andverify with a safety chek.  \nEddited:  \nBTW on which OS you are trying to run it ?  \nMac, Linux, Windows etc ?']",1,6,0.67,Help: Project,1751530507.0
1lqjxlu,computervision,OpenAI Board Member on Superintelligence,,[],[],0,0,0.14,Discussion ,1751529625.0
1lqhj3e,computervision,AlphaGenome – A Genomics Breakthrough,,[],[],0,0,0.5,Discussion ,1751520350.0
1lqgofj,computervision,Paper with code is completely down,"Paper with Code was being spammed (https://www.reddit.com/r/MachineLearning/comments/1lkedb8/d\_paperswithcode\_has\_been\_compromised/) before, and now it is completely down. It was also down a coupld times before, but seems like this time it has lasted for days. (https://github.com/paperswithcode/paperswithcode-data/issues)

","['It was working for me yesterday, but was down for a few days for me before that.']","['It was working for me yesterday, but was down for a few days for me before that.']",12,1,0.88,Discussion ,1751517368.0
1lqgbnw,computervision,Looking for a Technical Co-Founder to Lead AI Development,"For the past few months, I’ve been developing ProseBird—originally a collaborative online teleprompter—as a solo technical founder, and recently decided to pivot to a script-based AI speech coaching tool.

Besides technical and commercial feasibility, making this pivot really hinges on finding an awesome technical co-founder to lead development of what would be such a crucial part of the project: AI.

We wouldn’t be starting from scratch, both the original and the new vision for ProseBird share significant infrastructure, so much of the existing backend, architecture, and codebase can be leveraged for the pivot.

So if (1) you’re experienced with LLMs / ML / NLP / TTS & STT / overall voice AI; and (2) the idea of working extremely hard building a product of which you own 50% excites you, shoot me a DM so we can talk.

Web or mobile dev experience is a plus.",['What does this have to do with computer vision though?'],['What does this have to do with computer vision though?'],0,2,0.13,Discussion ,1751516159.0
1lqgajg,computervision,Opinions on PaddlePaddle / PaddleDetection for production apps?,"Since the professor at OpenMMLab unfortunately passed away, and that library is slowly decaying away, is PaddlePaddle / PaddleDetection the next best for open source CV model toolbox?

I know it's still not very popular in the Western world. If you have tried it, I'd love to hear your opinions if any. :)","['I used PaddleSeg for semantic segmentation and it was good for training and deployment', ""Using it for development and it is just one throw away to a deployment stage....Use case is Defect detection of X-ray Radiography images in real time... PaddleDetection's PP-YOLOE models are the ones am developing... Initially, when I ventured into it to look for alternatives to regular Yolo's, which are mostly locked by Ultralytics with AGPL 3.0 licensing, Paddle Apache 2.0 licence seemed to be the viable alternative... They may not be popular in the West, but I think entire PRC uses them as the first choice.... Learning curve and understanding how the code base is structured will take some time...But it's worth it....PM me for any help....""]","['I used PaddleSeg for semantic segmentation and it was good for training and deployment', ""Using it for development and it is just one throw away to a deployment stage....Use case is Defect detection of X-ray Radiography images in real time... PaddleDetection's PP-YOLOE models are the ones am developing... Initially, when I ventured into it to look for alternatives to regular Yolo's, which are mostly locked by Ultralytics with AGPL 3.0 licensing, Paddle Apache 2.0 licence seemed to be the viable alternative... They may not be popular in the West, but I think entire PRC uses them as the first choice.... Learning curve and understanding how the code base is structured will take some time...But it's worth it....PM me for any help....""]",4,5,0.75,Discussion ,1751516059.0
1lqetej,computervision,"I am building Codeflash, an AI code optimization tool that sped up Roboflow's Yolo models by 25%!","Latency is so crucial for computer vision and I like to make my models and code performant. I realized that all optimizations follow a similar pattern -

1. Create a performance benchmark and profile to find the slow sections

2. Think how the code could be improved, make edits and rerun the benchmark to verify optimizations.

The point 2 here is what LLMs are very good at, which made me think - can LLMs automate code optimization? To answer this questions, I've began building codeflash. The results seem promising...


[Codeflash](https://www.codeflash.ai) follows all the steps an expert takes while optimizing code, it profiles the code, analyzes the code for code to optimize, creates regression tests to ensure correctness, benchmarks the original code vs a new LLM generated code for performance and correctness. If a new code is indeed faster while being correct, it creates a Pull Request with the optimization to review!

Codeflash can optimize entire code bases function by function, or when given a script try to find the most performant optimizations for it. Since I believe most of the performance problems should be caught before they are shipped to prod, I built a GitHub action that reviews and optimizes all the new code you write when you [open a Pull Request](https://docs.codeflash.ai/getting-started/codeflash-github-actions)!

We are still early, but have managed to speed up yolov8 and RF-DETR models by Roboflow! The optimizations are better non-maximum suppression algorithms and even sorting algorithms.

Codeflash is free to use while in beta, and our code [is open source](https://github.com/codeflash-ai/codeflash/). You can install codeflash by \`pip install codeflash\` and \`codeflash init\`. Give it a try to see if you can find optimizations for your computer vision models. For best performance, [trace your](https://docs.codeflash.ai/optimizing-with-codeflash/trace-and-optimize) code to define the benchmark to optimize against. I am currently building GPU optimization and VS Code extension. I would appreciate your support and feedback! I would love to hear what results you find, and what you think about such a tool.

Thank you.","['>We are still early, but have managed to speed up yolov8 and RF-DETR models by Roboflow\n\nFYI, YOLOv8 is from r/Ultralytics , not Roboflow.', 'Link to all the PRs created for Roboflow - [https://github.com/roboflow/inference/pulls?q=is%3Apr+is%3Amerged+codeflash+sort%3Acreated-asc](https://github.com/roboflow/inference/pulls?q=is%3Apr+is%3Amerged+codeflash+sort%3Acreated-asc)  \nWe also sped up Albumentations - Link to PRs - [https://github.com/albumentations-team/albumentations/issues?q=state%3Amerged%20is%3Apr%20author%3Akrrt7%20OR%20state%3Amerged%20is%3Apr%20author%3Aaseembits93%20](https://github.com/albumentations-team/albumentations/issues?q=state%3Amerged%20is%3Apr%20author%3Akrrt7%20OR%20state%3Amerged%20is%3Apr%20author%3Aaseembits93%20)']","['>We are still early, but have managed to speed up yolov8 and RF-DETR models by Roboflow\n\nFYI, YOLOv8 is from r/Ultralytics , not Roboflow.', 'Link to all the PRs created for Roboflow - [https://github.com/roboflow/inference/pulls?q=is%3Apr+is%3Amerged+codeflash+sort%3Acreated-asc](https://github.com/roboflow/inference/pulls?q=is%3Apr+is%3Amerged+codeflash+sort%3Acreated-asc)  \nWe also sped up Albumentations - Link to PRs - [https://github.com/albumentations-team/albumentations/issues?q=state%3Amerged%20is%3Apr%20author%3Akrrt7%20OR%20state%3Amerged%20is%3Apr%20author%3Aaseembits93%20](https://github.com/albumentations-team/albumentations/issues?q=state%3Amerged%20is%3Apr%20author%3Akrrt7%20OR%20state%3Amerged%20is%3Apr%20author%3Aaseembits93%20)']",32,17,0.86,Showcase,1751511329.0
1lqdrfp,computervision,What is the best model for realtime video understanding?,"What is the state of the art on realtime video understanding with language?

Clarification:

What I would want is to be able to query video streams in natural language. I want to know how far away we are from AI that can “understand” what it “sees”

In this case hardware is not a limitation.","['See V-JEPA2 and the works it refers to', 'Sorry mate, just leaving a comment to see the responses later. \n\nYou should probably specify tho do you want it to run locally or do you want it to be an api that you can stream to? \n\nAnd you should also specify what do you mean by understanding? \n\nIf you want it to like segment stuff, then you need to train your own with your own annotated images or an existing model that recognizes images and can segment them or just make a bounding box around them? \n\nIf you want context for example, like real world underrating where ai tells you wtf is happening on screen, then you can pull a screenshot every x frames and pass it to an LLM to tell you. \n\nJust answer these questions in your post and I’m sure one of the local Reddit magicians will find the right model for you.', 'Understand is unspecified. None of them understand physics. Understand what is the question.', 'Note sure if its ""the best"" as the depends, however [YOLO-World](https://github.com/AILab-CVC/YOLO-World) is one such model.']","['See V-JEPA2 and the works it refers to', 'Sorry mate, just leaving a comment to see the responses later. \n\nYou should probably specify tho do you want it to run locally or do you want it to be an api that you can stream to? \n\nAnd you should also specify what do you mean by understanding? \n\nIf you want it to like segment stuff, then you need to train your own with your own annotated images or an existing model that recognizes images and can segment them or just make a bounding box around them? \n\nIf you want context for example, like real world underrating where ai tells you wtf is happening on screen, then you can pull a screenshot every x frames and pass it to an LLM to tell you. \n\nJust answer these questions in your post and I’m sure one of the local Reddit magicians will find the right model for you.', 'Understand is unspecified. None of them understand physics. Understand what is the question.', 'Note sure if its ""the best"" as the depends, however [YOLO-World](https://github.com/AILab-CVC/YOLO-World) is one such model.']",10,7,0.78,Discussion ,1751508089.0
1lqdm4x,computervision,3D reconstruction with only 4 calibrated cameras - COLMAP viable?,"Hi,

I'm working on 3D reconstruction of a 100m × 100m parking lot using only 4 fixed CCTV cameras. The cameras are mounted 9m high at \~20° downward angle with decent overlap between views. I have accurate intrinsic/extrinsic calibration (within 10cm) for all cameras.

The scene is a planar asphalt surface with painted parking markings, captured in good lighting conditions. My priority is reconstruction accuracy rather than speed, not real-time processing.

**My challenge:** Only 4 views to cover such a large area makes this extremely sparse.

**Proposed COLMAP approach:**

* Skip SfM entirely since I have known calibration
* Extract maximum SIFT features (32k per image) with lowered thresholds
* Exhaustive matching between all camera pairs
* Triangulation with relaxed angle constraints (0.5° minimum)
* Dense reconstruction using patch-based stereo with planar priors
* Aggressive outlier filtering and ground plane constraints

Since I have accurate calibration, I'm planning to fix all camera parameters and leverage COLMAP's geometric consistency checks. The parking lot's planar nature should help, but I'm concerned about the sparse view challenge.

Given only 4 cameras for such a large area, does this COLMAP approach make sense, or would learning-based methods (DUSt3R, MASt3R) handle the sparse views better despite my having good calibration? Has anyone successfully done similar large-area reconstructions with so few views?","['Yes.  I have had good luck with three cameras (arual imagery)', 'Have you tried VGGT Visual Geometry Grounded Transformer ? Be aware of the non commercial license. The results look promising with only a few view shots.']","['Yes.  I have had good luck with three cameras (arual imagery)', 'Have you tried VGGT Visual Geometry Grounded Transformer ? Be aware of the non commercial license. The results look promising with only a few view shots.']",10,5,0.92,Help: Project,1751507656.0
1lq8pd3,computervision,Need Help Converting Chessboard Image with Watermarked Pieces to Accurate FEN,"https://preview.redd.it/yj0n4bgh9jaf1.png?width=384&format=png&auto=webp&s=adc0b039a8afe5b5fe5b9cb8eaada3865d242422

Struggling to Extract FEN from Chessboard Image Due to Watermarked Pieces – Any Solutions?",['What are you using for the detection? Can you retrain the model? Are you using a pre built solution?'],['What are you using for the detection? Can you retrain the model? Are you using a pre built solution?'],2,5,1.0,Help: Project,1751493760.0
1lq6w0s,computervision,Live Face Swap and Voice Cloning,"Hey guys! Just wanted to share a little repo I put together that live face swaps and voice clones a reference person. This is done through zero shot conversion, so one image and a 15 second audio of the person is all that is needed for the live cloning. Let me know what you guys think! Here's a little demo. (Reference person is Elon Musk lmao). Link: [https://github.com/luispark6/DoppleDanger](https://github.com/luispark6/DoppleDanger)

https://reddit.com/link/1lq6w0s/video/mt3tgv0owiaf1/player

",[],[],3,0,1.0,Showcase,1751489228.0
1lpzru8,computervision,How To Actually Use MobileNetV3 for Fish Classifier[project],"https://preview.redd.it/yjg72fucihaf1.png?width=1280&format=png&auto=webp&s=06109cecebe26c72c6f65f431942073e97a5f894

This is a transfer learning tutorial for image classification using TensorFlow involves leveraging pre-trained model MobileNet-V3 to enhance the accuracy of image classification tasks.

By employing transfer learning with MobileNet-V3 in TensorFlow, image classification models can achieve improved performance with reduced training time and computational resources.

We'll go step-by-step through:

 

·         Splitting a fish dataset for training & validation 

·         Applying transfer learning with MobileNetV3-Large 

·         Training a custom image classifier using TensorFlow

·         Predicting new fish images using OpenCV 

·         Visualizing results with confidence scores

 

You can find link for the code in the blog  : [https://eranfeit.net/how-to-actually-use-mobilenetv3-for-fish-classifier/](https://eranfeit.net/how-to-actually-use-mobilenetv3-for-fish-classifier/)

 

You can find more tutorials, and join my newsletter here : [https://eranfeit.net/](https://eranfeit.net/)

 

Full code for Medium users : [https://medium.com/@feitgemel/how-to-actually-use-mobilenetv3-for-fish-classifier-bc5abe83541b](https://medium.com/@feitgemel/how-to-actually-use-mobilenetv3-for-fish-classifier-bc5abe83541b)

 

**Watch the full tutorial here**: [https://youtu.be/12GvOHNc5DI](https://youtu.be/12GvOHNc5DI)

 

Enjoy

Eran","['Let me guess, the ""transfer learning"" part is to use imagenet weights instead of random initialisation?', 'how to *actually* use a fish classifier:\n\n[https://youtu.be/E627QYFjPy0](https://youtu.be/E627QYFjPy0)']","['Let me guess, the ""transfer learning"" part is to use imagenet weights instead of random initialisation?', 'how to *actually* use a fish classifier:\n\n[https://youtu.be/E627QYFjPy0](https://youtu.be/E627QYFjPy0)']",2,4,0.63,Showcase,1751472311.0
1lpwv5o,computervision,Detecting surfaces of stacked boxes,"Hi everyone,

I’m working on a projection mapping project for a university course. The idea is to create a simple 3D jump-and-run experience projected onto two cardboard boxes stacked on top of each other.

To detect the front-facing surfaces, I’m using OpenCV. My current approach involves capturing two images (image red and image green) and computing their difference to isolate the areas of interest. This results in the masked image shown below.￼

Now I’m looking for a reliable method to detect exactly the 4 front surfaces of the boxes (See image below). Ideally, I want to end up with a clean, rectangular segmentation of each face.

My question is: what approach would you recommend to reliably detect the four front-facing surfaces of the boxes so I end up with something like the result shown in the last image below?

Thanks a lot in advance!

[Red Input Image](https://preview.redd.it/juwug866xgaf1.png?width=1920&format=png&auto=webp&s=fe13039dd63453137943acde91bfd142597aa9fd)

[Green Input Image](https://preview.redd.it/ixsmi6g7xgaf1.png?width=1920&format=png&auto=webp&s=4869285a2657a2524c0de87b26fb8ec536ae6b35)

[Difference based Image](https://preview.redd.it/bb3nda58xgaf1.png?width=1920&format=png&auto=webp&s=8ad8e75b3d57ff17b25733003b4687251b8478d9)

[Surfaces I am trying to detect of my Cardboards](https://preview.redd.it/7mhzxinbxgaf1.png?width=1920&format=png&auto=webp&s=2ee36f94a33fbbc2abc365704e932b88b397e869)

Edit:

Ok, so what I am currently doing Is using a Gaussian blur to smooth the image and to detect edges with Canny. Afterwards I am applying a dilation (3x) to connect broken edges and then filtering contours for large convex quadrilaterals. But this does not work very good, and I am only able to detect a part of one of the surfaces.

[Canny Edge Detection](https://preview.redd.it/81ciet0h6jaf1.png?width=822&format=png&auto=webp&s=3bd64b6ce91d75db777a1be39f1ecfabe1cdc55d)

[Repaired Edges \(Dilated\)](https://preview.redd.it/6iy06e6s6jaf1.png?width=894&format=png&auto=webp&s=87ba068de802370664e913d8e0a79ce55b57e6be)

[Final detected Faces](https://preview.redd.it/p5abihft6jaf1.png?width=804&format=png&auto=webp&s=2ba663aebeb67d70d0028256c3be01be7211bfc6)

","['thresholding + contour detection doesnt work?', 'I think you need to look at hough line transform.\xa0']","['thresholding + contour detection doesnt work?', 'I think you need to look at hough line transform.\xa0']",2,4,0.75,Help: Project,1751465441.0
1lpvw9e,computervision,Generate internal structure/texture of a 3d model,"Hey guys!
I saw many pipelines where you give a set of sparse images of an object, it generates 3d model.
I want to know if there's an approach for creating the internal structure and texture as well.

For example:
Given a set of images of a car and a set of images of its internal structure (seat, steering wheel etc.) The pipeline will generate the 3d model of the car as well as internal structure.

Any idea/approach will be immensely appreciated.

-R","[""I was at CVPR this year and [this paper](https://cvpr.thecvf.com/virtual/2025/poster/34660) on gaussian splatting of the inside of fruits might be relevant.\n\nedit: here's [the website](https://fanguw.github.io/FruitNinja3D/)"", 'Maybe try uv from the knowledge of 3d modeling.\n\nAlso you need to be able to segment different composites of the object, like the car will have a wheel , door etc, then wheel have the metal part and rubber etc']","[""I was at CVPR this year and [this paper](https://cvpr.thecvf.com/virtual/2025/poster/34660) on gaussian splatting of the inside of fruits might be relevant.\n\nedit: here's [the website](https://fanguw.github.io/FruitNinja3D/)"", 'Maybe try uv from the knowledge of 3d modeling.\n\nAlso you need to be able to segment different composites of the object, like the car will have a wheel , door etc, then wheel have the metal part and rubber etc']",2,5,1.0,Help: Project,1751462966.0
1lprx6a,computervision,Looking for AI-powered smart crop library (content-aware crop),"
Hey everyone!

https://preview.redd.it/9iy7j5wrpfaf1.png?width=1492&format=png&auto=webp&s=280299ceaf4484ffd1ec21cfa6661a33b36d8131

I'm currently using [smartcrop.py](https://github.com/smartcrop/smartcrop.py) for image cropping in Python, but it's pretty basic. It only detects edges and color gradients, not actual objects.

For example, if I have a photo with a coffee cup, I want it to recognize the cup as the main subject and crop around it. But smartcrop just finds areas with most edges/contrast, which often misses the actual focal point.

**Looking for:**

* Python library that uses AI/ML for object-aware cropping
* Can identify main subjects (people, objects, etc.)
* More modern than just edge detection

Any recommendations for libraries that actually understand what's in the image?

Thanks!","['Clip + groundingdino?', 'Depending on what your final goal is, maybe this external tool can help you with that [https://croppy.at](https://croppy.at)', 'Sounds like you need this: [https://segment-anything.com/](https://segment-anything.com/)', 'I believe the term is salient object detection.\xa0\n\nThis library can probably help. It goes one step beyond and masks the object, but you could “undo” that by cropping based on the mon/max coordinates of the mask.\xa0\n\nhttps://pypi.org/project/rembg/']","['Clip + groundingdino?', 'Depending on what your final goal is, maybe this external tool can help you with that [https://croppy.at](https://croppy.at)', 'Sounds like you need this: [https://segment-anything.com/](https://segment-anything.com/)', 'I believe the term is salient object detection.\xa0\n\nThis library can probably help. It goes one step beyond and masks the object, but you could “undo” that by cropping based on the mon/max coordinates of the mask.\xa0\n\nhttps://pypi.org/project/rembg/']",1,4,0.57,Help: Project,1751450595.0
1lpozk0,computervision,OCR project ideas,"I want to do a project on OCR, but I think datasets like traffic signs are too common and simple. It makes more sense to work with datasets that are closer to real-life problems. If you have any suggestions, please share them.","['try recognition of music notation.', ""How about recognizing handwriting, or is that too yesterday? Like one of the first projects I worked on at my last job was helping USPS set up computer vision parcel sorting (you can actually still find the case study on the hardware supplier's website although they obscured the customer's name\xa0https://www.gigabyte.com/Article/logistics-leader-initiates-smart-transformation-with-customized-server-solutions?lan=en) This is very useful and very hands-on real-life but again, not sure if there's anything new in this field after all this time."", ""Grocery store items for auto-checkout.   \n  \nText such as specific QR identifiers, size of the item (for example, an extra large cheerio box vs regular one might have very similar packaging, so you'd need to look at the oz/gram amount), or changing label visuals with consistent branding words all becomes relevant to this task."", 'Build your own OCR from scratch.', 'Maybe on a dataset of grocery products in a store so you can verify whether they are not spoiled. \nSo detect the spoil dates.', 'Take a look at a ""typical"" page in a magazine or newspaper.\n\nGet the text in columns, interrupted by pictures. diagrams, tables. Often there are kind of ""watermarks"" as design elements which complicates OCR.\n\nTo me it looks like people want to read less - and get more graphics and diagrams, requiring OCRs to shift focus a bit.\n\nIn general, have a look into e.g. [https://platform.entwicklerheld.de/challenge/document-scan?technology=java](https://platform.entwicklerheld.de/challenge/document-scan?technology=java) (ignore the programming language if you want to) with some implementation aspects \\_around\\_ OCR.', 'how about recognizing car equipment from pictures? e.g. leather seats or fabric seats etc.', 'Can you please make a where did my screwdriver go ? Or any tool really just on a workbench setting that you can do simple object tracking would be useful and I’m sure people will use it', 'how about receipt/invoice reader? Or business card scanner', 'Scanned mail documents, preliminary ocr phase to extract layout aware text then trying to make sense of it with an additional model depending on your needs to pinpoints certain specific elements for automating certain repetitive typing tasks']","['try recognition of music notation.', ""How about recognizing handwriting, or is that too yesterday? Like one of the first projects I worked on at my last job was helping USPS set up computer vision parcel sorting (you can actually still find the case study on the hardware supplier's website although they obscured the customer's name\xa0https://www.gigabyte.com/Article/logistics-leader-initiates-smart-transformation-with-customized-server-solutions?lan=en) This is very useful and very hands-on real-life but again, not sure if there's anything new in this field after all this time."", ""Grocery store items for auto-checkout.   \n  \nText such as specific QR identifiers, size of the item (for example, an extra large cheerio box vs regular one might have very similar packaging, so you'd need to look at the oz/gram amount), or changing label visuals with consistent branding words all becomes relevant to this task."", 'Build your own OCR from scratch.', 'Maybe on a dataset of grocery products in a store so you can verify whether they are not spoiled. \nSo detect the spoil dates.']",10,20,0.92,Discussion ,1751438756.0
1lpkrf9,computervision,Any projects that use tracking and querying?,"So I'm working on a project that involves a cloud-edge split. The edge runs a tracking algorithm, stores the frames locally and sends the data, such as the frame id, timestamp, detected objects and bounding box coordinates, in JSON format to the server. The server stores it on a SQL server for x amount of days (depending on how long we can store the images on the edge) and allows us to retrirve only certain frames of interest (i.e. only a certain car, or a car crossing the road on red lights, etc), therefore significantly reducing bandwidth.

I'd like to know if anyone heard of similar projects? Ideally, I'd like to publish my results and would appreciate either references to similar projects or just overall feedback regarding the high level description of my project.

Thanks!","[""I don't see any problem with what you describe if it satisfies the business requirements.\n\nAs for similar projects, we don't do it quite like that, our architecture typically follows;    All inference and decision making is made on the Edge, then we copy the data/frames/logs from the Edge into the Cloud where we can  access it all centrally.  Bandwidth isn't much of a concern for us as each session only creates around 5MB of data that needs to be copied and there are typically around 2-10 sessions occurring per 24 hour period."", ""Looks like one of the possible data treatment with its pros and cons. I wrote a video app which was processing live feed on the edge and slicing out events of interest, then sending a slice API link to the server. When user wanted to check the event server would pull the slice from the edge and show it. The drawback was waiting time when edge was down or had a network issue, so we decided to upgrade the server in the end and move on.\n\nI don't really see anything novel here that is worth publishing, but I am not discouraging you in any way. Unless by publishing you mean releasing the code, in which case I am encouraging you.""]","[""I don't see any problem with what you describe if it satisfies the business requirements.\n\nAs for similar projects, we don't do it quite like that, our architecture typically follows;    All inference and decision making is made on the Edge, then we copy the data/frames/logs from the Edge into the Cloud where we can  access it all centrally.  Bandwidth isn't much of a concern for us as each session only creates around 5MB of data that needs to be copied and there are typically around 2-10 sessions occurring per 24 hour period."", ""Looks like one of the possible data treatment with its pros and cons. I wrote a video app which was processing live feed on the edge and slicing out events of interest, then sending a slice API link to the server. When user wanted to check the event server would pull the slice from the edge and show it. The drawback was waiting time when edge was down or had a network issue, so we decided to upgrade the server in the end and move on.\n\nI don't really see anything novel here that is worth publishing, but I am not discouraging you in any way. Unless by publishing you mean releasing the code, in which case I am encouraging you.""]",3,2,0.72,Help: Project,1751424156.0
1lp9fa4,computervision,Screen recording movies,Hello there. So I’m a huge fan of movies. And I’m also glued to Instagram more than I’d like to admit. I see tons of videos of movie clips. I’d like to record my own and make some reviews or suggestions for Instagram. How do people do that? I have a Mac Studio M4. OBS won’t allow recording on anything. Even websites/browsers. Any suggestions? I’ve tried a bunch of different ways but can’t seem to figure it out. Also I’ve screen recorded from YouTube but I want better quality. I’m not looking to do anything other than use this for my own personal reviews and recommendations. ,"['You’re asking in the wrong subreddit', 'press cmd+shift+5 hit record']","['You’re asking in the wrong subreddit', 'press cmd+shift+5 hit record']",0,2,0.22,Help: Project,1751394576.0
1lov3bi,computervision,Help improving 3 D reconstruction with the VGGT model on an 8‑camera Jetson AGX Orin + Seeed Studio J501 rig?,"https://reddit.com/link/1lov3bi/video/s4fu6864c7af1/player

Hey everyone! 👋

I’m experimenting with Seeed Studio’s **J501 carrier board + GMSL extension** and **eight synchronized GMSL cameras** on a **Jetson AGX Orin**. ([deploy vggt on jetson](https://wiki.seeedstudio.com/multiple_cameras_with_jetson/#quickly-deploy-vggt-for-3d-reconstruction)) I attempted to use the multi-angle image input of the [VGGT](https://vgg-t.github.io/) model for 3D modeling. I envisioned that multiple angles of image input could enable the model to capture more features of the three-dimensional space. However, when I used eight cameras for image capture and model inference, I found that the more image inputs there were, the worse the quality of the model's output results became!

# What I’ve tried so far

* Use the [latitude and longitude correction method](https://github.com/duducosmos/defisheye) to correct the fish-eye camera.
* Cranking the AGX Orin clocks to max (60 W power mode) and locking the GPU at 1.2 GHz.
* Increased the pixel count for image input.

# Where I’m stuck

1. I used the MAX96724 defaults from the wiki, but I’m not 100 % sure the exposure sync is perfect.
2. How to calculate the adjustment of the angles of different cameras?
3. How does Jetson AGX Orin optimize to achieve real-time multi-camera model inference?

Thanks in advance, and hope the wiki brings you some value too. 🙌","['Do you really need to de-fisheye them? Unless you can do it very accurately it might only make things worse.\xa0', ""1) Exposure sync on the jetson should be pretty good as long as the cameras are hardware triggered. But for this application i doubt ms level syncing is necessary unless the object or your camera rig is moving quickly. But regardless the timestamp should basically be the first image packet that arrives to the jetson. In my tests for a stereo rig timestamps on images were within 1 ms of each other when using a hardware trigger.\n\n2) If you want to calculate the relative poses of the cameras you need to do camera calibration. Kinda guessing this is what you're asking.\n\n3) The AGX orin will use the onboard GPU and the two DLAs via tensorrt to do inference. Post/pre processing probably uses cuda kernels.\n\n\nThis is actually a pretty expensive and sophisticated setup. Single board computers like the orin are definitely the future but they are very difficult to make products with since you sometimes need custom carrier boards and kernel programming. The costs and complexity are just too high unless you have a large engineering team and a large budget. Just my 2 cents."", 'Looks like your cameras are configured in a compact array. Try spacing them out more']","['Do you really need to de-fisheye them? Unless you can do it very accurately it might only make things worse.\xa0', ""1) Exposure sync on the jetson should be pretty good as long as the cameras are hardware triggered. But for this application i doubt ms level syncing is necessary unless the object or your camera rig is moving quickly. But regardless the timestamp should basically be the first image packet that arrives to the jetson. In my tests for a stereo rig timestamps on images were within 1 ms of each other when using a hardware trigger.\n\n2) If you want to calculate the relative poses of the cameras you need to do camera calibration. Kinda guessing this is what you're asking.\n\n3) The AGX orin will use the onboard GPU and the two DLAs via tensorrt to do inference. Post/pre processing probably uses cuda kernels.\n\n\nThis is actually a pretty expensive and sophisticated setup. Single board computers like the orin are definitely the future but they are very difficult to make products with since you sometimes need custom carrier boards and kernel programming. The costs and complexity are just too high unless you have a large engineering team and a large budget. Just my 2 cents."", 'Looks like your cameras are configured in a compact array. Try spacing them out more']",5,5,0.86,Help: Project,1751353371.0
1lonr7c,computervision,Made a Handwriting->LaTex app that also does natural language editing of equations,,"['Is the llm running locally? If demand is high, openai might rate limit you. Btw awesome app !! Love it']","['Is the llm running locally? If demand is high, openai might rate limit you. Btw awesome app !! Love it']",22,2,0.93,Showcase,1751329183.0
1longzo,computervision,Question about computer OS for CV,"I mainly just lurk here to learn some things. I'm curious if you are running Windows for real time processing needs or a different OS. I use CAD on a laptop with specifications recommended by the software manufacturer, and it will still lag occasionally. A long time ago, I controlled a machine via printer port outputs using C and Unix. It's been so long, but I remember being able to dedicate almost all the Unix resources to the program. I also work with PLCs where the processing is 100% committed to the program.

I've done Cognex vision projects where the processing is on the camera and completely dedicated to the task. Cognex also has pc software, but I've never used it. I'm curious how a fast and complex vision program runs without the OS doing some sort of system task or whatever that causes lag.

I know most everyone here is programming rather using an off the shelf solution. Are custom programmed vision projects being used much in automation settings? ","['I assume you mean real time in the colloquial, non technical sense?\n\nI pretty much exclusively use Linux and Mac', 'I have co-workers that build their ""edge"" systems on windows. I find they have a lot more headaches getting stuff to work than I do on linux']","['I assume you mean real time in the colloquial, non technical sense?\n\nI pretty much exclusively use Linux and Mac', 'I have co-workers that build their ""edge"" systems on windows. I find they have a lot more headaches getting stuff to work than I do on linux']",3,6,0.72,Discussion ,1751328366.0
1loi79a,computervision,How to Build a Prototype for Querying and Summarizing Video,"Hi everyone,I have a video of someone touring a house. I’d like to build a prototype system that can extract visual and contextual details from this video so that:

* Later, I can ask questions in natural language like: “Was there a gas stove or an electric stove in the kitchen?” or “How many bedrooms did I see?”.
* I want to produce a summary of what the buyer saw during the tour, focusing only on the visuals (no audio transcript).

I’m probably going to use a vector database to store the extracted information for easy searching later. But my main questions are:

* What models could I use to extract and structure this visual/contextual information from the video? Should I look into video captioning models, object detection, scene segmentation, or something else?
* Is retrieval-augmented generation (RAG) a good option here for answering natural language questions, or might there be a better approach for this kind of video content?
* What tech stack would you use?",[],[],1,0,1.0,Help: Project,1751315049.0
1lofruz,computervision,Need open source Vlm for Trading chart analysis,"Need open source Vlm for Trading chart analysis
comment the name of model that are on huggingface or github . ",[],[],0,0,0.5,Help: Project,1751309376.0
1loavus,computervision,Low-Cost Open Source Stereo-Camera System,"
Hello Computer Vision Community,

I'm building an open-source stereo depth camera system to solve the cost barrier problem. Current depth cameras ($300-500) are pricing out too many student researchers.

What I'm building:
- Complete Desktop app(executable), Use any two similar webcams (~$50 total cost), adjustable baseline as per the need.
- Camera calibration, stereo processing, Point Cloud visualization and Processing and other Photogrammetry algorithms.
- Full algorithm transparency + ROS2 support
-Will extend support for edge devices

Quick questions:
1. Have you skipped depth sensing projects due to hardware costs?
2. Do you prefer plug-and-play solutions or customizable algorithms?
3. What's your typical sensor budget for research/projects?

Just validating if this solves a real problem before I invest months of development time!","[""I absolutely agree with the need for a cheap hobby 3D sensor, my main concern is that already the Realsense doesn't have great depth output, and that's a $300 device, with two identical and calibrated cameras. Assuming you'd go for some kind of SGM approach in your algorithm, plus two low-end cameras with unknown distortions, you'd probably get junk out.\n\nIMHO, only ML-based depth will work for low-end cameras. Or, which would be also an interesting thought, a structured light camera."", 'i am not saying ai depth models will replace all stereoscopic hardware, but especially for students(your target audience) it will probably be more than enough. worst case scenario they can train with new edge-cases, so i am not sure.\n\nimo the problem is your target audience not the venture itself. i would target SMB with a bit better gear.\n\nalso dont forget: what do i know, if there is a market go for it.\n\nedit: re read your post. it seems im completely off. so here is my new answer YOU MUST implement gaussian splatting, it has sooooo much future potential.', ""Using stereo camera setup to get absolute depth of objects is a great direction. It sets itself apart from depth sensors (most applications do not want to increase the cost of the product, and mostly use more than two camera setup) and ml-based depth estimation models (these models aren't as accurate as they are required to be.\n\nI would be interested in knowing the direction you are trying to go. When you say plug and play, any camera system without any prior camera calibration should work right? How exactly do you plan to achieve that?\n\nSecondly, are you targeting to achieve absolute depth of objects or approximate depth?"", 'Calibration has been my issue. I bought a $100 stereo camera like [this](https://www.amazon.com/dp/B07R8LQKV4/ref=sspa_mw_detail_4?ie=UTF8&psc=1&sp_csd=d2lkZ2V0TmFtZT1zcF9waG9uZV9kZXRhaWwp13NParams) \n\nA few years ago and it worked really well but calibration was Sooooo time consuming. I ended up printing a giant 3ft checkerboard and used so many photos only to get to a point where it was -pretty good- good enough for projects.\n\nPre calibration would make me buy lol. Like if you already had cam matrix deformations and a set of checkerboard images taken with the exact camera']","[""I absolutely agree with the need for a cheap hobby 3D sensor, my main concern is that already the Realsense doesn't have great depth output, and that's a $300 device, with two identical and calibrated cameras. Assuming you'd go for some kind of SGM approach in your algorithm, plus two low-end cameras with unknown distortions, you'd probably get junk out.\n\nIMHO, only ML-based depth will work for low-end cameras. Or, which would be also an interesting thought, a structured light camera."", 'i am not saying ai depth models will replace all stereoscopic hardware, but especially for students(your target audience) it will probably be more than enough. worst case scenario they can train with new edge-cases, so i am not sure.\n\nimo the problem is your target audience not the venture itself. i would target SMB with a bit better gear.\n\nalso dont forget: what do i know, if there is a market go for it.\n\nedit: re read your post. it seems im completely off. so here is my new answer YOU MUST implement gaussian splatting, it has sooooo much future potential.', ""Using stereo camera setup to get absolute depth of objects is a great direction. It sets itself apart from depth sensors (most applications do not want to increase the cost of the product, and mostly use more than two camera setup) and ml-based depth estimation models (these models aren't as accurate as they are required to be.\n\nI would be interested in knowing the direction you are trying to go. When you say plug and play, any camera system without any prior camera calibration should work right? How exactly do you plan to achieve that?\n\nSecondly, are you targeting to achieve absolute depth of objects or approximate depth?"", 'Calibration has been my issue. I bought a $100 stereo camera like [this](https://www.amazon.com/dp/B07R8LQKV4/ref=sspa_mw_detail_4?ie=UTF8&psc=1&sp_csd=d2lkZ2V0TmFtZT1zcF9waG9uZV9kZXRhaWwp13NParams) \n\nA few years ago and it worked really well but calibration was Sooooo time consuming. I ended up printing a giant 3ft checkerboard and used so many photos only to get to a point where it was -pretty good- good enough for projects.\n\nPre calibration would make me buy lol. Like if you already had cam matrix deformations and a set of checkerboard images taken with the exact camera']",12,7,0.88,Discussion ,1751298237.0
1lo7rws,computervision,COCO test-dev is completely down?,"I used to check COCO test-dev to see what methods were performing the best, but it looks like it's completely down? I checked last week, and it's been broken the whole time.

[https://paperswithcode.com/sota/instance-segmentation-on-coco](https://paperswithcode.com/sota/instance-segmentation-on-coco)",[],[],7,0,1.0,Discussion ,1751290743.0
1lo54hs,computervision,I created a little computer vision app builder (C++/OpenGL/Tensorflow/OpenCV/ImGUI),,[],[],5,0,1.0,Showcase,1751283026.0
1lo4vb2,computervision,Need Help in order to build a cv library,"You, as a computer vision developer, what would you expect from this library?

Asking because i don't want to develop something that's only useful for me, but i lack the experience to take some decisions.
I Wish to focus on robotics and some machine learning, but those are not the initial steps i have to take.

I need to be able to implement this in about a month for my Image Processing assignment in college, not exactly the most fancy methods but rather the basics that will allow the project to evolve properly in the future. ","['Designing source of light in an image has lot of applications. Make sure your method is novel. Better if you can manipulate the source of light once you detect it. There are several papers on this. “Changing light sources on portraits images.”', 'Maybe dumb question, but would this let you take an ROI an determine the effective lighting acting on that area? Maybe remove it?']","['Designing source of light in an image has lot of applications. Make sure your method is novel. Better if you can manipulate the source of light once you detect it. There are several papers on this. “Changing light sources on portraits images.”', 'Maybe dumb question, but would this let you take an ROI an determine the effective lighting acting on that area? Maybe remove it?']",34,11,0.9,Help: Project,1751282181.0
1lo3smk,computervision,Building a face recognition app for event photo matching,"I'm working on a project and would love some advice or guidance on how to approach the face recognition..

we recently hosted an event and have around 4,000 images taken during the day. I'd like to build a simple web app where:

* ﻿﻿Visitors/attendees can scan their face using their webcam or phone.
* ﻿﻿The app will search through the 4,000 images and find all the ones where they appear.
* ﻿﻿The user will then get their personal gallery of photos, which they can download or share.

The approach I'm thinking of is the following:

embed all the photos and store the data in a vector database (on google cloud, that is a constrain).

then, when we get a query, we embed that photo as well and search through the vector database.

Is this the best approach?

for the model i'm thinking of using facenet through deepface","['You can load all images as video frames, the the ones with user face detected using any third party Facial Recognition dependency, or as Optional you can also go with mobile app (built in face recognition)', 'Checkout my repo: https://github.com/ajaymin28. And look for face-detection-zoo. There is an app I’ve built that does it (check FaissFinder section)']","['You can load all images as video frames, the the ones with user face detected using any third party Facial Recognition dependency, or as Optional you can also go with mobile app (built in face recognition)', 'Checkout my repo: https://github.com/ajaymin28. And look for face-detection-zoo. There is an app I’ve built that does it (check FaissFinder section)']",4,5,1.0,Help: Project,1751278320.0
1lo3qib,computervision,Looking for good multilingual/swedish OCR,"Hi, im looking for a good ocr, localizing the text in the image is not necessary i just want to read it.  The images are of real scenes of cars with logos, already localized the logos with Yolo v11. The text is swedish","[""PaddleOCR is my usual default, but these days I'm using Qwen VLM more and more for these OCR tasks""]","[""PaddleOCR is my usual default, but these days I'm using Qwen VLM more and more for these OCR tasks""]",2,1,1.0,Help: Project,1751278103.0
1lnwwd1,computervision,The First Version Design of reCamera V1 with the PoE & HD Camera Module is Here and Ask for Help!,"Our team has just carried out design iterations for the reCamera with a PoE and high-definition camera version. Here are our preliminary renderings.

This is a preliminary rendering of the PoE version with the HD camera module. Do you think this looks good for you?

If you have good suggestions on the location of the interface opening and the overall structure, please let me know. 💚",[],[],0,0,0.5,Help: Project,1751252293.0
1lnsmyk,computervision,SMPL-X (3d obj from image),Anyone know if SMPL-X is still working? I tried installing its dependencies but seems a couple are outdated leaving the SMPL-X incapable of running.,[],[],0,0,0.33,Help: Project,1751239256.0
1lnq17y,computervision,I need career advice (CV/ML roles),"Hi everyone,

I'm currently working in the autonomous driving domain as a perception and mapping software engineer. While I work at a well-known large company, my current team is not involved in production-level development, which limits my growth and hands-on learning opportunities.

My long-term goal is to transition into a computer vision or machine learning role at a Big Tech company, ideally in applied CV/ML areas like 3D scene understanding and general perception. However, I’ve noticed that Big Tech firms seem to have fewer applied CV/ML positions compared to startups, especially for those focused on deployment rather than model architecture.

Most of my experience is in deploying and optimizing perception models, improving inference speed, handling integration with robotics stacks, and implementing existing models. However, I haven’t spent much time designing or modifying model architectures, and my understanding of deep learning fundamentals is relatively shallow.

I'm planning to start some personal projects this summer to bridge the gap, but I’d like to get some feedback from professionals:

* Is it realistic to aim for applied CV/ML roles in Big Tech with my background?
* Would you recommend focusing on open-source contributions, personal research, or something else?
* Is there a better path, such as joining a strong startup team, before applying to Big Tech?

Thanks in advance for your advice!","['I feel like this normally requires a PhD, especially if you want to work at big tech companies. The fact you’re doing a masters is good and gives you a chance but I feel like it’s still insanely competitive/difficult. And yeah, many positions now ask for publications at top conferences/journals. Best of luck to you!', 'Startups and mid-size companies will care less, if at all, about the lack of a PhD and papers and will value your experience with deployment, integration, and performance optimization (things researchers are usually pretty awful at).\n\nBig (FAANG) companies are overrated. \n\nWorking on applied perception, you’ve already got what many people would consider a dream job. If you feel your growth needs aren’t being met, by all means job hop. It might be your org is the problem, not you or your skill set / publications.\n\nIf you want more experience with customizing models and DL fundamentals you can do that on your own time and your own pace (IMHO). A Big Tech job by no means will give you that opportunity.', 'Do you have any CVPR/ICCV/ECCV  publications?', ""I work in multiple ML areas and do things from deployments, backend, frontend, and spend the majority of my time developing models and training tools for other developers.\n\nI like doing a lot of things and knowing the whole pipeline. With that being said I would caution you working at big tech. You won't be working in multiple areas and doing many different things. I don't even know if you will develop or research novel models and algorithms and put them into production. You may get stuck doing a small part of the pipeline you don't enjoy. It all depends on the team of course. \n\nI really enjoy mid and small size companies. I feel I grow more and solve more problems. The base salary is comparable but obviously the stock options at big tech are insane. \n\nWith all this said if you just really want to live the big tech or large company life just for the money or the fading reputation of being a big tech nerd then go for it. I've worked with ex big tech devs and many are average. It can be fun to work on large scale projects so there is that. \n\nYou can have a very fulfilling career outside of big tech and many times you learn more and can do more. You may also find it easier to get into CV positions at small or mid size companies. \n\nAgain you do you and getting into big tech just takes a bit of luck (good interview skills, good interviewer) and leetcode grinding""]","['I feel like this normally requires a PhD, especially if you want to work at big tech companies. The fact you’re doing a masters is good and gives you a chance but I feel like it’s still insanely competitive/difficult. And yeah, many positions now ask for publications at top conferences/journals. Best of luck to you!', 'Startups and mid-size companies will care less, if at all, about the lack of a PhD and papers and will value your experience with deployment, integration, and performance optimization (things researchers are usually pretty awful at).\n\nBig (FAANG) companies are overrated. \n\nWorking on applied perception, you’ve already got what many people would consider a dream job. If you feel your growth needs aren’t being met, by all means job hop. It might be your org is the problem, not you or your skill set / publications.\n\nIf you want more experience with customizing models and DL fundamentals you can do that on your own time and your own pace (IMHO). A Big Tech job by no means will give you that opportunity.', 'Do you have any CVPR/ICCV/ECCV  publications?', ""I work in multiple ML areas and do things from deployments, backend, frontend, and spend the majority of my time developing models and training tools for other developers.\n\nI like doing a lot of things and knowing the whole pipeline. With that being said I would caution you working at big tech. You won't be working in multiple areas and doing many different things. I don't even know if you will develop or research novel models and algorithms and put them into production. You may get stuck doing a small part of the pipeline you don't enjoy. It all depends on the team of course. \n\nI really enjoy mid and small size companies. I feel I grow more and solve more problems. The base salary is comparable but obviously the stock options at big tech are insane. \n\nWith all this said if you just really want to live the big tech or large company life just for the money or the fading reputation of being a big tech nerd then go for it. I've worked with ex big tech devs and many are average. It can be fun to work on large scale projects so there is that. \n\nYou can have a very fulfilling career outside of big tech and many times you learn more and can do more. You may also find it easier to get into CV positions at small or mid size companies. \n\nAgain you do you and getting into big tech just takes a bit of luck (good interview skills, good interviewer) and leetcode grinding""]",24,12,0.96,Discussion ,1751232247.0
1lnpauf,computervision,"Will industrial cameras (IDS, Allied Vision, Basler, etc.) work in emulation mode on Windows arm?","I'd love to test the new Surface Pro that comes with a Snapdragon CPU. As far as I understand, emulation of x64 application works pretty well, some wifi/ethernet devices also work like a charm but I was wondering what will happen for industrial cameras that do not necessarily have arm drivers.
Will vision software written in c++ and compiled for x64 work in emulation mode?
Has anyone tried this kind of setup?","['Usually industrial cameras come with a c++ library, some wrappers (like c# or python) and they give you a software suite to configure and test cameras (that internally use the sdk they given to you). You could try to install one and check if it works (basler pylon viewer could emulate a camera too for example).']","['Usually industrial cameras come with a c++ library, some wrappers (like c# or python) and they give you a software suite to configure and test cameras (that internally use the sdk they given to you). You could try to install one and check if it works (basler pylon viewer could emulate a camera too for example).']",1,4,1.0,Discussion ,1751230414.0
1lnosde,computervision,Trouble Getting Clear IR Images of Palm Veins (850nm LEDs + Bandpass Filter),"Hey y’all,
I’m working on a project where I’m trying to capture images of a person’s palm veins using infrared. I’m using:

* 850nm IR LEDs (10mm) surrounding the palm
* An IR camera (compatible with Raspberry Pi)
* An 850nm bandpass filter directly over the lens

The problem is:

1. The images are super noisy, like lots of grain even in a dark room
2. I’m not seeing any veins at all — barely any contrast or detail

I’ve attached a few of the images I’m getting. The setup has the palm held \~3–5 cm from the lens. I’m powering the LEDs off 3.3V with 220Ω resistors, and the filter is placed flat on top of the camera lens. I’ve tried diffusing the light a bit but still no luck.

Any ideas what I might be doing wrong? Could it be the LED intensity, camera sensitivity, filter placement, or something else? Appreciate any help from folks who’ve worked with IR imaging or vein detection before!

https://preview.redd.it/slvikcfxgx9f1.jpg?width=1280&format=pjpg&auto=webp&s=55721a53af396b39fa2bf7df15bd8a6923c690e8

https://preview.redd.it/ilmidbfxgx9f1.jpg?width=1280&format=pjpg&auto=webp&s=349491ed469e04dcd14dcb2bd6084afc8a760e47

","['The IR intensity is likely far too low. Grainy noisy images are the camera maxing out its gain.\n\nYou could confirm this by taking an image with the filter in direct sunlight - you should see a clear image.\n\nMore LEDs', ""You can enhance contrast by using a retinex-like operation.\n\nAlso the lower end of your range is way too low. You don't need that many values for the background, it's just useless info\n\nIt also looks like your LEDs aren't overlapping at all, if that's the case you're basically just wasting 2 LEDs""]","['The IR intensity is likely far too low. Grainy noisy images are the camera maxing out its gain.\n\nYou could confirm this by taking an image with the filter in direct sunlight - you should see a clear image.\n\nMore LEDs', ""You can enhance contrast by using a retinex-like operation.\n\nAlso the lower end of your range is way too low. You don't need that many values for the background, it's just useless info\n\nIt also looks like your LEDs aren't overlapping at all, if that's the case you're basically just wasting 2 LEDs""]",2,3,1.0,Help: Project,1751229129.0
1lnlq5e,computervision,Building an AR manufacturing assembly assistant similar to LightGuide. Anyone know how I can leverage AI coding tools to assist through the capture and inputing of images from the overhead camera?,"Hello, I'm building a system that uses a projector and a camera mounted above a workbench. The idea is the projector will project info and guiding UI features onto the workbench and the camera monitors the assembly process and aids in locating the projected content. I love using tools like Cline or Claude code for development so Im trying to figure out a way to have the code capture frames from the camera and have the coding agent process them to confirm successful feature implementation, troubleshoot etc. Any ideas on how I could do this? And any ideas for other AI coding tools useful for computer vision application development? I'm wondering if platforms like n8n could be useful, but I'm not sure. ",[],[],1,0,1.0,Discussion ,1751221490.0
1lnbikq,computervision,Why is my Faster Rcnn Detectron2 object detection model still detecting null images?,"
Ok so I was able to train a faster rcnn model with detectron2 using a custom book spine dataset from Roboflow in colab. My dataset from roboflow includes 20 classes/books and atleast 600 random book spine images labeled as “NULL”. It’s working already and detects the classes, even have a high accuracy at 98-100%.

However my problem is, even if I test upload images from the null or even random book spine images from the internet, it still detects them and even outputs a high accuracy and classifies them as one of the books in my classes. Why is that happening?

I’ve tried the suggestion of chatgpt to adjust the threshold but whats happening now if I test upload is “no object is detected” even if the image is from my classes.

This is my colab: https://colab.research.google.com/drive/1-ZIPqCtrabJFZoPKOhcesoT8GjXt7Ucp?usp=sharing","['Hello!\nTwo rules to solve your problem:\n1. Do not expose your API Key..never\n2. Do not use index 0 for a class (class 0) because it is already allocated for background index. Check your annotations, add one unit in dataloader so the first labelled class is 1.\nYour model is trained like your first class is the same with the background', 'When I run your notebook I can’t get past the cell that logs into your roboflow account.\n\nThat part works fine but it fails for something related to AVIF.\xa0\n\nBtw - do you mind if I use your account to train a model? My account is out of credits…']","['Hello!\nTwo rules to solve your problem:\n1. Do not expose your API Key..never\n2. Do not use index 0 for a class (class 0) because it is already allocated for background index. Check your annotations, add one unit in dataloader so the first labelled class is 1.\nYour model is trained like your first class is the same with the background', 'When I run your notebook I can’t get past the cell that logs into your roboflow account.\n\nThat part works fine but it fails for something related to AVIF.\xa0\n\nBtw - do you mind if I use your account to train a model? My account is out of credits…']",0,2,0.33,Help: Project,1751193094.0
1lnb022,computervision,[Update]Open source astronomy project: need best-fit circle advice,,"[""Hi,\n\nI'm maintaining an open-source tool called [DFTFringe](https://github.com/atsju/DFTFringe/issues/37) that analyzes interferometry images to deduce the shape of telescope mirrors. It's used by many amateur telescope makers and works well overall.\n\nThere's one manual step we'd like to automate: fitting a circle to an image feature, with ~1 pixel accuracy. \n\nI have set up a small github workflow to make it easy to try out algorithms. If you're interested, feel free to open a Pull Request with your code. Or suggest an approach in the thread.\n\nPrevious Reddit posts and more details are linked from the [GitHub description](https://github.com/atsju/DFTFringe/issues/37).\n\nThanks in advance!"", 'Always try simple solutions first. Go for RANSAC', 'Which circle are you meant to pick up in image 3?\n\nImage 5 is radically different than the other images. That’s a problem.']","[""Hi,\n\nI'm maintaining an open-source tool called [DFTFringe](https://github.com/atsju/DFTFringe/issues/37) that analyzes interferometry images to deduce the shape of telescope mirrors. It's used by many amateur telescope makers and works well overall.\n\nThere's one manual step we'd like to automate: fitting a circle to an image feature, with ~1 pixel accuracy. \n\nI have set up a small github workflow to make it easy to try out algorithms. If you're interested, feel free to open a Pull Request with your code. Or suggest an approach in the thread.\n\nPrevious Reddit posts and more details are linked from the [GitHub description](https://github.com/atsju/DFTFringe/issues/37).\n\nThanks in advance!"", 'Always try simple solutions first. Go for RANSAC', 'Which circle are you meant to pick up in image 3?\n\nImage 5 is radically different than the other images. That’s a problem.']",24,21,0.96,Help: Project,1751191058.0
1ln7hue,computervision,What Would You Do? Career Pivot Toward Autonomous Systems,"Hello everyone,

I'm a senior Mechanical Engineering student currently working full-time as a mechanical designer and I'm exploring a master’s degree in Autonomous Systems and Robotics. While my current field isn’t directly related, there are skills that transfer. Throughout college I’ve taken technical electives in computer science and discrete math, and I’m comfortable coding in a few languages. I’m especially interested in vehicle dynamics and computer vision, and I hope to contribute in both areas. Would like to hear insights or advice from anyone working in autonomous systems or computer vision; or even from those outside the field that would like to share their perspectives. My research is pointing me in that direction, I know I can be biased or overconfident in my reasoning, so I’m seeking honest input. Thank you for your time and responses.


Lastly, would love to hear about projects you are working on! ",[],[],4,0,0.84,Discussion ,1751176983.0
1ln402j,computervision,Need advice: Low confidence and flickering detections in YOLOv8 project,"I am working on an object detection project that focuses on identifying restricted objects during a hybrid examination (for example, students can see the questions on the screen and write answers on paper or type them into the exam portal).

We have created our own dataset with around 2,500 images. It consists of 9 classes: **Answer script**, **calculator**, **cheat sheet**, **earbuds**, **hand**, **keyboard**, **mouse**, **pen**, and **smartphone**.

Also Data split is 94% for training , 4% test and 2% valid

We applied the following data augmentations :

* **Flip**: Horizontal, Vertical
* **90° Rotate**: Clockwise, Counter-Clockwise, Upside Down
* **Rotation**: Between -15° and +15°
* **Shear**: ±10° Horizontal, ±10° Vertical
* **Brightness**: Between -15% and +15%
* **Exposure**: Between -15% and +15%

We annotated the dataset using **Roboflow**, then trained a model using [YOLOv8m.pt](http://YOLOv8m.pt) for about 50 epochs. After training, we exported and used the [best.pt](http://best.pt) model for inference. However, we faced a few issues and would appreciate some advice on how to fix them.

# Problems:

1. **The model struggles to differentiate between ""answer script"" and ""cheat sheet""** : The predictions keep flickering and show low confidence when trying to detect these two. The answer script is a full A4 sheet of paper, while the cheat sheet is a much smaller piece of paper. We included clear images of the answer script during training, as this project is for our college.
2. **Cheat sheet is rarely detected when placed on top of the hand or answer script** : Again, the results flicker and the confidence score is very low whenever it does get detected.
3. **The pen is detected very rarely** : Even when it's detected, the confidence score is quite low.
4. **The model works well in landscape mode but fails in portrait mode** : We took pictures in various scenarios showing different object combinations on a student's desk during the exam (permutation and combination of objects we are trying to detect in our project) — all in **landscape mode**. However, when we rotate the camera to **portrait mode**, it hardly detects anything. We don't need to detect in portrait mode, but we are curious why this issue occurs.
5. **Should we use a large yolov8 model instead of medium model during training?** Also, how many epochs are appropriate when training a model with this kind of dataset?
6. **Open to suggestions** We are open to any advice that could help us improve the model's performance and detection accuracy.

Reposting as I received feedback that the previous version was unclear. Hopefully, this version is more readable and easier to follow. Thanks!","['The pen not being detected is probably because its too small.  What is your raw image size and I assume your YOLOv8m model has input tensor size 640x640?      Small objects need to use SAHI to be picked up for inferencing as without it the resize of source image to tensor input results in it being lost/too small to detect.\n\nThe fact your model performs differently in landscape vs portrait also suggests small object detection problems due to the image resizing.', 'Can you share the performance on the validation set? Also, the testing and validation sets should be larger. I would recommend 10% for each validation and testing.\n\n1. If low confidence is for both of the classe, it makes me think it’s learning paper, but not the differences between them. Either increase more samples of each, increase model size, or use a chained model where you first detect the paper, then use a binary classifier to decide between a cheat sheet and answer script.\n\n2. This is an issue of not having enough samples for it in the case where it’s obscured. This is a hard problem, as you can’t really learn what something is without some additional information, i.e., previous frame information. Maybe try some type of object tracking, or attempt to use something like an LSTM to feed in additionally positional information. \n\n3. This is just an issue of small object detection. If you resize your images, don’t resize them as small. Otherwise, either crop your images during training to get “smaller images” and then do inference on the crops, or just use SAHI during inference (what i just described above but only for inference)\n\n4. If you don’t need to use portrait mode, don’t. As for why it’s happening, you’re resizing your images and the objects will look different when you squish them vertically rather than horizontally. So they just look different from one another, imo.\n\n5. If you want! I suggested it above. If inference latency isn’t an issue, then sure. With regard to epochs, you just gotta see—training and validation curves should plateau. \n\n6. I don’t have anything other than the above.', 'Sounds like you might need more training data. It’s hard to know without seeing your dataset.\xa0\n\nPotentially you can create it synthetically. I don’t think roboflow can help there, since its augmentations are not aware of the semantics of your dataset.\xa0\n\nOther things they may help are training a seperstr model to “smooth out” the flickering, basically you can train this model on the output of yolo for a series of frames and maybe adding some yolo embeddings. That is NOT beginner level though (it sounds like you’re a beginner) so maybe don’t try it first! \xa0Similar to this, I’ve heard of yolo models which consider adjacent video frames…also not beginner level though.\xa0', '50 epoches is nothing, what scores you get for it?\xa0', 'Is there a link to the dataset on RF?']","['The pen not being detected is probably because its too small.  What is your raw image size and I assume your YOLOv8m model has input tensor size 640x640?      Small objects need to use SAHI to be picked up for inferencing as without it the resize of source image to tensor input results in it being lost/too small to detect.\n\nThe fact your model performs differently in landscape vs portrait also suggests small object detection problems due to the image resizing.', 'Can you share the performance on the validation set? Also, the testing and validation sets should be larger. I would recommend 10% for each validation and testing.\n\n1. If low confidence is for both of the classe, it makes me think it’s learning paper, but not the differences between them. Either increase more samples of each, increase model size, or use a chained model where you first detect the paper, then use a binary classifier to decide between a cheat sheet and answer script.\n\n2. This is an issue of not having enough samples for it in the case where it’s obscured. This is a hard problem, as you can’t really learn what something is without some additional information, i.e., previous frame information. Maybe try some type of object tracking, or attempt to use something like an LSTM to feed in additionally positional information. \n\n3. This is just an issue of small object detection. If you resize your images, don’t resize them as small. Otherwise, either crop your images during training to get “smaller images” and then do inference on the crops, or just use SAHI during inference (what i just described above but only for inference)\n\n4. If you don’t need to use portrait mode, don’t. As for why it’s happening, you’re resizing your images and the objects will look different when you squish them vertically rather than horizontally. So they just look different from one another, imo.\n\n5. If you want! I suggested it above. If inference latency isn’t an issue, then sure. With regard to epochs, you just gotta see—training and validation curves should plateau. \n\n6. I don’t have anything other than the above.', 'Sounds like you might need more training data. It’s hard to know without seeing your dataset.\xa0\n\nPotentially you can create it synthetically. I don’t think roboflow can help there, since its augmentations are not aware of the semantics of your dataset.\xa0\n\nOther things they may help are training a seperstr model to “smooth out” the flickering, basically you can train this model on the output of yolo for a series of frames and maybe adding some yolo embeddings. That is NOT beginner level though (it sounds like you’re a beginner) so maybe don’t try it first! \xa0Similar to this, I’ve heard of yolo models which consider adjacent video frames…also not beginner level though.\xa0', '50 epoches is nothing, what scores you get for it?\xa0', 'Is there a link to the dataset on RF?']",6,6,0.88,Help: Project,1751164547.0
1ln2gdq,computervision,[Open Source] TrackStudio – Multi-Camera Multi Object Tracking System with Live Camera Streams,"We’ve just open-sourced **TrackStudio** ([https://github.com/playbox-dev/trackstudio](https://github.com/playbox-dev/trackstudio)) and thought the CV community here might find it handy. TrackStudio is a modular pipeline for multi-camera multi-object tracking that works with both prerecorded videos and live streams. It includes a built-in dashboard where you can adjust tracking parameters like Deep SORT confidence thresholds, ReID distance, and frame synchronization between views.

Why bother?

* **MCMOT code is scarce.** We struggled to find a working, end-to-end *multi-camera* MOT repo, so decided to release ours.
* **Early access = faster progress.** The project is still in heavy development, but we’d rather let the community tinker, break things and tell us what’s missing than keep it private until “perfect”.

>

Hope this is useful for anyone playing with multi-camera tracking. Looking forward to your thoughts!","['Do you have a bev of all cams combined in one room not one per cam?', ""Wow, if you combine this with London's CCTV network you could track someone through the whole city."", 'Looks interesting, having troubles launching it. toml file specifies numpy>=1.24.0 which installs numpy 2.x and then it complains some precompiled binaries were built with numpy 1.x. Fails to launch on Windows. Launches of linux with errors. Installing numpy 1.x removes the error, but complains that trackers module requires numpy>=2.0.2. All attempts to launch on windows return page with\n\n|| || |detail|""Not Found""detail\t""Not Found""|\n\nand linux wont load page at all', 'Are both the cameras calibrated ?', ""This looks really cool! Are you based in a research lab? How fast runs the pipeline (what's the latency from image taken to BEV output)?""]","['Do you have a bev of all cams combined in one room not one per cam?', ""Wow, if you combine this with London's CCTV network you could track someone through the whole city."", 'Looks interesting, having troubles launching it. toml file specifies numpy>=1.24.0 which installs numpy 2.x and then it complains some precompiled binaries were built with numpy 1.x. Fails to launch on Windows. Launches of linux with errors. Installing numpy 1.x removes the error, but complains that trackers module requires numpy>=2.0.2. All attempts to launch on windows return page with\n\n|| || |detail|""Not Found""detail\t""Not Found""|\n\nand linux wont load page at all', 'Are both the cameras calibrated ?', ""This looks really cool! Are you based in a research lab? How fast runs the pipeline (what's the latency from image taken to BEV output)?""]",79,16,0.96,Showcase,1751159514.0
1lmxeng,computervision,Looking for computer vision developer for object tracking project,Hi and thanks for reading this. Hopefully you’re a computer vision developer looking for an exciting opportunity to help in a brand new project I am currently working on. I’m on the ground floor of a product that people want and has a low barrier to entry with a TAM of over $3B today and growing. I’d like to have a working prototype within three months. If this is something that sounds interesting please DM me and we can discuss more details.,['Please check DM'],['Please check DM'],0,1,0.31,Help: Project,1751144823.0
1lmwjam,computervision,Missing moviepy.editor file in FER.,"I am working on face emotion recognition. I installed FER in my project using pip. No when i run a simple test code, i get the error no module named moviepy.editor. I uninstalled and reinstalled moviepy and still no fix. Tried installing from github too, still there is no moviepy/editor. Chatgpt seems confused too. Please let me know if there is a fix or a lightweight alternative for emotion detection.","['Try to install older version of moviepy(1.0.3) may fix the issue since ""moviepy.editor"" was the used earlier.', '[deleted]', 'Nice screenshots /s']","['Try to install older version of moviepy(1.0.3) may fix the issue since ""moviepy.editor"" was the used earlier.', '[deleted]', 'Nice screenshots /s']",0,6,0.11,Help: Project,1751142495.0
1lmw0ri,computervision,Would you list Copyrights and patents on your resume?,"Hey folks, I’d love some honest feedback on this.

I'm currently in my final year of a CS-related degree and have filed **3 software-related copyrights** and **1 patent**. The patent isn’t groundbreaking it’s about an **indexing system** designed to reflect a country’s status in a specific area (I’d prefer not to go into detail). It’s innovative in concept, but I understand it’s not a massive tech breakthrough.

What I’m more confident about are the **copyrights**, which are based on **fully conceptualized software ideas**. While I haven’t built the actual apps, I **used my experience in UI/UX, cloud/web deployment, and software design** to **thoroughly conceptualize the ideas** including app flow, layout, core logic, and features. These are **idea-level projects**, but I’ve documented and structured them well enough that a professional developer could easily turn them into functional apps.

They’ve already been filed, and are about 6 months in I should receive the official registrations soon.

# My question is:

👉 **Would it make sense to list these copyrights (and the one patent) on my resume?**

* Should I create a separate section like *“Intellectual Property”*?
* Should I add short descriptions for each, or just the titles and status?
* Or would it seem unnecessary or out of place for a fresh grad?

I’ve read mixed opinions ,some say it shows initiative and innovation, while others say it could look like filler if not explained properly.

Would appreciate any guidance, from those who’ve been on the hiring side and my fellow Software enthusiasts

One thing to note is :  I am just going to sit in my first placement season, I am going to complete my Engineering Soon ",['Answer to the title: Yes \n\nBut tell chatgpt to write shorter messages next time'],['Answer to the title: Yes \n\nBut tell chatgpt to write shorter messages next time'],0,3,0.2,Discussion ,1751141100.0
1lmrusf,computervision,Is a SWE with CS background and MS statistics a good fit for CV jobs?,Currently have my BS in CS with 7 years in software engineering and data engineering. Starting my MS in applied statistics this fall. Hoping to get into the CV field upon graduating. ,"['Maybe, that word “applied” gives you some hope, but the statisticians have a really **really** bad habit of uhhhh not actually being practical / useful. Think of them like pure mathematicians: utterly insufferable if you need to actually solve a problem. They have a really prevalent tendency to prefer to just want to play with their math, which isn’t the fucking point. The majority of the people in the field kinda lost the plot literally generations ago.\n\nThink about what a colossal condemnation of the field it is that the machine learning people had to reinvent the wheel from scratch, over and over, instead of pulling from the statisticians.\n\nI have many examples I could use, but a recent personal anecdote is so on its face absurd I have to share. Last month I posted to some statistics subreddits looking for more information on robust estimates for central tendency. And I mean I have a decent background, it’s not like I don’t know about M estimators, etc. My question was well constructed and clear, obviously not coming from a novice. Yet I had my post to AskStatistics removed, lol, and when I reposted I caught flack from some regulars asking what that had to do with statistics.\n\n> “What does central tendency of data have to do with statistics??”\n\nIf literally the simplest, most fundamental thing you can do with data is considered by statisticians to be unrelated to statistics, then I **very strongly** question the value of a formal statistics education to anyone hoping to do anything practical with their career. \n\nThe best that can be said is that they will force you to work with some ugly math that will “put hair on your chest”, mathematically speaking, but there are a lot better ways to get that!\n\nAlso there is a hidden minority of people in the field that are very practically oriented that you can learn a lot from. Case in point, I eventually found a statistician with an extremely impressive resume that has an amazing blog who has essentially built his career on answering that “unrelated to statistics” question I asked. 🙃\n\nEither way, you’re going to have to do a lot of self teaching and self direction to succeed in computer vision, even if you went in for a CV specific masters. Most people can’t or won’t do that though and rely on their externally directed formal education. You… kinda can’t do that and be in this field. Kinda. So maybe if you have what it takes, then it won’t matter too much what your formal education is.', 'Yeah']","['Maybe, that word “applied” gives you some hope, but the statisticians have a really **really** bad habit of uhhhh not actually being practical / useful. Think of them like pure mathematicians: utterly insufferable if you need to actually solve a problem. They have a really prevalent tendency to prefer to just want to play with their math, which isn’t the fucking point. The majority of the people in the field kinda lost the plot literally generations ago.\n\nThink about what a colossal condemnation of the field it is that the machine learning people had to reinvent the wheel from scratch, over and over, instead of pulling from the statisticians.\n\nI have many examples I could use, but a recent personal anecdote is so on its face absurd I have to share. Last month I posted to some statistics subreddits looking for more information on robust estimates for central tendency. And I mean I have a decent background, it’s not like I don’t know about M estimators, etc. My question was well constructed and clear, obviously not coming from a novice. Yet I had my post to AskStatistics removed, lol, and when I reposted I caught flack from some regulars asking what that had to do with statistics.\n\n> “What does central tendency of data have to do with statistics??”\n\nIf literally the simplest, most fundamental thing you can do with data is considered by statisticians to be unrelated to statistics, then I **very strongly** question the value of a formal statistics education to anyone hoping to do anything practical with their career. \n\nThe best that can be said is that they will force you to work with some ugly math that will “put hair on your chest”, mathematically speaking, but there are a lot better ways to get that!\n\nAlso there is a hidden minority of people in the field that are very practically oriented that you can learn a lot from. Case in point, I eventually found a statistician with an extremely impressive resume that has an amazing blog who has essentially built his career on answering that “unrelated to statistics” question I asked. 🙃\n\nEither way, you’re going to have to do a lot of self teaching and self direction to succeed in computer vision, even if you went in for a CV specific masters. Most people can’t or won’t do that though and rely on their externally directed formal education. You… kinda can’t do that and be in this field. Kinda. So maybe if you have what it takes, then it won’t matter too much what your formal education is.', 'Yeah']",9,4,1.0,Discussion ,1751130392.0
1lmq78d,computervision,Need help form experts regarding object detection,"I am working on object detection project of restricted object in hybrid examination(for ex we can see the questions on the screen and we can write answer on paper or type it down in exam portal). We have created our own dataset with around 2500 images and it consist of 9 classes in it Answer script , calculator , chit , earbuds , hand , keyboard , mouse , pen  and smartphone . So we have annotated our dataset on roboflow and then we extracted the model [best.pt](http://best.pt) (while training the model we used was [yolov8m.pt](http://yolov8m.pt) and epochs used were around 50) for using and we ran it we faced few issue with it so need some advice with how to solve it
problems:
1)it is not able to tell a difference between answer script and chit used in exam (results keep flickering and confidence is also less whenever it shows) so we have answer script in A4 sheet of paper and chit is basically smaller piece of paper . We are making this project for our college so we have the picture of answer script to show how it looks while training.

2)when the chit is on the hand or on the answer script it rarely detects that (again results keep flickering and confidence is also less whenever it shows)

3)pen it detect but very rarely also when it detects its confidence score is less

4)we clicked picture with different scenarios possible on students desk during the exam(permutation and combination of objects we are trying to detect in out project) in landscape mode , but we when we rotate our camera to portrait mode it hardly detects anything although we don't need to detect in portrait mode but why is this problem occurring?

5)should we use large yolov8 model during training? also how many epochs is appropriate while training a model?

6)open for your suggestion to improve it

sorry for reposting it title was misspelled in previous post","['I am sorry but your post is seriously unreadable. I dont care if you put it in chatgpt to reformat it but i tried to read this post 3 times and its semi gibberish. Please repost with clear explanation of the challenge domain and images for examples', ""More data and use augmentation. Record where it fails, correct it, put it back into your training set. Don't neglect your data preparation."", 'It seems to be unbalanced data issue in the first sight. Can you get class wise accuracy? You already mention some issues on special cases. Validate how many samples you have for those cases in train/val set. \n\nIf you cant balance the data, see if current implementation uses focal loss, focus loss helps focusing on hard cases.', 'Are you training the full yolo model or just the head? (You should just be training the head probably)']","['I am sorry but your post is seriously unreadable. I dont care if you put it in chatgpt to reformat it but i tried to read this post 3 times and its semi gibberish. Please repost with clear explanation of the challenge domain and images for examples', ""More data and use augmentation. Record where it fails, correct it, put it back into your training set. Don't neglect your data preparation."", 'It seems to be unbalanced data issue in the first sight. Can you get class wise accuracy? You already mention some issues on special cases. Validate how many samples you have for those cases in train/val set. \n\nIf you cant balance the data, see if current implementation uses focal loss, focus loss helps focusing on hard cases.', 'Are you training the full yolo model or just the head? (You should just be training the head probably)']",4,10,0.75,Help: Project,1751126213.0
1lmnxm5,computervision,Segment Layer Integrated Vision System (SLIVS),"I have an idea for a project, but before I start I wanted to know if there is anything like it that exists.  Essentially I plan to use [SAM2](https://huggingface.co/facebook/sam2-hiera-large) to segment all objects in a frame.  Then use [MiDAS](https://huggingface.co/Intel/dpt-hybrid-midas) to estimate depth in the scene.  Then take a 'deck of cards' approach to objects.  So each segment on the 'top layer' extends back based on a smooth depth gradient from the midas estimate x layers.  Midas is relative so i am only using it as a way to stack my objects 'in front' or 'in back' the same way you would with photoshop layers for example, not rely on it as frame to frame depth comparison.  The system then assumes

* no objects can move.
* no objects can teleport
* objects can not be traversed (you can't just pass through a couch. you move behind it or in front of it).
* objects are permanent, if you didn't see them leave off screen they are still there just not visible
* objects move based on physics. things fall, things move sequentially (remember no teleport) between frames. objects continue to move in the same direction.

 The result is 255 layers (midas 0 - 255),  my segments would be overlayed on the depth so that i can create the 'deck of cards' concept for each object. So a book on on a table in the middle of the room, it would be identified as a segmented object by SAM2. That segment would correlate with the depth map estimate, specifically the depth gradient, so we can estimate that the book is at depth 150 (which again we want relative so it just means it's stacked in the middle of our objects in terms of depth) and it is about 20 layers deep so any other objects in that range the back or front of the book may be on the same depth layer as a few other objects.

Save all of the objects, based on segment count in local memory, with some attributes like can it move.

On frame 2, which is where the tracking begins, we assume nothing moved. so we predict frame 2 to be a copy of frame 1.  we overlay frame 2 on 1 (just the rgb v rgb), any place there is difference an optical flow check, we go back to our knowledge about objects in that area established from frame 1 and begin an update relying on our depth stack and segments such that we update or prediction of frame 2 to match the reality of frame 2 AND update the properties of those changed objects in memory.  Now we predict frame 3, etc.

It seems like a lot, my thought is once it gets rolling it really wouldn't be that bad since it is relatively low computation requirements to move the 'deck of card' representation of an object.

Here is an LLM Chat I did with a lot more detail. [https://claude.ai/share/98f93e57-5a8b-4d4f-a1c7-32c695435a13](https://claude.ai/share/98f93e57-5a8b-4d4f-a1c7-32c695435a13)

Any insight on this greatly appreciated. Also DM me if you're interested in prototyping and messing around with this concept to see if it could work.","[""Started experimenting on the first step.  Here is an output example of full depth map, and 5 depth layers.  the green dots are what will be the input points to SAM2 to get segments for this layer [OUTPUT](https://github.com/reliableJARED/multimodal-perception/blob/main/vision/depth_output.png)\n\nThe demo code is: [https://github.com/reliableJARED/multimodal-perception/blob/main/vision/slivs\\_1stream\\_pts.py](https://github.com/reliableJARED/multimodal-perception/blob/main/vision/slivs_1stream_pts.py)\n\nI have to mess around with clustering the points because I want to stay under about 100 across all depth layers.  Once this is worked out I'll end up getting my segments from SAM.  Those segments will likely span multiple depth frames.  thats basically the core of the idea.  it's like a converting the SAM output to a topographical map.  Once I establish objects (we say object tracking all the time but it's really pixel tracking), I'll revert to using optical flow to track the objects and have a working memory of all the objects which again are treated like a deck of cards where a object segment has 'depth slices' that follow normal physics rules. no teleport, can't pass through solid objects, etc."", ""Still early on but I'm deff encouraged by the results so far of using the depth estimation with MiDAS to help SAM2 create the correct segments of an object. I have a demo [HERE](https://github.com/reliableJARED/multimodal-perception/blob/main/vision/slivs_depth_segment_demo.py) in the repo.  Note this repo is VERY active and I am changing the code a lot, there is a lot of testing and other things at the moment so it's not all clean and ready for primetime). I saved a single round of demo output images [here](https://github.com/reliableJARED/multimodal-perception/tree/main/vision/slivs_depth_segment_demo_output).\n\nIt takes about 1.8 seconds to process everything.  However, once that step is done the goal again is to create representations of objects as 'decks of cards' via depth layers.  Then I'll use optical flow to track the objects or at least focus on them for updating.""]","[""Started experimenting on the first step.  Here is an output example of full depth map, and 5 depth layers.  the green dots are what will be the input points to SAM2 to get segments for this layer [OUTPUT](https://github.com/reliableJARED/multimodal-perception/blob/main/vision/depth_output.png)\n\nThe demo code is: [https://github.com/reliableJARED/multimodal-perception/blob/main/vision/slivs\\_1stream\\_pts.py](https://github.com/reliableJARED/multimodal-perception/blob/main/vision/slivs_1stream_pts.py)\n\nI have to mess around with clustering the points because I want to stay under about 100 across all depth layers.  Once this is worked out I'll end up getting my segments from SAM.  Those segments will likely span multiple depth frames.  thats basically the core of the idea.  it's like a converting the SAM output to a topographical map.  Once I establish objects (we say object tracking all the time but it's really pixel tracking), I'll revert to using optical flow to track the objects and have a working memory of all the objects which again are treated like a deck of cards where a object segment has 'depth slices' that follow normal physics rules. no teleport, can't pass through solid objects, etc."", ""Still early on but I'm deff encouraged by the results so far of using the depth estimation with MiDAS to help SAM2 create the correct segments of an object. I have a demo [HERE](https://github.com/reliableJARED/multimodal-perception/blob/main/vision/slivs_depth_segment_demo.py) in the repo.  Note this repo is VERY active and I am changing the code a lot, there is a lot of testing and other things at the moment so it's not all clean and ready for primetime). I saved a single round of demo output images [here](https://github.com/reliableJARED/multimodal-perception/tree/main/vision/slivs_depth_segment_demo_output).\n\nIt takes about 1.8 seconds to process everything.  However, once that step is done the goal again is to create representations of objects as 'decks of cards' via depth layers.  Then I'll use optical flow to track the objects or at least focus on them for updating.""]",4,2,0.83,Help: Project,1751120380.0
1lmnx4p,computervision,Would combining classes together cause any problems ?,"So im training a yolo v8 small model using the visdrone dataset. I get good results but what happens is that sometimes it mistakes a vehicle for a truck etc. I need it to track the objects as good as possible so I can get their trajectory data to train LSTM. Dataset currently has 10 classes, what I wonder is if I can combine them together ? Would that cause any problems ? Like its going to call every type of vehicle it sees, just a vehicle ?","['The only reason you’d need individual classes for a vehicle is if it’s relevant to detect cars and trucks individually.\n\nIf you don’t care and just care about tracking vehicle trajectory, feel free to combine the classes! :)', '>\xa0Dataset currently has 10 classes, what I wonder is if I can combine them together ? Would that cause any problems ? Like its going to call every type of vehicle it sees, just a vehicle ?\n\nMaybe I’m misunderstanding your question, but that is exactly what it will do. If you combine all classes into a single “vehicle” then it will no longer tell you the specific type of vehicle.']","['The only reason you’d need individual classes for a vehicle is if it’s relevant to detect cars and trucks individually.\n\nIf you don’t care and just care about tracking vehicle trajectory, feel free to combine the classes! :)', '>\xa0Dataset currently has 10 classes, what I wonder is if I can combine them together ? Would that cause any problems ? Like its going to call every type of vehicle it sees, just a vehicle ?\n\nMaybe I’m misunderstanding your question, but that is exactly what it will do. If you combine all classes into a single “vehicle” then it will no longer tell you the specific type of vehicle.']",2,5,0.75,Discussion ,1751120348.0
1lmmdgm,computervision,Paper Digest: ICML 2025 Papers & Highlights,"[https://www.paperdigest.org/2025/06/icml-2025-papers-highlights/](https://www.paperdigest.org/2025/06/icml-2025-papers-highlights/)

ICML 2025 will be held from July 13th to July 19th 2025 at the Vancouver Convention Center. This year ICML accepted \~3,300 papers (600 more than the last year) from 13,000 authors. Paper proceeding is available.",['they will reach 10k sooner or later.'],['they will reach 10k sooner or later.'],14,1,1.0,Research Publication,1751115938.0
1lmitk8,computervision,Help a local airfield prevent damage to aircraft.,"I work at a small GA airfield and in the past we had some problems with FOD (foreign object damage) where pieces of plastic or metal were damaging passing planes and helicopters.

My solution would be to send out a drone every morning along the taxiways and runway to make a digital twin. Then (or during the droneflight) scan for foreign objects and generate a rapport per detected object with a close-up photo and GPS location.

Now I am a BSc, but unfortunately only with basic knowledge of coding and CV. But this project really has my passion so I’m very much willing to learn. So my questions are this:

1. Which deep learning software platform would be recommended and why? The pictures will be 75% asphalt and 25% grass, lights, signs etc. I did research into YOLO ofcourse, but efficiënt R-CNN might be able to run on the drone itself. Also, since I’m no CV wizard, a model which isbeasy to manipulate and with a large community behind it would be great.

2. How can I train the model? I have collected some pieces of FOD which I can place on the runway to train the model. Do I have to sit through a couple of iterations marking all the false positives?

3. Which hardware platform would be recommended? If visual information is enough would a DJI Matrice + Dock work?

4. And finally, maybe a bit outside the scope of this subreddit. But how can I control the drone to start an autonomous mission every morning with a push of a button. I read about DroneDeploy but that is 500+ euro per month.

Thank you very much for reading the whole post. I’m not officially hired to solve this problem, but I’d really love to present an efficient solution and maybe get a promotion! Any help is greatly appreciated.","['It would be helpful to provide details regarding type, look, and size of debris that you’re trying to automatically flag!', ""There is a lot to unpack here but I just want to add that you don't necessarily have to run your models and algorithms on the drone. If you have lightweight compute you can always make the drone only responsible for collecting footage and uploading it.\n\nAs always, pictures and examples help a lot."", 'This might become fairly complicated, and I’m wondering if there are regulatory obstacles that you have to consider?\xa0\n\nThat said, I think it’s a great idea and it sounds like you’re on the right path in general! Consider whether you really need to run the models on the drone, or if you can just have it record video that you can then analyze on a more powerful computer or in the cloud.\xa0', 'I don’t know what you really mean by “digital twin”, but you only need photographs for this application. You don’t have to worry about stitching the photos together or anything, when you detect an object you’re just going to go out there and look for it.   \n  \nThis whole project might be easier/faster/potentially cheaper by simply using a laser pointer or similar across your (presumably straight and flat) runway.   \n  \nIf a laser elevated 1” off the ground can traverse your entire runway unobstructed and reach some kind of camera or detector, you know that path is unobstructed.', 'Check out LatentAgent from LatentAI you mind it useful. It is a complete MLOps platform with the ability build models for the edge platforms.', 'Would recommend stationary stills to begin with, just to reduce motion blur related issues. \n\nAn actual lidar sensor probably would be somewhat useful. But just suggesting depth models for a low commitment attempt to begin with. I recently used videodepthanything for traffic segmentation in cctv footage at work (granted it’s an easier case to yours), but I reckon there’s no harm giving it a shot here.']","['It would be helpful to provide details regarding type, look, and size of debris that you’re trying to automatically flag!', ""There is a lot to unpack here but I just want to add that you don't necessarily have to run your models and algorithms on the drone. If you have lightweight compute you can always make the drone only responsible for collecting footage and uploading it.\n\nAs always, pictures and examples help a lot."", 'This might become fairly complicated, and I’m wondering if there are regulatory obstacles that you have to consider?\xa0\n\nThat said, I think it’s a great idea and it sounds like you’re on the right path in general! Consider whether you really need to run the models on the drone, or if you can just have it record video that you can then analyze on a more powerful computer or in the cloud.\xa0', 'I don’t know what you really mean by “digital twin”, but you only need photographs for this application. You don’t have to worry about stitching the photos together or anything, when you detect an object you’re just going to go out there and look for it.   \n  \nThis whole project might be easier/faster/potentially cheaper by simply using a laser pointer or similar across your (presumably straight and flat) runway.   \n  \nIf a laser elevated 1” off the ground can traverse your entire runway unobstructed and reach some kind of camera or detector, you know that path is unobstructed.', 'Check out LatentAgent from LatentAI you mind it useful. It is a complete MLOps platform with the ability build models for the edge platforms.']",9,20,1.0,Help: Project,1751103402.0
1lmd02f,computervision,In search of a de-ID model for patient and staff privacy,Looking for a model that can provide a privacy mask for patient and staff in a procedural room environment. The one I've created simply isn't working well and patient privacy is required for HIPAA. Any models out there that do this well? ,"['Maybe you’re looking to achieve something like [this](https://github.com/TeCSAR-UNCC/PoseLift). \n\nYou can do that with any person segmentation model (frame-level masking). If you need key points, that can also be done with a key point detector.', 'What do you mean exactly... are you trying to blur out the faces of people?   If so then RetinaFace could do this to detect the region of the head, then you can blur those pixels out.']","['Maybe you’re looking to achieve something like [this](https://github.com/TeCSAR-UNCC/PoseLift). \n\nYou can do that with any person segmentation model (frame-level masking). If you need key points, that can also be done with a key point detector.', 'What do you mean exactly... are you trying to blur out the faces of people?   If so then RetinaFace could do this to detect the region of the head, then you can blur those pixels out.']",5,6,1.0,Help: Project,1751081320.0
1lm19cd,computervision,Labeled images for tornado,"Hi,

I am working as a research intern on tornado prediction project using optical, labeled images in CNN.

Which are good places to find dataset? I have tried [images.cv](http://images.cv), [images.google](http://images.google), pexels.

Tried CNN with deep layers as well as pretrained models. ResNet 50 is hovering around 92% accuracy while ResNet18 and VGG16 around 50-60%.

My current dataset has around 950 images (which is less for image training). Adding more data can improve metrics, I believe.

Any idea, where I could find more real tornado images (not tornado aftermath)?

Thanks","['I always love when people discover that data (and compute) is the actual gold of AI.', ""So are you trying to identify a tornado as or after it's formed via a camera? I feel like this is the wrong approach. There are so many other tools that measure things in the atmosphere that I feel would make a better detection method"", 'Here are a couple of options: \n\n* [https://universe.roboflow.com/projet-pfe/detection-de-tornade/browse](https://universe.roboflow.com/projet-pfe/detection-de-tornade/browse)\n* [https://universe.roboflow.com/margaretcapybara/tornado-derection/browse](https://universe.roboflow.com/margaretcapybara/tornado-derection/browse)\n* [https://universe.roboflow.com/line-stickman-ogd1b/tornado-oj30p/browse](https://universe.roboflow.com/line-stickman-ogd1b/tornado-oj30p/browse)', 'Now that you have annotated the data, simply augment it to like 10 times. Increase brightness, do horizontal flip, zoom out, add noise etc.', 'Have you tried creating synthetic image data generation?']","['I always love when people discover that data (and compute) is the actual gold of AI.', ""So are you trying to identify a tornado as or after it's formed via a camera? I feel like this is the wrong approach. There are so many other tools that measure things in the atmosphere that I feel would make a better detection method"", 'Here are a couple of options: \n\n* [https://universe.roboflow.com/projet-pfe/detection-de-tornade/browse](https://universe.roboflow.com/projet-pfe/detection-de-tornade/browse)\n* [https://universe.roboflow.com/margaretcapybara/tornado-derection/browse](https://universe.roboflow.com/margaretcapybara/tornado-derection/browse)\n* [https://universe.roboflow.com/line-stickman-ogd1b/tornado-oj30p/browse](https://universe.roboflow.com/line-stickman-ogd1b/tornado-oj30p/browse)', 'Now that you have annotated the data, simply augment it to like 10 times. Increase brightness, do horizontal flip, zoom out, add noise etc.', 'Have you tried creating synthetic image data generation?']",0,6,0.5,Help: Project,1751048828.0
1llqoc2,computervision,What to care for in Computer Vision,"Hello everyone,

I'm currently just starting out with computer vision theory and i'm using CS231A from stanford as my roadmap and guide for that , one thing that I'm not sure about is what to actually focus on and what to not focus on , for example in the first lectures they ask you to read the first chapter of the book Computer Vision : A Modern Approach but the book at the start goes through various setups of  lenses and light rays related things and so on also the book Multiple View Geometry that goes deep into math related things and i'm finding a hard time to decide if i should take these math related things as simply a tool that solves a specific problem in the field of CV and move on or actually go and read the theory behind it all and why it solves such a problem and look up proofs , if these things are supposed to be skipped for now then when do you think would be a good timing to actually focus on them ?","[""This is your only chance to learn that stuff. You will never have time to come back and learn it properly.\n\nAlso if you know it you might use it one day. And obviously if you don't learn it you will never use it."", 'That’s an excellent question!\n\nI don’t think there’s a one size fits all solution. To be any good at computer vision you’re going to have to self teach quite a lot, and that essentially never means you learn things in some well structured or “optimal” way.\n\nYou’re going to have to make several passes over the same material, preferably presented in multiple formats, over a significant period of time. This means stuff will be overlapping, which can feel a bit chaotic. But if you look at the research on how people learn and retain knowledge over the long term it’s not by sitting for a lecture, doing a homework, and then simply moving on!\n\nBut I will say that the sooner you internalize the math the better and smoother the rest of your journey will be, so it’s worth prioritizing. Building your mathematical foundation should absolutely be your highest priority.\n\nBut of course if you try to master everything all the time you’ll choke on the size of the task! There are a lot of things you’ll have to abstract, approximate, merely-accept, contextualize, remember-where-to-learn-more, etc instead of truly master. How you decide which things to master is… up to you! You can always make another pass over the material in greater depth later if you decide you need more technical depth. Most people don’t actually do that, but basically everyone who is really good does.\n\nYou know that quote “don’t allow your schooling to interfere with your education”? It takes a lot of intellectual maturity to do this, that many people don’t have, but it’s realistically what’s required.\n\nIn a more concrete sense, I think Hartley and Zissermann make things *way* more complex than they need to be, and multi view geometry is pretty much my area of specialization! You should consider an alternative resource with better pedagogy. I’ve not read it but I’ve heard good things about “an invitation to 3d vision”. \n\nRegardless, if your goal is to do structure from motion or SLAM you actually need a lot of stuff in that textbook. Heck, you don’t actually even need to know what a fundamental matrix is! To say nothing of trifocal tensors etc. \n\nI also don’t like Forsyth and Ponce but it’s been so long I sincerely don’t remember why :). Szeliski is basically the best survey of the field you could ask for, it just focuses primarily on classical methods. It has a reading guide in the intro I recommend you read: it also encourages you to skim it then dig deeper. (But pay close attention for the first few chapters to establish those fundamentals.)\n\nIn early 2017 some coworkers and I gossiped in shock that the new hotshot computer vision PhD grad we had just hired didn’t know what a pinhole camera matrix was. He was a pure deep learning guy. Having that blind spot (often in chapter 1 of any CV textbook) that far into your CV education is a *huge* unforced error, but it wasn’t actually that relevant to his work... at the time, as far as he knew. \n\nYou’re gonna miss some stuff no matter what you do, so you need to spend time both on depth but also breadth. Adaptability and knowledge-of (potential access to) a large bag of tricks is a huge asset as a computer vision engineer. In the literature about decision making under uncertainty you’ll hear people talk about the tradeoffs between exploitation and exploration… you’ll need to just use your judgment to tweak the hyperparameters of your own personal learning, like you would for a machine learning model.', 'You want to be great or mediocre at computer vision? \n\nGreat means understanding lenses and the very complicated math behind CV.\n\nBest, \nComputer Vision AI startup founder.', 'Best part of learning is to this decision to start this had part, if you start and see interest part you will love it and time of learning will go like Cruze. I suggest go ahead. One day you will appraise this decision.']","[""This is your only chance to learn that stuff. You will never have time to come back and learn it properly.\n\nAlso if you know it you might use it one day. And obviously if you don't learn it you will never use it."", 'That’s an excellent question!\n\nI don’t think there’s a one size fits all solution. To be any good at computer vision you’re going to have to self teach quite a lot, and that essentially never means you learn things in some well structured or “optimal” way.\n\nYou’re going to have to make several passes over the same material, preferably presented in multiple formats, over a significant period of time. This means stuff will be overlapping, which can feel a bit chaotic. But if you look at the research on how people learn and retain knowledge over the long term it’s not by sitting for a lecture, doing a homework, and then simply moving on!\n\nBut I will say that the sooner you internalize the math the better and smoother the rest of your journey will be, so it’s worth prioritizing. Building your mathematical foundation should absolutely be your highest priority.\n\nBut of course if you try to master everything all the time you’ll choke on the size of the task! There are a lot of things you’ll have to abstract, approximate, merely-accept, contextualize, remember-where-to-learn-more, etc instead of truly master. How you decide which things to master is… up to you! You can always make another pass over the material in greater depth later if you decide you need more technical depth. Most people don’t actually do that, but basically everyone who is really good does.\n\nYou know that quote “don’t allow your schooling to interfere with your education”? It takes a lot of intellectual maturity to do this, that many people don’t have, but it’s realistically what’s required.\n\nIn a more concrete sense, I think Hartley and Zissermann make things *way* more complex than they need to be, and multi view geometry is pretty much my area of specialization! You should consider an alternative resource with better pedagogy. I’ve not read it but I’ve heard good things about “an invitation to 3d vision”. \n\nRegardless, if your goal is to do structure from motion or SLAM you actually need a lot of stuff in that textbook. Heck, you don’t actually even need to know what a fundamental matrix is! To say nothing of trifocal tensors etc. \n\nI also don’t like Forsyth and Ponce but it’s been so long I sincerely don’t remember why :). Szeliski is basically the best survey of the field you could ask for, it just focuses primarily on classical methods. It has a reading guide in the intro I recommend you read: it also encourages you to skim it then dig deeper. (But pay close attention for the first few chapters to establish those fundamentals.)\n\nIn early 2017 some coworkers and I gossiped in shock that the new hotshot computer vision PhD grad we had just hired didn’t know what a pinhole camera matrix was. He was a pure deep learning guy. Having that blind spot (often in chapter 1 of any CV textbook) that far into your CV education is a *huge* unforced error, but it wasn’t actually that relevant to his work... at the time, as far as he knew. \n\nYou’re gonna miss some stuff no matter what you do, so you need to spend time both on depth but also breadth. Adaptability and knowledge-of (potential access to) a large bag of tricks is a huge asset as a computer vision engineer. In the literature about decision making under uncertainty you’ll hear people talk about the tradeoffs between exploitation and exploration… you’ll need to just use your judgment to tweak the hyperparameters of your own personal learning, like you would for a machine learning model.', 'You want to be great or mediocre at computer vision? \n\nGreat means understanding lenses and the very complicated math behind CV.\n\nBest, \nComputer Vision AI startup founder.', 'Best part of learning is to this decision to start this had part, if you start and see interest part you will love it and time of learning will go like Cruze. I suggest go ahead. One day you will appraise this decision.']",29,12,0.91,Help: Theory ,1751020832.0
1llpen5,computervision,Do you know the best model for hand tracking?,I am trying to build a project for hand tracking. Do you know any open-source libraries for hand tracking?,"['Depends on what you want to do, but mediapipe can be used']","['Depends on what you want to do, but mediapipe can be used']",6,1,1.0,Discussion ,1751015934.0
1llp6ti,computervision,"Resume Review : Hard to land Interviews , Need Guidance","I am new to job search and interviews, I didnt go for a job after my bachelors in India, Now doing my MS in US.

My eperience is in labs, I have not published any papers so far. I am not sure where to improve, I so far tried reimplementation existing works.

I would love to hear all your opinions, feedback. I was aiming for roles like CV/DL Engineer, Robotics Perception roles, Sensor Calibration and Integration roles.

https://preview.redd.it/1xy89lxyqf9f1.png?width=999&format=png&auto=webp&s=b02f5ccff5c6f5caab60f6d076d1a6b4ce4ff917


","['I think you are selling yourself short on a lot of this by leaving out the “so what”. Like this one:\n\n“Validated sensors with Raspberry Pi for autonomous underwater glider during initial development phase with the Professor.” \n\nThat could be a very cool thing, but the limited details provided leave me wondering what exactly you did. Validated doesn’t really tell me much. Sensors, what sensors? Underwater glider? You mean like a submarine? \n\nConsider something like this instead:\n\n“Integrated pressure, velocity, orientation and acceleration sensors into PID controller utilizing Kalman filtering for error-correction while staying within strict memory and CPU budget to create autonomous submarine with Professor X’s Nautical Nonsense Research Group. Achieved sub-centimeter positional accuracy in GPS-denied environment with self-correcting inertial navigation with automatic reorientation based on external data fusion while accounting for underwater drift due to tidal forces.”\n\nI think we are both saying the same thing - but adding the details makes your prior work not only look more impressive but more closely aligns to business needs. It takes your work beyond a simple cut and paste from a GitHub repository to demonstrating your ability to then utilize that information in a way that solves a meaningful problem.']","['I think you are selling yourself short on a lot of this by leaving out the “so what”. Like this one:\n\n“Validated sensors with Raspberry Pi for autonomous underwater glider during initial development phase with the Professor.” \n\nThat could be a very cool thing, but the limited details provided leave me wondering what exactly you did. Validated doesn’t really tell me much. Sensors, what sensors? Underwater glider? You mean like a submarine? \n\nConsider something like this instead:\n\n“Integrated pressure, velocity, orientation and acceleration sensors into PID controller utilizing Kalman filtering for error-correction while staying within strict memory and CPU budget to create autonomous submarine with Professor X’s Nautical Nonsense Research Group. Achieved sub-centimeter positional accuracy in GPS-denied environment with self-correcting inertial navigation with automatic reorientation based on external data fusion while accounting for underwater drift due to tidal forces.”\n\nI think we are both saying the same thing - but adding the details makes your prior work not only look more impressive but more closely aligns to business needs. It takes your work beyond a simple cut and paste from a GitHub repository to demonstrating your ability to then utilize that information in a way that solves a meaningful problem.']",1,3,0.57,Discussion ,1751015040.0
1llnju8,computervision,Any deep learning models for object following (not just detection/tracking)?,"Looking for models that go beyond object detection and tracking — specifically for real-time object following (e.g., generating movement cues to follow a target).
Ideally something that can run on edge devices and maybe use monocular depth.
Any suggestions or keywords to look into?","[""well if you want to do motion planning it probably makes sense to at least share what you're trying to plan the motion of\n\nquadcopters and cars behave a bit differently \n\nand both have plenty of ink spilled on motion planning, and they're not the only modalities"", 'Are you looking into ""reidentification""? That is: tracking an object that is visually changing and moving between cameras?  Or are you tracking an object with images from a single camera?  If the latter, just do pose detection as fast as you can and interpolate the motion, or use a Kalman filter to generate motion predictions once you have some data.', 'Kalman filter']","['Are you looking into ""reidentification""? That is: tracking an object that is visually changing and moving between cameras?  Or are you tracking an object with images from a single camera?  If the latter, just do pose detection as fast as you can and interpolate the motion, or use a Kalman filter to generate motion predictions once you have some data.', ""well if you want to do motion planning it probably makes sense to at least share what you're trying to plan the motion of\n\nquadcopters and cars behave a bit differently \n\nand both have plenty of ink spilled on motion planning, and they're not the only modalities"", 'Kalman filter']",6,5,1.0,Discussion ,1751008192.0
1llnggy,computervision,Bytetrack efficiency,"Hello all,

This is regarding a personal project in the field of computer vision i will be working with yolo+Bytetrack i do wan't to know it's efficiency in fast-moving scenarios people say they are better than DeepSort is it so.Thanks in advance.","['Recently I found boxmot in GitHub quite good performance, detection, tracking, Reid. All in 1\n\nhttps://github.com/mikel-brostrom/boxmot', 'Bytetrack is very efficient, more or less as good/fast as it gets when only using boxes.']","['Recently I found boxmot in GitHub quite good performance, detection, tracking, Reid. All in 1\n\nhttps://github.com/mikel-brostrom/boxmot', 'Bytetrack is very efficient, more or less as good/fast as it gets when only using boxes.']",1,2,0.67,Help: Project,1751007817.0
1llkjhb,computervision,Speculative Emergence of Ant-Like Consciousness in Large Language Models,,[],[],0,0,0.29,Discussion ,1750997319.0
1lljid4,computervision,How to remove unwanted areas and use contour detection for locating characters?,"As my project I am trying to detect Nepali number plate and extract the numbers from it. I used YOLOv8 model to detect number plates. It successfully detects the number plate and crops it. The second image is converted to grayscale, gaussian blur is applied then otsu's thresholding is used. I am facing an issue in removing screws from the plate and detecting the numbers. I want to remove screws and noise and then use contour detection to detect individual letters in the plate. Can you help me with this process?","['If you want the text use OCR, it can ignore screws/background automatically. Also it would be better to pass in original colored image in that case.', ' use morphology transformation then detect contour area > threshold', ""You can detect the four corners of the plate and use perspective transform to rectify the image. Then you'd know the coordinates of the area of interest if the plates are consistent."", 'How about OCR ?  Will that work ?', 'Count pixel for contours make 0s below the threshold.']","['If you want the text use OCR, it can ignore screws/background automatically. Also it would be better to pass in original colored image in that case.', ' use morphology transformation then detect contour area > threshold', ""You can detect the four corners of the plate and use perspective transform to rectify the image. Then you'd know the coordinates of the area of interest if the plates are consistent."", 'How about OCR ?  Will that work ?', 'Count pixel for contours make 0s below the threshold.']",19,5,1.0,Help: Project,1750993964.0
1lliiux,computervision,Could someone please suggest a project on segmentation?,"I've been studying object segmentation for days, the theoretical part, but I'd like to apply it to a personal project, a real-life case. Honestly, I can't think of anything, but I want something different from the classic one (fitting a segmentation model to your custom dataset). I want something different. Also, links to websites, blogs, etc., would be very grateful. thanks.","[""Try weakly supervised segmentation. The degree of supervision can vary. The provided labels can be just bounding boxes, could even just be classes of the objects in the image. One technique that's been tried is using the activation maps for CNNs I think (I've heard of weakly supervised object detectors doing this). Maybe you can find some way to sharpen those activation maps, so they localize objects better.\n\nIn general I'd say, you can always make a problem more interesting by dealing with a low data regime, or weak supervision."", 'One real world case I have seen is to use segmentation models to analyse photos of metal parts for detecting stress fractures/cracks in them.', 'I’m doing a segment project, just started https://www.reddit.com/r/computervision/s/iqmhFNWzDG', 'What kinds of things are you interested in? Like if you watch a lot of tv you could make a model that segments a tv from a photo and warps it to be rectangular, so you can watch a tv from a side angle. Obviously not very practical but it’s an application of segmentation!\xa0']","[""Try weakly supervised segmentation. The degree of supervision can vary. The provided labels can be just bounding boxes, could even just be classes of the objects in the image. One technique that's been tried is using the activation maps for CNNs I think (I've heard of weakly supervised object detectors doing this). Maybe you can find some way to sharpen those activation maps, so they localize objects better.\n\nIn general I'd say, you can always make a problem more interesting by dealing with a low data regime, or weak supervision."", 'One real world case I have seen is to use segmentation models to analyse photos of metal parts for detecting stress fractures/cracks in them.', 'I’m doing a segment project, just started https://www.reddit.com/r/computervision/s/iqmhFNWzDG', 'What kinds of things are you interested in? Like if you watch a lot of tv you could make a model that segments a tv from a photo and warps it to be rectangular, so you can watch a tv from a side angle. Obviously not very practical but it’s an application of segmentation!\xa0']",0,5,0.5,Help: Project,1750990864.0
1llg6wd,computervision,Image Classification with Web-DINO,"Image Classification with Web-DINO

[https://debuggercafe.com/image-classification-with-web-dino/](https://debuggercafe.com/image-classification-with-web-dino/)

DINOv2 models led to several successful downstream tasks that include image classification, semantic segmentation, and depth estimation. Recently, the DINOv2 models were trained with web-scale data using the Web-SSL framework, terming the new models as Web-DINO. We covered the motivation, architecture, and benchmarks of Web-DINO in our last article. In this article, we are going to use one of the ***Web-DINO models for image classification***.

https://preview.redd.it/zje0oev06d9f1.png?width=1000&format=png&auto=webp&s=7a35503b83bdba55686f6f1f93e2b03c4c1ae426

",[],[],1,0,0.6,Showcase,1750983886.0
1llcyvn,computervision,"Why does it seem so easy to remove an object's background using segmentation, but it's so complicated to remove a segmented object and fill in the background naturally? Is it actually possible?","Hi,Why does it seem so easy to remove the background of an object using segmentation, but it's so complicated to remove a segmented object and fill the background naturally?



I'm using YOLO11-seg to segment a bottle. I have its mask. But when I try to remove it, all the methods fail or simply cover the object without actually removing it.

What I want is to delete the segmented object and then replace it with a new one.

https://preview.redd.it/voltlejsfc9f1.png?width=1366&format=png&auto=webp&s=47b5fce362f17925ba192c1f36dc0bf0ba56b493

I appreciate your help or recommending an article to help me learn more.
","['Check r/StableDiffusion subreddit. What you are trying to do is inpainting - removing the blank space and filling it with a matching content related to its surroundings. You should encounter practical info there.', 'What specifically does it mean to “remove the background of an object”? I don’t understand what you mean.\n\nIt sounds kinda like you’re confused why a generative task is harder than simple segmentation and masking. The answer is so obvious it’s trite: because the camera didn’t see that part of the world.', 'Well just think about it.\xa0\n\nMost five year olds can cut stuff out of a magazine in a matter of seconds. Tell them to do it carefully and they will.\xa0\n\nBut even some of the best artists in the world struggle to draw photo realistically.\xa0', 'Is call inpainting \n\nhttps://github.com/advimman/lama\n\nMeta lama inpainting, Stable diffusion, control net able to do it.', 'As object is an obstacle from the camera’s point of view, you can only remove the object by using segmentation. Nevertheless , you can fill the missing pixels by using inpainting algorithms. Take a look at https://huggingface.co/docs/diffusers/en/using-diffusers/inpaint. There are also inpainting algorithms without text prompts (they fill the image based on the segmentation boundaries. Rather than YOLO, you can try SAM2 for better segmentation boundaries as well.', ""You can see what you can see (the bottle) but you can't see what you can't see (what is behind the bottle)"", 'Also YOLO models aren’t the best at segmentation. You can improve the results using something like SAM.\xa0']","['Check r/StableDiffusion subreddit. What you are trying to do is inpainting - removing the blank space and filling it with a matching content related to its surroundings. You should encounter practical info there.', 'What specifically does it mean to “remove the background of an object”? I don’t understand what you mean.\n\nIt sounds kinda like you’re confused why a generative task is harder than simple segmentation and masking. The answer is so obvious it’s trite: because the camera didn’t see that part of the world.', 'Well just think about it.\xa0\n\nMost five year olds can cut stuff out of a magazine in a matter of seconds. Tell them to do it carefully and they will.\xa0\n\nBut even some of the best artists in the world struggle to draw photo realistically.\xa0', 'Is call inpainting \n\nhttps://github.com/advimman/lama\n\nMeta lama inpainting, Stable diffusion, control net able to do it.', 'As object is an obstacle from the camera’s point of view, you can only remove the object by using segmentation. Nevertheless , you can fill the missing pixels by using inpainting algorithms. Take a look at https://huggingface.co/docs/diffusers/en/using-diffusers/inpaint. There are also inpainting algorithms without text prompts (they fill the image based on the segmentation boundaries. Rather than YOLO, you can try SAM2 for better segmentation boundaries as well.']",1,9,0.56,Help: Project,1750975180.0
1llalzr,computervision,ShowUI-2B is simultaneously impressive and frustrating as hell.,"Spent the last day hacking with ShowUI-2B, here's my takeaways...

## ✅ The Good

- **Dual output modes**: Simple coordinates OR full action dictionaries - clean AF

- **Actually fast**: Only 1.5x slower with massive system prompts vs simple grounding

- **Clean integration**: FiftyOne keypoints just work with existing ML pipelines

## ❌ The Bad

- **Zero environment awareness**: Uses TAP on desktop, CLICK on mobile - completely random

- **OCR struggles**: Small text and high-res screens expose major limitations

- **Positioning issues**: Points *around* text links instead of *at* them

- **Calendar/date selection**: Basically useless for fine-grained text targets

## What I especially don't like

- **Unified prompts sacrifice accuracy** but make parsing way simpler

- **Works for buttons, fails for text links** - your clicks hit nothing

- **Technically correct, practically useless** positioning in many cases

- **Model card suggests environment-specific prompts** but I want agents that figure it out

## 🚀 Redeeming qualities

- **Foundation is solid** - core grounding capability works

- **Speed enables real-time workflows** - fast enough for actual automation

- **Qwen2.5VL coming** - hopefully fixes the environmental awareness gap

- **Good enough to bootstrap** more sophisticated GUI understanding systems

**Bottom line: Imperfect but fast enough to matter. The foundation for something actually useful.**

#### 💻 Notebook to get started:

https://github.com/harpreetsahota204/ShowUI/blob/main/using-showui-in-fiftyone.ipynb

Check out the full code and ⭐️ the repo on GitHub: https://github.com/harpreetsahota204/ShowUI",['yeah the ui hella annoying'],['yeah the ui hella annoying'],16,1,1.0,Showcase,1750969381.0
1ll7dfr,computervision,Computer vision and ai in robotics,"Ai engineers who have work with robots. Can you explain, which tool you used, programming languages, fields(nlp, computer vision) in your projects?","[""python and c++\n\nagricultural robot and i-can't-tell-you\n\nprimarily custom code, but did use ROS, some off the shelf object object detectors, etc"", 'Python and pytorch for the vision and ML part, Ros to interact with robot hardware again using the Python bindings to keep everything in a single language.', 'Adding to what others have already mentioned! Definitely look into inference optimization with TensorRT or other dedicated inference chips like Hailo.\n\n\nEdit: I assumed you’d be deploying on edge.', 'Roboflow for labeling pictures, ROS for Robots, Python - Copilot - LLMs for help etc. \nSurvey Papers for quick oversight on certain topics', 'In manufacturing, everything is pretty off the shelf and built into a larger system. Robots may be UR, FANUC, ABB, Epson. Vision might come from Cognex, Keyence, Halcon, Zebra. PLCs might be from Allen Bradley, Siemens, Beckhoff', 'From my experience I would say ROS, Roboflow for image detection are two important softwares. Also be prepared to ditch roboflow and use a lighter image model, especially if you are working with limited computing power such as a Raspberry Pi.']","[""python and c++\n\nagricultural robot and i-can't-tell-you\n\nprimarily custom code, but did use ROS, some off the shelf object object detectors, etc"", 'Python and pytorch for the vision and ML part, Ros to interact with robot hardware again using the Python bindings to keep everything in a single language.', 'Adding to what others have already mentioned! Definitely look into inference optimization with TensorRT or other dedicated inference chips like Hailo.\n\n\nEdit: I assumed you’d be deploying on edge.', 'Roboflow for labeling pictures, ROS for Robots, Python - Copilot - LLMs for help etc. \nSurvey Papers for quick oversight on certain topics', 'In manufacturing, everything is pretty off the shelf and built into a larger system. Robots may be UR, FANUC, ABB, Epson. Vision might come from Cognex, Keyence, Halcon, Zebra. PLCs might be from Allen Bradley, Siemens, Beckhoff']",10,9,0.78,Discussion ,1750961684.0
1ll4gvb,computervision,On-device monocular depth estimation on iOS—looking for feedback on performance & models,"Hey r/computervision 👋

I’m the creator of **Magma – Depth Map Extractor**, an iOS app that generates depth maps and precise masks from photos/videos entirely **on-device** using pretrained models like Depth‑Anything V1/V2, MiDaS, MobilePydnet, U2Net, and VisionML. What the app does?

* Imports images/videos from camera/gallery
* Runs depth estimation locally
* Outputs depth maps, matte masks, and lets you apply customizable colormaps (e.g., Magma, Inferno, Plasma)

# I’m excited about how deep learning-based monocular depth estimation (like MiDaS, Depth‑Anything) is becoming usable on mobile devices. I'd love to sparkle a convo around:

1. **Model performance**
   * Are models like MiDaS/Depth‑Anything V2 effective for **on-device video depth mapping**?
   * How do they compare quality-wise with stereo or LiDAR-based approaches?
2. **Real-time / streaming use-cases**
   * Would it be feasible to do continuous depth map extraction on video frames at \~15–30 FPS?
   * What are best practices to optimize throughput on mobile GPUs/NPUs?
3. **Colormap & mask use**
   * Are depth‑based masks useful in your workflows (e.g. segmentation, compositing, AR)?
   * Which color maps lend better interpretability or visualization in production pipelines?

# Questions for the CV community:

* Curious about your experience with **MiDaS-small vs Depth‑Anything** on-device—how reliable are edges, consistency, occlusions?
* Any suggestions for **optimizing depth inference** frame‑by‑frame on mobile (padding, batching, NPU‑specific ops)?
* Do you use depth maps extracted on mobile for **AR, segmentation, background effects** – what pipelines/tools handle these well?

[App Store Link](https://apps.apple.com/us/app/magma-depth-map-extractor/id6535647130)

https://preview.redd.it/iuvcyqcira9f1.png?width=1242&format=png&auto=webp&s=f777b085d1f0cb2f4d46a960e95504239e06d535

","['Have you checked DepthPro by Apple? I guess the pretrained models are already optimized for Apple SoCs.', ""I (try to) use monocular metric depth for multiple tasks, e.g. 3D reconstruction or substituting expensive LiDAR sensors through camera-only solutions for other projects.\n\nThe core challenges I mostly experience are:\n- trustworthyness (not just accuracy)\n- handling of different cameras (intrinsics, some models perform worse on certain fovs or resolutions)\n- inference speed\n- blurry edges (good at this are depth anything v2, patchfusion or apple depth pro)\n\n\nsince i'm on android, would you mind sharing some inference time benchmarks for your app if you have them available? Also, could you elaborate on your deployment process? :)\n\nRegarding videos, i think some fancy post processing could be done over a sequence of single frame predictions, e.g. plausibility checks. There are also multi-frame depth prediction models but i have never tried them.\n\nI would also highly recommend checking out Metric3D V2 for amazingly accurate depths or UniDepth V2 for extra utility.\n\nedit: since you are on ios, why did you decide against apple depth pro (which is integrated into the os i think?)"", 'Very cool!\n\nThere’s a model, I can’t remember the name, which couples monocular depth estimation with sparse lidar points. It’s be really cool if you implemented that too!\xa0\n\nBasically it combines the metric accuracy and precision of lidar with the density of a monocular model, so you get accurate metric depth at every pixel.\xa0']","['Have you checked DepthPro by Apple? I guess the pretrained models are already optimized for Apple SoCs.', ""I (try to) use monocular metric depth for multiple tasks, e.g. 3D reconstruction or substituting expensive LiDAR sensors through camera-only solutions for other projects.\n\nThe core challenges I mostly experience are:\n- trustworthyness (not just accuracy)\n- handling of different cameras (intrinsics, some models perform worse on certain fovs or resolutions)\n- inference speed\n- blurry edges (good at this are depth anything v2, patchfusion or apple depth pro)\n\n\nsince i'm on android, would you mind sharing some inference time benchmarks for your app if you have them available? Also, could you elaborate on your deployment process? :)\n\nRegarding videos, i think some fancy post processing could be done over a sequence of single frame predictions, e.g. plausibility checks. There are also multi-frame depth prediction models but i have never tried them.\n\nI would also highly recommend checking out Metric3D V2 for amazingly accurate depths or UniDepth V2 for extra utility.\n\nedit: since you are on ios, why did you decide against apple depth pro (which is integrated into the os i think?)"", 'Very cool!\n\nThere’s a model, I can’t remember the name, which couples monocular depth estimation with sparse lidar points. It’s be really cool if you implemented that too!\xa0\n\nBasically it combines the metric accuracy and precision of lidar with the density of a monocular model, so you get accurate metric depth at every pixel.\xa0']",0,3,0.5,Help: Project,1750954984.0
1ll1g9j,computervision,The best course platform except youtube.,"If we take udemy platform, some courses are incompleteness. In these courses, some computer vision techniques aren’t included, buy next one, no required section like segmentation, buy one more, no explanations towards code. On coursera, no quality explanations(I mean techniques). So, does someone know the best free/paid platform for professional computer vision roadmap, where all important themes are included?","['The one I use by far the most is [https://arxiv.org/](https://arxiv.org/)', ""Many universities publish the materials of some of their courses online for free. Google OCW + the university you're interested in"", 'Check your public libraries and libraries your your schools and universities.']","['The one I use by far the most is [https://arxiv.org/](https://arxiv.org/)', ""Many universities publish the materials of some of their courses online for free. Google OCW + the university you're interested in"", 'Check your public libraries and libraries your your schools and universities.']",1,3,0.67,Discussion ,1750947852.0
1ll0jdv,computervision,Best local OCR for multilingual/swedish text in real life scenes,"Hi, i have been looking around for a OCR that works better for Swedish text in photos taken irl. The text is mainly logos/printed text on vehicles which can be very angled and sometimes small.

One of the OCRs which worked great but only knows english is GOT-OCR2\_0. Does anyone know any better ocr? ",[],[],2,0,1.0,Help: Project,1750945523.0
1ll0c4b,computervision,Help me find a video!,"I watched a (YouTube?) video a while ago about a guy using 2 or 3 cameras in various positions in a field. They were all pointed at a similar region of sky and he used it to accurately triangulate birds and planes in 3D space. He wanted to market it towards airports for bird detection to prevent bird strikes. There was no calibration involved to setup the position of the cameras. The video was mostly of blue sky with annotations showing birds. He was able to track incredibly distant objects using the smallest pixel movements.

Similar projects but not the same thing:

[Multi-camera real-time three-dimensional tracking of multiple flying animals](https://pmc.ncbi.nlm.nih.gov/articles/PMC3030815/)

[Multi-camera multi-object tracking: A review of current trends and future advances](https://www.sciencedirect.com/science/article/pii/S0925231223006811)

Optical localisation?


Starting to think it was all a dream...","[""I've got that video in my chat history, I can pull it up for you later. Suffice to say fielding a real system like that is a biiiiiit more complex than he's letting on.""]","[""I've got that video in my chat history, I can pull it up for you later. Suffice to say fielding a real system like that is a biiiiiit more complex than he's letting on.""]",5,3,0.86,Discussion ,1750944984.0
1lkxpoh,computervision,What are some top tier/well reviewed conferences/workshops? How to get those publications?,"I'm curios about reading from some of the top journals/conferences/workshops. If there's any way to read these papers, and how to get access. I'm no academic. So would like to know the names too. ","['The top conference in CV is CVPR. The papers from CVPR are freely available. Other top conferences are ICCV, ECCV, BMVC, and WACV. There are also good adjacent conferences that have a lot of CV papers like AAAI, NeurIPS, IJCAI, etc.', 'International Journal of Computer Vision is an excellent journal with a high impact factor. Some articles are available for free as the authors paid for the open-access feature. Nevertheless, if there are any other articles that interest you and they are behind pay wall, you can always reach out to the authors directly and ask them to send you a copy (it happens quite often and authors are more than happy to share it).']","['The top conference in CV is CVPR. The papers from CVPR are freely available. Other top conferences are ICCV, ECCV, BMVC, and WACV. There are also good adjacent conferences that have a lot of CV papers like AAAI, NeurIPS, IJCAI, etc.', 'International Journal of Computer Vision is an excellent journal with a high impact factor. Some articles are available for free as the authors paid for the open-access feature. Nevertheless, if there are any other articles that interest you and they are behind pay wall, you can always reach out to the authors directly and ask them to send you a copy (it happens quite often and authors are more than happy to share it).']",1,3,0.67,Discussion ,1750937292.0
1lkudx6,computervision,Can I estimate camera pose from an image using a trained YOLO model (no SLAM/COLMAP)?,"Hi all, I'm pretty new to computer vision and I had a question about using YOLO for localization.

Is it possible to estimate the **camera pose (position and orientation)** from a **single input image** using a YOLO model trained on a specific object or landmark (e.g., a building with distinct features)? My goal is to **calibrate the view direction of the camera** one time, without relying on SLAM or COLMAP.

I'm not trying to track motion over time—just determine the direction I'm looking at when the object is detected.
If this is possible, could anyone point me to relevant resources, papers, or give guidance on how I’d go about setting this up?","['Let\'s assume a few things: you know the camera\'s intrinsic calibration, and YOLO\'s detections are exact points, and have little noise. If the points you detect are known points over a building, then you know their 3D coordinates in the building\'s reference frame. This means you\'re in a position to run the PnP algorithm ( [https://en.wikipedia.org/wiki/Perspective-n-Point](https://en.wikipedia.org/wiki/Perspective-n-Point) ) for which there are implementations in openCV and everywhere else.\n\nIf you don\'t know the camera\'s intrinsic calibration, then there is a version of PnP that works for cameras with unknown focal length. I suppose it\'s limited to pinhole cameras, so don\'t do this with a fisheye camera for instance. [https://ieeexplore.ieee.org/document/9184857](https://ieeexplore.ieee.org/document/9184857)\n\nNow, the detections by YOLO will not be super precise: you get a bounding box around your points, but hte corners are sometimes quite free to go their merry way. It\'s not super realistic to say we can re-train YOLO to become perfect with additional efforts. Usually, you just work around this by using the most points you can and hope it\'ll ""average out"" the errors. You can also use may detections over time, but that is more complex since you need to work with the camera\'s change of position between frames...', ""You can theoretically find vanishing points from an image: https://bmva-archive.org.uk/bmvc/2013/Papers/paper0090/paper0090.pdf\n\nThis should give you every camera parameter except scale. If you have a detected object you can use its dimensions to calibrate scale and determine camera's position in real world units relative to the ground plane for example.\n\nFrom a single image it's all going to be super unreliable with like 50% error margin."", 'I think what you want to do is very similar to ""map free visual relocalization"". If so, you can have a look at the leaderboard on https://research.nianticlabs.com/mapfree-reloc-benchmark\n\nIn particular, have a look at mast3r:\nif you have a reference image that contains these landmarks, you can obtain the relative pose and 3d pointcloud of it.', 'I think its important to step back and ask ""but why?""\n\nYou\'re trying to force a square peg into a round hole. Sure, you can make it fit, but why not just use the round peg?']","['Let\'s assume a few things: you know the camera\'s intrinsic calibration, and YOLO\'s detections are exact points, and have little noise. If the points you detect are known points over a building, then you know their 3D coordinates in the building\'s reference frame. This means you\'re in a position to run the PnP algorithm ( [https://en.wikipedia.org/wiki/Perspective-n-Point](https://en.wikipedia.org/wiki/Perspective-n-Point) ) for which there are implementations in openCV and everywhere else.\n\nIf you don\'t know the camera\'s intrinsic calibration, then there is a version of PnP that works for cameras with unknown focal length. I suppose it\'s limited to pinhole cameras, so don\'t do this with a fisheye camera for instance. [https://ieeexplore.ieee.org/document/9184857](https://ieeexplore.ieee.org/document/9184857)\n\nNow, the detections by YOLO will not be super precise: you get a bounding box around your points, but hte corners are sometimes quite free to go their merry way. It\'s not super realistic to say we can re-train YOLO to become perfect with additional efforts. Usually, you just work around this by using the most points you can and hope it\'ll ""average out"" the errors. You can also use may detections over time, but that is more complex since you need to work with the camera\'s change of position between frames...', ""You can theoretically find vanishing points from an image: https://bmva-archive.org.uk/bmvc/2013/Papers/paper0090/paper0090.pdf\n\nThis should give you every camera parameter except scale. If you have a detected object you can use its dimensions to calibrate scale and determine camera's position in real world units relative to the ground plane for example.\n\nFrom a single image it's all going to be super unreliable with like 50% error margin."", 'I think what you want to do is very similar to ""map free visual relocalization"". If so, you can have a look at the leaderboard on https://research.nianticlabs.com/mapfree-reloc-benchmark\n\nIn particular, have a look at mast3r:\nif you have a reference image that contains these landmarks, you can obtain the relative pose and 3d pointcloud of it.', 'I think its important to step back and ask ""but why?""\n\nYou\'re trying to force a square peg into a round hole. Sure, you can make it fit, but why not just use the round peg?']",0,4,0.5,Help: Project,1750924780.0
1lktqq6,computervision,anyone have a pimeyes subscription? opinions?,i‘m thinking of purchase but have some concerns,"['I reccomend you to check results first on pimeyes, [lenso.ai](http://lenso.ai) and facecheck - compare them and choose the best one for you']","['I reccomend you to check results first on pimeyes, [lenso.ai](http://lenso.ai) and facecheck - compare them and choose the best one for you']",2,2,0.75,Commercial ,1750922196.0
1lks0yr,computervision,Looking for: researcher networking in south Silicon Valley,"Hello Computer Vision Researchers,

With 4+ years in Silicon Valley and a passion for cutting-edge CV research, I have ongoing projects (outside of work) in stereo vision, multi-view 3D reconstruction and shallow depth-of-field synthesis.

I would love to connect with Ph.D. students, recent graduates or independent researchers in south bay, who

* Enjoy solving challenging problems and pushing research frontiers
* Are up for brainstorming over a cup of coffee or a nature hike

Seeking:

1. Peer-to-peer critique, paper discussions, innovative ideas
2. Accountability partners for steady progress

If you’re working on multi-view geometry, depth learning / estimation, 3D scene reconstruction, depth-of-field, or related topics, feel free to DM me.

Let’s collaborate and turn ideas into publishable results!",[],[],6,0,0.8,Research Publication,1750915724.0
1lkr5lc,computervision,"[RevShare] Vision Correction App Dev Needed (Equity Split) – Flair: ""Looking for Team""","#Accessibility #AppDev #EquitySplit
**Title:** Vision Correction App Dev Needed (Equity Split) – Documented IP, NDA Ready


**Title:** [#VisionTech] Vision Correction App Dev Needed (Equity for MVP + Future AR)

**Body:**
Seeking a developer to build an MVP that distorts device screens to compensate for uncorrected vision (like digital glasses).

- **Phase 1 (6 weeks):** Static screen correction (GPU shaders for text/images).
- **Phase 2 (2025):** Real-time AR/camera processing (OpenCV/ARKit).
- **Offer:** 25% equity (negotiable) + bonus for launching Phase 2.

I’ve documented the IP (NDA ready) and validated demand in vision-impaired communities.

**Reply if you want to build foundational tech with huge upside.**
","['This is not possible without adding lenses to the display', ""Im not sure if that's true but it could be.  I have read some things that say it is possible.  I have perfect vision so it's hard for me to test myself and without the developers I need it's hard for me to test. I have my doubts on it as well .  I appreciate your response no matter what."", '[deleted]', 'I could see doing this to replace prism-only prescriptions, and I don’t think Apple lets you sell apps that access the cameras. But for cylinder/sphere correction, you need extra elements between eyes and display. Might be possible with a pinhole insert, but that is hardware only. won’t even need an app. And it would take some creativity to not break eye-tracking.']","['This is not possible without adding lenses to the display', ""Im not sure if that's true but it could be.  I have read some things that say it is possible.  I have perfect vision so it's hard for me to test myself and without the developers I need it's hard for me to test. I have my doubts on it as well .  I appreciate your response no matter what."", '[deleted]', 'I could see doing this to replace prism-only prescriptions, and I don’t think Apple lets you sell apps that access the cameras. But for cylinder/sphere correction, you need extra elements between eyes and display. Might be possible with a pinhole insert, but that is hardware only. won’t even need an app. And it would take some creativity to not break eye-tracking.']",1,13,0.6,Help: Theory ,1750912699.0
1lkr4ib,computervision,3D Point Cloud Segmentation of scence to access particular object,"I have a point cloud of a scene, and would like to segment out a particular object from the scene, for instance a football field scene and the goal post, I’m more and only interested in getting the goal post point cloud out of and from this scene ignoring everyother thing in the scene point cloud, how do I do this, has anyone ever something like this. Most state-of-the-art methods/algorithms I have seen just focus on classification and just mere semantic segmentation and identification of the objects in the scene including PointNet++, RandLA-Net etc. Can you drop ideas on how I can approach or perform this? Would really appreciate that.

Edit: I’m assuming that there maybe other goal posts /players/spectators or in general noise but interested in the one immediately closer or obvious from the LiDAR source/device","[""Aren't you just describing semantic segmentation in 3D? You could use a 3D UNet."", 'If the object is isolated like a goal post you could just do semantic. If you need instance you can look at repos like Mask3D, SoftGroup']","[""Aren't you just describing semantic segmentation in 3D? You could use a 3D UNet."", 'If the object is isolated like a goal post you could just do semantic. If you need instance you can look at repos like Mask3D, SoftGroup']",1,5,1.0,Discussion ,1750912592.0
1lkpryh,computervision,Low-Budget Sensor Fusion Setup with DE10-Nano and Triggered Cameras – Need Advice,"Hi everyone,

I'm working on a sensor fusion research project for my PhD and a future paper publication. I need to acquire synchronized data from multiple devices in real time. The model I'm building is offline, so this phase is focused entirely on low-latency data acquisition.

The setup includes:

* An RGB camera with external triggering and reliable timestamping.
* A distance perception device (my lab provides access to a stereographic camera).
* A GNSS receiver for localization.

The main platform Im considering for the data acquisition and synchronization will be a DE10-Nano FPGA board.

I'm currently considering two RGB camera options:

1. See3CAM\_CU135 (e-con Systems)
   * Pros: Hardware MJPEG/H.264 compression, USB 3.0, external trigger (GPIO), UVC compliant
   * Cons: Expensive (\~USD $450 incl. shipping and import fees)
2. Arducam OV9281 (USB 3.0 Global Shutter)
   * Pros: Global shutter, external trigger (GPIO), more affordable (\~USD $120)
   * Cons: I've read that it has no hardware compression and is not that reliable on deterministic times

My budget is very limited, so I'm looking for advice on:

* Any more affordable RGB cameras that support triggering and ≥1080p@30fps
* Experience using the DE10-Nano for real-time data fusion or streaming
* Whether offloading data via Ethernet to another computer is a viable low-latency alternative to onboard RAM/SD writing

Any insights, experience, or recommendations would be hugely appreciated. Thanks in advance!

**Edit:** Forgot to mention — I’m actually a software engineer, so I don’t have much hands-on experience with FPGAs. That’s one of the reasons I went with the DE10-Nano. I do have a solid background in concurrency and parallel programming in C/C++, though.",[],[],2,0,1.0,Help: Project,1750908186.0
1lklw23,computervision,how long did it take to understand the Transformer such that you can implement it in Python code?,.,"['For implementation purposes I don’t think you really need to understand the transformer that deeply. The most naive implementation basically involves just doing the linear projections (get query, key and value vectors) seeing it as a matrix multiplication (to compute the attention map), normalization, soft max, then multiply against value vectors to get your final answer. \n\nThe encoder is particularly straightforward. The decoder is pretty similar but uses cross attention (not just self attention).\n\nSome things do get a bit tricky though. First issue is that you’re probably training on sequences of variable length. You’ll need to have some sort of padding mechanism, with a max sequence length, so you can batch sequences. You’ll probably also need masks you indicate portions of the sequence that are relevant (for example, for a sequence that is actually 100 elements but padded to be 200 elements, you need a Boolean mask of true values for the first 100 elements of a sequence and then 100 false values). Using these masks, you ensure you don’t compute losses for tokens that are out of bounds of the sequence (correspond to false values in the mask). Sometimes we drop really long and really short sequences from the training data as well, it tends to cause the model to perform worse (I guess they’re kind of like outliers in a way).\n\nYou need a setup involving BOS and EOS tokens of course. Make sure you don’t make the mistake of training the model to output a BOS given a BOS — it’s a dumb mistake I made because when I first implemented the transformer, I added BOS and EOS tokens beforehand before doing teacher forcing.\n\nYou can try key value caching later when you feel more confident, to cache key value vectors for the layers of the decoder. Speeds up inference.\n\nOne final note — the original paper on transformers is worth reading but does skim over implementation. I highly recommend Jay Alammars writeup, looking at code in GitHub if you’re stuck, I think Harvard used to have a Python notebook you could look at too that implemented the transformer (albeit with no key value caching).\n\nThere’s also stuff on learning rate schedules and initialization (I forget what the original transformer used) that you should certainly look at (it’s all in the original paper). \n\nI’ve also avoided discussing multiheaded attention, but that’s not too bad.', 'I recommend [annotated transformer](https://nlp.seas.harvard.edu/annotated-transformer/) to read.', 'What helped me understand the transformer/attention was looking at the code others have written and debugging through the shapes in a forward pass. [Here’s](https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/simple_vit.py#L80) an example.\n\nHowever, if I didn’t get involved in custom network building for some time, I have to admit that I’d need a quick refresher on the topic before getting into that again.', ""I didn't, other people already implemented it. I simply imported it.""]","['For implementation purposes I don’t think you really need to understand the transformer that deeply. The most naive implementation basically involves just doing the linear projections (get query, key and value vectors) seeing it as a matrix multiplication (to compute the attention map), normalization, soft max, then multiply against value vectors to get your final answer. \n\nThe encoder is particularly straightforward. The decoder is pretty similar but uses cross attention (not just self attention).\n\nSome things do get a bit tricky though. First issue is that you’re probably training on sequences of variable length. You’ll need to have some sort of padding mechanism, with a max sequence length, so you can batch sequences. You’ll probably also need masks you indicate portions of the sequence that are relevant (for example, for a sequence that is actually 100 elements but padded to be 200 elements, you need a Boolean mask of true values for the first 100 elements of a sequence and then 100 false values). Using these masks, you ensure you don’t compute losses for tokens that are out of bounds of the sequence (correspond to false values in the mask). Sometimes we drop really long and really short sequences from the training data as well, it tends to cause the model to perform worse (I guess they’re kind of like outliers in a way).\n\nYou need a setup involving BOS and EOS tokens of course. Make sure you don’t make the mistake of training the model to output a BOS given a BOS — it’s a dumb mistake I made because when I first implemented the transformer, I added BOS and EOS tokens beforehand before doing teacher forcing.\n\nYou can try key value caching later when you feel more confident, to cache key value vectors for the layers of the decoder. Speeds up inference.\n\nOne final note — the original paper on transformers is worth reading but does skim over implementation. I highly recommend Jay Alammars writeup, looking at code in GitHub if you’re stuck, I think Harvard used to have a Python notebook you could look at too that implemented the transformer (albeit with no key value caching).\n\nThere’s also stuff on learning rate schedules and initialization (I forget what the original transformer used) that you should certainly look at (it’s all in the original paper). \n\nI’ve also avoided discussing multiheaded attention, but that’s not too bad.', 'I recommend [annotated transformer](https://nlp.seas.harvard.edu/annotated-transformer/) to read.', 'What helped me understand the transformer/attention was looking at the code others have written and debugging through the shapes in a forward pass. [Here’s](https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/simple_vit.py#L80) an example.\n\nHowever, if I didn’t get involved in custom network building for some time, I have to admit that I’d need a quick refresher on the topic before getting into that again.', ""I didn't, other people already implemented it. I simply imported it.""]",17,13,0.9,Discussion ,1750896695.0
1lkez10,computervision,Is there a better model than D-FINE?,"Hello everyone,

Are you aware of any newer or better permisive license model series for object detection than D-FINE?

D-FINE works good for me except for small objects and I am trying to avoid cropping image due to latency.","['RFDetr is OpenSource, latest alternative of YOLO from Roboflow and I did a brief benchmarking last month. It was better than YOLOX for our usecase.', 'Thanks, will check rfdetr out. Base model does seem to do a bit worse against DFine M on Coco though', 'rf detr', 'D-FINE is available here with Apache 2.0 https://github.com/open-edge-platform/training_extensions\n\nOr integrated into the annotation/training/optimisation platform here https://github.com/open-edge-platform/geti', '[deleted]', 'Check out DEIM. Apache 2, improved results over DFINE. Published in CVPR 2025\n\nhttps://github.com/ShihuaHuang95/DEIM', 'jozhang97/deta-swin-large has been my goto.', 'DEIM-DFine', 'I have tried all of these stuff but still no model was able to beat RT-DETR in terms of small object for my case. Not even RT-DETR v2. These SOTA models are benchmarked on large datasets. Real world dataset is different story']","['RFDetr is OpenSource, latest alternative of YOLO from Roboflow and I did a brief benchmarking last month. It was better than YOLOX for our usecase.', 'Thanks, will check rfdetr out. Base model does seem to do a bit worse against DFine M on Coco though', 'rf detr', 'D-FINE is available here with Apache 2.0 https://github.com/open-edge-platform/training_extensions\n\nOr integrated into the annotation/training/optimisation platform here https://github.com/open-edge-platform/geti', '[deleted]']",12,16,0.94,Discussion ,1750879485.0
1lke81m,computervision,Real-Time Inference Issues!! need advice,"Hello. I have built a live image-classification model on Roboflow, and have deployed it using VScode. Now I use a webcam to scan for certain objects while driving on the road, and I get live feed from the webcam.

However inference takes at least a second per update, and when certain objects i need detected (particularly small items that performed accurately while at home testing) are passed by and it just says 'clean'.

I trained my model on Resnet50, should I consider using a smaller (or bigger model)? Or switch to ViT, which Roboflow also offers.


All help would be very appreciated, and I am open to answering questions.","['Are you running the model on your device (and if so, what type of hardware are you using) or in the cloud?', 'Try running the model locally. We do local real-time inference in our open--source project HUB - [https://github.com/securade/hub](https://github.com/securade/hub) on a intel i5 laptop with 16 gb ram we get 15 fps with onnx yolov7 model.']","['Are you running the model on your device (and if so, what type of hardware are you using) or in the cloud?', 'Try running the model locally. We do local real-time inference in our open--source project HUB - [https://github.com/securade/hub](https://github.com/securade/hub) on a intel i5 laptop with 16 gb ram we get 15 fps with onnx yolov7 model.']",3,7,0.8,Help: Project,1750877748.0
1lkdy5i,computervision,How to retrieve K matrix from smartphone cameras?,I would like to deploy my application as PWA/webapp. Is there any convenient way to retrieve the K intrinsic matrix from the camera input?,"['check the image metadata. if its not there, youll have to calibrate it yourself. honestly, if you can, you should do that anyways.', 'Calibrate the camera?', ""Camera calibration is the simplest and most reliable, I've heard of deep learning methods that estimate camera parameters from images (but the basic camera calibration method is simple, reliable, always works)"", 'Depends on the phone. iPhones for example expose intrinsics with an API.']","['check the image metadata. if its not there, youll have to calibrate it yourself. honestly, if you can, you should do that anyways.', 'Calibrate the camera?', ""Camera calibration is the simplest and most reliable, I've heard of deep learning methods that estimate camera parameters from images (but the basic camera calibration method is simple, reliable, always works)"", 'Depends on the phone. iPhones for example expose intrinsics with an API.']",5,6,0.86,Help: Project,1750877121.0
1lkd1mc,computervision,How did you guys get a computer vision engineer internship?,What are the things you did to get one? What are the things I should know to get a computer vision engineer internship?,"['I got my internship in Computer Vision due to my Masters Thesis. The work done in my thesis was very relevant to the project in an early start up. Nowadays, I think companies would want to see your publications even when hiring for intern. Most candidates who gets hired these days as CV intern are actually PhDs with extremely successful publication tracks and past experience. I would advise taking up thesis under a professor who has PhD students with good publications.', 'I run a computer vision startup and honestly I had literally 2400 applications for my last CV engineer role. \nHere’s who I interviewed:\nPeople who have worked with direct camera optimization I always interviewed. CV is a lot of edge technology with performance issues\nPeople who low level CV coding experience since CV has a lot of direct to hardware programming opportunities\nNovel AI techniques that expand the anomaly or generalizability of CV\nPeople who got CV to work well in real world environments\n\nStuff that I ignored:\nFine tuning a model using your own data \nUsing a VLM or LLM to do something that… those models can do\nUsing a public data set with more augmentations to beat SOTA… that’s not the point', ""I asked. Unless they are through a university, I think it's more who you know for a field like CV lol"", 'Sort of got into it 5+ years ago, basically the right person at the right time kind of thing.', 'I gave a few presentations on my PhD project at industry events, got offered something after the talks.  Now when I hire, I get a massive pile of applicants so look for interesting project work on Github as a good indicator.', 'We’re hiring! DM us your resume.', 'not an internship but a full time computer vision engineer position on a DARPA project\n\nI came back from abroad after finishing up my physics BSc, set up my LinkedIn for the first time, and got a message just a few days later asking me to come in for an interview next week. i got an offer the day after my interview.\n\nthey later told me they did a keyword search on LinkedIn and i was the *only* person to come up in a 500 mile radius. there are 100 million people in that circle!']","['I got my internship in Computer Vision due to my Masters Thesis. The work done in my thesis was very relevant to the project in an early start up. Nowadays, I think companies would want to see your publications even when hiring for intern. Most candidates who gets hired these days as CV intern are actually PhDs with extremely successful publication tracks and past experience. I would advise taking up thesis under a professor who has PhD students with good publications.', 'I run a computer vision startup and honestly I had literally 2400 applications for my last CV engineer role. \nHere’s who I interviewed:\nPeople who have worked with direct camera optimization I always interviewed. CV is a lot of edge technology with performance issues\nPeople who low level CV coding experience since CV has a lot of direct to hardware programming opportunities\nNovel AI techniques that expand the anomaly or generalizability of CV\nPeople who got CV to work well in real world environments\n\nStuff that I ignored:\nFine tuning a model using your own data \nUsing a VLM or LLM to do something that… those models can do\nUsing a public data set with more augmentations to beat SOTA… that’s not the point', ""I asked. Unless they are through a university, I think it's more who you know for a field like CV lol"", 'Sort of got into it 5+ years ago, basically the right person at the right time kind of thing.', 'I gave a few presentations on my PhD project at industry events, got offered something after the talks.  Now when I hire, I get a massive pile of applicants so look for interesting project work on Github as a good indicator.']",27,21,0.89,Discussion ,1750875058.0
1lkai9z,computervision,Texture more important feature than color,Working on a computer vision model where I want to reduce color's effect as a feature and increase the weight of the texture and topography type feature more. Would like to know some processes and previous work if someone has done it. ,"['Well… I’m assuming you’ve heard of grayscale? Single channel images?', 'Grayscale and/or massively distorting colors during training. Randomly swapping channels.\xa0\n\nBasically just common sense stuff that makes color into a random feature so there’s nothing to learn from it anymore.\xa0']","['Well… I’m assuming you’ve heard of grayscale? Single channel images?', 'Grayscale and/or massively distorting colors during training. Randomly swapping channels.\xa0\n\nBasically just common sense stuff that makes color into a random feature so there’s nothing to learn from it anymore.\xa0']",0,5,0.5,Help: Project,1750869374.0
1lka0nz,computervision,Replacing 3D chest topography with Monocular depth estimation for Medical Screening,"I’m investigating whether monocular depth estimation can be used to replicate or approximate the kind of spatial data typically captured by 3D topography systems in front-facing chest imaging, particularly for screening or tracking thoracic deformities or anomalies.

The goal is to reduce dependency on specialized hardware (e.g., Moiré topography or structured light systems) by using more accessible 2D imaging, possibly from smartphone-grade cameras, combined with recent monocular depth estimation models (like DepthAnything or Boosting Monocular Depth).

Has anyone here tried applying monocular depth estimation in clinical or anatomical contexts especially for curved or deformable surfaces like the chest wall?

Any suggestions on:
•	Domain adaptation strategies for such biological surfaces?
•	Datasets or synthetic augmentation techniques that could help bridge the general-domain → medical-domain gap?
•	Pitfalls with generalization across body types, lighting, or posture?

Happy to hear critiques or point-outs to similar work I might’ve missed!","[""I have not but I'm 99% sure it would not work of the shelf as there are no such images in the usual datasets used for that. Training a network yourself would also be very difficult. Getting data to do that would be a nightmare and even if you do you run a big risk of getting patients with out of distribution morphologies or issues."", ""Let's say that for specific cases such as a medical environment I have never tested, but on more common objects yes. But moving from technologies that work on a 2.5D (let's say) to monocular depth estimation I don't think is very advantageous, to be precise. Usually monocular depth estimations are good for an approximation, don't think at the current state of being able to have all the precision of a moire or a structured light. However, if you want to do some testing at the moment I recommend you try marigold, it seems to be among the best. You could try looking directly to 3D acquisition tools like lidar or 3D scanner.""]","[""I have not but I'm 99% sure it would not work of the shelf as there are no such images in the usual datasets used for that. Training a network yourself would also be very difficult. Getting data to do that would be a nightmare and even if you do you run a big risk of getting patients with out of distribution morphologies or issues."", ""Let's say that for specific cases such as a medical environment I have never tested, but on more common objects yes. But moving from technologies that work on a 2.5D (let's say) to monocular depth estimation I don't think is very advantageous, to be precise. Usually monocular depth estimations are good for an approximation, don't think at the current state of being able to have all the precision of a moire or a structured light. However, if you want to do some testing at the moment I recommend you try marigold, it seems to be among the best. You could try looking directly to 3D acquisition tools like lidar or 3D scanner.""]",2,5,1.0,Help: Theory ,1750868285.0
1lk50nv,computervision,"Chnage Image Background, Help","Hello guys, I'm trying to remove the background from images and keep the car part of the image constant and change the background to studio style as in the above images. Can you please suggest some ways by which I can do that?","['Python has a library called rembg', 'If you’re ok doing diffusion model. Use SAM model to segment car first, and replace background with any prompt you like. You need to use Diffusion + Dreambooth']","['Python has a library called rembg', 'If you’re ok doing diffusion model. Use SAM model to segment car first, and replace background with any prompt you like. You need to use Diffusion + Dreambooth']",7,4,0.73,Help: Project,1750856385.0
1lk4ff9,computervision,Looking for AI-powered CCTV system for my retail store — any recommendations?,"I’m running a mid-size retail store and starting to look into AI-powered CCTV or video analytics systems. Ideally something that can do real-time people counting, detect shoplifting behavior and help with queue management.

I've read a bit about AI cameras but honestly don’t know which brands are actually reliable vs pure hype. Has anyone here used any AI surveillance systems that actually work well? Not looking for some overpriced enterprise system — just something accurate, scalable, and reasonably priced. Appreciate any recommendations based on actual experience!","['If you want to roll your own, Frigate is a popular open-source project to look into. You could use it for computer vision and do analysis with Home Assistant. \n\n[Frigate NVR](https://frigate.video/)', 'Anything that is out of the box will have limited accuracy as the model will need to be able to generalize to your new environment. If you want something that will have extreme accuracy, I highly recommend to look into custom models for your store that will be trained and deployed in the same environment (especially that you are looking for illegal behavior where you need to be very certain that it is occurring). It is all very doable and pretty easy to deploy, especially that you have access to export images from your camera for potential training.', 'Have you thought about a custom setup? Sony released some example apps for the [Raspberry Pi AI Camera](https://www.raspberrypi.com/products/ai-camera/). You can pretty easily get it running for stuff like people counting or smart triggers on the device.  Super flexible and way more affordable than big enterprise systems, I think. [https://github.com/SonySemiconductorSolutions/aitrios-rpi-sample-apps/tree/main/examples](https://github.com/SonySemiconductorSolutions/aitrios-rpi-sample-apps/tree/main/examples) \n\n[https://imgur.com/a/qROTHKi](https://imgur.com/a/qROTHKi)', 'My company deals in solutions similar to what you’re looking for - dm and we can discuss more!', 'If you want ""real-time people counting, detect shoplifting behavior and help with queue management"" then its Hikvision and second place to that Dahua.\n\nHowever you seem to think these systems are overpriced...', ""I think you're looking for CrowdIQ by DevelMo. Check this out on their website and socials, https://develmo.com/our-products/crowdiq\nhttps://www.facebook.com/share/v/15f83KeC3C/"", 'Check out Roboflow..', ""Any CCTV is good enough\n\nFor shoplifting behavior, you need to ensure there are no blind spots. For computer vision you will need shared Reid for multiple camera and action detection like tears barcode, put items into bag, pocket, inside cloth and etc.\n\nFor counting can see number if Reid for count\n\nI'm doing a dine and dash AI project and I constructed the room floor plan and calibrated all the person reid current position in the floor plan."", 'Check out [Unifi](https://ui.com/us/en/physical-security) \n\nCameras [https://www.youtube.com/watch?v=IBzY06tH0sQ](https://www.youtube.com/watch?v=IBzY06tH0sQ)\n\nSoftware [https://www.youtube.com/watch?v=pdb-aZe-1GE](https://www.youtube.com/watch?v=pdb-aZe-1GE)', 'You can try our open source solution HUB - https://github.com/securade/hub']","['If you want to roll your own, Frigate is a popular open-source project to look into. You could use it for computer vision and do analysis with Home Assistant. \n\n[Frigate NVR](https://frigate.video/)', 'Anything that is out of the box will have limited accuracy as the model will need to be able to generalize to your new environment. If you want something that will have extreme accuracy, I highly recommend to look into custom models for your store that will be trained and deployed in the same environment (especially that you are looking for illegal behavior where you need to be very certain that it is occurring). It is all very doable and pretty easy to deploy, especially that you have access to export images from your camera for potential training.', 'My company deals in solutions similar to what you’re looking for - dm and we can discuss more!', 'Have you thought about a custom setup? Sony released some example apps for the [Raspberry Pi AI Camera](https://www.raspberrypi.com/products/ai-camera/). You can pretty easily get it running for stuff like people counting or smart triggers on the device.  Super flexible and way more affordable than big enterprise systems, I think. [https://github.com/SonySemiconductorSolutions/aitrios-rpi-sample-apps/tree/main/examples](https://github.com/SonySemiconductorSolutions/aitrios-rpi-sample-apps/tree/main/examples) \n\n[https://imgur.com/a/qROTHKi](https://imgur.com/a/qROTHKi)', 'If you want ""real-time people counting, detect shoplifting behavior and help with queue management"" then its Hikvision and second place to that Dahua.\n\nHowever you seem to think these systems are overpriced...']",8,14,0.84,Discussion ,1750854773.0
1lk0uyw,computervision,"Multi-page instance segmentation, help","I am working on a project where I am handling images of physical paper documents. Most images have one paper page per image, however many users have uploaded one image with several papers inside. This is causing problems, and I am trying to find a solution. See the image attached as an example (note: it is pixelated intentionally for anonymization just for this sample).

Ideally I'd like to get a bounding box or instance segmentation of each page such I can perform OCR on each page separately. If this is not possible, I would simply like a page count of the image.

These are my findings so far:

* [SegmentAnything](https://segment-anything.com/demo) \- cannot segment papers accurately, instead segments layout.
* [BLIP 3o](https://huggingface.co/spaces/BLIP3o/blip-3o) \- can detect number of pages accurately
* [BLIP](https://huggingface.co/spaces/doevent/blip) \- cannot detect number of pages accurately
* [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) \- can detect number of pages accurately

The dream would be to find a lightweight model that can segment each paper/page instance. Considering YOLO's performance on other tasks, I feel like this should exist - but have not been able to find such a model.

Can anyone suggest any open-source models that can help me solve this page/paper instance segmentation problem, or alternatively page count?

Thanks!

[Sample image](https://preview.redd.it/6kjuavtry19f1.png?width=4160&format=png&auto=webp&s=b53feb05efbf66cda0af440503b7ebb9974a97a3)",['Can you upload a sample image?'],['Can you upload a sample image?'],0,2,0.5,Help: Project,1750842725.0
1ljppg4,computervision,Epic Games Interview for Research Engineer Computer Vision,Does anyone has experience interviewing with Epic Games for Research Engineer position? Would you mind sharing your experience please? Thank you!,"['Mostly talk about ai in game creation.  The field is very experienced, no one knows if it can work or not with a lot of ambitious ideas. For me it is more about the generative AI part, generating 3d mesh, image, texture etc. is in between computer vision and computer graphics, also improving the version of procedural generate.']","['Mostly talk about ai in game creation.  The field is very experienced, no one knows if it can work or not with a lot of ambitious ideas. For me it is more about the generative AI part, generating 3d mesh, image, texture etc. is in between computer vision and computer graphics, also improving the version of procedural generate.']",1,2,0.67,Discussion ,1750805876.0
1ljp2kn,computervision,Book Detection System,"I am developing a book detection system in python for a university project. Based on the spine in the model image, it needs to find the corresponding matches in the scene image through keypoints detection. I have used sift and ransac for this. However, even when there are multiple books visible, it identifies only one of them, and not the others. Also, some of the books are shown from the front, and not the spine, but I don't know how to detect them. Also, when a book is detected, its area is highlighted. I hope you can help me with this. Thank you in advance. If you need any further information on what I have done, I can give it to you. ","[""Hi,\nThere are a lot of points here, but here are a few insights:\n- Regarding multiple books, I assume your matching algorithm only outputs the best pairing and thus only detects one book. If you want to keep using the same algorithm, you could loop the matching process while discarding the features that have already been detected to see if other occurrences are found.\n- Otherwise, you could look into object detection algorithms (deep learning or not).\n- You are comparing your image with known spines, so you cannot detect the front cover. You would need to add the front covers to your matching collection to detect them.\n- I'm not sure what your question is regarding the highlight.""]","[""Hi,\nThere are a lot of points here, but here are a few insights:\n- Regarding multiple books, I assume your matching algorithm only outputs the best pairing and thus only detects one book. If you want to keep using the same algorithm, you could loop the matching process while discarding the features that have already been detected to see if other occurrences are found.\n- Otherwise, you could look into object detection algorithms (deep learning or not).\n- You are comparing your image with known spines, so you cannot detect the front cover. You would need to add the front covers to your matching collection to detect them.\n- I'm not sure what your question is regarding the highlight.""]",1,2,1.0,Help: Project,1750804231.0
1ljot7u,computervision,Running YOLO and Deep SORT on RK3588,Is it possible to run both YOLO and Deep SORT on an RK3588 chip? im planning to use it for my human detection and tracking robot. I heard that you have to change the YOLO model to RKNN but what about the Deep SORT? Or is there other more optimal Object tracking algorithm that I should consider for my RK3588?,"['I have an example with [YOLO and Bytetrack here](https://github.com/swdee/go-rknnlite/tree/master/example/stream).  \n\nAs for your questions;  The YOLO model is converted to RKNN to run with [rknn-toolkit2](https://github.com/airockchip/rknn-toolkit2), you need to use Rockchips fork of YOLO from their [Model Zoo](https://github.com/airockchip/rknn_model_zoo) as they have made optimisations for it to run faster on the NPU.   As for the object tracking/Deep Sort, that is all implemented in your application code and works by feeding it the object detections from the YOLO output.']","['I have an example with [YOLO and Bytetrack here](https://github.com/swdee/go-rknnlite/tree/master/example/stream).  \n\nAs for your questions;  The YOLO model is converted to RKNN to run with [rknn-toolkit2](https://github.com/airockchip/rknn-toolkit2), you need to use Rockchips fork of YOLO from their [Model Zoo](https://github.com/airockchip/rknn_model_zoo) as they have made optimisations for it to run faster on the NPU.   As for the object tracking/Deep Sort, that is all implemented in your application code and works by feeding it the object detections from the YOLO output.']",1,2,0.67,Help: Project,1750803575.0
1ljkyoh,computervision,MiMo-VL is good at agentic type of tasks but leaves me unimpressed for OCR but maybe I'm not prompt engineering enough,"The MiMo-VL model is seriously impressive for UI understanding right out of the box.

I've spent the last couple of days hacking with MiMo-VL on the [WaveUI dataset](https://huggingface.co/datasets/Voxel51/WaveUI-25k), testing everything from basic object detection to complex UI navigation tasks. The model handled most challenges surprisingly well, and while it's built on Qwen2.5-VL architecture, it brings some unique capabilities that make it a standout for UI analysis. If you're working with interface automation or accessibility tools, this is definitely worth checking out.

The right prompts make all the difference, though.

1. Getting It to Point at Things Was a Bit Tricky

The model really wants to draw boxes around everything, which isn't always what you need.

I tried a bunch of different approaches to get proper keypoint detection working, including XML tags like `<point>x y</point>` which worked okay. Eventually I settled on a JSON-based system prompt that plays nicely with FiftyOne's parsing. It took some trial and error, but once I got it dialed in, the model became remarkably accurate at pinpointing interactive elements.

Worth the hassle for anyone building click automation systems.

2. OCR Is Comprehensive But Kinda Slow

The text recognition capabilities are solid, but there's a noticeable performance hit.

OCR detection takes significantly longer than other operations (in my tests it takes 2x longer than regular detection...but I guess that's expected because it's generating that many more tokens). Weirdly enough, if you just use VQA mode and ask ""Read the text"" it works great. While it catches text reliably, it sometimes misses detections and screws up the requested labels for text regions. It's like the model understands text perfectly but struggles a bit with the spatial mapping part.

Not a dealbreaker, but something to keep in mind for text-heavy applications.

3. It Really Shines as a UI Agent

This is where MiMo-VL truly impressed me - it actually understands how interfaces work.

The model consistently generated sensible actions for navigating UIs, correctly identifying clickable elements, form inputs, and scroll regions. It seems well-trained on various action types and can follow multi-step instructions without getting confused. I was genuinely surprised by how well it could ""think through"" interaction sequences.

If you're building any kind of UI automation, this capability alone is worth the integration.

4. I Kept the ""Thinking"" Output and It's Super Useful

The model shows its reasoning, and I decided to preserve that instead of throwing it away.

MiMo-VL outputs these neat ""thinking tokens"" that reveal its internal reasoning process. I built the integration to attach these to each detection/keypoint result, which gives you incredible insight into why the model made specific decisions. It's like having an explainable AI that actually explains itself.

Could be useful for debugging weird model behaviors.

5. Looking for Your Feedback on This Integration

I've only scratched the surface and could use community input on where to take this next.

I've noticed huge performance differences based on prompt wording, which makes me think there's room for a more systematic approach to prompt engineering in FiftyOne. While I focused on UI stuff, early tests with natural images look promising but need more thorough testing.

If you give this a try, drop me some feedback through GitHub issues - would love to hear how it works for your use cases!","['Shoot, forgot to send a link to the integration. You can find it here: https://github.com/harpreetsahota204/MiMo_VL']","['Shoot, forgot to send a link to the integration. You can find it here: https://github.com/harpreetsahota204/MiMo_VL']",13,1,0.79,Showcase,1750794361.0
1ljkd4u,computervision,I built a local deepfake detection tool that works on photos/videos — open-source.,,[],[],2,0,1.0,Help: Project,1750792955.0
1ljanft,computervision,Where are all the Americans?,"I was recently at CVPR looking for Americans to hire and only found five. I don’t mean I hired 5, I mean I found five Americans. (Not including a few later career people; professors and conference organizers indicated by a blue lanyard). Of those five, only one had a poster on “modern” computer vision.

This is an event of 12,000 people! The US has 5% of the world population (and a lot of structural advantages), so I’d expect at least 600 Americans there. In the demographics breakdown on Friday morning Americans didn’t even make the list.

I saw I don’t know how many dozens of Germans (for example), but virtually no Americans showed up to the premier event at the forefront of high technology… and CVPR was held in *Nashville, Tennessee* this year.

You can see online that about a quarter of papers came from American universities but they were almost universally by international students.

So what gives? Is our educational pipeline **that** bad? Is it always like this? Are they all publishing in NeurIPS or one of those closed doors defense conferences? I mean I doubt it but it’s that or 🤷‍♂️","[""That's America. Visit any known tech university and you'll see most of the students, especially graduate students are foreign. And it's very similar in tech companies."", 'When I was in grad school for PhD about a decade ago, there were only 2 American students I’ve ever known in the program - not just counting my year or in my lab,  but everyone I knew in the computer vision program during my 6 years there. And our program was quite large: easily 50+ students at any given time. I suspect it’s getting worse since then. \nMy interpretation is American kids who are smart and hard-working enough to publish at CVPR could easily make big money right out of college with higher status job (think FAANG, VC, hedge funds, consulting etc.) or go to law or medicine. There’s no incentive to grind many years in a specialized technical field.', ""One factor when it comes to CS PhDs is that generally speaking, the jobs you can get with a PhD don't pay enough more to make it worth spending an extra six years in school.  On the other hand, for a foreign student, the PhD makes it much easier to get a work visa in the U.S.\n\nI don't think the situation is very healthy for the U.S., but that's a large part of what's happening."", ""We don't have that many US students in grad school and their math + programming skills are generally a bit behind their peers when they start the grad school. I still like them a lot though - since they have good mentality and communication skills. (This is just my own observation from a batch size of about 100 students with about 10 of them being American.)"", 'Most of the people I know doing work in this realm are focusing on ""sexier"" career paths with AI, specifically LLMs which seems to be all the rage. I love doing computer vision and find that to be a shame. Im trained as a biologist but got into computer vision because I found it fascinating and have been able to use it to solve quite a few problems on the conservation side. I have no interest in making a chatbot that can pass the Turing test.', '[https://www.youtube.com/watch?app=desktop&v=-fphPeRvhjQ&t=2s](https://www.youtube.com/watch?app=desktop&v=-fphPeRvhjQ&t=2s)', 'So, I’m an American getting my PhD in security/AI, but I’ve worked in the computer vision space for 7~ years prior to starting my PhD. I went to CVPR back in 2019 and what I’ve learned over the years is that most Americans in CS/Engineering go straight into industry after their bachelors and those who decide to get their masters frequently will do it while working for a company that pays for part of it. \n\nGoing to get your PhD doesn’t often make financial sense for most Americans given the state of the market a few years ago, and I’m not sure it makes financial sense even now.\n\nI’m one of maybe 4 Americans in my cohort of 35~ PhD students. I have an NSF/CRA fellowship specifically geared towards bringing more Americans who have worked for a bit back into research and academia in computer/information science. \n\nWhat do you consider to be “modem” CV? Remember that research often can be a bit cyclical and breakthroughs can happen by revisiting decades old work with new hardware and specialized implementations. Look at how the AI boom happened after 2011-2012 ish.', 'Did you talk to more than 100 people? If not 5 is what you would expect. Also sampling bias.\n\n>  \nYou can see online that about a quarter of papers came from American universities but they were almost universally by intentional students.\n\nHow do you know they are not unintentional students?', 'American PhD in CV + few years of experience now working at a startup. When I did the FAANG research internships I believe I only interacted with 2-3 Americans at all companies. Growing up in the Midwest I have noticed a few explanations. The smartest people tend to go into medicine, law, finance since there legitimately aren’t tech jobs in 90% of the country. Manufacturing engineering jobs abound though, for a quarter of the pay. Most people aren’t crazy about moving far away from home to get a career. There is just a cultural desert in most of the country when it comes to tech. No one talks about it or encourages it, at least where I am from. It is viewed as reclusive and weird haha. Maybe with AI and subsequently education spreading more now that will change. \n\nAll to say, you aren’t seeing Americans in CV because there aren’t any.', ""Obviously there is a longer term trend here, but I wonder if there was a cohort of American researchers that couldn't travel and didn't submit stuff because they're funded by grants that got pulled back.  \n\nI know that the lab I was in in Grad School lost a lot of random grants that could only be used to fund Grad Students who are American Citizens, so that might have borked everything with the papers they were working on. Although the grants were obviously pulled AFTER the CVPR deadline last year.""]","[""That's America. Visit any known tech university and you'll see most of the students, especially graduate students are foreign. And it's very similar in tech companies."", 'When I was in grad school for PhD about a decade ago, there were only 2 American students I’ve ever known in the program - not just counting my year or in my lab,  but everyone I knew in the computer vision program during my 6 years there. And our program was quite large: easily 50+ students at any given time. I suspect it’s getting worse since then. \nMy interpretation is American kids who are smart and hard-working enough to publish at CVPR could easily make big money right out of college with higher status job (think FAANG, VC, hedge funds, consulting etc.) or go to law or medicine. There’s no incentive to grind many years in a specialized technical field.', ""One factor when it comes to CS PhDs is that generally speaking, the jobs you can get with a PhD don't pay enough more to make it worth spending an extra six years in school.  On the other hand, for a foreign student, the PhD makes it much easier to get a work visa in the U.S.\n\nI don't think the situation is very healthy for the U.S., but that's a large part of what's happening."", ""We don't have that many US students in grad school and their math + programming skills are generally a bit behind their peers when they start the grad school. I still like them a lot though - since they have good mentality and communication skills. (This is just my own observation from a batch size of about 100 students with about 10 of them being American.)"", 'Most of the people I know doing work in this realm are focusing on ""sexier"" career paths with AI, specifically LLMs which seems to be all the rage. I love doing computer vision and find that to be a shame. Im trained as a biologist but got into computer vision because I found it fascinating and have been able to use it to solve quite a few problems on the conservation side. I have no interest in making a chatbot that can pass the Turing test.']",119,123,0.93,Discussion ,1750770449.0
1ljan02,computervision,Differing results from YOLOv8,"Follow up from last post- I am training a basketball computer vision model to automatically detect made and missed shots.
An issue I ran into is I had a shot that was detected as a miss in a really long video, when it should have been a make.
I edited out that video in isolation and tried it again, and the graph was completely different and it was now detected as a make.
Two things i can think of
1. the original video was rotated, so everytime i ran YOLOv8, I had to rotate the vid back first, but in the edited version, it was not rotated to begin with, so I didn't run rotate every frame
2. Maybe editing it somehow changed what frames the ball is detected in? It felt a lot more fast and accurate

Here is the differing graphs
graph 1, the incorrect detection, where I'm rotating the whole frame every time
graph 2, the model ran on the edited version|

https://preview.redd.it/9hwr9pt5jv8f1.png?width=578&format=png&auto=webp&s=0fc45d3f2456402694ba62af1e10ac4965ff8c82

https://preview.redd.it/hazqo2e7jv8f1.png?width=614&format=png&auto=webp&s=7c0010dd9f0be6bf34340e5297674e488a13045d



","['Why do the plots show significantly different trajectory times?', 'Most likely when you cropped and rotated a video segment you reencoded the video and it changed the quality slightly. And due to random variance you get more detections this time around.\n\nIf you want a clean experiment, crop a longer segment, so that keyframes are preserved. Without rotating or changing the codec. Then do exactly the same processing you do with a long video. Grab a frame, rotate, do inference. Results should be identical.', 'Your best-fit parabola is different between the two runs because you have additional detections in the second run.   \nWhat kind of algorithm or model are you using for object tracking? trajectory fitting?', ""is any of the code on github? if so please share i'd like to see a few things under the hood""]","['Why do the plots show significantly different trajectory times?', 'Most likely when you cropped and rotated a video segment you reencoded the video and it changed the quality slightly. And due to random variance you get more detections this time around.\n\nIf you want a clean experiment, crop a longer segment, so that keyframes are preserved. Without rotating or changing the codec. Then do exactly the same processing you do with a long video. Grab a frame, rotate, do inference. Results should be identical.', 'Your best-fit parabola is different between the two runs because you have additional detections in the second run.   \nWhat kind of algorithm or model are you using for object tracking? trajectory fitting?', ""is any of the code on github? if so please share i'd like to see a few things under the hood""]",8,8,1.0,Help: Project,1750770417.0
1lj7slh,computervision,"Lightweight frame selection methods for downstream human analysis (RGB+LiDAR, varying human poses)","Hey everyone I am working on a project using synchronized RGB and LiDAR feeds, where the scene includes human actors or mannequin in various poses which are for example lying down, sitting up, fetal position, etc.

Downstream the pipeline we have VLM-Based trauma detection models with high inference times(\~15s per frame), so passing every frame through them is not viable. I am looking for lightweight frame selection /forwarding methods to pick the most informative frames from a human analysis perspective for example, clearest visibility, minimal occlusion maximum body parts are visible (like arms,legs,torso,head)etc.

One approach I thought of was Human part segmentation from point clouds using [Human3D](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://human-3d.github.io/&ved=2ahUKEwjWzJ6a7YmOAxVde2wGHYP8KvAQFnoECAkQAQ&usg=AOvVaw1nCXR7xwStEYYqcEiY8RnR) but It didn't work on my LiDAR data (maybe because it was sparse \~9000 points in my scene)

If anyone have experience or have idea on efficient approaches especially for RBG+Depth/LiDAR Data I would love to here your thoughts. Ideally looking for something fast and lightweight that can run ahead of heavier models.

currently using [Blickfeld Cube 1](https://www.blickfeld.com/lidar-sensor-products/cube-1/) LiDAR and iPhone 12 Max Camera for RGB stream

[point cloud data captured from my LiDAR](https://preview.redd.it/g1z4lh6ltu8f1.png?width=890&format=png&auto=webp&s=0015fddb988a57c275688512de92be74cc429717)","[""Why do you want to use the sparse point-cloud instead of the dense image?\nAssuming that they are both captured from roughly the same location (iphone perhaps) and capture the same objects, you camera image will be much more dense and informative and you'll have a vast range of pre-trained models (person detection, pose estimation, vlm) to use...\nAm I missing something trivial?""]","[""Why do you want to use the sparse point-cloud instead of the dense image?\nAssuming that they are both captured from roughly the same location (iphone perhaps) and capture the same objects, you camera image will be much more dense and informative and you'll have a vast range of pre-trained models (person detection, pose estimation, vlm) to use...\nAm I missing something trivial?""]",3,6,1.0,Help: Project,1750761825.0
1lj1kaq,computervision,Please refer to ideas for using a camera and OpenCV,"**I have the following idea:**

A laser sensor will detect objects moving on a conveyor belt. When the sensor starts shining on an object and continues until the object is no longer detected, it will send a start signal.

This signal will activate four LEDs positioned underneath, which will illuminate the four edges of the object. Four industrial cameras, fixed above, will capture the four corners of the object.

From these four corner images, we can calculate the lengths of each side (a, b, c, d), the lengths of the two diagonals, and the four angles between the long and short sides. Based on these measurements, we can evaluate the quality of the object according to three criteria: size, diagonal, and corner angle.

I plan to use **OpenCV** to extract these values.
Is this feasible? Do I need to be aware of anything? Do you have any suggestions?
Thank you verymuch.","[""That sounds overly complex.\xa0\nWhy would you need a camera for each edge and not a single one? Why would you need a camera at all if you already know where to put LED on the 4 edges?\n\n\nPlease elaborate your use case, how you expect people to help you if you don't explain your challenge?"", 'It sounds like you really just want to do ""structure from motion"".\n\nThere are much better tools for this than OpenCV.']","[""That sounds overly complex.\xa0\nWhy would you need a camera for each edge and not a single one? Why would you need a camera at all if you already know where to put LED on the 4 edges?\n\n\nPlease elaborate your use case, how you expect people to help you if you don't explain your challenge?"", 'It sounds like you really just want to do ""structure from motion"".\n\nThere are much better tools for this than OpenCV.']",1,4,0.6,Help: Project,1750738225.0
1lizdzu,computervision,Tesla Autopilot (AP) Hiring,"Any Vision/Robotics Masters/PhDs interviewing for Vision roles, DM me about Tesla AP openings. Pay is very good. I also have insight into the interview process and can link you up.

My motivation: I'm looking for 1-2 collaborators in the job hunt process. I also have insight into other roles (Waymo, Snap, Runway). DM me!

Edit: Please forgive me to those I can't get back to, but I'm prioritizing folks with a similar bg as myself!

Edit 2: To new folks, u/OverfitMode666 and u/RelationshipLong9092 are internet trolls. I'm not a recruiter, I offered to identify myself but they ghosted me. Do text me, I have many interview insights and would like to hear from yours!","['Recruiter. He will share your CV and squeeze out a commission. Any company but most likely not Tsla, Waymo et al.', 'Woof, I wouldn’t want the inevitable blood on my hands of this task. Best of luck yall', 'Interested, PhD in vision. but this does look a bit shady tbh', 'Clicking the ""ignore"" button when you try to take your bullshit to DM\'s is not what ""ghosting"" is btw.', 'Dmd', ""Dm'd""]","['Recruiter. He will share your CV and squeeze out a commission. Any company but most likely not Tsla, Waymo et al.', 'Woof, I wouldn’t want the inevitable blood on my hands of this task. Best of luck yall', 'Interested, PhD in vision. but this does look a bit shady tbh', 'Clicking the ""ignore"" button when you try to take your bullshit to DM\'s is not what ""ghosting"" is btw.', 'Dmd']",0,18,0.5,Discussion ,1750731429.0
1lirfzl,computervision,Advanced Anomaly Detection,"Hello!

I am looking for a ways to become a pro in computer vision, with an emphasis on anomaly detection.

I know python and computer vision basics, built couple of classsifiers via transfer learning (with mobilenet, resnet, vgg) and I am now trying to solve a problem with a quality control of prints, with the use of linear camera.

I'm aware of the other factors like light, focus etc, but by now I want to build as great knowledge as I want, and there I have a question.

Do you recommend any learning paths, online courses so that could help me become more advanced in this topic? Every response will be appreciated.
Thanks :) ","['A package called Anomalib could be something you are looking for.', 'what is a linear camera? like a line-scan single-row camera?\n\nat some point you will have to simply read papers. you can\'t rely on courses anymore. once you start talking about ""advanced computer vision"" you\'re probably at that point.']","['A package called Anomalib could be something you are looking for.', 'what is a linear camera? like a line-scan single-row camera?\n\nat some point you will have to simply read papers. you can\'t rely on courses anymore. once you start talking about ""advanced computer vision"" you\'re probably at that point.']",6,4,0.88,Discussion ,1750710417.0
1liorjm,computervision,COCO pretrained YOLO v8 debugging (class index issues),"I'm using a YOLOv8 pretrained on COCO on my class dataset, focused on 3 classes that are also in COCO. Using Roboflow webapp Grounding Dino annotater I annotated a dataset on bicycles, boats, cars. This dataset is indexed, after extracting, as 0,1,2 respectively, because I extracted it as YOLOv8. I need it as YOLOv8, because after running it like this, I will fine-tune using that dataset.

This is not the same as COCO, where those 3 classes have 1,2,8 as index. Now I'm facing issues when Im validating on my test dataset labels. The data is running, predicting correctly and locating the labels for my test data correctly.



image 28/106 test-127-\_jpg.rf.08a36d5a3d959b4abe0e5a267f293f59.jpg: Predicted: 1 boat \[GT: 1 boat\]
image 29/106 test-128-\_jpg.rf.bf3f57e995e27e68da74691a1c30effd.jpg: Predicted: 1 boat \[GT: 1 boat\]
image 30/106 test-129-\_jpg.rf.01163a19c5b241dcd9fbb765afae533c.jpg: Predicted: 4 boat \[GT: 2 boat\]
image 31/106 test-13-\_jpg.rf.40a610771968be6fda3931ec1063182f.jpg: Predicted: 2 boat \[GT: 1 boat\]
image 32/106 test-130-\_jpg.rf.296913d2a5cb563a4e81f7e656adac59.jpg: Predicted: 7 boat \[GT: 3 boat\]
image 33/106 test-14-\_jpg.rf.b53326d248c7e0bb309ea45292d49102.jpg: Predicted: 3 bicycle \[GT: 1 bicycle\]


GT shows that the ground truth label is the same as the one predicted. However.

                       all        106         86      0.381      0.377      0.384      0.287
                   bicycle         21         25          0          0   0.000833    0.00066
                       car         54         61      0.762      0.754      0.767      0.572
    Speed: 6.1ms preprocess, 298.4ms inference, 0.0ms loss, 4.9ms postprocess per image
    Results saved to runs/detect/val16

    --- Evaluation Metrics ---
    mAP50: 0.3837555367935218
    mAP50-95: 0.28657243641136704

This statistics showw that boats was not even validated and bicycle was indexed wrong. I have not been able to fix this and have currently made my tables by going around it and using the GT label values.



Does anyone know how to fix this?",[],[],2,0,1.0,Help: Project,1750704240.0
1lijd01,computervision,Help me find a birthday gift for my boyfriend who works with CV,"Hello! I'm really sorry if this is not the place to ask this, but I am looking for some help with finding a computer vision-related gift for my boyfriend. He not only works with CV but also loves learning about it and studying it. That is not my area of expertise at all, so I was thinking, is there anything I could gift him that is related to CV and that he'll enjoy or use? I've tried looking it up online but either I don't understand what is said or I can't find stuff related specifically to computer vision... I would appreciate any suggestion!!","[""A truly unique gift would be a properly curated net new dataset that's highly targeted to his interests."", 'Nvidia GPU', 'Get him a 3d scanner. They are in 300-1000s range.', 'This is really tough because most CV stuff is work related so it’s gonna be big ticket. Dream gift is Nvidia 5090 ofc, or A100/H100 if you’re balling like crazy. Those will run you 2k and 10-50k respectively. \n\nDataset is a great one if you have like 40+ hours to make something. Can be done for free.\n\nMore realistically I’d say maybe you can get him a nice camera? There are cameras suitable for capturing computer vision data; multimodal infrared or depth with RGB capture is consumer grade (the old Kinect is a very early example) and could be a decent gift — still pricey, but won’t break the bank (100-1000+$). For an even smaller gift just buy him compute time with an A100 or H100 through a provider — at 2$ an hour you can choose how much you’d like to spend.', ""As someone who works in Computer Vision, not a lot of things come to mind to be honest, but if he's into hardware a little bit like me as well, maybe he would appreciate something like the Nvidia Raspberry Pi, with a mini cam and some other tools. \n\nIt's a good gift for him to test some personal projects in real life."", 'Give him the Oculus VR or Ray Ban smart glasses. Or one that supports Snapchat VR. If he works in CV, he might be interested in creating an application that could work with it.', 'Frame lena photo of course', 'Thank you so much for all your suggestions! I will def look them up', 'If that man doesn’t put a ring on it…', 'Programming snacks, essential for getting things done. Or some Teledyne FLIR cameras or Sony IMX sensor boards. But snacks are still the better gift.']","[""A truly unique gift would be a properly curated net new dataset that's highly targeted to his interests."", 'Nvidia GPU', 'Get him a 3d scanner. They are in 300-1000s range.', 'This is really tough because most CV stuff is work related so it’s gonna be big ticket. Dream gift is Nvidia 5090 ofc, or A100/H100 if you’re balling like crazy. Those will run you 2k and 10-50k respectively. \n\nDataset is a great one if you have like 40+ hours to make something. Can be done for free.\n\nMore realistically I’d say maybe you can get him a nice camera? There are cameras suitable for capturing computer vision data; multimodal infrared or depth with RGB capture is consumer grade (the old Kinect is a very early example) and could be a decent gift — still pricey, but won’t break the bank (100-1000+$). For an even smaller gift just buy him compute time with an A100 or H100 through a provider — at 2$ an hour you can choose how much you’d like to spend.', ""As someone who works in Computer Vision, not a lot of things come to mind to be honest, but if he's into hardware a little bit like me as well, maybe he would appreciate something like the Nvidia Raspberry Pi, with a mini cam and some other tools. \n\nIt's a good gift for him to test some personal projects in real life.""]",11,32,0.77,Discussion ,1750691999.0
1liihht,computervision,Medical images Semantic segmentation,"I am working on this medical image segmentation project for burn images. After reading a bunch of papers and doing some lit reviews….I started with unet based architecture to set the baseline with different encoders on my dataset but seems like I can’t get a IoU over .35 any way. Thinking of moving on to unet++ and HRnetv2 based architecture but wondering if anyone has worked here what tricks or recipes might have worked.

Ps- i have tried a few combinations of loss function including bce, dice, jaccard and focal. Also few different data augs and learning rate schedulers with adam. I have a dataset of around 1000 images of not so great quality though. ( if anyone is aware of public availability of good burn images dataset that would be good too ).",[],[],1,0,1.0,Help: Project,1750689974.0
1lidk1i,computervision,ReID in football,"Hi, I need help in re-identifying football players with consistently mapped IDs even if the exit the frame an re-enter. Players are being tracked by the model I have but the IDs are not consistent. If anybody can give me some tips on how to move forward please do so. Thanks!","[""Try some pretrained person re-id network, retrain on manually checked trajectories without id swaps or try DINOv2 on some player crops (31st encoder layer + cosine similarity). When there is a distinguishable feature (e.g. player number) visible only in parts of trajectory frames you'd need to extract re-id features from multiple frames and compute single similarity measure out of these two sets.""]","[""Try some pretrained person re-id network, retrain on manually checked trajectories without id swaps or try DINOv2 on some player crops (31st encoder layer + cosine similarity). When there is a distinguishable feature (e.g. player number) visible only in parts of trajectory frames you'd need to extract re-id features from multiple frames and compute single similarity measure out of these two sets.""]",1,1,1.0,Help: Project,1750676102.0
1liat1t,computervision,Any Coursera course recommendation to get started with computer vision?,"I have free access to every course on Coursera from my university and I wanted to explore the field of computer vision.

As for programming and math experience, I can code in C++ and taken courses of Calculus 1, Calculus 2 and linear algebra. So should I take a course from the Coursera or should I go on personalized route?
Thanks for your time.","['I recommend starting with CS231n for deep learning in computer vision. You can find the 2017 lecture videos on yt, along with notes and assignments on Stanford CS231n website. It isn’t the most up-to-date with the latest advancements like GenAI, Stable Diffusion, or VLMs, that said, it provides DL fundamentals before diving into those more advanced topics. As for traditional (non-deep-learning) computer vision, perhaps other Redditors can suggest good resources', ""The best resource for getting started with computer vision is by Szeliski. It is legally available for free online. I also recommend Prince's textbook. \n\nYou will want to do something different for deep learning. Goodkind is highly recommended.""]","['I recommend starting with CS231n for deep learning in computer vision. You can find the 2017 lecture videos on yt, along with notes and assignments on Stanford CS231n website. It isn’t the most up-to-date with the latest advancements like GenAI, Stable Diffusion, or VLMs, that said, it provides DL fundamentals before diving into those more advanced topics. As for traditional (non-deep-learning) computer vision, perhaps other Redditors can suggest good resources', ""The best resource for getting started with computer vision is by Szeliski. It is legally available for free online. I also recommend Prince's textbook. \n\nYou will want to do something different for deep learning. Goodkind is highly recommended.""]",13,11,1.0,Discussion ,1750665501.0
1liaazx,computervision,Need Help with Thermal Image/Video Analysis for fault detection,"Hi everyone,

I’m working on a project that involves analyzing thermal images and video streams to detect anomalies in an industrial process. think of it like monitoring a live process with a thermal camera and trying to figure out when something “wrong” is happening.

I’m very new to AI/ML. I’ve only trained basic image classification models. This project is a big step up for me, and I’d really appreciate any advice or pointers.

Specifically, I’m struggling with:
What kind of neural networks/models/techniques are good for video-based anomaly detection?

Are there any AI techniques or architectures that work especially well with thermal images/videos?

How do I create a ""quality index"" from the video – like some kind of score or decision that tells whether the frame/segment is “normal” or “abnormal”?

If you’ve done anything similar or can recommend tutorials, open-source projects, or just general advice on how to approach this problem — I’d be super grateful. 🙏
Thanks a lot for your time!","[""https://github.com/open-edge-platform/anomalib is the go to for anything anomaly related - image modality shouldn't really matter"", 'look for related papers from cvpr this year in defect detection, anomaly detection, etc\n\nthere were several doing this exact same thing, except usually without the thermal camera']","[""https://github.com/open-edge-platform/anomalib is the go to for anything anomaly related - image modality shouldn't really matter"", 'look for related papers from cvpr this year in defect detection, anomaly detection, etc\n\nthere were several doing this exact same thing, except usually without the thermal camera']",4,4,1.0,Help: Project,1750663402.0
1li9i1z,computervision,What pipeline would you use to segment leaves with very low false positives?,"For different installations  with a single crop each. We need to segment leaves of 5 different types of plants in a productive setting, day and night, angles may vary between installations but don’t change

Almost no time limit We don’t need real time. If an image takes ten seconds to segment, it’s fine.

No problem if we miss leaves or we accidentally merge them.

⚠️False positives are a big NO.

We are currently using Yolo v13 and it kinda works but false positives are high and even even we filter by confidence score > 0.75 there are still some false positives.

🤔I’m considering to just keep labelling leaves, flowers, fruits and retrain but i strongly suspect that i may be missing something: wrong yolo configuration or wrong model or missing a pre-filtering or not labelling the background and objects…

# Edit: Added sample images

Color Legend: Red: Leaves, Yellow: Flowers, Green: Fruits

https://preview.redd.it/aqnvsq1cvn8f1.png?width=4&format=png&auto=webp&s=e05864396915fc037231e4d6d6ed77d7cbd0288d

https://preview.redd.it/14e8en1cvn8f1.png?width=674&format=png&auto=webp&s=ecba7474db2bc83f84b380b59d748717ae34c16e

https://preview.redd.it/7ap8ro1cvn8f1.png?width=932&format=png&auto=webp&s=6e6d9e54957d7880027806069dfb9a5a83843de2

https://preview.redd.it/eee6bo1cvn8f1.png?width=946&format=png&auto=webp&s=0931c3380ad8f74593b4b0c0a0054940629dd7b5

https://preview.redd.it/qlbprn1cvn8f1.png?width=826&format=png&auto=webp&s=61feedc0df543699dd8dce372cb6a5b3f661b9ce

","['Share images. We should make this a rule of the subreddit.', 'So you don’t care if it misses leaves, but do require that when it does segment a leaf it does so very accurately?\n\xa0\nI would be looking at foundation models like SAM, Grounded SAM, Florence, and so on.\xa0\n\nCheckout “T-Rex Label” too.\xa0', 'So you are trying to segment a single leaf on a background? But the important part is the classification label applied to the segmentation? \n\nDo you have control of the background?', 'What do you think of this gpt advice:\n\nYou’re on the right track — and yes, it’s a complex decision, especially when you’re optimizing for zero false positives and building a trustworthy computer vision pipeline in agriculture. Let’s unpack the two options you mentioned and guide you toward the best next step.\n\n⸻\n\n🧩 Current Context\n\t•\tYou’re using YOLO-Seg\n\t•\tYou want zero false positives\n\t•\tYou’ve observed hallucinated leaves (objects being misdetected as leaves)\n\t•\tYou want to guide the system’s next steps intelligently\n\n⸻\n\n🔀 Option A: Switch to YOLO + SAM (Modular, Precision-Oriented)\n\n✅ When to choose this:\n\t•\tYou want full control over filtering and verification\n\t•\tYou’re okay with slower inference or batch mode\n\t•\tYou want to run experiments per-class or region\n\t•\tYou’re okay building a pipeline with multiple stages\n\n🧠 What it gives you:\n\t•\tYOLO handles object detection (you can set confidence ≥ 0.9)\n\t•\tSAM receives only the trusted detections\n\t•\tYou can add filtering logic post-SAM (mask area, shape, position)\n\t•\tYou eliminate hallucinations by clamping input to known contexts\n\n⚒️ Action plan:\n\t•\tIdentify which YOLO predictions are suspect (e.g. wrong class or shape)\n\t•\tCreate a filter module: size, position, edge-sharpness, or even rule-based logic\n\t•\tPlug-in SAM to only process vetted boxes\n\t•\tVisualize results and adjust filter thresholds\n\n🧪 Bonus: use SAM + bounding box + center-point simultaneously for more stable masks.\n\n⸻\n\n🌱 Option B: Retrain YOLO-Seg to Reduce Leaf Hallucinations\n\n✅ When to choose this:\n\t•\tYou’re already heavily invested in YOLO-Seg\n\t•\tYou want fast, integrated segmentation\n\t•\tYou suspect poor data quality or class confusion\n\t•\tYou want to fix hallucinations at the root\n\n🧠 What it gives you:\n\t•\tClean, single-stage inference\n\t•\tBetter generalization if data improves\n\t•\tSimpler deployment\n\n⚠️ BUT: YOLO-Seg is not easily interpretable — if it hallucinates, you can’t easily filter it out.\n\n⚒️ Action plan:\n\t1.\tAudit predictions → are leaves being detected where there are no visible objects?\n\t2.\tIf yes → label a negative dataset with “non-leaf background” and retrain\n\t3.\tAdd “background” or “unknown” class explicitly\n\t4.\tUse confidence-weighted loss functions to penalize high-confidence hallucinations\n\t5.\tConsider data rebalancing or hard negative mining\n\n💡 Train YOLO-Seg with a “hard precision” mode (confidence > 0.95 required, low-recall acceptable)\n\n⸻\n\n🔍 Recommendation Based on Goals\n\nCriteria\tOption A: YOLO + SAM\tOption B: Fix YOLO-Seg\nMax Precision\t✅ Best\t❌ Less controllable\nDeployment simplicity\t❌ More complex\t✅ Easy\nInterpretable failures\t✅ Very\t❌ Difficult\nSpeed\t❌ Slower\t✅ Faster\nControl over hallucinations\t✅ Full\t⚠️ Data-dependent\nLong-term flexibility\t✅ Excellent\t⚠️ Limited\n\n\n⸻\n\n🎯 Final Suggestion: Start with A, use B to enhance\n\t1.\tBuild a hybrid pipeline with YOLO + SAM\n\t•\tAdd filters to accept only “leaf-looking” detections\n\t•\tManually inspect cases to refine rules\n\t2.\tLog false positives (hallucinated leaves)\n\t•\tUse those logs as hard negatives to retrain YOLO-Seg later\n\t•\tThis turns Option A into a data engine to feed Option B\n\t3.\tLater, simplify\n\t•\tOnce YOLO-Seg is reliable with cleaned data, phase out SAM for production\n\n⸻']","['Share images. We should make this a rule of the subreddit.', 'So you don’t care if it misses leaves, but do require that when it does segment a leaf it does so very accurately?\n\xa0\nI would be looking at foundation models like SAM, Grounded SAM, Florence, and so on.\xa0\n\nCheckout “T-Rex Label” too.\xa0', 'So you are trying to segment a single leaf on a background? But the important part is the classification label applied to the segmentation? \n\nDo you have control of the background?', 'What do you think of this gpt advice:\n\nYou’re on the right track — and yes, it’s a complex decision, especially when you’re optimizing for zero false positives and building a trustworthy computer vision pipeline in agriculture. Let’s unpack the two options you mentioned and guide you toward the best next step.\n\n⸻\n\n🧩 Current Context\n\t•\tYou’re using YOLO-Seg\n\t•\tYou want zero false positives\n\t•\tYou’ve observed hallucinated leaves (objects being misdetected as leaves)\n\t•\tYou want to guide the system’s next steps intelligently\n\n⸻\n\n🔀 Option A: Switch to YOLO + SAM (Modular, Precision-Oriented)\n\n✅ When to choose this:\n\t•\tYou want full control over filtering and verification\n\t•\tYou’re okay with slower inference or batch mode\n\t•\tYou want to run experiments per-class or region\n\t•\tYou’re okay building a pipeline with multiple stages\n\n🧠 What it gives you:\n\t•\tYOLO handles object detection (you can set confidence ≥ 0.9)\n\t•\tSAM receives only the trusted detections\n\t•\tYou can add filtering logic post-SAM (mask area, shape, position)\n\t•\tYou eliminate hallucinations by clamping input to known contexts\n\n⚒️ Action plan:\n\t•\tIdentify which YOLO predictions are suspect (e.g. wrong class or shape)\n\t•\tCreate a filter module: size, position, edge-sharpness, or even rule-based logic\n\t•\tPlug-in SAM to only process vetted boxes\n\t•\tVisualize results and adjust filter thresholds\n\n🧪 Bonus: use SAM + bounding box + center-point simultaneously for more stable masks.\n\n⸻\n\n🌱 Option B: Retrain YOLO-Seg to Reduce Leaf Hallucinations\n\n✅ When to choose this:\n\t•\tYou’re already heavily invested in YOLO-Seg\n\t•\tYou want fast, integrated segmentation\n\t•\tYou suspect poor data quality or class confusion\n\t•\tYou want to fix hallucinations at the root\n\n🧠 What it gives you:\n\t•\tClean, single-stage inference\n\t•\tBetter generalization if data improves\n\t•\tSimpler deployment\n\n⚠️ BUT: YOLO-Seg is not easily interpretable — if it hallucinates, you can’t easily filter it out.\n\n⚒️ Action plan:\n\t1.\tAudit predictions → are leaves being detected where there are no visible objects?\n\t2.\tIf yes → label a negative dataset with “non-leaf background” and retrain\n\t3.\tAdd “background” or “unknown” class explicitly\n\t4.\tUse confidence-weighted loss functions to penalize high-confidence hallucinations\n\t5.\tConsider data rebalancing or hard negative mining\n\n💡 Train YOLO-Seg with a “hard precision” mode (confidence > 0.95 required, low-recall acceptable)\n\n⸻\n\n🔍 Recommendation Based on Goals\n\nCriteria\tOption A: YOLO + SAM\tOption B: Fix YOLO-Seg\nMax Precision\t✅ Best\t❌ Less controllable\nDeployment simplicity\t❌ More complex\t✅ Easy\nInterpretable failures\t✅ Very\t❌ Difficult\nSpeed\t❌ Slower\t✅ Faster\nControl over hallucinations\t✅ Full\t⚠️ Data-dependent\nLong-term flexibility\t✅ Excellent\t⚠️ Limited\n\n\n⸻\n\n🎯 Final Suggestion: Start with A, use B to enhance\n\t1.\tBuild a hybrid pipeline with YOLO + SAM\n\t•\tAdd filters to accept only “leaf-looking” detections\n\t•\tManually inspect cases to refine rules\n\t2.\tLog false positives (hallucinated leaves)\n\t•\tUse those logs as hard negatives to retrain YOLO-Seg later\n\t•\tThis turns Option A into a data engine to feed Option B\n\t3.\tLater, simplify\n\t•\tOnce YOLO-Seg is reliable with cleaned data, phase out SAM for production\n\n⸻']",3,7,1.0,Help: Project,1750660208.0
1li6baf,computervision,Is it feasible to build my own small-scale VPS for one floor of a building?,"I’m working on a project where I want to implement a small-scale Visual Positioning System (VPS) — not city-wide, just for a single floor of a building (like a university lab or hallway).

I know large-scale VPS systems use tons of data and cloud services, but for my case, I’m trying to do it locally and on a smaller scale.

I could capture the environment (record footage) and then use extracted key frames with COLMAP to form a 3D point cloud then store that locally. Then i can implement real time localization.

My question is, is this feasible? Is it a lot more complex than it sounds? I’m quite new to this concept so I’m worried i’m missing out on something important.
","[""yes this is totally possible. you can even do this without preprocessing at all: its called SLAM (simultaneous localization and mapping)\n\nmake your life easy: do not use a rolling shutter camera. use a global shutter camera. \n\nit would help if your camera was calibrated, but if you had a single camera with fixed intrinsics (no autofocus etc) that will also work fine. i personally wouldn't use video for this but would manually take the pictures.\n\na calibrated stereo camera pair is the ideal, but monocular will work fine. you may have to approximately set the scale yourself, if you even care about that. \n\ndo you want to do this for localizing a robot or an AR headset or what? what are your performance requirements and what sort of hardware do you have to play with? VGGT might have won best paper at CVPR but you may not want to commit yourself to the hardware required for the deep learning approach. with care you can definitely get this to run on just CPU.\n\nCOLMAP is just one option. there are many others that have succeeded it in a variety of ways. is visualizing the map super-important, somewhat useful, or do you just care about localization?\n\nis the environment going to look the same for your final system as it does during data acquisition? will there people people etc in the building, how consistent is the lighting, are there large areas with no features, is there a lot of glass/mirrors/specular materials? depending on your method these things might matter enough for you to care.\n\ndoes this one floor of a building have areas that look essentially identical to other areas? industrial/commercial buildings can sometimes be quite tricky in this way.\n\nlong featureless hallways that dont loop back can always be tricky if you care about precision of the map. its much easier if you can get a good 'loop closure' hit. \n\nthere are stadnard datasets in the structure from motion SFM and SLAM communities you can use online that are essentially what you describe, so you don't need to gather data before you begin testing.""]","[""yes this is totally possible. you can even do this without preprocessing at all: its called SLAM (simultaneous localization and mapping)\n\nmake your life easy: do not use a rolling shutter camera. use a global shutter camera. \n\nit would help if your camera was calibrated, but if you had a single camera with fixed intrinsics (no autofocus etc) that will also work fine. i personally wouldn't use video for this but would manually take the pictures.\n\na calibrated stereo camera pair is the ideal, but monocular will work fine. you may have to approximately set the scale yourself, if you even care about that. \n\ndo you want to do this for localizing a robot or an AR headset or what? what are your performance requirements and what sort of hardware do you have to play with? VGGT might have won best paper at CVPR but you may not want to commit yourself to the hardware required for the deep learning approach. with care you can definitely get this to run on just CPU.\n\nCOLMAP is just one option. there are many others that have succeeded it in a variety of ways. is visualizing the map super-important, somewhat useful, or do you just care about localization?\n\nis the environment going to look the same for your final system as it does during data acquisition? will there people people etc in the building, how consistent is the lighting, are there large areas with no features, is there a lot of glass/mirrors/specular materials? depending on your method these things might matter enough for you to care.\n\ndoes this one floor of a building have areas that look essentially identical to other areas? industrial/commercial buildings can sometimes be quite tricky in this way.\n\nlong featureless hallways that dont loop back can always be tricky if you care about precision of the map. its much easier if you can get a good 'loop closure' hit. \n\nthere are stadnard datasets in the structure from motion SFM and SLAM communities you can use online that are essentially what you describe, so you don't need to gather data before you begin testing.""]",3,3,1.0,Help: Project,1750648814.0
1li5782,computervision,Success at feeding in feature predictions to sem seg model training?,I’m curious how useful it is using semantic seg feature masks to re-train models? What’s the best pipeline for doing this?,[],[],1,0,1.0,Help: Project,1750645255.0
1li523x,computervision,How to achieve real-time video stitching of multiple cameras？,"Hey everyone, I'm having issues while using the Jetson AGX Orin 64G module to complete a real-time panoramic stitching project. My goal is to achieve 360-degree panoramic stitching of eight cameras. I first used the latitude and longitude correction method to remove the distortion of each camera, and then input the corrected images for panoramic stitching. However, my program's real-time performance is extremely poor. I'm using the panoramic stitching algorithm from OpenCV. I reduced the resolution to improve the real-time performance, but the result became very poor. How can I optimize my program? Can any experienced person take a look and help me?Here are my code:

    import cv2
    import numpy as np
    import time
    from defisheye import Defisheye


    camera_num = 4
    width = 640
    height = 480
    fixed_pano_w = int(width * 1.3)
    fixed_pano_h = int(height * 1.3)

    last_pano_disp = np.zeros((fixed_pano_h, fixed_pano_w, 3), dtype=np.uint8)


    caps = [cv2.VideoCapture(i) for i in range(camera_num)]
    fourcc = cv2.VideoWriter_fourcc(*'MJPG')
    # out_video = cv2.VideoWriter('output_panorama.avi', fourcc, 10, (fixed_pano_w, fixed_pano_h))

    stitcher = cv2.Stitcher_create()
    while True:
        frames = []
        for idx, cap in enumerate(caps):
            ret, frame = cap.read()
            frame_resized = cv2.resize(frame, (width, height))
            obj = Defisheye(frame_resized)
            corrected = obj.convert(outfile=None)
            frames.append(corrected)
        corrected_img = cv2.hconcat(frames)
        corrected_img = cv2.resize(corrected_img,dsize=None,fx=0.6,fy=0.6,interpolation=cv2.INTER_AREA )
        cv2.imshow('Original Cameras Horizontal', corrected_img)

        try:
            status, pano = stitcher.stitch(frames)
            if status == cv2.Stitcher_OK:
                pano_disp = np.zeros((fixed_pano_h, fixed_pano_w, 3), dtype=np.uint8)
                ph, pw = pano.shape[:2]
                if ph > fixed_pano_h or pw > fixed_pano_w:
                    y0 = max((ph - fixed_pano_h)//2, 0)
                    x0 = max((pw - fixed_pano_w)//2, 0)
                    pano_crop = pano[y0:y0+fixed_pano_h, x0:x0+fixed_pano_w]
                    pano_disp[:pano_crop.shape[0], :pano_crop.shape[1]] = pano_crop
                else:
                    y0 = (fixed_pano_h - ph)//2
                    x0 = (fixed_pano_w - pw)//2
                    pano_disp[y0:y0+ph, x0:x0+pw] = pano
                last_pano_disp = pano_disp
                # out_video.write(last_pano_disp)
            else:
                blank = np.zeros((fixed_pano_h, fixed_pano_w, 3), dtype=np.uint8)
                cv2.putText(blank, f'Stitch Fail: {status}', (50, fixed_pano_h//2), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
                last_pano_disp = blank
        except Exception as e:
            blank = np.zeros((fixed_pano_h, fixed_pano_w, 3), dtype=np.uint8)
            # cv2.putText(blank, f'Error: {str(e)}', (50, fixed_pano_h//2), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
            last_pano_disp = blank
        cv2.imshow('Panorama', last_pano_disp)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    for cap in caps:
        cap.release()
    # out_video.release()
    cv2.destroyAllWindows()


    ","['With out looking too deeply at the code l have some questions: \n1. Are you calculating the points for stitching and then the transformation every frame? \n2: are these cameras ridged?\n3: can you calibrate before hand?', 'You should know the cameras positions relative to each other, so instead of calculating the homography you just use the known transformations.  Regardless, the compute time on the image transforms will be high so you may only achieve a couple FPS.', ""You have the cameras most probably fixed on a rig, haven't you? If it's the case you don't have to perform image matching every frame, which is exactly what is the OpenCV stitcher doing. It may even perform optimal image seam computation by default which may be quite expensive and is intended to stitch images taken in succession without cutting moving people in half. The frames from individual cameras are also highly unlikely to be undistorted correctly by `defisheye` with the default settings.\n\nYou should do this first before running the realtime pipeline:\n\n1. calibrate individual cameras with printed chessboard pattern to get distortion parameters (both calibration and undistortion is in opencv, you may skip this when there is almost no visible image distortion)\n2. calibrate relative poses of neighboring cameras: for few cameras just a homography / perspective transformation of a chessboard pattern is ok, for more cameras covering more than \\~ 150 deg field of view you need some kind of cylindrical or spherical mapping to accommodate the large field of view, you may use the stitcher and save the camera parameters\n\nrealtime processing:\n\n1. undistort individual images using calibration parameters\n2. for few cameras map all the frames to the central one using \\`cv.WarpPerspective\\` (you'll need to think about how to apply transformations correctly to map everything to a single image, it is good to try this on individual pairs first) or use the saved camera parameters with the camera stitcher disabling all image matching features and image seam optimization\n\nThe image warping is quite fast, but can take some time on large images. You may downscale the images first to reduce the load. You should do the calibration / stitcher initialization on the downscaled images to avoid need of correcting the calibration parameters and camera poses for reduced image size. You may also separate image loading and image stitching to individual threads."", 'That looks like a jetson desktop, but youre doing everything on the cpu, which is rather weak. An LLM should be able to help you port the code to something that use the gpu.', 'This looks very interesting. What cameras are you using? Perhaps I can jump and help you out.', ""Are your cameras fixed on a rig?\nThe function you are calling is recomputing the stitching parameters at every frame but you should precompute them once and reuse.\nBased on what I see on the documentation, you should call estimateTransform() one time to estimate the stitching parameters, then composePanorama() for each frame.\n\n\nAlso, don't use Defisheye to undistort your images at every frames. Use opencv to calibrate and compute a lookup table (remap) so you can undistort really fast."", 'Oaky, so I have worked on image stitching in real time using 3 cameras.\n\nLike everyone suggested, fix your cameras. \n\nAfter that find key points using any feature extraction method. Create H-matrix and use it for image stitching.', ""Doing that in real time is challenging. After calibration, you need to run the projections and the merging on a GPU. I don't think openCV has that in its cuda module.\n\nYou can have a look at\n\n[https://github.com/stitchEm/stitchEm](https://github.com/stitchEm/stitchEm)\n\nIt's a cuda implementation of what you're trying to do"", 'This will be easier and faster if you know the camera positions relative to each other at the time the photos are taken.\xa0\n\nThat requires the cameras are sturdily mounted on a rig, and that their shutters are coordinates.', 'Even if the cameras are static and you just do the same transformation over and over again you will still end up with a seam in your image as things move, stitching definitely needs to happen each frame or you will never see a smooth image. So i think OP is correct to run the stitch on every frame if his project truly requires smooth images.\n\nI think your first step should be profiling each section of the code. add timing print outs and look for the sections with the largest time or use a profiling library.  \nFor example if the defisheye is taking up 10% of the cpu time then maybe moving that to its own subprocess/thread would speed things up.']","['With out looking too deeply at the code l have some questions: \n1. Are you calculating the points for stitching and then the transformation every frame? \n2: are these cameras ridged?\n3: can you calibrate before hand?', 'You should know the cameras positions relative to each other, so instead of calculating the homography you just use the known transformations.  Regardless, the compute time on the image transforms will be high so you may only achieve a couple FPS.', ""You have the cameras most probably fixed on a rig, haven't you? If it's the case you don't have to perform image matching every frame, which is exactly what is the OpenCV stitcher doing. It may even perform optimal image seam computation by default which may be quite expensive and is intended to stitch images taken in succession without cutting moving people in half. The frames from individual cameras are also highly unlikely to be undistorted correctly by `defisheye` with the default settings.\n\nYou should do this first before running the realtime pipeline:\n\n1. calibrate individual cameras with printed chessboard pattern to get distortion parameters (both calibration and undistortion is in opencv, you may skip this when there is almost no visible image distortion)\n2. calibrate relative poses of neighboring cameras: for few cameras just a homography / perspective transformation of a chessboard pattern is ok, for more cameras covering more than \\~ 150 deg field of view you need some kind of cylindrical or spherical mapping to accommodate the large field of view, you may use the stitcher and save the camera parameters\n\nrealtime processing:\n\n1. undistort individual images using calibration parameters\n2. for few cameras map all the frames to the central one using \\`cv.WarpPerspective\\` (you'll need to think about how to apply transformations correctly to map everything to a single image, it is good to try this on individual pairs first) or use the saved camera parameters with the camera stitcher disabling all image matching features and image seam optimization\n\nThe image warping is quite fast, but can take some time on large images. You may downscale the images first to reduce the load. You should do the calibration / stitcher initialization on the downscaled images to avoid need of correcting the calibration parameters and camera poses for reduced image size. You may also separate image loading and image stitching to individual threads."", 'That looks like a jetson desktop, but youre doing everything on the cpu, which is rather weak. An LLM should be able to help you port the code to something that use the gpu.', 'This looks very interesting. What cameras are you using? Perhaps I can jump and help you out.']",100,23,0.98,Help: Project,1750644812.0
1li4sw9,computervision,How to achieve real-time video stitching of multiple cameras?,"Hey everyone, I'm having issues while using the Jetson AGX Orin 64G module to complete a real-time panoramic stitching project. My goal is to achieve 360-degree panoramic stitching of eight cameras. I first used the latitude and longitude correction method to remove the distortion of each camera, and then input the corrected images for panoramic stitching. However, my program's real-time performance is extremely poor. I'm using the panoramic stitching algorithm from OpenCV. I reduced the resolution to improve the real-time performance, but the result became very poor. How can I optimize my program? Can any experienced person take a look and help me?","['Are the camera positions constant? Can you just compute the homographies in an initial calibration step and apply them to the incoming pixels as a static transformation?', 'Assuming the cameras are very close to each other, relative to the scene observed, then for each image you can first remove radial distorsion and remap it to a sphere or cylinder, depending on the type of panorama you want.\nThese transformations can be precomputed and run realtime.']","['Are the camera positions constant? Can you just compute the homographies in an initial calibration step and apply them to the incoming pixels as a static transformation?', 'Assuming the cameras are very close to each other, relative to the scene observed, then for each image you can first remove radial distorsion and remap it to a sphere or cylinder, depending on the type of panorama you want.\nThese transformations can be precomputed and run realtime.']",5,2,1.0,Help: Project,1750644008.0
1li1k36,computervision,Any ideas or better strategies for feature engineering to use YOLOv8 to detect shipwrecks in a Digital Elevation Model (DEM)?,I haven’t found too much literature on fine-tuning YOLOv8 on DEMs. Anyone have experience and some best practices? ,"['Interesting concept.\n\nI guess I would focus first on generating synthetic training data using 3D models of ships added to a DEM of the ocean floor. Maybe run it through a style transfer model to give it the low fidelity noisy look? If you can’t find 3D ship models you could use a “image to 3D model” model. Get the images from Google or serial photos.\xa0\n\nAnd the hill shading stuff probably doesn’t hurt, but why not just train on the elevation alone? Not 100% sure but I think Ultralytics provides the option to vary the number of channels, assuming you want to use their specific YOLO implementation.']","['Interesting concept.\n\nI guess I would focus first on generating synthetic training data using 3D models of ships added to a DEM of the ocean floor. Maybe run it through a style transfer model to give it the low fidelity noisy look? If you can’t find 3D ship models you could use a “image to 3D model” model. Get the images from Google or serial photos.\xa0\n\nAnd the hill shading stuff probably doesn’t hurt, but why not just train on the elevation alone? Not 100% sure but I think Ultralytics provides the option to vary the number of channels, assuming you want to use their specific YOLO implementation.']",8,4,0.9,Help: Project,1750634198.0
1li0niv,computervision,more accurate basketball tracking ideas?,"[This is what the clips look like](https://preview.redd.it/j42wttnf2k8f1.png?width=715&format=png&auto=webp&s=b4ff30851d68b9c3aa48c904d886bd50d2356dee)

Currently using rectangular bounding boxes on a dataset of around 1400 images all from the same game using the same ball. Running my model (YOLOv8) back on the same video, the detection sometimes doesnt work fast enough or it doesn't register some really fast shots, any ideas?
I've considered potentially getting different angles? Or is it simply that my dataset isnt big enough and I should just annotate more data
Moreover another issue is that I have annotated lots of basketballs where my hand was on it, and I think this might be affecting the accuracy of the model?
","['you could gather data from different angles and annotate them. Also you could predict the ball path and interpolate between detections.  \nI did the ball path prediction here: [https://www.reddit.com/r/computervision/comments/1klp3so/using\\_python\\_cv\\_to\\_visualize\\_quadratic\\_equations/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/computervision/comments/1klp3so/using_python_cv_to_visualize_quadratic_equations/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)', ""Additional data should help, because current size quite small. Recently I worked on a similar project but about golf, with dataset around 2000 images - network quite frequently detect end of club as ball, so it's similar to your case with hand. Our final solution was to extend dataset (10000+ images) and its help alot - model were more robust and works quite well. I think here help fact that quantity of static and a ball without any object close increased compare to original dataset which helps model to capture ball features and reduce overall accuracy (so you could increase number of such cases more, to reduce mistakes with hand classified as ball or ball classified as a ball only with hand on it, if such error persist). For you such dataset size could be large, but netherless any extend is good and should help. Another recommendation will be to adjust parameters of the model, as for start - resolution of the model (try something large, like with minimum 1080 pixel count by any side)"", ""\\- Do you leave `imgsz` parameter default? It's 640 by default and your images during both training and inference are downscaled to this resolution. The ball could be relatively small after downsizing and reduce detection accuracy. You may 1) zoom in to get larger ball 2) perform training and inference in higher resolution, but not overdo this. Check [https://github.com/ultralytics/ultralytics/issues/1037](https://github.com/ultralytics/ultralytics/issues/1037) and [https://github.com/ultralytics/ultralytics/issues/2546](https://github.com/ultralytics/ultralytics/issues/2546)\n\n\\- Do you have enough blurred samples in your dataset? If you intend to detect both a ball in player's possession and in flight you need enough samples of both type. \n\n\\- You may set the video recording to higher frame rate to reduce the motion blur.\n\n\\- A few missed detections, particularly in flight doesn't matter. You can apply Kalman filter to fill the gaps and smooth out detection jitter.""]","['you could gather data from different angles and annotate them. Also you could predict the ball path and interpolate between detections.  \nI did the ball path prediction here: [https://www.reddit.com/r/computervision/comments/1klp3so/using\\_python\\_cv\\_to\\_visualize\\_quadratic\\_equations/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/computervision/comments/1klp3so/using_python_cv_to_visualize_quadratic_equations/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)', ""Additional data should help, because current size quite small. Recently I worked on a similar project but about golf, with dataset around 2000 images - network quite frequently detect end of club as ball, so it's similar to your case with hand. Our final solution was to extend dataset (10000+ images) and its help alot - model were more robust and works quite well. I think here help fact that quantity of static and a ball without any object close increased compare to original dataset which helps model to capture ball features and reduce overall accuracy (so you could increase number of such cases more, to reduce mistakes with hand classified as ball or ball classified as a ball only with hand on it, if such error persist). For you such dataset size could be large, but netherless any extend is good and should help. Another recommendation will be to adjust parameters of the model, as for start - resolution of the model (try something large, like with minimum 1080 pixel count by any side)"", ""\\- Do you leave `imgsz` parameter default? It's 640 by default and your images during both training and inference are downscaled to this resolution. The ball could be relatively small after downsizing and reduce detection accuracy. You may 1) zoom in to get larger ball 2) perform training and inference in higher resolution, but not overdo this. Check [https://github.com/ultralytics/ultralytics/issues/1037](https://github.com/ultralytics/ultralytics/issues/1037) and [https://github.com/ultralytics/ultralytics/issues/2546](https://github.com/ultralytics/ultralytics/issues/2546)\n\n\\- Do you have enough blurred samples in your dataset? If you intend to detect both a ball in player's possession and in flight you need enough samples of both type. \n\n\\- You may set the video recording to higher frame rate to reduce the motion blur.\n\n\\- A few missed detections, particularly in flight doesn't matter. You can apply Kalman filter to fill the gaps and smooth out detection jitter.""]",3,6,1.0,Help: Project,1750631660.0
1lhvlc4,computervision,"Struggling with Traffic Violation Detection ML Project — Need Help with Types, Inputs, GPU & Web Integration","
Hey everyone 👋
I’m working on a traffic violation detection project using computer vision, and I could really use some guidance.

So far, I’ve implemented red light violation detection using YOLOv10. But now I’m stuck with the following challenges:

1. Multiple Violation Types
There are many types of traffic violations (e.g., red light, wrong lane, overspeeding, helmet detection, etc.).
How should I decide which ones to include, or how to integrate multiple types effectively?
Should I stick to just 1-2 violations for now? If so, which ones are best to start with (in terms of feasibility and real-world value)?


2. GPU Constraints
I’m training on Kaggle’s free GPU, but it still feels limiting—especially with video processing.
Any tips on optimizing model performance or alternatives to train faster on limited resources?


3. Input for Functional Prototype
I want to make this project usable on a website (like a tool for traffic police or citizens).
What kind of input should I take on the website?



Upload video?

Upload frame?

Real-time feed?

Would love advice on what’s practical


4. ML + Web Integration
Lastly, I’m facing issues integrating the ML model with a  frontend + Flask backend.
Any good tutorials or boilerplate projects that show how to connect a CV model with a web interface?


I am having a time shortage
💡 Would love your thoughts, experiences, or links to similar projects.
Thanks in advance!

",['I assume this is a homework assignment?'],['I assume this is a homework assignment?'],1,4,0.56,Help: Project,1750618577.0
1lhv1fx,computervision,"I need your help, I honestly don't know what logic or project to carry out on segmented objects.","I can't believe it can find hundreds of tutorials on the internet on how to segment objects and even adapt them to your own dataset, but in reality, it doesn't end there. You see, I want to do a personal project, but I don't know what logic to apply to a segmented object or what to do with a pixel mask.

Please give me ideas, tutorials, or links that show this and not the typical ""segment objects with this model.""

    for r in results:  
        if r.masks is not None:
            mask = r.masks.data[0].cpu().numpy()
    Here I contain the mask of the segmented object but I don't know what else to do.","['The problem is that you are not trying to solve any problems. When you are, you will use masks for tasks like: how big is the object  how many are the objects, what is the ratio of the object with respect to the frame dimension, and so on. \n\nTry to think it as an application for a proboem you are trying to solve. You have a small garden where you grow tomatoes. You have a little furry friend at home who love to play with the soil. You do not want that to happen. You setup a small pi camera to observe it and use a dog segmentation model. That model gives you the segmentation mask for the frame passed. You record every frame and pass that to the model. When you get 0 mask, you can sleep calmly but when it is not, your furry friend is doing something at your farm. You send an alarm via email with some of such frames and you rush down there.', 'Why did you decide to use segmentation in the first place?\xa0']","['The problem is that you are not trying to solve any problems. When you are, you will use masks for tasks like: how big is the object  how many are the objects, what is the ratio of the object with respect to the frame dimension, and so on. \n\nTry to think it as an application for a proboem you are trying to solve. You have a small garden where you grow tomatoes. You have a little furry friend at home who love to play with the soil. You do not want that to happen. You setup a small pi camera to observe it and use a dog segmentation model. That model gives you the segmentation mask for the frame passed. You record every frame and pass that to the model. When you get 0 mask, you can sleep calmly but when it is not, your furry friend is doing something at your farm. You send an alarm via email with some of such frames and you rush down there.', 'Why did you decide to use segmentation in the first place?\xa0']",4,10,1.0,Help: Project,1750617193.0
1lhsovk,computervision,soccer team detection using jerseys,"Here's the description of what I'm trying to solve and need input on how to model the problem.

Problem Statement: Given a room/stadium filled with soccer (or any sport) fans, identify and count the soccer fans belonging to each team. For the moment, I'd like to focus on just still images. As an example, given an image of ""World cup starting ceremony"" with 15 different fans/players, identify the represented teams and proportion.

Given the scale of teams (according to Google, there are about 4k professional soccer clubs worldwide), what is the right way to model this problem?

My current thoughts are to model each team as a different object category (a specialization of PERSON / T-SHIRT). Annotate enough examples per team(?) and fine tune a SAM(or another one). Then, count the objects of each category. Is this the right approach?

I see that there is some overlap between this problem and logo detection. Folks who have worked on similar problems, what are your thoughts?",['This isn’t an easy problem and you wouldn’t go about it the way you have described. The best way I can think of without thinking too hard would be to detect each person individually and compare the embedding of each person to an embedding of the shirt that you are looking for'],['This isn’t an easy problem and you wouldn’t go about it the way you have described. The best way I can think of without thinking too hard would be to detect each person individually and compare the embedding of each person to an embedding of the shirt that you are looking for'],3,2,0.72,Help: Project,1750611427.0
1lhl26o,computervision,Any way to perform OCR of this image?,"Hi! I'm a newbie in image processing and computer vision, but I need to perform an OCR of a huge collection of images like this one. I've tried Python + Tesseract, but it is not able to parse it correctly (it always makes mistakes in at least 1-2 digits, usually even more). I've also tried EasyOCR and PaddleOCR, but they gave me even less than Tesseract did. The only way I can perform OCR right now is.... well... ChatGPT, it was correct 100% times, but, I can't feed such huge amount of images to it. Is there any way this text could be recognized correctly, or it's something too complex for existing OCR libraries?","['50.3918852 no need to thank me', ""Try some image processing before throwing it into tesseract, boost the contract, improve the edges etc.\n\nThe image isn't too complicated for OCR, you just need a good OCR pipeline"", 'Are the crops, sizes, and fonts always the same?\n\nIf so you can find examples of each character and the do a simple pattern match to find the closest character. Eg find an example of 1 and 2 and 3 etc and then when you decode an image you compare each region to your set of examples, pixel-wise correlation may be effective for this.\n\nIf the digits move about or change font this would be more challenging.', 'Using classical CV, I would try with template matching with a little bit of preprocessing. If each number is always at the same position, you could create a ROI for each digits. Also, if the number are in a logical sequence, you could filter some data.', ""  \nIn tesseract you want to   \n\\--tessedit\\_char\\_whitelist 0123456789  \nNow it's not going to return you SO.39lBBS2  \n  \nThen have convert about 100 samples including occurences of each digit.  \nUse tesseract training to teach it those samples.  \n[https://github.com/tesseract-ocr/tessdoc/blob/main/tess5/TrainingTesseract-5.md](https://github.com/tesseract-ocr/tessdoc/blob/main/tess5/TrainingTesseract-5.md)  \nOrder your output by tesseract confidence (available in tsv and hocr outputs)  \nRun the low confidence results over with an LLM or hand check them depending on the quantity.\n\nNo method will 100% this, aim for 98%. It's a fair metric."", 'I have a boring solution for you. OpenAI models has OCR capabilities. You can send the image via API. If you have not too many images, the total price will be reasonable.', 'Is the image always low quality like this? Its probably possible to do it, youre gonna probably need to do heavy pre-processing. \n\nHow crucial is it that the numbers is always 100% correct?', 'Preprocessing is everything', 'I think if you tune your contrast and brightness during pre processing', 'Are the errors across the various libraries you use common mode?  That is, do they all make the same errors or different errors for a hard image?  \n\nIf you want to use your free local libraries, you could use all of them and compare their outputs.  If they all agree, mark it as high confidence and move on.  If there is disagreement on an OCR, then you could decide to look for majority agreement across the various tools or simply choose to send off that subset of difficult images to ChatGPT, Gemini, or Claude to have it analyze them only in the cases where you are not getting consensus across your local tools.\n\nYou could also increase this pool of results by adding noise to the baseline image or slightly translating it or rotating it to get various versions of the input image and feed those into the various local pipelines to get more results to look at consistency across.']","['50.3918852 no need to thank me', ""Try some image processing before throwing it into tesseract, boost the contract, improve the edges etc.\n\nThe image isn't too complicated for OCR, you just need a good OCR pipeline"", 'Are the crops, sizes, and fonts always the same?\n\nIf so you can find examples of each character and the do a simple pattern match to find the closest character. Eg find an example of 1 and 2 and 3 etc and then when you decode an image you compare each region to your set of examples, pixel-wise correlation may be effective for this.\n\nIf the digits move about or change font this would be more challenging.', 'Using classical CV, I would try with template matching with a little bit of preprocessing. If each number is always at the same position, you could create a ROI for each digits. Also, if the number are in a logical sequence, you could filter some data.', ""  \nIn tesseract you want to   \n\\--tessedit\\_char\\_whitelist 0123456789  \nNow it's not going to return you SO.39lBBS2  \n  \nThen have convert about 100 samples including occurences of each digit.  \nUse tesseract training to teach it those samples.  \n[https://github.com/tesseract-ocr/tessdoc/blob/main/tess5/TrainingTesseract-5.md](https://github.com/tesseract-ocr/tessdoc/blob/main/tess5/TrainingTesseract-5.md)  \nOrder your output by tesseract confidence (available in tsv and hocr outputs)  \nRun the low confidence results over with an LLM or hand check them depending on the quantity.\n\nNo method will 100% this, aim for 98%. It's a fair metric.""]",49,90,0.9,Help: Project,1750589349.0
1lhkg71,computervision,Open source astronomy project: need best-fit circle advice,,"[""Hi,\n\nI'm maintaining an open-source tool called [DFTFringe](https://github.com/atsju/DFTFringe/issues/37) that analyzes interferometry images to deduce the shape of telescope mirrors. It's used by many amateur telescope makers and works well overall.\n\nThere's one manual step we'd like to automate: fitting a circle to an image feature, with ~1 pixel accuracy. More background here: [discussion thread](https://groups.io/g/Interferometry/topic/113665708#msg36232).\n\nIf you have suggestions for good approaches or algorithms, I’d love to hear them. Specific advice is very welcome — and if anyone feels like going further with a proof of concept, that would be fantastic (but absolutely not expected).\n\nYou can reply here or comment on [GitHub](https://github.com/atsju/DFTFringe/issues/37).\n\nThanks!"", 'Others have provided good suggestions but preprocessing is more important in this case and it might not be trivial. You could look into removing low frequency noise or looking at the image gradients. E.g. horizontal gradients if the lines are often vertical', 'Hough transformation (circle)', 'I think you could do RANSAC and try to find a circle such that the distribution of the intensity values of the pixels inside and the pixels outside have the maximum difference in mean.', ""As previously mentioned, circular hough transform. Scipy's implementation works very well. You can provide a range of expected radii. If you could mask the inner part of the circle and it's exterior (assuming that you have a rough estimate of the center and the radius), that will definitely help. If you have some labeled data (images+circle params), you could train a DL model to regress coarse circle params and then use hough transform for a fine estimate."", ""Do not use machine learning for this. \n\nYou will need to do some preprocessing to select out the boundary pixels of the circle before you apply the Hough transform. Do you know how to do that preprocessing? \n\nYou can use a numerical optimization after you've found a best initial guess. Do you know how to do that?"", ""This is all about the preprocessing, once you have the coordinates that make up the circle, just about anything will work to fit coefficients to it. This package has a bunch of methods you can try: https://pypi.org/project/circle-fit\n\nFor preprocessing, look at ridge detection algorithms: https://scikit-image.org/docs/0.25.x/auto_examples/edges/plot_ridge_filter.html\n\nYou may need some brightness normalization before and you'll need to threshold after the ridge detection."", ""If classical methods don't work well, training a simple segmentation model is not a terrible idea either. It depends a bit on how the other images in your dataset look like.""]","[""Hi,\n\nI'm maintaining an open-source tool called [DFTFringe](https://github.com/atsju/DFTFringe/issues/37) that analyzes interferometry images to deduce the shape of telescope mirrors. It's used by many amateur telescope makers and works well overall.\n\nThere's one manual step we'd like to automate: fitting a circle to an image feature, with ~1 pixel accuracy. More background here: [discussion thread](https://groups.io/g/Interferometry/topic/113665708#msg36232).\n\nIf you have suggestions for good approaches or algorithms, I’d love to hear them. Specific advice is very welcome — and if anyone feels like going further with a proof of concept, that would be fantastic (but absolutely not expected).\n\nYou can reply here or comment on [GitHub](https://github.com/atsju/DFTFringe/issues/37).\n\nThanks!"", 'Others have provided good suggestions but preprocessing is more important in this case and it might not be trivial. You could look into removing low frequency noise or looking at the image gradients. E.g. horizontal gradients if the lines are often vertical', 'Hough transformation (circle)', 'I think you could do RANSAC and try to find a circle such that the distribution of the intensity values of the pixels inside and the pixels outside have the maximum difference in mean.', ""As previously mentioned, circular hough transform. Scipy's implementation works very well. You can provide a range of expected radii. If you could mask the inner part of the circle and it's exterior (assuming that you have a rough estimate of the center and the radius), that will definitely help. If you have some labeled data (images+circle params), you could train a DL model to regress coarse circle params and then use hough transform for a fine estimate.""]",24,32,0.9,Help: Project,1750586854.0
1lhitgz,computervision,Issue with face embeddings in face recognition system,"Hey guys, I have been building a face recognition system using face embeddings and similarity checking. For that I first register the user by taking 3-5 images of their faces from different angles, embed them and store in a db. But I got issues with embedding the side profiles of the user's face. The embedding model is not able to recognize the face features from the side profile and thus the embedding is not good, which results in the system false recognizing people with different id. Has anyone worked on such a project? I would really appreciate any help or advise from you guys. Thank you :)","['What is the pretrained model you used to calculate the face embeddings?\n\nAlso, for a sanity check, you can check if the stored embeddings in the db can be grouped by person correctly - if this has issues, it’s a good idea to make that work first.', ""Ideally in this case, all the variations of a person's face, when embedded should form a cluster. In the best case, you should see one cluster per person. If you are sure that the embedding model is the problem, try using an image encoder which is trained for low level classification tasks (identifying different bird species or different dog breeds etc) which is trained using triplet loss. Make sure to fine tune such a model on human faces if it is pre-trained on a different dataset."", ""We had the same problem with face orientation and embeddings, that's why we decided to apply FaceID only when people were facing the camera. In our case, we used MTCNN to get faces and landmarks, and validated the orientation with the landmarks' positions.\n\nWe used a MobileFaceNet (for faster inference) to get the embeddings and then ArcFace for classification. We used a similar strategy for the embeddings, different photos but computing and saving the mean embedding.\n\nThat worked really well, but always limited to the face orientation for a proper identification."", 'Use sota model for the embeddings(InsightFace), try to do retrieval from embeddings space using FAISS or similar.']","['What is the pretrained model you used to calculate the face embeddings?\n\nAlso, for a sanity check, you can check if the stored embeddings in the db can be grouped by person correctly - if this has issues, it’s a good idea to make that work first.', ""Ideally in this case, all the variations of a person's face, when embedded should form a cluster. In the best case, you should see one cluster per person. If you are sure that the embedding model is the problem, try using an image encoder which is trained for low level classification tasks (identifying different bird species or different dog breeds etc) which is trained using triplet loss. Make sure to fine tune such a model on human faces if it is pre-trained on a different dataset."", ""We had the same problem with face orientation and embeddings, that's why we decided to apply FaceID only when people were facing the camera. In our case, we used MTCNN to get faces and landmarks, and validated the orientation with the landmarks' positions.\n\nWe used a MobileFaceNet (for faster inference) to get the embeddings and then ArcFace for classification. We used a similar strategy for the embeddings, different photos but computing and saving the mean embedding.\n\nThat worked really well, but always limited to the face orientation for a proper identification."", 'Use sota model for the embeddings(InsightFace), try to do retrieval from embeddings space using FAISS or similar.']",5,17,0.78,Help: Project,1750580030.0
1lhflnq,computervision,Is AI tracking in Supervisely processed on client side?,"Hey everyone,
I’ve been using Supervisely for some annotation tasks and recently noticed something. When I use the AI tracking feature on my own laptop, the performance is noticeably slower and less accurate. But when I tried the same task on a friend’s laptop (with better hardware), the tracking seemed faster and more precise.
This got me wondering:
Dose Supervisely perform AI tracking locally on client machine, or is the processing done server-side?

I’d appreciate any insights or official clarification. Thanks!",['Their own documentation says “maybe”.\n\nhttps://supervisely.com/feature-comparison/'],['Their own documentation says “maybe”.\n\nhttps://supervisely.com/feature-comparison/'],0,1,0.25,Help: Theory ,1750567476.0
1lhaf1v,computervision,2 Android AI agents running at the same time - Object Detection and LLM,"Hi, guys!

I added a support for running several AI agents at the same time to my project - deki.
It is a model that understands what’s on your screen and can perform tasks based on your voice or text commands.

Some examples:
\* ""Write my friend ""some\_name"" in WhatsApp that I'll be 15 minutes late""
\* ""Open Twitter in the browser and write a post about something""
\* ""Read my latest notifications""
\* ""Write a linkedin post about something""

Android, ML and Backend codes are fully open-sourced.
I hope you will find it interesting.

Github: [https://github.com/RasulOs/deki](https://github.com/RasulOs/deki)

License: GPLv3","['What is this ?', '>”\xa0Write a linkedin post about something""\n\nPlease do the world a favor and block that functionality 😂\xa0', 'By the way, just deployed the model on huggingface space:\n\n[https://huggingface.co/spaces/orasul/deki](https://huggingface.co/spaces/orasul/deki) \n\nYou can check Analyze & and get YOLO and then action endpoint to see the capabilities of the model']","['What is this ?', '>”\xa0Write a linkedin post about something""\n\nPlease do the world a favor and block that functionality 😂\xa0', 'By the way, just deployed the model on huggingface space:\n\n[https://huggingface.co/spaces/orasul/deki](https://huggingface.co/spaces/orasul/deki) \n\nYou can check Analyze & and get YOLO and then action endpoint to see the capabilities of the model']",41,5,0.89,Discussion ,1750550306.0
1lhaea0,computervision,Question: using computer vision for detection on pickle ball court,"Hey folks,

Was hoping someone could point me in the right direction....

Main Question:

* What tools or libraries could be used to create a device/tool that can detect how many courts are currently busy vs not busy.


Context:

* I'm thinking of making a device for my local pickle ball court that can detect how many courts are open at any given moment.

* My courts are always packed and I think it would be cool if I could no ahead of time if there are openings or not.
* I have permission to hang a device on the court
* I am technical but not knowledgable in this domain",['You will have to look at object detection in a region of interest. this is a video but it is more of the answer than an explanation:  \n[https://www.youtube.com/watch?v=wsSNz1R3ej8&t=39s](https://www.youtube.com/watch?v=wsSNz1R3ej8&t=39s)\n\nYou can look at object detection is specific region'],['You will have to look at object detection in a region of interest. this is a video but it is more of the answer than an explanation:  \n[https://www.youtube.com/watch?v=wsSNz1R3ej8&t=39s](https://www.youtube.com/watch?v=wsSNz1R3ej8&t=39s)\n\nYou can look at object detection is specific region'],4,2,0.83,Help: Project,1750550239.0
1lgsckl,computervision,I just got some free time on my hands - any recommended course/book/articles?,"Hello,
I just got some free time on my hands and want to dedicate my time for brushing up on latest knowledge gaps.
I have been mainly working on vision problems (classificationm, segmentation) but also 3D related ones like camera pose estimation including some gen AI related (Nerf, GS) etc...

I am not bounding myself to Vision. also LLM or other ML fields that could be benefciail in today's changing world.

Any useful resource on multimodal models?

Thanks!","['Following!!', 'It is a very new domain so not much books around it.\n\nFor classical CV - [https://fpcv.cs.columbia.edu/](https://fpcv.cs.columbia.edu/)  \nDeep Learning CV (Not much theory) - [https://huggingface.co/learn/computer-vision-course/](https://huggingface.co/learn/computer-vision-course/)']","['Following!!', 'It is a very new domain so not much books around it.\n\nFor classical CV - [https://fpcv.cs.columbia.edu/](https://fpcv.cs.columbia.edu/)  \nDeep Learning CV (Not much theory) - [https://huggingface.co/learn/computer-vision-course/](https://huggingface.co/learn/computer-vision-course/)']",23,2,0.9,Discussion ,1750497262.0
1lgel0t,computervision,VGGT was best paper at CVPR and kinda impresses me,"VGGT eliminates the need for geometric post-processing altogether.

The paper introduces a feed-forward transformer that directly predicts camera parameters, depth maps, point maps, and 3D tracks from arbitrary numbers of input images in **under a second**. Their alternating-attention architecture (switching between frame-wise and global self-attention) outperforms traditional approaches that rely on expensive bundle adjustment and geometric optimization. What's particularly impressive is that this purely neural approach achieves this without specialized 3D inductive biases.

VGGT show that large transformer architectures trained on diverse 3D data might finally render traditional geometric optimization obsolete.

Project page: https://vgg-t.github.io

Notebook to get started: https://colab.research.google.com/drive/1Dx72TbqxDJdLLmyyi80DtOfQWKLbkhCD?usp=sharing

⭐️ Repo for my integration into FiftyOne: https://github.com/harpreetsahota204/vggt
","['Yeah it’s pretty incredible. I’d love something even close to this good available with a permissive commercial license.', 'Talked with one of the authors at the show, incredibly impressive stuff.', 'What type of traditional geometric optimization does it replace?', ""Jeez, that's something else.\nI haven't read the paper, but will do and try the software.\n\nI'd like to see how this could work with my 3D microscopy imaging.\n\nedit: Holy crap, this is astonishing."", ""After a local install, here are a few comments :\n- Easy to install. The requirements.txt file set specific version of torch,torchvision,... but it also works with more recent version. The gradio demo returns a TypeError : argument of type 'bool' is not iterable. it can be fixed by installing pydantic==2.10.6\n- The reported memory consumption doesn't include the model itself, so we should add 4.68Gb to the VRAM required.\nOn a 8Gb GPU, I could run the model with maximum 2 images. The increment per image is 0.22Gb, so it should be usable with a lot more images on a 16Gb GPU.\n- I tried a few driving scenes (without dynamic objects) on the online demo with 16 images, I get either really good results or very bad reconstructions. It doesn't seem able to handle well when it doesn't have enough point features in close range (like trees on the side of the road). discontinuous lanelines don't seem enough to get a good alignment. It also doesn't handle the slopes well."", ""VRAM consumption seems great. Approximately 1.7Gb + 0.2Gb/image, so easy to try even on low cost gpu.\nInput resolution is low but I guess it should be possible to increase it in post-process.\nI'll test it on some difficult sequences to see how good it is.\n\nEdit : The reported consumption didn't include the 4.68Gb for loading the model. Max 2 images on a 8Gb GPU, but probably around 40 images on a 16Gb GPU which is reasonable."", 'This work is absolutely insane. The only bad thing is that the work has non commercial licence.', 'Remindme! 3 days', 'This is so great, a year back or so I was working on a problem similar to this ie helping LLMs with geo-spatial data. \n\nMaybe I could work on my long left project by taking some inspiration from this.', 'How well does it work on large scale scenes?\n\nCan it pinpoint the position of something a mile away at the same time as locating things ten feet away in the same set of photos?\n\nMost of the outdoor scene datasets tend to cut off at about 100 meters, and models trained on them tend to inherit that limitation.\xa0']","['Yeah it’s pretty incredible. I’d love something even close to this good available with a permissive commercial license.', 'Talked with one of the authors at the show, incredibly impressive stuff.', ""Jeez, that's something else.\nI haven't read the paper, but will do and try the software.\n\nI'd like to see how this could work with my 3D microscopy imaging.\n\nedit: Holy crap, this is astonishing."", 'What type of traditional geometric optimization does it replace?', ""After a local install, here are a few comments :\n- Easy to install. The requirements.txt file set specific version of torch,torchvision,... but it also works with more recent version. The gradio demo returns a TypeError : argument of type 'bool' is not iterable. it can be fixed by installing pydantic==2.10.6\n- The reported memory consumption doesn't include the model itself, so we should add 4.68Gb to the VRAM required.\nOn a 8Gb GPU, I could run the model with maximum 2 images. The increment per image is 0.22Gb, so it should be usable with a lot more images on a 16Gb GPU.\n- I tried a few driving scenes (without dynamic objects) on the online demo with 16 images, I get either really good results or very bad reconstructions. It doesn't seem able to handle well when it doesn't have enough point features in close range (like trees on the side of the road). discontinuous lanelines don't seem enough to get a good alignment. It also doesn't handle the slopes well.""]",289,29,1.0,Showcase,1750452313.0
1lgbqpp,computervision,YOLOv8 for Falling Nails Detection + Classification – Seeking Advice on Improving Accuracy from Real Video,"Hey folks,
I’m working on a project where I need to detect and classify **falling nails** from a video. The goal is to:

* Detect only the nails that **land on a wooden surface..**
* Classify them as **rusted** or **fresh**
* Count valid nails and match similar ones by height/weight

**What I’ve done so far:**

* Made a synthetic dataset (\~700 images) using fresh/rusted nail cutouts on wooden backgrounds
* Labeled the background as a separate class (""wood"")
* Trained a YOLOv8n model (100 epochs) with tight rotated bounding boxes
* Results were decent on synthetic test images

**But...**

When I ran it on the actual video (10s clip), the model tanked:

* Missed nails, loose or no bounding boxes
* detecting the ones not on wooden surface as well
* Poor generalization from synthetic to real video
* many things are messed up..

I’ve started manually labeling video frames now to retrain with better data... but any tips on improving real-world detection, model settings, or data realism would be hugely appreciated.

https://preview.redd.it/j2rzl9bwn48f1.png?width=1919&format=png&auto=webp&s=9950002b6c660d5806ebdfd5f8bbbdd32ed6058c

https://reddit.com/link/1lgbqpp/video/e29zx1ain48f1/player

","[""Could you show some examples from synthetic dataset?\n\nFrom your detection results, it seems the dataset itself correlate poorly with the real data. Labeling real-world data should give you much better results. From the video you attached as an example - quite blurry one, but even such examples the network could process quite good. As quality improvement of the dataset - improve visibility so all area on the wood itself will be visible clear. Maybe even change position of camera itself, for instance shot from the top.\n\nAs for the pipeline itself, detect the wood - crop it - detect\\\\classify nails from crop - seems good plan to start. As I understand you do something similar here. My concern here that quantity of nails quite large and they create sort of a metal-blob which is itself a hard task to detect (individually), but I'm not sure. \n\nYou don't specify which framework you use, but I suppose Ultralytics (cause of YOLOv8) - out of box it has I would say great parameters to achieve good detection results, so even with such small dataset achieve something is possible. In your situation, I would change number of epochs and resolution for more higher one (up to minimal 1080 pixels per side for examples).\n\nAs for another solution (if detections fails), I would try to replace detection\\\\classification of nails with instance segmentation model. Or maybe you even could try SegmentationAnything model to get separable instances of each nail, based on this you could classify each set of pixels (of each nail) with some small network."", ""Have you thought of applying background subtraction to detect moving objects as the nail falls. Then when stationary i.e the track for that blob ends -> check what the background is once it's stationary. And you have a few ways of doing that: train a classifier based on some convnet features, or CLIP embeddings (with wooden background<>without  or rusted <> fresh ). Hope this helps.\n\nedit: I would also try yolo world or Grounding DINO - they might have a way of working with your prompt to detect. You could also try multiple prompts and arrive at a consensus if a single prompt isn't cutting it."", 'I wouldn’t use a bounding box model for this.\xa0\n\nTry either a key point model (like the kind used to infer a person’s head, hands, legs, etc) and have it infer each end of the nail, or an instance segmentation model that infers the specific pixels belonging to each nail. I’d probably try the later since many nails have one end hidden. You can also add a class for “wood” to the same model.\xa0', 'Is the wooden slab and environment the final deployment environment?\n\nIf so then I would strongly recommend creating synthetic data based on that and with more variations in the light, nails etc to avoid over-training.\n\nYou can easily remodel this in Blender 3D and create pixel-perfect segmentation masks inside of Nvidia Omniverse.']","[""Could you show some examples from synthetic dataset?\n\nFrom your detection results, it seems the dataset itself correlate poorly with the real data. Labeling real-world data should give you much better results. From the video you attached as an example - quite blurry one, but even such examples the network could process quite good. As quality improvement of the dataset - improve visibility so all area on the wood itself will be visible clear. Maybe even change position of camera itself, for instance shot from the top.\n\nAs for the pipeline itself, detect the wood - crop it - detect\\\\classify nails from crop - seems good plan to start. As I understand you do something similar here. My concern here that quantity of nails quite large and they create sort of a metal-blob which is itself a hard task to detect (individually), but I'm not sure. \n\nYou don't specify which framework you use, but I suppose Ultralytics (cause of YOLOv8) - out of box it has I would say great parameters to achieve good detection results, so even with such small dataset achieve something is possible. In your situation, I would change number of epochs and resolution for more higher one (up to minimal 1080 pixels per side for examples).\n\nAs for another solution (if detections fails), I would try to replace detection\\\\classification of nails with instance segmentation model. Or maybe you even could try SegmentationAnything model to get separable instances of each nail, based on this you could classify each set of pixels (of each nail) with some small network."", ""Have you thought of applying background subtraction to detect moving objects as the nail falls. Then when stationary i.e the track for that blob ends -> check what the background is once it's stationary. And you have a few ways of doing that: train a classifier based on some convnet features, or CLIP embeddings (with wooden background<>without  or rusted <> fresh ). Hope this helps.\n\nedit: I would also try yolo world or Grounding DINO - they might have a way of working with your prompt to detect. You could also try multiple prompts and arrive at a consensus if a single prompt isn't cutting it."", 'I wouldn’t use a bounding box model for this.\xa0\n\nTry either a key point model (like the kind used to infer a person’s head, hands, legs, etc) and have it infer each end of the nail, or an instance segmentation model that infers the specific pixels belonging to each nail. I’d probably try the later since many nails have one end hidden. You can also add a class for “wood” to the same model.\xa0', 'Is the wooden slab and environment the final deployment environment?\n\nIf so then I would strongly recommend creating synthetic data based on that and with more variations in the light, nails etc to avoid over-training.\n\nYou can easily remodel this in Blender 3D and create pixel-perfect segmentation masks inside of Nvidia Omniverse.']",7,6,0.89,Help: Project,1750445161.0
1lg48rx,computervision,Optimal SBC for human tracking?,"whats the best SBC to use and optimal FPS for tracking a human? im planning to use the YOLO model, ive researched the Raspi 4 but it only gave 1 fps and im pretty sure it is not optimal, any recommendations that i should consider for this project?","['You can get 30 FPS on a YOLO model with any Rockchip [RK3588](https://radxa.com/products/rock5/5c/) or [RK3576](https://radxa.com/products/rock4/4d/) based SBC.', 'Jetson, either nano or orin nano (super). Nano is a bit cheaper, orin nano has much more performance.']","['You can get 30 FPS on a YOLO model with any Rockchip [RK3588](https://radxa.com/products/rock5/5c/) or [RK3576](https://radxa.com/products/rock4/4d/) based SBC.', 'Jetson, either nano or orin nano (super). Nano is a bit cheaper, orin nano has much more performance.']",2,6,1.0,Help: Project,1750426764.0
1lfz6cv,computervision,looking for collaboration on computer vision projects,"hello everyone, i know basic computer vision algorithms and have good knowledge of image processing techniques. currently i am learning about vision transformers by implementing from scratch. i want to build some cool computer vision projects, not sure what to build yet. so if you're interested to team up, let me know. Thanks.","['Here is an experiment we did nearly a year ago. May be you are interested.\nhttps://github.com/Crazz-Zaac/computer-vision-experiments', ""If you're interested (or someone else) we could work on a kaggle image dataset/competition together."", ""Hi! I've got an interesting project that's been on the back burner, waiting for collaboration.\n\nI have build several robotic microscopes with fast XYZ motion. What I have always wanted to do, is to set up a GUI where you identify a 'fast moving' (or to start, a slow moving) target object with a single click, an example being a ciliate or paramecium (see video below), with a single click on the object/target, and then predict approximately it's moving trajectory (maybe fit a spline? if that's fast enough) and automatically move the XY stage to manually keep the target within the camera view. \n\nIt's really challenging to keep the target/specimen in the view. They often rotate rapidly and unpredictably. If you watch any video on protozoa video microscopy, you'll see people (like me :) struggling very hard to keep up with the target.\n\nI can definitely see research potential here. Longer-term tracking of protozoa motion could be highly beneficial for understanding their behavior, and how it relates to their intercellular structure/triggering. I'm actually amazed that no one has even attempted this, but that's likely because most 3D robotic microscopes are very expensive.\n\nHere's a link to an older video that shows the type of images/specimens/targets that I'm talking about. I think using ML to rapidly predict motion is possible, or using fast optical flow, or other means.\n\nWell, if this is interesting, just PM me. I have written my current GUI code in C#, but moving to Python is also fine. The motors use [TIC controllers](https://www.pololu.com/category/212/tic-stepper-motor-controllers) with about 20-40 ms response time. Right now I'm moving with just simple command-line statement(s), but there is probably a better way (eg. TTL), that I haven't explored.\n\nBest! -Todd\n\nhttps://youtu.be/-gidj8eBPHA"", 'Can a data annotator join?', 'I would love to team up with you, if given an opportunity', '(disclaimer promotion) If interested, consider contributing to my open source project -\nhttps://github.com/Udayraj123/OMRChecker', 'So I am currently working on a good project. It is real time cctv detection. In this i am trying to build a face detector which takes live feed from multiple IP cameras and runs algorithms such as tracking and recognition. If the person detected is unknown, then there is an integration of an Alert System. This is an end to end project.\n\nLet me know if you are interested.', 'Im currently doing my thesis in this domain, let me know if you need any help!', 'hi human mesh recovery', 'Hey, I am working on identifying diseases in livestock using computer vision by analyzing the images over time.\nThere are many interesting problem statements in this domain as I work with rgb as well as infrared images.\n\nSome of the topics include multimodal image fusion, time series image analysis and anomaly detection, image segmentation and tracking, depth estimation, and vision language.\n\nLet me know if you are interested.']","['Here is an experiment we did nearly a year ago. May be you are interested.\nhttps://github.com/Crazz-Zaac/computer-vision-experiments', ""If you're interested (or someone else) we could work on a kaggle image dataset/competition together."", ""Hi! I've got an interesting project that's been on the back burner, waiting for collaboration.\n\nI have build several robotic microscopes with fast XYZ motion. What I have always wanted to do, is to set up a GUI where you identify a 'fast moving' (or to start, a slow moving) target object with a single click, an example being a ciliate or paramecium (see video below), with a single click on the object/target, and then predict approximately it's moving trajectory (maybe fit a spline? if that's fast enough) and automatically move the XY stage to manually keep the target within the camera view. \n\nIt's really challenging to keep the target/specimen in the view. They often rotate rapidly and unpredictably. If you watch any video on protozoa video microscopy, you'll see people (like me :) struggling very hard to keep up with the target.\n\nI can definitely see research potential here. Longer-term tracking of protozoa motion could be highly beneficial for understanding their behavior, and how it relates to their intercellular structure/triggering. I'm actually amazed that no one has even attempted this, but that's likely because most 3D robotic microscopes are very expensive.\n\nHere's a link to an older video that shows the type of images/specimens/targets that I'm talking about. I think using ML to rapidly predict motion is possible, or using fast optical flow, or other means.\n\nWell, if this is interesting, just PM me. I have written my current GUI code in C#, but moving to Python is also fine. The motors use [TIC controllers](https://www.pololu.com/category/212/tic-stepper-motor-controllers) with about 20-40 ms response time. Right now I'm moving with just simple command-line statement(s), but there is probably a better way (eg. TTL), that I haven't explored.\n\nBest! -Todd\n\nhttps://youtu.be/-gidj8eBPHA"", 'Can a data annotator join?', 'I would love to team up with you, if given an opportunity']",9,24,0.8,Discussion ,1750409670.0
1lfxev3,computervision,Cognex/Keyence Machine Vision Cameras without their software?,"To people who have worked with industrial machine vision cameras, like those from Cognex/Keyence. Can you use them for merely capturing data and running your own algorithms instead of relying on their software suite?

I heard that cognex runtime licenses cost from 2-10k USD/yr, which would be a massive cost but also completely avoidable since my requirements are something I can code. I just wanted if they're not cutting off your ability to capture streams unless you specifically use their software suite.

I will be working with 3D line and area scanners.","['At that point, why use their cameras. Use any camera with gige or USB.', ""There are many (the majority) similar industrial cameras that stream the frames to a PC for processing.  The APIs are simple and there's lots of software you can use.  Cognex is fairly top-end price wise.  You can pick up a basic industrial camera for under $500 and the SDK to stream frames will (usually) be included in that price.   \nIf you specifically want a smart camera that you can deploy your own software onto, that is also quite common - just search for industrial smart cameras."", 'There are lot of 3D line alternatives without software like Teledyne, one of my favourite is LMI wich have software, but you can also use without it. Same for area scan like Zivid. If you wants any suggests feel free to DM me', 'Have you ever heard about SICK Ranger 3 camera, or AT C6 or similar GigE with line extraction on board? Or area scan like Teledyne Dalsa Genie Nano or Basler Pylon?\n\nThe are many industrial products that give you only images to process, you don’t have to use Cognex or Keyence if you don’t need their software.', 'Plenty of basler machine vision cameras for less than $500', 'When you say ""machine vision"" ... do you mean as in an industrial automation \\[manufacturing\\] machine with stand alone smart cameras? or do you mean some larger computer system with non-smart cameras simply taking the images?\n\nSmart cameras from Cognex and Keyence have a high up-front cost, but offer on-board software that processes each image on-board the camera as it is acquired. The programming software for these is ""free"" and downloadable (""free"" in quotes because the programming software license fee is buried into the cost of the hardware). Once the programming is complete, the computer can be connected and the camera runs ""autonomously"". These cameras are commonly used in manufacturing and logistics applications.\n\nBut if you\'re talking about a system where the image is simply sent from the dumb camera to the PC where it is processed ... then the other comments here are correct -- purchase any brand camera that has the specs and transfer protocol of your chose (GigE or USB for example). The images are transferred directly from the camera to the PC and the PC does all of the processing. You\'ll need to provide all of your own software (written, purchased, open source, etc).']","['At that point, why use their cameras. Use any camera with gige or USB.', ""There are many (the majority) similar industrial cameras that stream the frames to a PC for processing.  The APIs are simple and there's lots of software you can use.  Cognex is fairly top-end price wise.  You can pick up a basic industrial camera for under $500 and the SDK to stream frames will (usually) be included in that price.   \nIf you specifically want a smart camera that you can deploy your own software onto, that is also quite common - just search for industrial smart cameras."", 'There are lot of 3D line alternatives without software like Teledyne, one of my favourite is LMI wich have software, but you can also use without it. Same for area scan like Zivid. If you wants any suggests feel free to DM me', 'Have you ever heard about SICK Ranger 3 camera, or AT C6 or similar GigE with line extraction on board? Or area scan like Teledyne Dalsa Genie Nano or Basler Pylon?\n\nThe are many industrial products that give you only images to process, you don’t have to use Cognex or Keyence if you don’t need their software.', 'Plenty of basler machine vision cameras for less than $500']",2,6,0.75,Commercial ,1750402575.0
1lfvmyo,computervision,"Best Face Recognition Model in 2025? Also, How to Build One from Scratch for Industry-Grade Use?","I'm working on a project that involves **face recognition at an industry level** (think large-scale verification, security, access control, or personalization). I’d appreciate any insights from people who’ve worked with or deployed FR systems recently.","['It\'s always funny to think about how over-saturated the ML labour market is with people that want to work in the field and how posts like these come up time to time where either it\'s completely made up or else someone with apparently zero domain knowledge has been hired to do something they aren\'t remotely qualified to do, not just because they don\'t know exactly what to do but because they think it will be productive to ask reddit to do their research for them and put so little effort into the request.\n\nIt just goes to show that (1) even though the labour market is brimming with ""candidates"" that a lot of them really do not have the hard or soft skills for the job and (2) hiring is really hard, specifically filtering out people that can say all of the right stuff in interviews but that lack the requisite experience, technical skills, and problem solving / research skills.', 'lol', 'I hear Haar-cascade is making waves at CVPR', 'First, look at a lot of faces, then think real hard about what makes em ""facey,"" then implement a C algorithm that figures out if it\'s facey, write python bindings, segfault, and enroll in grad school.', 'Viola Jones', 'if we talk only about the quality of recognition, here is a rating for you https://pages.nist.gov/frvt/html/frvt11.html. \n\nonly it will not help if you really want to build a normally working system. i have been working for more than 10 years in companies from the top NIST rating. \n\nevery year we now implement hundreds of projects around the world. and how everything is sometimes neglected...', 'Consistent lighting(day/night), all ethnicities, always looking straight ahead, face masks allowed etc ??', ""I wrote the server for one of the industry leading FR enterprise systems, but I did not do the FR model. The guy that wrote the FR model is probably the most extreme technology developer I have ever worked with, and I have worked with several high profile famous developers. The FR model developer is a PhD in Statistics, and has his own everything, where his work predates tensors, cuda, and the entire ecosystem. He lives and breathes in C/C++/Assembly, he counts op codes, he knows exactly what every byte in his entire system is doing at every moment. I was working with him in '06 using machine learning with all our own terminology because at the time neural nets and machine learning was not recognized yet. \n\nThe FR models he creates are after 30 years of experience, where before doing facial recognition he was doing in-womb 3D reconstruction of babies for identification of deformities. The guy is the smartest most comprehensive developer I have ever encountered. He's also a bear to work with, so I no longer do. \n\nBut his work is undeniably globally leading. I say from experience, don't even bother trying to beat this guy, just use his software: www.cyberextruder.com  All the 3-lettered spook agencies do...""]","['It\'s always funny to think about how over-saturated the ML labour market is with people that want to work in the field and how posts like these come up time to time where either it\'s completely made up or else someone with apparently zero domain knowledge has been hired to do something they aren\'t remotely qualified to do, not just because they don\'t know exactly what to do but because they think it will be productive to ask reddit to do their research for them and put so little effort into the request.\n\nIt just goes to show that (1) even though the labour market is brimming with ""candidates"" that a lot of them really do not have the hard or soft skills for the job and (2) hiring is really hard, specifically filtering out people that can say all of the right stuff in interviews but that lack the requisite experience, technical skills, and problem solving / research skills.', 'lol', 'I hear Haar-cascade is making waves at CVPR', 'First, look at a lot of faces, then think real hard about what makes em ""facey,"" then implement a C algorithm that figures out if it\'s facey, write python bindings, segfault, and enroll in grad school.', 'Viola Jones']",15,15,0.69,Discussion ,1750395786.0
1lfuuhm,computervision,Is there a survey on object detection for best of CNN vs transformers models,"I am really keen to know which models are best for object detection in current day.

Cnn or transformers.

Based on multiple factors like efficiency, accuracy among others.","['Just check paperswithcode.', 'Just add ""survey"" with object detection papers and hit search. Or paper with code']","['Just check paperswithcode.', 'Just add ""survey"" with object detection papers and hit search. Or paper with code']",0,3,0.5,Help: Theory ,1750393026.0
1lfqig4,computervision,Web-SSL: Scaling Language Free Visual Representation,"Web-SSL: Scaling Language Free Visual Representation

[https://debuggercafe.com/web-ssl-scaling-language-free-visual-representation/](https://debuggercafe.com/web-ssl-scaling-language-free-visual-representation/)

For more than two years now, vision encoders with language representation learning have been the go-to models for multimodal modeling. These include the CLIP family of models: OpenAI CLIP, OpenCLIP, and MetaCLIP. The reason is the belief that language representation, while training vision encoders, leads to better multimodality in VLMs. In these terms, SSL (Self Supervised Learning) models like DINOv2 lag behind. However, a methodology, **Web-SSL**, trains DINOv2 models on web scale data to create **Web-DINO** models without language supervision, surpassing CLIP models.

https://preview.redd.it/7kfxu6g58z7f1.png?width=1000&format=png&auto=webp&s=4ad6d556c88f407ec4268ccb22262208649d5005

",[],[],10,0,0.92,Showcase,1750379301.0
1lfhzvy,computervision,this is built in computer vision techniques??,,[],[],0,0,0.15,Discussion ,1750357183.0
1lfh76l,computervision,t-SNE Explained,"Hi there,

I've created a video [here](https://youtu.be/b-AvYLqLWd0) where I break down t-distributed stochastic neighbor embedding (or t-SNE in short), a widely-used non-linear approach to dimensionality reduction.

I hope it may be of use to some of you out there. Feedback is more than welcomed! :)","['Very nice!!', 'what is its relevance in the field of Computer Vision?Would love to know about it', ""Thanks for the video!\n\nI've been very interested in dimensionality reduction recently.  I wonder how t-SNE compares with Principal Component Analysis. \n\nPCA is my go-to technique right now for most things, including squashing down data from higher dimensions into 2D, but I was wondering if t-SNE could be a better alternative.""]","['Very nice!!', 'what is its relevance in the field of Computer Vision?Would love to know about it', ""Thanks for the video!\n\nI've been very interested in dimensionality reduction recently.  I wonder how t-SNE compares with Principal Component Analysis. \n\nPCA is my go-to technique right now for most things, including squashing down data from higher dimensions into 2D, but I was wondering if t-SNE could be a better alternative.""]",11,6,0.79,Showcase,1750355325.0
1lffqvq,computervision,.engine model way faster when created via Ultralytics compared to trtexec/TensorRT,"Hey everyone.


Got a yolov12 .pt model which I try to convert to .engine to make the process faster via 5090 GPU.

If I convert it in Python with Ultralytics then it works great and is fast. However I only can go up to batchsize 139 because then my VRAM is completely used during conversion.

When I first convert the .pt to .onnx and then use trtexec or TensorRT in Python then I can go way higher with the batchsize until my VRAM is completely used. For example I converted with a batchsize of 288.

Both work fine HOWEVER no matter which batchsize, the model created from Ultralytics is 2.5x faster.

I have read that Ultralytics does some optimizations during conversion, how can I achieve the same speed with trtexec/TensorRT?

Thank you very much!","['FP16 and depends a lot of initial version of onnx and on tensorrt version. Use onnx opset >=11\nFor ""ultralytics optimization"", it s just about the preprocessing and postprocessing phases, not the inference with tensorrt itself.\nUse albumentations and libtorch for preprocessing, FP16, min ONNX op >=11 and you ll achieve similar results.', 'You didn’t mention running any onnx simplification so I would assume you missed that step. When PyTorch exports to onnx it in includes many layers and operations which are non-essential. If you use onnxsim or onnxslim (ultralytics uses this) you can shrink the model and thus get optimization. \n\nBehind the scenes ultralytics does this slimming step and compiles the engine with FP16. Additionally, if you are using NMS built into the engine you may notice higher latency. Even with built engines (last I read/contributed) they run NMS with torchvision after engine execution. Thus, their published or reported GPU time values may be lower than a true end to end engine.', ""You're welcome my friend :)\n\nAll our export source code is in the Ultralytics repo at [https://github.com/ultralytics/ultralytics/](https://github.com/ultralytics/ultralytics/)"", 'Can you post the trtexec command you’re running?']","['FP16 and depends a lot of initial version of onnx and on tensorrt version. Use onnx opset >=11\nFor ""ultralytics optimization"", it s just about the preprocessing and postprocessing phases, not the inference with tensorrt itself.\nUse albumentations and libtorch for preprocessing, FP16, min ONNX op >=11 and you ll achieve similar results.', 'You didn’t mention running any onnx simplification so I would assume you missed that step. When PyTorch exports to onnx it in includes many layers and operations which are non-essential. If you use onnxsim or onnxslim (ultralytics uses this) you can shrink the model and thus get optimization. \n\nBehind the scenes ultralytics does this slimming step and compiles the engine with FP16. Additionally, if you are using NMS built into the engine you may notice higher latency. Even with built engines (last I read/contributed) they run NMS with torchvision after engine execution. Thus, their published or reported GPU time values may be lower than a true end to end engine.', ""You're welcome my friend :)\n\nAll our export source code is in the Ultralytics repo at [https://github.com/ultralytics/ultralytics/](https://github.com/ultralytics/ultralytics/)"", 'Can you post the trtexec command you’re running?']",5,4,0.86,Help: Project,1750351926.0
1lfezq1,computervision,How To Actually Fine-Tune MobileNetV2 | Classify 9 Fish Species [project],"https://preview.redd.it/vo7p8mogtw7f1.png?width=1280&format=png&auto=webp&s=369a6a3b7a5260c3ecbf7edea48428185947af5e

🎣 **Classify Fish Images Using MobileNetV2 & TensorFlow** 🧠



In this hands-on video, I’ll show you how I built a deep learning model that can **classify 9 different species of fish** using **MobileNetV2** and **TensorFlow 2.10** — all trained on a real Kaggle dataset!
From dataset splitting to live predictions with OpenCV, this tutorial covers the entire **image classification pipeline** step-by-step.

 

🚀 **What you’ll learn:**

* How to preprocess & split image datasets
* How to use ImageDataGenerator for clean input pipelines
* How to customize MobileNetV2 for your own dataset
* How to freeze layers, fine-tune, and save your model
* How to run predictions with OpenCV overlays!

 

You can find link for the code in the blog: [https://eranfeit.net/how-to-actually-fine-tune-mobilenetv2-classify-9-fish-species/](https://eranfeit.net/how-to-actually-fine-tune-mobilenetv2-classify-9-fish-species/)

 

You can find more tutorials, and join my newsletter here : [https://eranfeit.net/](https://eranfeit.net/)

 

**👉 Watch the full tutorial here**: [**https://youtu.be/9FMVlhOGDoo**](https://youtu.be/9FMVlhOGDoo)

 

 

Enjoy

Eran",[],[],0,0,0.5,Showcase,1750350102.0
1lf95ei,computervision,cv.Videocapture(0) does not work on raspberry pi camera module 2,I am trying to learn computer vision on a raspberry pi with opencv and a raspberry pi 4/5 and a raspberry pi camera module2 ( like this https://www.raspberrypi.com/products/camera-module-v2/) but whatever tutorial i do or find i still get the same error that it cannot read frame. but if wanna see a image or a or a terminal command to test a image that works but if i wanna use cv.Videocapture(0) function in c++ or python it does not work.Can anyone help?,"['you need to use picamera2 library to capture , cv2.Videocapture(0) is not compatable with picamera', 'Alternatively to using picamera2 that u/sethumadhav24 suggested, you can use Gstreamer backend in OpenCV to read from Camera Module 2/3/4. It\'s a headache though.\n\nYou need to compile libcamerasrc from https://github.com/raspberrypi/libcamera. Then use smth like cv2.VideoCapture(""libcamerasrc ! video/x-raw, width=AAAAA, height=BBBBB, framerate=CCCCC/1, format=DDDDD ! videoconvert ! video/x-raw,format=(string)BGR ! appsink max-buffers=5"", cv2.CAP_GSTREAMER).\n\nFor the above to work your OpenCV should be built with Gstreamer support though, which it is likely not the case for Pi from apt package. So you will also have to build OpenCV from source with -DWITH_GSTREAMER=ON.', 'Search the camera from the terminal and see what its descriptor is(the path to it)\n\n\nSearch command in google', 'i got picamera2 working with venv. but does anyone know how to setup  open cv with libcamera and cmake?\n\ni look were i could but could not find a good example', ""Oh 1 more question does anybody know if cv.videocapture(0) works with this usb camera? [https://www.tinytronics.nl/nl/sensoren/optisch/camera's-en-scanners/dfrobot-0.3mp-usb-camera-voor-raspberry-pi-en-nvidia-jetson-nano](https://www.tinytronics.nl/nl/sensoren/optisch/camera's-en-scanners/dfrobot-0.3mp-usb-camera-voor-raspberry-pi-en-nvidia-jetson-nano)""]","['you need to use picamera2 library to capture , cv2.Videocapture(0) is not compatable with picamera', 'Alternatively to using picamera2 that u/sethumadhav24 suggested, you can use Gstreamer backend in OpenCV to read from Camera Module 2/3/4. It\'s a headache though.\n\nYou need to compile libcamerasrc from https://github.com/raspberrypi/libcamera. Then use smth like cv2.VideoCapture(""libcamerasrc ! video/x-raw, width=AAAAA, height=BBBBB, framerate=CCCCC/1, format=DDDDD ! videoconvert ! video/x-raw,format=(string)BGR ! appsink max-buffers=5"", cv2.CAP_GSTREAMER).\n\nFor the above to work your OpenCV should be built with Gstreamer support though, which it is likely not the case for Pi from apt package. So you will also have to build OpenCV from source with -DWITH_GSTREAMER=ON.', 'Search the camera from the terminal and see what its descriptor is(the path to it)\n\n\nSearch command in google', 'i got picamera2 working with venv. but does anyone know how to setup  open cv with libcamera and cmake?\n\ni look were i could but could not find a good example', ""Oh 1 more question does anybody know if cv.videocapture(0) works with this usb camera? [https://www.tinytronics.nl/nl/sensoren/optisch/camera's-en-scanners/dfrobot-0.3mp-usb-camera-voor-raspberry-pi-en-nvidia-jetson-nano](https://www.tinytronics.nl/nl/sensoren/optisch/camera's-en-scanners/dfrobot-0.3mp-usb-camera-voor-raspberry-pi-en-nvidia-jetson-nano)""]",2,5,0.67,Help: Project,1750334875.0
1lf8m7p,computervision,Roboflow Auto Labelling/Annotation stuck,"So just before this, I annotated 40 images using the exact same class description and it completed pretty quickly. But now, with this new batch of 288 images, it’s been stuck like this for the past 15 minutes.
I even tried canceling the process once since earlier it got stuck around 24 images, but I just ended up losing credits and had to start all over again. :(",['Contact roboflow support.\xa0'],['Contact roboflow support.\xa0'],0,2,0.14,Help: Project,1750333158.0
1lf8hyy,computervision,Implementing a CNN from scratch,I built a CNN from scratch in C++ and Vulkan without any machine learning or math libraries. It was a lot of fun and I learned a lot. Here is my detailed write up. Hope it helps someone :),"[""That's actually super interesting! \n\nI did a similar project some time ago, but I coded a multilayer perceptron rather than a CNN. \n\nHow did you implement gradient descent? Did you do it like pytorch does and make your own computational graph with automatic differentiation or did you use finite differences for gradient approximation?\n\nLike\n\n```\nf'(w) = ( f(w + epsilon) - f(w - epsilon) ) / (2 * epsilon)\n```\n\nMy first implementation used finite differences, but I quickly found out that doing it this way was unreasonably slow. It's basically running 2 forward passes for each parameter just to find one gradient. Even running this on a GPU was... Not ideal. And I quickly pivoted to coding an autograd implementation like pytorch does."", 'It’s an impressive effort! Well done. The deep learning.ai courses do also cover this though they don’t use Vulkan. But choosing to do this and completing it is a real achievement']","[""That's actually super interesting! \n\nI did a similar project some time ago, but I coded a multilayer perceptron rather than a CNN. \n\nHow did you implement gradient descent? Did you do it like pytorch does and make your own computational graph with automatic differentiation or did you use finite differences for gradient approximation?\n\nLike\n\n```\nf'(w) = ( f(w + epsilon) - f(w - epsilon) ) / (2 * epsilon)\n```\n\nMy first implementation used finite differences, but I quickly found out that doing it this way was unreasonably slow. It's basically running 2 forward passes for each parameter just to find one gradient. Even running this on a GPU was... Not ideal. And I quickly pivoted to coding an autograd implementation like pytorch does."", 'It’s an impressive effort! Well done. The deep learning.ai courses do also cover this though they don’t use Vulkan. But choosing to do this and completing it is a real achievement']",14,5,0.85,Showcase,1750332768.0
1lf4tu3,computervision,How can I analyze a vision transformer trained to locate sub-images?,"I'm trying to build real intuition about how vision transformers work — not just by using state-of-the-art models, but by experimenting and analyzing what a given model is actually learning, and using that understanding to improve it.

As a starting point, I chose a ""simple"" task:

>

I know this task can be solved more efficiently with classical computer vision techniques, but I picked it because it's easy to generate data and to visually inspect how different training examples behave. I normalize everything to the unit square, and with a basic vision transformer, I can get an average position error of about 0.1 — better than random guessing, but still not great.

What I’m really interested in is:
**How do I analyze the model to understand what it's doing, and then improve it?**
For example, this task has some clear structure — shifting the sub-image slightly should shift the output accordingly. Is there a way to discover such patterns from the weights themselves?

More generally, what are some useful tools, techniques, or approaches to probe a vision transformer in this kind of setting? I can of course just play with the topology of the model and see what is best, but I hope for ways which give more insights into the learning process.
I’d appreciate any suggestions — whether visualizations, model inspection methods, training tricks, etc (also, doesn't have to be just for vision, and I have already seen Andrej's YouTube videos). I have a strong mathematical background, so I should be able to follow more technical ideas if needed.","["">shifting the sub-image slightly should shift the output accordingly\n\nThere is no reason for this! it's virtually true for CNNs because convolutions are translationally equivariant (they're not strictly equivariant because of borders, padding and strides), but the operations in a ViT do not guarantee that at all. It will still be kinda true in practice, but just kinda.\n\n>Is there a way to discover such patterns from the weights themselves?\n\nthe weights are independent on the inputs, so no. And for the reason stated above, I don't think it's trivial to find sub images in the token representation of a bigger image. Now there are older works on using log-polar Fourier transform for registration. They might be of interest to you."", ""I can't remember the paper, but I've seen visualizations of intermediate attention maps within a transformer network and they showed that different maps tended to represent different regions of the image. It was not a straight forward mapping, tiling the image nearly from token 1 to N, but they did suggest that the attention maps were each specializing in certain regions. So, you could presumably do a similar analysis and then try to attribute features via these specialization regions."", 'Try out explainable AI methods, one very popular method is grad cam.']","["">shifting the sub-image slightly should shift the output accordingly\n\nThere is no reason for this! it's virtually true for CNNs because convolutions are translationally equivariant (they're not strictly equivariant because of borders, padding and strides), but the operations in a ViT do not guarantee that at all. It will still be kinda true in practice, but just kinda.\n\n>Is there a way to discover such patterns from the weights themselves?\n\nthe weights are independent on the inputs, so no. And for the reason stated above, I don't think it's trivial to find sub images in the token representation of a bigger image. Now there are older works on using log-polar Fourier transform for registration. They might be of interest to you."", ""I can't remember the paper, but I've seen visualizations of intermediate attention maps within a transformer network and they showed that different maps tended to represent different regions of the image. It was not a straight forward mapping, tiling the image nearly from token 1 to N, but they did suggest that the attention maps were each specializing in certain regions. So, you could presumably do a similar analysis and then try to attribute features via these specialization regions."", 'Try out explainable AI methods, one very popular method is grad cam.']",2,6,1.0,Help: Project,1750318572.0
1leuxhs,computervision,NVIDIA's C-RADIOv3 model is pretty good for embeddings and feature maps,"RADIOv2.5 distills CLIP, DINO, and SAM into a single, resolution-robust vision encoder.

It solves the ""mode switching"" problem where previous models produced different feature types at different resolutions. Using multi-resolution training and teacher loss balancing, it maintains consistent performance from 256px to 1024px inputs. On benchmarks, RADIOv2.5-B beats DINOv2-g on ADE20k segmentation despite being 10x smaller.

One backbone that handles both dense tasks and VLM integration is the holy grail of practical CV.

Token compression is all you need!

This is done through a bipartite matching approach that preserves information where it matters.

Unlike pixel unshuffling that blindly reduces tokens, it identifies similar regions and selectively merges them. This intelligent compression improves TextVQA by 4.3 points compared to traditional methods, making it particularly strong for document understanding tasks. The approach is computationally efficient, applying only at the output layer rather than throughout the network.

Smart token merging is what unlocks high-resolution vision for LLMs.

Paper: https://arxiv.org/abs/2412.07679

Implementation in FiftyOne to get started: https://github.com/harpreetsahota204/NVLabs_CRADIOV3",[],[],62,0,0.96,Showcase,1750286640.0
1leqfjy,computervision,Is there an Ai tool that can automatically censor the same areas of text in different images?,"I have a set of files (mostly screenshots) and i need to censor specific areas in all of them, usually the same regions (but with slightly changing content, like names) I'm looking for an AI-powered solution that can detect those areas based on their position, pattern, or content, and automatically apply censorship (a black box) in batch.

The ideal tool would:

•	⁠detect and censor dynamic or semi-static text areas. -work in batch mode (on multiple files)
•	⁠require minimal to no manual labeling (or let me train a model if needed).

I am aware that there are some programs out there designed to do something similar (in +18 contexts) but i'm not sure they are exactly what i'm looking for.

I have a vague idea of using maybe an OCR + filtering for the text with the yolov8 model but im not quite sure how i would make it work tbh.

Any tips?

I'm open to low-code or python-based solutions as well.

Thanks in advance!","['\\> specific areas in all of them, usually the same regions  \ndifferent images and those regioins can always be found at the same coordinate, like X=50 and Y=87 and WIDTH=156 and HIGHT=96 ?  \nThat sounds like a simple ""computer vision"" task, to just draw an non-transparent overlay.\n\n\\> that can detect those areas based on their position, pattern, or content  \nor the areas are at different position?\n\nCan you maybe provide some ""dummy"" images with dummy symbols to show what is the dynamic part, how ""areas"" and ""content"" would look like and name some attributes to recognize them?', ""You don't even need AI for this. What format are your docs in?"", ""I have something similar already made, in case it's useful: [https://soyunomas.github.io/pequenos-proyectos/censuretext.html](https://soyunomas.github.io/pequenos-proyectos/censuretext.html)""]","['\\> specific areas in all of them, usually the same regions  \ndifferent images and those regioins can always be found at the same coordinate, like X=50 and Y=87 and WIDTH=156 and HIGHT=96 ?  \nThat sounds like a simple ""computer vision"" task, to just draw an non-transparent overlay.\n\n\\> that can detect those areas based on their position, pattern, or content  \nor the areas are at different position?\n\nCan you maybe provide some ""dummy"" images with dummy symbols to show what is the dynamic part, how ""areas"" and ""content"" would look like and name some attributes to recognize them?', ""You don't even need AI for this. What format are your docs in?"", ""I have something similar already made, in case it's useful: [https://soyunomas.github.io/pequenos-proyectos/censuretext.html](https://soyunomas.github.io/pequenos-proyectos/censuretext.html)""]",2,9,0.67,Help: Project,1750275462.0
1lensd1,computervision,What are some good resources for learning classical Computer Vision.,"Ok so I have experience working with deep learning side of computer vision made some projects & also working on a video segmentation project right now.
The one thing that I noticed after asking for review for my resume is that I lack classical Computer vision knowledge which is quite evident in my resume.
So I wanted to know what are some good resources for learning classical Computer Vision.
Like I found a playlist from Tubingen University:
https://youtube.com/playlist?list=PL05umP7R6ij35L2MHGzis8AEHz7mg381_&si=YykHRoJS81ONRSM9
Also, I would love if I can get some feedbacks from my resume because I am trying to find internships right now so any advice would be really helpful!!
","['Pattern recognition, Theodoridis. CV a modern approach, Forsyth Ponce.\n\nCV is too much ""NN user/tuner"", lacks wider and more fondamental points in maths/physics/CS/ML IMHO. But maybe education is what is missing.\n\nEg., wavelets/Fourier/patches, algorithmes, complexity, etc and classical CV/ML methods, kernels, optimisation, SVM, modern OOP, DI etc', 'What is “from this research paper”? Does your PDF link to said paper? Maybe mention the model name or paper name instead.', 'Try here :\n\nhttps://youtube.com/@eranfeit?si=nYijRFC9JBar6XvB', ""Pro tip: when you send people an image containing text, it makes you seem like a newb doofus. You probably don't want that.""]","['Pattern recognition, Theodoridis. CV a modern approach, Forsyth Ponce.\n\nCV is too much ""NN user/tuner"", lacks wider and more fondamental points in maths/physics/CS/ML IMHO. But maybe education is what is missing.\n\nEg., wavelets/Fourier/patches, algorithmes, complexity, etc and classical CV/ML methods, kernels, optimisation, SVM, modern OOP, DI etc', 'What is “from this research paper”? Does your PDF link to said paper? Maybe mention the model name or paper name instead.', 'Try here :\n\nhttps://youtube.com/@eranfeit?si=nYijRFC9JBar6XvB', ""Pro tip: when you send people an image containing text, it makes you seem like a newb doofus. You probably don't want that.""]",32,7,0.94,Discussion ,1750269253.0
1lemkr7,computervision,Landing lens for image labeling,"Hi , did anyone use Landing Lens for image annotation in real-time business case ?
If yes. , is it good for enterprise level to automate the annotation for images ? .

Apart from this , are there any better tools they support semantic and instance segmentation , bounding box etc. and automatic annotation support for production level.  I have around 30GB of images and need to annotate it all .","['Roboflow is my go to labeling platform, it’s fast, responsive, handles multiple project types and has tools like auto label and box prompting to speed up labeling efforts. Even with great tools the unfortunate reality is that it’s still a manual process and takes time.\xa0', ""Also is there any possibility of AWS backend integration ? I'm using the python backend with AWS sagemaker"", 'You\'ve already received some great recommendations, but it would be remiss of me to not throw Intel Geti into the mix! (being the product owner)  \n[https://github.com/open-edge-platform/geti](https://github.com/open-edge-platform/geti) \\- repo  \n[https://docs.geti.intel.com/](https://docs.geti.intel.com/) \\- installer\n\n\\- Manual annotation, semi-automated or automated via visual prompting.  \n\\- Free! zero cost for commercial usage  \n\\- Offline, on-prem for private data  \n\\- Fully integrated training platform with Apache 2.0 models in OpenVINO, PyTorch, ONNX format  \n\\- Supports Nvidia and Intel GPUs for training  \n\\- Automatic model optimisation of OpenVINO models to FP16 and INT8 optimised for inference on Intel HW\n\n""*is it good for enterprise level to automate the annotation for images*"" - yes, Geti is used across our fabs :)\n\nAny questions just give me a shout!', 'Haven’t used lending lens but used label studio and CVAT for enterprise level use case. Both have pros and cons but label studio with ML backend which is fully open sourced (and also you can run your custom ML backend there) is the clear winner in my opinion.']","['Roboflow is my go to labeling platform, it’s fast, responsive, handles multiple project types and has tools like auto label and box prompting to speed up labeling efforts. Even with great tools the unfortunate reality is that it’s still a manual process and takes time.\xa0', ""Also is there any possibility of AWS backend integration ? I'm using the python backend with AWS sagemaker"", 'You\'ve already received some great recommendations, but it would be remiss of me to not throw Intel Geti into the mix! (being the product owner)  \n[https://github.com/open-edge-platform/geti](https://github.com/open-edge-platform/geti) \\- repo  \n[https://docs.geti.intel.com/](https://docs.geti.intel.com/) \\- installer\n\n\\- Manual annotation, semi-automated or automated via visual prompting.  \n\\- Free! zero cost for commercial usage  \n\\- Offline, on-prem for private data  \n\\- Fully integrated training platform with Apache 2.0 models in OpenVINO, PyTorch, ONNX format  \n\\- Supports Nvidia and Intel GPUs for training  \n\\- Automatic model optimisation of OpenVINO models to FP16 and INT8 optimised for inference on Intel HW\n\n""*is it good for enterprise level to automate the annotation for images*"" - yes, Geti is used across our fabs :)\n\nAny questions just give me a shout!', 'Haven’t used lending lens but used label studio and CVAT for enterprise level use case. Both have pros and cons but label studio with ML backend which is fully open sourced (and also you can run your custom ML backend there) is the clear winner in my opinion.']",1,12,0.67,Help: Project,1750266442.0
1lelkct,computervision,Learned keypoints vs Superpoint for 6 Dof pose,"Hi all,

I am working on a personal project which initially uses a SLAM based feature matching to find the 6 DoF camera pose for sports video footages.

I am thinking of using a learned keypoints model, that has a set number of keypoints that describes the playing field/arena and use them for matching.

Is this a good idea ? What should I do further once I have the keypoint model (thinking of a YOLO pose model) trained and ready to predict the 2D keypoints ?
",[],[],1,0,1.0,Help: Project,1750264108.0
1lek7rg,computervision,Looking for the most accurate face recognition model,"Hi, I'm looking for the most accurate face recognition model that I can use in an on-premise environment. We yave no problems buying a license for a solution if it is accurate enough and can be used without internet connection.

Can someone please guide me to some models or solutions that are considered on the moat accurate ones as of 2025.

Thanks a lot in advance ","['Have you already tried pre-trained face recognition models? Which have you tried, what made them look like ""inaccurate""?\n\nHave you (re-)trained a new model or fine-tuned existing models to ""taylor"" them for your ""specific faces"", if you want to recognize (or (re-)identify?) specific faces only to e.g. unlock a door for your colleagues only?\n\nWhat would your environment look like, what could make your used model being inaccurate? Lightning? Fast movement, people wearing masks, sun-glasses, hats, shaddows?  \nCould it maybe improved programmatically, like filtering, weighting, applying NMS (see e.g. [https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/) ), averaging, tracking?\n\nHave a look at this example (from 2023, there might be updated versions of the code, of OpenVINO and of the models in the meantime):\n\n[https://docs.openvino.ai/2023.3/omz\\_demos\\_face\\_recognition\\_demo\\_python.html](https://docs.openvino.ai/2023.3/omz_demos_face_recognition_demo_python.html)\n\nListing quite a few supported models (which could be different for this demo with a 2025 version of OpenVINO):\n\n>Supported Models\n\n>   face-detection-adas-0001\n\n>   face-detection-retail-0004\n\n>   face-recognition-resnet100-arcface-onnx\n\n>   face-reidentification-retail-0095\n\n>   facenet-20180408-102900\n\n>   landmarks-regression-retail-0009\n\n>   Sphereface\n\nHow would faces be recognized in your environment and your use-case with these models?', ""I doubt that you really need the most accurate face recognition model, but if you're sure you do, this is the industry standard benchmark: https://pages.nist.gov/frvt/html/frvt11.html (Verification) or https://pages.nist.gov/frvt/html/frvt1N.html (Identification)"", ""You can try [plumerai.com](https://plumerai.com) I had decent performance with their solution. I haven't tried other commercial models but the company is dedicated to people and face detection/identification.""]","['Have you already tried pre-trained face recognition models? Which have you tried, what made them look like ""inaccurate""?\n\nHave you (re-)trained a new model or fine-tuned existing models to ""taylor"" them for your ""specific faces"", if you want to recognize (or (re-)identify?) specific faces only to e.g. unlock a door for your colleagues only?\n\nWhat would your environment look like, what could make your used model being inaccurate? Lightning? Fast movement, people wearing masks, sun-glasses, hats, shaddows?  \nCould it maybe improved programmatically, like filtering, weighting, applying NMS (see e.g. [https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/) ), averaging, tracking?\n\nHave a look at this example (from 2023, there might be updated versions of the code, of OpenVINO and of the models in the meantime):\n\n[https://docs.openvino.ai/2023.3/omz\\_demos\\_face\\_recognition\\_demo\\_python.html](https://docs.openvino.ai/2023.3/omz_demos_face_recognition_demo_python.html)\n\nListing quite a few supported models (which could be different for this demo with a 2025 version of OpenVINO):\n\n>Supported Models\n\n>   face-detection-adas-0001\n\n>   face-detection-retail-0004\n\n>   face-recognition-resnet100-arcface-onnx\n\n>   face-reidentification-retail-0095\n\n>   facenet-20180408-102900\n\n>   landmarks-regression-retail-0009\n\n>   Sphereface\n\nHow would faces be recognized in your environment and your use-case with these models?', ""I doubt that you really need the most accurate face recognition model, but if you're sure you do, this is the industry standard benchmark: https://pages.nist.gov/frvt/html/frvt11.html (Verification) or https://pages.nist.gov/frvt/html/frvt1N.html (Identification)"", ""You can try [plumerai.com](https://plumerai.com) I had decent performance with their solution. I haven't tried other commercial models but the company is dedicated to people and face detection/identification.""]",1,11,0.57,Help: Project,1750260915.0
1lek6cw,computervision,Computer vision for Football/Soccer: Need help with camera setup.,"**Context**
I am looking for advice and help on selecting cameras for my Football CV Project. The match is going to be played on a local Futsal ground. The idea is to track players and the ball to get useful insights.

I plan on setting up 4 cameras, one on each corner of the ground. Using stereo triangulation (or other viable methods) I plan on tracking the ball.

**Problem:**

I am having trouble *selecting* the 4 cameras due to constraints such as power delivery and data transfer to my laptop. My laptop will be \~30m (100ft) away. Here are the constraints for the camera:

1. Output: 1080p 60fps (To track fast moving ball)
2. Angle: FOV (>100 deg) (To see the entire field, with edges)
3. Data streaming over 100ft
4. Power delivery to camera (Battery may die over the duration of the game)


Please provide suggestions on what type of camera setup is suitable for this. Feel free to tell me if the constraints I have decided are wrong, based on the context I have provided.","['An Initial solution that has come to my mind is using Action cameras in the webcam mode, such as cameras from Akaso and GoPro. However, from reading posts online and some YouTube videos , I gathered that the  webcam outputs for action cameras are limited to 30 fps.', ""Lol this sounds like what I've been doing/saying here passed through an AI chatbot.\n\nIf you're not AI, DM me.""]","['An Initial solution that has come to my mind is using Action cameras in the webcam mode, such as cameras from Akaso and GoPro. However, from reading posts online and some YouTube videos , I gathered that the  webcam outputs for action cameras are limited to 30 fps.', ""Lol this sounds like what I've been doing/saying here passed through an AI chatbot.\n\nIf you're not AI, DM me.""]",2,4,0.67,Help: Project,1750260824.0
1lek44w,computervision,Question about the SimSiam loss in Multi-Resolution Pathology-Language Pre-training models,"I was reading this paper [Multi-Resolution Pathology-Language Pre-training](https://arxiv.org/abs/2504.18856), and they define their SimSiam loss as:

https://preview.redd.it/v113i59pgp7f1.png?width=724&format=png&auto=webp&s=469233f9592b640eb1f8cf7a3cfb45fc3da113fb

But shouldn’t it actually be:

1/2(L(hp, sg(gc)) + L(hc, sg(gp)))

Like, the standard SimSiam loss compares the prediction from one view with the stop-gradient of the other view’s projection, not the other way around, right? The way they wrote it looks like they swapped predictions and projections in the second term.

Could someone help clarify this issue?",[],[],2,0,1.0,Discussion ,1750260678.0
1lejoe5,computervision,"[Help] Issues with LabelMe Annotations using ""AI Masks""","Hi everyone,

I'm running into some issues using the **latest version of LabelMe** with the **""AI-masks""** feature for automatic segmentation.

# What I did:

* I used the **AI-masks** functionality to annotate images with **binary masks**.
* The annotations are saved in the `.json` file with `""shape_type"": ""mask""` and a `""mask""` field containing the mask image encoded in **base64**.
* Instead of using polygons (`""points""`), each shape now includes an embedded **mask image**.

# Where the problems arise:

1. **Common tools and scripts don't support this format**:
   * Scripts like [`labelme2coco.py`](http://labelme2coco.py) throw errors such as: ValueError: shape\_type='mask' is not supported
   * These tools typically assume segmentation annotations are polygons (`""shape_type"": ""polygon""` with `""points""`).
2. **Incompatibility with standard frameworks**:
   * Tools like **COCO, VOC, Detectron2, Roboflow**, etc., expect polygons or masks in standard formats like **RLE** or structured bitmaps — **not base64-encoded images embedded in JSON**.
3. **Lack of interoperability**:
   * While binary masks are often more precise for segmentation, the **lack of direct support** makes them hard to integrate into common pipelines without preprocessing or conversion.

# Questions:

* Has anyone dealt with this and found a **practical way to convert ""shape\_type"": ""mask"" annotations to polygons or other compatible formats (COCO/VOC/RLE)?**
* Are there any updated scripts or libraries that **support this newer LabelMe mask format directly**?
* Any recommended workflows to make use of these AI-generated masks **without losing compatibility** with training frameworks?

Any guidance, suggestions, or useful links would be greatly appreciated!

https://preview.redd.it/41jgli7wcp7f1.png?width=1392&format=png&auto=webp&s=6b9efea98322c32d41730d6da652c4f3b7f0b911

",[],[],2,0,0.75,Help: Project,1750259644.0
1leixey,computervision,How do you use zero-shot models/VLMs in your work other than labelling/retrieval?,"I’m interested in hearing about the technical details on how have you used these models’ out of the box image understanding capabilities in serious projects. If you’ve fine-tuned them with minimal data for a custom use case, that’ll be interesting to hear too.

I have personally used them for speeding up the data labelling workflows, by sorting them out to custom classes and using textual prompts to search the datasets. ","['Agreed. We generally use these models for speeding up the data labelling. The throughput (speed) is very important aspect real vision applications so we try to avoid bigger models for productions.', 'Aside from data labelling, I sometimes incorporate them into quality control processes.\xa0\n\nI mostly process video using my own custom models (like yolo) and will check every 100th frame using a VLM to help understand if data drift is occurring. A specific example is that the VLM is expected to always respond “Yes” to the prompt “Does this photo depict an outdoor scene in broad daylight?”. If it says anything other than Yes then I log the image and do some additional checks to make sure nothing is wrong with the cameras. \xa0\n\nAnother thing I often do is feed a VLM closeup crops of objects detected by my own model and ask it if it see’s a certain thing. Say I’m detecting dog breeds, I’ll ask the VLM “Is this a photo of a real dog”? Helps to catch errors like my model detecting a stuffed animal when I only want it to detect real dogs.\xa0', ""We use VLMs to get proof of concepts going and then sample the production data from those projects for training faster/smaller purpose built models if we need real-time or don't want to use big GPUs. If an application only run inference every few seconds, we sometimes leave the VLM as the solution because it's not worth building a custom model."", 'We do. It makes sense if you have a pipeline that only sends a small number of images to the VLM.', 'I used it recently as an OCR model to extract the names from CVPR badges\n\n[https://www.linkedin.com/posts/droliverhamilton\\_cvpr-activity-7339421958683389954-7ZIu](https://www.linkedin.com/posts/droliverhamilton_cvpr-activity-7339421958683389954-7ZIu)']","['Aside from data labelling, I sometimes incorporate them into quality control processes.\xa0\n\nI mostly process video using my own custom models (like yolo) and will check every 100th frame using a VLM to help understand if data drift is occurring. A specific example is that the VLM is expected to always respond “Yes” to the prompt “Does this photo depict an outdoor scene in broad daylight?”. If it says anything other than Yes then I log the image and do some additional checks to make sure nothing is wrong with the cameras. \xa0\n\nAnother thing I often do is feed a VLM closeup crops of objects detected by my own model and ask it if it see’s a certain thing. Say I’m detecting dog breeds, I’ll ask the VLM “Is this a photo of a real dog”? Helps to catch errors like my model detecting a stuffed animal when I only want it to detect real dogs.\xa0', 'Agreed. We generally use these models for speeding up the data labelling. The throughput (speed) is very important aspect real vision applications so we try to avoid bigger models for productions.', ""We use VLMs to get proof of concepts going and then sample the production data from those projects for training faster/smaller purpose built models if we need real-time or don't want to use big GPUs. If an application only run inference every few seconds, we sometimes leave the VLM as the solution because it's not worth building a custom model."", 'We do. It makes sense if you have a pipeline that only sends a small number of images to the VLM.', 'I used it recently as an OCR model to extract the names from CVPR badges\n\n[https://www.linkedin.com/posts/droliverhamilton\\_cvpr-activity-7339421958683389954-7ZIu](https://www.linkedin.com/posts/droliverhamilton_cvpr-activity-7339421958683389954-7ZIu)']",11,11,0.92,Discussion ,1750257848.0
1leib6s,computervision,dinotool: CLI tool for extracting DINOv2/CLIP/SigLIP2 global and local features for images and videos.,"Hi r/computervision,

I have made some updates to [dinotool](https://github.com/mikkoim/dinotool), which is a python command line tool that lets you extract and visualize global and local DINOv2 features from images and videos. I have just added the possibility of extracting also CLIP/SigLIP2 features, which have shown to be useful in retrieval and few-shot tasks.

I hope this tool can be useful for folks in fields where the user is interested in image embeddings for downstream tasks. I have found it to be a useful tool for generating features for k-nn classification and image retrieval.

If you are on a linux system / WSL and have `uv` and `ffmpeg` installed you can try it out simply by running

    uvx dinotool my/image.jpg -o output.jpg

which produces a side-by-side view of the PCA transformed feature vectors you might have seen in the DINO demos. Installation via `pip install dinotool` is also of course possible. (I noticed uvx might not work on all systems due to xformers problems, but normal venv/pip install should work in this case.

Feature export is supported for local patch-level features (in `.zarr` and `parquet` format)

    dinotool my_video.mp4 -o out.mp4 --save-features flat

saves features to a parquet file, with each row being a feature patch. For videos the output is a partitioned parquet directory, which makes processing large videos scalable.

The new functionality that I recently added is the possibility of processing directories with images of varying sizes, in this example with SigLIP2 features

    dinotool my_folder -o features --save-features 'frame' --model-name siglip2

Which produces a parquet file with the global feature vector for each image. You can also process local patch feature in a similar way. If you want batch processing, all images have to be resized to a predefined size via `--input-size W H.`

Currently the feature export modes are `frame`, which saves one global vector per frame/image, `flat`, which saves a table of patch-level features, and `full` that saves a `.zarr` data structure with the 2D spatial structure.

I would love to have anyone to try it out and to suggest features to make it even more useful.","['Very cool stuff. Have you looked at FeatUp? It might be easy to add their weights\n\nhttps://github.com/mhamilton723/FeatUp', 'Cool stuff']","['Very cool stuff. Have you looked at FeatUp? It might be easy to add their weights\n\nhttps://github.com/mhamilton723/FeatUp', 'Cool stuff']",75,3,1.0,Showcase,1750256332.0
1le1k7z,computervision,Saw a cool dataset at CVPR - UnCommon Objects in 3D,"You can download the dataset from HF here: https://huggingface.co/datasets/Voxel51/uco3d

The code to parse it in case you want to try it on a different subset: https://github.com/harpreetsahota204/uc03d_to_fiftyone

Note: This dataset doesn't include camera intrinsics or extrinsics, so the point clouds may not be perfectly aligned with the RGB videos.

","['Im using this to learn point cloud downsampling and the point clouds are pretty good all things considered. Most are 100,000+ points and they have color.']","['Im using this to learn point cloud downsampling and the point clouds are pretty good all things considered. Most are 100,000+ points and they have color.']",26,2,0.97,Showcase,1750201128.0
1ldyl9s,computervision,Autonomous Drone Tracks Target with AI Software | Computer Vision in Action,,"['Umm this is not AI… this is classic cv. You literally clicked on where you wanted the drone to go. That’s cool and good job but no where in here is AI…', 'What do you mean by AI?', 'Autonomous? You are loading the VideoCapture mp4… it’s a prerecorded video you controlled the drone', 'Looks cool but why are you selling it? Is that allowed on this subreddit?', ""he's back...\n\nit's just a tracking demo"", 'Now attach a gun to it and go play!', 'It’s got AI trust me', 'Me and my team actually used a pre-trained model and then just fed it a few images which helped it identify things.\n\nAlso I think the flying part is autonomous and done with the help of AI', 'Looking good, what are you using for tracking?', 'source video: [https://youtu.be/vM8n4BM4c5Q](https://youtu.be/vM8n4BM4c5Q)']","['Umm this is not AI… this is classic cv. You literally clicked on where you wanted the drone to go. That’s cool and good job but no where in here is AI…', 'What do you mean by AI?', 'Autonomous? You are loading the VideoCapture mp4… it’s a prerecorded video you controlled the drone', ""he's back...\n\nit's just a tracking demo"", 'Looks cool but why are you selling it? Is that allowed on this subreddit?']",7,14,0.56,Showcase,1750193631.0
1ldvhjn,computervision,How much code do you write by yourself at workplace?,"This is a broad and vague question especially for those who are professional CV engineers. These days I am noticing that my brain has kind of become forgetful. If you ask me to write any function, I would know math and logic behind it, but I can't write it from scratch (like college days). So these days I start with code generation from chatgpt and then tweak it accordingly. But I feel dumb doing this (like I am slowly becoming dumber and dumber and relying too much on LLM)
Can anyone relate? is there any better way to work especially in Computer Vision fields ?","['Well, I work like that, I think it’s more important knowing the math and logic behind than writing code these days, it enables us to write code faster and we can focus only on writing more complex stuff', 'A lot of code. Besides meetings and such, yes, writing a lot of code.\n\nHowever, at some point you will have a solid base of code for algorithms and ""design patterns"" you wrote for earlier tasks, you have templates, you created many helper methods, tools, libraries, maybe even a framework - and often you are not alone and colleagues (and even customers) are contributing to the code pase.\n\nWhat\'s the difference between copying from Stackoverflow or from a blog or tutorial or LLM? As long as you study the code snippets, debug them, adopt them, optimize them, integrate them (considering the license(s))?', 'i only use LLM\'s to get a rough idea of something if i have no idea how to do it, recently used it to get a samle code for using FFMPEG as a library. otherwise i wouldn\'t use code from LLM without fully vetting it. \n\ni also hate the loop of ""hey, this piece of code does not work, rewrite it"".', ""Using an LLM, specifically one trained to code, is fine. Personally it's given me time to do more research on the side, and thus I find more ways to better do my job. I iterate faster, I make mistakes faster, I learn faster. It's all about how you use it. If you want it to be a crutch, it will be. I use it like transportation. I *could* walk, but why would I want to do that when I have a car and can get to my destination in 5 minutes instead of 5 hours?"", 'I don’t see any problems using chatGPT to generate your code but you absolutely need to understand every single line it produces. It’s like having an assistant to do tedious work for you but you need to be able to follow and understand it at 100%. If shit hits the fan during deployment, you have a limited time to rewrite certain parts and you will be damn happy to deal with code that you understand well and can quickly locate the error.', 'actually writing code is the majority of my workday\n\nwell, i guess reading and researching is more, but its usually intertwined with the coding', 'In a same boat here. \n\nBut in contrast I like the code assistant since it automate code which by the way very repetitive like simple class and function definition and some for loops and all. \n\nIn a positive way, now I can test and train vision models very quickly. With these code assistant I am evaluating and implementing many models as such fast pace.', 'In my view, it’s not necessary to understand every line of code in detail. What truly matters is the ability to scale, maintain, and upgrade your systems effectively. These are the key skills for success today. LLM excel at handling complexity in code and data, while your strength as a human lies in thinking creatively, solving problems, and driving innovation.', ""I was using Claude to hack together a demo using the realsense camera. It worked great, had it running in no time. But randomly my internet connection died for a couple of days. I thought it didn't matter I'll continue to hack around with the demo while it gets fixed... I was totally lost. I had learned nothing."", 'If the purpose is to get work done, and to have an impact on your work, then I wouldn’t worry TOO much about being forgetful.\n\nThat aside, if you apply for a new job, some interviewers will ask you to write some functions. It’s common enough for interviewers to mess up this part of the process.\n\nIf someone insists that you have a live coding exercise to write some function you will never in your life be pressured to write on the spot, in a hurry, in some IDE in a browser, in some default light/dark mode you don’t use, etc., then consider walking away from such an interview. They don’t know how to interview and/or they’re looking for someone who will work as a replaceable cog. Or they only ask questions to answers they can answer well themselves. Big waste of everyone’s time.\n\nSome people can type accurate code in a plain vanilla IDE. They may or may not take risks. But good for them. That represents a fraction of coders. \n\nBut people working in R&D creating new tech that’s actually useful? Their coding style, personality, IDE requirements, manner of expressing themselves, etc., all vary considerably. Hiring two different people who been presented the exact same questions during their interviews is a failure in the hiring process.\n\nThat said, if you’re working in CV and relying on LLMs for some functions, you may need to partner with someone who loves coding as coding. I’ve met people like that, and they can be great work partners. \n\nThat aside, if I knew you were math and logic oriented, and if you were applying for a job that didn’t relying on being able to write code from scratch, then that pretty much just leaves work on hard vision problems as the only possible job left. And in that case I’d try to find out how much you know as soon in the process as possible.\n\nFor jobs like that, I would start (and have started) with questions like this:\n\nCan you explain the math and the programmatic “trick” that distinguishes an FFT from a DFT? (Getting the answer mostly right, or getting the core idea, is fine.)\n\nWhat are half a dozen (or more) ways to fit a plane through a cloud of 3D points? What does one have to look out for each?\n\nFor some classic problem in CV, and the current solutions that “everyone knows” are the right approaches, what’s the best performance one can expect for real-world uses? What do you think a system that can do that would be worth to a customer? How do you know?\n\nHow would you find the optimal solution in an 8-dimensional solution space? (Then I edit for someone to start talking, or to ask for clarification.)\n\nHow would you calculate the standard deviation of a data set that keeps growing, and without having to reprocess the whole set each time one new data point is added?\n\nWhat are important considerations for function names in an API?\n\nAnd questions like that. Sometimes a co versatile spins off for half an hour on one question. Sometimes it’s necessary to ask a simpler question, then see how that goes.\n\n—\n\nAnd, to wrap up: if you rely on LLMs too much, then the chance diminishes that you can think through and write new CV code that will be valuable for years to come. \n\nIf you use LLMs to quickly create prototypes, and if that’s hard for others to do as fast as you do, then that’s a good skill. But don’t count on it being unusual for long.']","['Well, I work like that, I think it’s more important knowing the math and logic behind than writing code these days, it enables us to write code faster and we can focus only on writing more complex stuff', 'A lot of code. Besides meetings and such, yes, writing a lot of code.\n\nHowever, at some point you will have a solid base of code for algorithms and ""design patterns"" you wrote for earlier tasks, you have templates, you created many helper methods, tools, libraries, maybe even a framework - and often you are not alone and colleagues (and even customers) are contributing to the code pase.\n\nWhat\'s the difference between copying from Stackoverflow or from a blog or tutorial or LLM? As long as you study the code snippets, debug them, adopt them, optimize them, integrate them (considering the license(s))?', 'i only use LLM\'s to get a rough idea of something if i have no idea how to do it, recently used it to get a samle code for using FFMPEG as a library. otherwise i wouldn\'t use code from LLM without fully vetting it. \n\ni also hate the loop of ""hey, this piece of code does not work, rewrite it"".', ""Using an LLM, specifically one trained to code, is fine. Personally it's given me time to do more research on the side, and thus I find more ways to better do my job. I iterate faster, I make mistakes faster, I learn faster. It's all about how you use it. If you want it to be a crutch, it will be. I use it like transportation. I *could* walk, but why would I want to do that when I have a car and can get to my destination in 5 minutes instead of 5 hours?"", 'I don’t see any problems using chatGPT to generate your code but you absolutely need to understand every single line it produces. It’s like having an assistant to do tedious work for you but you need to be able to follow and understand it at 100%. If shit hits the fan during deployment, you have a limited time to rewrite certain parts and you will be damn happy to deal with code that you understand well and can quickly locate the error.']",38,21,0.95,Discussion ,1750186360.0
1ldvdmx,computervision,Acne Detection model,"Hey guys!
I am planning to create an acne detection cum inpainting model. Till now I found only one dataset Acne04. The results though pretty accurate, fails to detect many edge cases. Though there's more data on the web, getting/creating the annotations is the most daunting part. Any suggestions or feedback in how to create a more accurate model?

Thank you.

-R","['Why a model is used? Sounds like something that classical computer vision techniques can solve.\n\n\nIn anycase, do some preprocess on the dataset and on the inference images so that edge cases look like the other non edge cases', ""if you don't want to annotate data, have you tried asking chatgpt if an image has acne or not?""]","['Why a model is used? Sounds like something that classical computer vision techniques can solve.\n\n\nIn anycase, do some preprocess on the dataset and on the inference images so that edge cases look like the other non edge cases', ""if you don't want to annotate data, have you tried asking chatgpt if an image has acne or not?""]",1,10,0.56,Help: Project,1750186114.0
1ldv5zg,computervision,V-JEPA 2 in transformers,"Hello folks 👋🏻 I'm Merve, I work at Hugging Face for everything vision!

Last week Meta released V-JEPA 2, their world video model, which comes with a transformers integration zero-day

the support is released with

\> fine-tuning script & notebook (on subset of UCF101)

\> four embedding models and four models fine-tuned on Diving48 and SSv2 dataset

\> FastRTC demo on V-JEPA2 SSv2

I will leave them in comments, wanted to open a discussion here as I'm curious if anyone's working with video embedding models 👀

https://reddit.com/link/1ldv5zg/video/20pxudk48j7f1/player

","['All models are here [https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6](https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6)\n\nTry the streaming demo on SSv2 checkpoint [https://huggingface.co/spaces/qubvel-hf/vjepa2-streaming-video-classification](https://huggingface.co/spaces/qubvel-hf/vjepa2-streaming-video-classification)\n\nWe made a fine-tuning notebook [https://colab.research.google.com/drive/16NWUReXTJBRhsN3umqznX4yoZt2I7VGc?usp=sharing](https://colab.research.google.com/drive/16NWUReXTJBRhsN3umqznX4yoZt2I7VGc?usp=sharing)', 'I want to know how to use this model for tasks like action recognition and localization. We have a dataset like AVA for this task.', ""Awesome - thank you for making this available! I never got around to hacking with the original VJEPA cuz it wasn't in transformers and I couldn't be bothered lol"", 'thanks for your work Merve! :)']","['All models are here [https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6](https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6)\n\nTry the streaming demo on SSv2 checkpoint [https://huggingface.co/spaces/qubvel-hf/vjepa2-streaming-video-classification](https://huggingface.co/spaces/qubvel-hf/vjepa2-streaming-video-classification)\n\nWe made a fine-tuning notebook [https://colab.research.google.com/drive/16NWUReXTJBRhsN3umqznX4yoZt2I7VGc?usp=sharing](https://colab.research.google.com/drive/16NWUReXTJBRhsN3umqznX4yoZt2I7VGc?usp=sharing)', 'I want to know how to use this model for tasks like action recognition and localization. We have a dataset like AVA for this task.', ""Awesome - thank you for making this available! I never got around to hacking with the original VJEPA cuz it wasn't in transformers and I couldn't be bothered lol"", 'thanks for your work Merve! :)']",35,7,0.96,Showcase,1750185617.0
1ldsgss,computervision,How to find Datasets?,"I am working on surface defect detection for Li-ion batteries. I have a small in-house dataset, as it's quite small I want to validate my results on a bigger dataset.

I have tried finding the dataset using simple Google search, Kaggle, some other dataset related websites.

I am finding a lot of dataset for battery life prediction but I want data for manufacturing defects. Apart from that I found a dataset from NEU, although those guys used some other dataset to augment their data for battery surface defects.

Any help would be nice.

P.S: I hope I am not considered Lazy, I tried whatever I could. ","['Have you tried google dataset search https://datasetsearch.research.google.com/ ? \n\nTry papers with code https://paperswithcode.com/datasets and huggingface datasets', ""Do you have an example image of the type of thing you're looking for? (Eg x-ray? car vs phone? installed vs on its own? what zoom level? what type of defect? etc)"", 'You think there is a pre-made dataset of exactly Li-Ion batteries with production defects open on the internet? Use common sense, any niche dataset is either private or does not exist.', '[deleted]']","['Have you tried google dataset search https://datasetsearch.research.google.com/ ? \n\nTry papers with code https://paperswithcode.com/datasets and huggingface datasets', ""Do you have an example image of the type of thing you're looking for? (Eg x-ray? car vs phone? installed vs on its own? what zoom level? what type of defect? etc)"", 'You think there is a pre-made dataset of exactly Li-Ion batteries with production defects open on the internet? Use common sense, any niche dataset is either private or does not exist.', '[deleted]']",6,9,0.81,Help: Project,1750179475.0
1ldpwbp,computervision,How to Automate QA on AI generated Images?,"I am currently generating realistic images, i want to develop an automated auality assurance method to  identify anomalies in the image.

An Idea on how to do it?

Edit:

Sorry, i had not added any background information.

The Images generated using online AI Image generator tool (Freepik). The anomalies include biological abnormalities like missing or additional body parts, weird or abnormal facial or body features, abnormal objects. The images do include abstract components, so it  find it to be a hard problem.

I shall try to add  images, when i find time. ","['No images, no description of how they are made, no explanation of what anomaly is. Yeah, good luck buddy, you gonna need it...']","['No images, no description of how they are made, no explanation of what anomaly is. Yeah, good luck buddy, you gonna need it...']",0,2,0.29,Discussion ,1750173566.0
1ldnids,computervision,[D] Can masking operations detach the tensors from the computational graph?,,[],[],1,0,1.0,Help: Project,1750167869.0
1ldj39i,computervision,How to forward a PyGame window from server to macOS (M1)?,"I'm trying to run a reinforcement learning environment on a remote Ubuntu server, and I need to manually interact with the game window rendered via PyGame. The idea is to run the environment on the server and forward the display to my macOS machine using X11. I'm on an Apple Silicon (M1) Mac.

I'm currently using **XQuartz** for X11 forwarding. I can connect via SSH with `-X` or `-Y` and basic X11 apps like `xeyes` display fine. However, when PyGame tries to open its window, I get the following OpenGL error when checking `glxinfo`:

name of display: localhost:10.0

libGL error: No matching fbConfigs or visuals found

libGL error: failed to load driver: swrast

display: localhost:10  screen: 0

...


I've searched all over and tried various suggestions (installing `mesa-utils`, using different display configs, etc.) but nothing resolves this. It seems like **XQuartz has very poor support for OpenGL forwarding**, and I haven’t found any working solution\[\^1\].

I also tried using **Xpra**, which forwards graphical apps via SSH, but it’s extremely finicky and hard to configure properly — especially with OpenGL apps like PyGame.


\[\^1\]: [https://github.com/XQuartz/XQuartz/issues/144#issuecomment-2481017077](https://github.com/XQuartz/XQuartz/issues/144#issuecomment-2481017077)",[],[],1,0,0.67,Help: Project,1750154101.0
1ldhy8i,computervision,Can YOLO be used to detect and identify specific objects (custom data sets) with the Meta Quest 3?,"Hello All,

I'm interested in object detection algorithms used in Mixed Reality and was wondering if one could train a tool like YOLO to detect and identify a specific object in physical space to trigger specific effects in MR? Thank you.","['yes you can.\xa0', 'according to this thread, it is not possible to access the camera feed\n\n[https://www.reddit.com/r/OculusQuest/comments/1b17dej/object\\_human\\_detection\\_using\\_quest\\_3/](https://www.reddit.com/r/OculusQuest/comments/1b17dej/object_human_detection_using_quest_3/)', 'You can definitely train yolo with a custom dataset']","['yes you can.\xa0', 'according to this thread, it is not possible to access the camera feed\n\n[https://www.reddit.com/r/OculusQuest/comments/1b17dej/object\\_human\\_detection\\_using\\_quest\\_3/](https://www.reddit.com/r/OculusQuest/comments/1b17dej/object_human_detection_using_quest_3/)', 'You can definitely train yolo with a custom dataset']",7,5,0.9,Discussion ,1750149479.0
1ldd1wr,computervision,What is the best way/industry standard way to properly annotate Video Data when you require multiple tasks/models as part of your application?,"Hello.


Let's say I'm building a Computer vision project where I am building an analytical tool for basketball games (**just using this as an example**)

There's 3 types of tasks involved in this application:

1. player detection, referee detection

2. Pose estimation of the players/joints

3. Action recognition of the players(shooting, blocking, fouling, steals, etc...)

**Q)** Is it customary to train on the same video data input, I guess in this case (**correct me if I'm wrong**) differently formatted video data, how would I deal with multiple video resolutions as input? Basketball videos can be streamed in 1440p, 360p, 1080p, w/ 4k resolution, etc... Should I always normalize to 3-d frames such as 224 x 224 x 3 x T(height, width, color channel, time) I am assuming?

**Q)** Can I use the same video data for all 3 of these tasks and label all of the video frames I have, **i.e. bounding boxes, keypoints, action classes per frame(s)** all at once.

**Q)** Or should I separate it, where I use the same exact videos, but create let's say 3 folders for each task (or more if there's more tasks/models required) where each video will be annotated separately based off the required task? (1 video -> same video for bounding boxes, same video for keypoints, same video for action recognition)


**Q) What is industry standard?** The latter seems to have much more overhead. But the 1st option takes a lot of time to do.

**Q) Also, what if I were to add in another element, let's say I wanted to track if a player is sprinting, vs jogging, or walking.**

**How would I even annotate this, also is there a such thing as too much annotation? B/c at this point it seems like I would need to annotate every single frame of data per video, which would take an eternity**","[""224x224x3 could be ok for action recognition, but is probably not enough for object detection and pose estimation. get started with any of the shelf bottom up pose estimator. calculate crops for each person and do classification for players and referees. build temporal sequences from those crops to feed into the action recognition. \n\ni suggest to avoid training a pose estimator yourself. annotating keypoints is very time consuming and it will be difficult to have the necessary variability in data. don't label every frame of your data. neighbouring frames contain very little relevant information. \n\nlastly i would say that the camera perspective plays a big role. do you have video footage of a static camera covering the whole playing field, or are you trying to analyze TV footage?""]","[""224x224x3 could be ok for action recognition, but is probably not enough for object detection and pose estimation. get started with any of the shelf bottom up pose estimator. calculate crops for each person and do classification for players and referees. build temporal sequences from those crops to feed into the action recognition. \n\ni suggest to avoid training a pose estimator yourself. annotating keypoints is very time consuming and it will be difficult to have the necessary variability in data. don't label every frame of your data. neighbouring frames contain very little relevant information. \n\nlastly i would say that the camera perspective plays a big role. do you have video footage of a static camera covering the whole playing field, or are you trying to analyze TV footage?""]",4,2,0.83,Help: Project,1750131043.0
1ldamip,computervision,Retrained our model on yolov8n instead of yolov8m and now our dataset is completely different than we used before,"We're doing a CV detection model on traffic signs and we found a nice and decent kaggle notebook to train our yolov8 models on a traffic sign dataset. The first model was yolov8m but it was extremely heavy on our systems but it did detect all of the traffic signs that we wanted to detect.

We made the decision to move yolov8n as its lighter and it is lighter but the issue is that it no longer detects the traffic signs but instead detects persons and mobile phones.

It seems that the dataset has changed while converting the pt file to onnx file and we're not sure how to handle it


This is our notebook for reference.

It's supposed to detect traffic signs only but not humans","['To confirm, all you did was change the ""m"" to an ""n"" and rerun the exact same code against the exact same input dataset?', 'did you tried to inspect your dataset anotations ?  \nBecause i last time face issue with data anotation afte i applied Data Augmentation using roboflow, and augmented data was really misaligned Annotations.  \nOnce i annotate using canvas and i used two tools ""make using rectangle"" and 2nd ""mark using poligon"", but when i trained my model using Yolo it didnt read ""rectangles"" properly.\n\nSo if theres anything similer in ur dataset also check it.']","['To confirm, all you did was change the ""m"" to an ""n"" and rerun the exact same code against the exact same input dataset?', 'did you tried to inspect your dataset anotations ?  \nBecause i last time face issue with data anotation afte i applied Data Augmentation using roboflow, and augmented data was really misaligned Annotations.  \nOnce i annotate using canvas and i used two tools ""make using rectangle"" and 2nd ""mark using poligon"", but when i trained my model using Yolo it didnt read ""rectangles"" properly.\n\nSo if theres anything similer in ur dataset also check it.']",2,4,0.63,Help: Project,1750123739.0
1lcv5hg,computervision,how to do perspective correction ?,"Hi, I would like to find a solution to correct the perspective in images, using a python package like scikit-image. Below an example. I have images of signs, with corresponding segmentation mask. Now I would like to apply a transformation so that the borders of the sign are parallel to the borders of the image. Any advice on how I should proceed, and which tools should I use? Thanks in advance for your wisdom.

https://preview.redd.it/xzd4g6oyza7f1.png?width=2145&format=png&auto=webp&s=e831d5e9e5b4754fffdfa98012aeb7fac50ada3d

","[""You can use [Roboflow Workflows](https://inference.roboflow.com) to do this. [I created a demo using your image with the Perspective Correction block that you can try here.](https://app.roboflow.com/workflows/embed/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ3b3JrZmxvd0lkIjoibHpjTWJLa3JNNk03UTdmNXJjSVIiLCJ3b3Jrc3BhY2VJZCI6IkVaNkYwWmtqUXNXTEVEYWpEVzJGdHNnOVlZcjIiLCJ1c2VySWQiOiJFWjZGMFpralFzV0xFRGFqRFcyRnRzZzlZWXIyIiwiaWF0IjoxNzUwMDk1Mzc5fQ.fZbhwFOJjWzSY-KX6kaZhzUREE1eUAm5poOTElfdzMo?defaultVisual=true&showGraph=false) It uses OpenCV behind the scenes for this.\n\nI created a static outline of your arrow to give you a feel of what you can achieve. If you want to find the edges automatically you'd train and add an instance segmentation model to the Workflow to output the zone dynamically (it sounds from your post like you may have already done that to create the cutout).\n\n(Disclaimer: I'm one of the co-founders of Roboflow.)"", '[deleted]', 'Agree with the other comment to read up on some theory — just so you know the caveats and limitations of the techniques.\n\nA simple correction would involve chaining cv2. getPerspectiveTransform with cv2. warpPerspective. Feed in the 4 corners of the signage, along with the 4 corners of the image.']","[""You can use [Roboflow Workflows](https://inference.roboflow.com) to do this. [I created a demo using your image with the Perspective Correction block that you can try here.](https://app.roboflow.com/workflows/embed/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ3b3JrZmxvd0lkIjoibHpjTWJLa3JNNk03UTdmNXJjSVIiLCJ3b3Jrc3BhY2VJZCI6IkVaNkYwWmtqUXNXTEVEYWpEVzJGdHNnOVlZcjIiLCJ1c2VySWQiOiJFWjZGMFpralFzV0xFRGFqRFcyRnRzZzlZWXIyIiwiaWF0IjoxNzUwMDk1Mzc5fQ.fZbhwFOJjWzSY-KX6kaZhzUREE1eUAm5poOTElfdzMo?defaultVisual=true&showGraph=false) It uses OpenCV behind the scenes for this.\n\nI created a static outline of your arrow to give you a feel of what you can achieve. If you want to find the edges automatically you'd train and add an instance segmentation model to the Workflow to output the zone dynamically (it sounds from your post like you may have already done that to create the cutout).\n\n(Disclaimer: I'm one of the co-founders of Roboflow.)"", '[deleted]', 'Agree with the other comment to read up on some theory — just so you know the caveats and limitations of the techniques.\n\nA simple correction would involve chaining cv2. getPerspectiveTransform with cv2. warpPerspective. Feed in the 4 corners of the signage, along with the 4 corners of the image.']",9,11,1.0,Help: Project,1750086164.0
1lcsnoj,computervision,Want to know how to break into the field of Computer Vision.,"Hey, I am an undergrad student from india doing my btech in mechanical engineering. I wanted to know how do people usually break into this field because I was looking for an internship opportunity in this field but couldn't find much results.","['You need research experience. Market for CV is very less. In that you have thousands competing.', 'Do your own projects. Start a blog where you document what you do. Build your portfolio up. Try to find something that is relevant to both mech eng and cv. If you were an aero eng you could do something with drone -- maybe navigate based on camera feed looking down at the ground? Think of stuff like this but in your line of education.']","['You need research experience. Market for CV is very less. In that you have thousands competing.', 'Do your own projects. Start a blog where you document what you do. Build your portfolio up. Try to find something that is relevant to both mech eng and cv. If you were an aero eng you could do something with drone -- maybe navigate based on camera feed looking down at the ground? Think of stuff like this but in your line of education.']",0,8,0.36,Discussion ,1750080111.0
1lcr9kt,computervision,What are some major research papers I need to understand in 2025?,"I am currently a computer science master student in the US and am looking for a fall ML engineer internship!

","['Attention is all you need\n\nhttps://huggingface.co/papers you can also filter the weekly and monthly papers. Then you can just read their references and explore from there', '[Paper digest](https://resources.paperdigest.org/category/computer-vision/)', 'Just pick a topic of interest and go to google scholar and start to read in order.', ""You have to narrow it down a bit, but here's one that I found most interesting:\n\nhttps://cvpr.thecvf.com/virtual/2025/poster/32881"", 'Good question 👍', 'Following', 'I found the one with convex optimization for locating vanishing points in Manhattan world very impressive. This reminds me of pre 2012 cvprs: \n\nhttps://github.com/WU-CVGL/GlobustVP', 'Following this been trying to build a solid reading list myself. Curious what papers people think really shaped the field']","['Attention is all you need\n\nhttps://huggingface.co/papers you can also filter the weekly and monthly papers. Then you can just read their references and explore from there', '[Paper digest](https://resources.paperdigest.org/category/computer-vision/)', 'Just pick a topic of interest and go to google scholar and start to read in order.', ""You have to narrow it down a bit, but here's one that I found most interesting:\n\nhttps://cvpr.thecvf.com/virtual/2025/poster/32881"", 'Good question 👍']",58,14,0.98,Discussion ,1750076199.0
1lcqxni,computervision,How would you want to fund your CV build?,"My company is providing a budget and access to our platform for building Computer Vision applications–what would get you interested in using it?

[View Poll](https://www.reddit.com/poll/1lcqxni)",['A proposal for an academic grant would be fantastic for me! mind sharing the company? would love to stay updated!'],['A proposal for an academic grant would be fantastic for me! mind sharing the company? would love to stay updated!'],0,3,0.29,Discussion ,1750075208.0
1lckcni,computervision,Has Anyone Applied Computer Vision for Micro Defect Detection in Manufacturing ?,"We have been looking into how computer vision can be applied to identify micro defects in manufacturing. Does anyone here have experience with similar applications or working in this field?
","[""I've worked in the semiconductor industry identifying nm scale defects on Semiconductor wafers? Is that micro enough?"", 'Explain micro defects', 'Yes. Please add more info', '""Micro"" sound like you ""just"" need a proper optics to make those defects visible.\n\nDepending on the material and its surface you might require multiple light sources, or projecting a specific pattern, or looking for interference patterns. Would the material allow to let specific light to shine through?  \nWould such a micro defect be visible on a homogenious surface, or does the surface has other structures?\n\nShould it be non-destructive or is destructive detection possible? In some cases applying a specific fluid (e.g. acid) (or oil!) (a colorant?) to the surface could reveal more details.\n\nIs it possible to pause the movement of the object for the time of the detection, or does it move fast and you would need an extremely short grabbibg and capturing of frames?\n\nWould you need realtime analysis, or could the result(s) be available a ""few moments"" later, too?', 'I work in surface inspection, we detect and classify defects in the millimeter range at up to 50m² per second. What would you like to know?', 'My client (full disclosure) www.3LC.ai has a great tool for data cleaning and model adjustment. They used it extensively for dent detection on car rentals with great results. I would imagine they are similar computer vision workflows. Hope that helps.', 'We do something similar in our institute. \n\nAs user u/herocoding  mentioned, first you need correct optics where such a defect is actually visible to your human eye.\n\nNext comes the alogorithm. If you want to go for Deep learning based approach, check for PatchCore anomaly detection OR and U-Net segmenation network', 'Multiple papers at CVPR this year about that', 'Yes.  Lots of people and lots of companies employ CV to detect sub-millimetre defects during manufacturing processes in every industry you can think of.', 'Look up research papers']","[""I've worked in the semiconductor industry identifying nm scale defects on Semiconductor wafers? Is that micro enough?"", 'Yes. Please add more info', '""Micro"" sound like you ""just"" need a proper optics to make those defects visible.\n\nDepending on the material and its surface you might require multiple light sources, or projecting a specific pattern, or looking for interference patterns. Would the material allow to let specific light to shine through?  \nWould such a micro defect be visible on a homogenious surface, or does the surface has other structures?\n\nShould it be non-destructive or is destructive detection possible? In some cases applying a specific fluid (e.g. acid) (or oil!) (a colorant?) to the surface could reveal more details.\n\nIs it possible to pause the movement of the object for the time of the detection, or does it move fast and you would need an extremely short grabbibg and capturing of frames?\n\nWould you need realtime analysis, or could the result(s) be available a ""few moments"" later, too?', 'Explain micro defects', 'I work in surface inspection, we detect and classify defects in the millimeter range at up to 50m² per second. What would you like to know?']",16,25,0.94,Discussion ,1750049980.0
1lcjvlz,computervision,Best VLMs for document parsing and OCR.,"Not sure if this is the correct sub to ask on, but I’ve been struggling to find models that meet my project specifications at the moment.

I am looking for open source multimodal VLMs (image-text to text) that are < 5B parameters (so I can run them locally).

The task I want to use them for is zero shot information extraction, particularly from engineering prints. So the models need to be good at OCR, spatial reasoning within the document and key information extraction. I also need the model to be able to give structured output in XML or JSON format.

If anyone could point me in the right direction it would be greatly appreciated!","['I’ve had good success with Llama 4 Maverick.', ""I've been super impressed with Qwen2-VL-2B"", 'Have you tried IBM Granite?', 'You can try with this:\n\nhttps://huggingface.co/nanonets/Nanonets-OCR-s\n\nOr with this:\n\nhttps://github.com/Yuliang-Liu/MonkeyOCR.git', 'I’ve been trying to figure this out too there are so many models out there, it’s a bit overwhelming. Hoping someone shares something beginner-friendly here!']","['I’ve had good success with Llama 4 Maverick.', ""I've been super impressed with Qwen2-VL-2B"", 'Have you tried IBM Granite?', 'You can try with this:\n\nhttps://huggingface.co/nanonets/Nanonets-OCR-s\n\nOr with this:\n\nhttps://github.com/Yuliang-Liu/MonkeyOCR.git', 'I’ve been trying to figure this out too there are so many models out there, it’s a bit overwhelming. Hoping someone shares something beginner-friendly here!']",9,8,1.0,Help: Project,1750048252.0
1lchcdx,computervision,TensorRT + SAHI ?,"Hello friends!
I am having hard times to get SAHI working with TensorRT. I know SAHI doesn't support "".engine"" so you need a workaround.

Did someone get it working somehow?

Background is that I need to detect small images and want to take profit of TensorRT Speed.

Any other alternative is also welcome for that usecase.

Thank you!!!!!","[""Last time I used TensorRT I just wrote something to handle lazily slicing the images into tiles in memory. It was some basic image processing code. I'll have to find it."", 'It s easy but it s not out-of-the-box\nYou have to feed the inference pipeline with a batch made from patches generated by SAHI.\nIf you use ultralytics, it s not so easy to customize... Use other frameworks and mostly focus on PyTorch', 'IMO the low hanging fruit in getting some speed up with SAHI is batched inference. The original SAHI is still running with batch size == 1, see [https://github.com/obss/sahi/blob/48258a7a35fc7f997ef6b720432411e34ea300cc/sahi/predict.py#L232](https://github.com/obss/sahi/blob/48258a7a35fc7f997ef6b720432411e34ea300cc/sahi/predict.py#L232) There are some unfinished pull requests implementing batching, but nothing ready for production. The SAHI implementation can be quite simple and the easiest way to get batched inference now is own implementation.']","[""Last time I used TensorRT I just wrote something to handle lazily slicing the images into tiles in memory. It was some basic image processing code. I'll have to find it."", 'It s easy but it s not out-of-the-box\nYou have to feed the inference pipeline with a batch made from patches generated by SAHI.\nIf you use ultralytics, it s not so easy to customize... Use other frameworks and mostly focus on PyTorch', 'IMO the low hanging fruit in getting some speed up with SAHI is batched inference. The original SAHI is still running with batch size == 1, see [https://github.com/obss/sahi/blob/48258a7a35fc7f997ef6b720432411e34ea300cc/sahi/predict.py#L232](https://github.com/obss/sahi/blob/48258a7a35fc7f997ef6b720432411e34ea300cc/sahi/predict.py#L232) There are some unfinished pull requests implementing batching, but nothing ready for production. The SAHI implementation can be quite simple and the easiest way to get batched inference now is own implementation.']",2,6,0.75,Help: Project,1750039891.0
1lcglyt,computervision,What logic/algorithms are applied after object segmentation? Beyond visual mask?,"Hello community I have a conceptual question about object segmentation. I understand how segmentation works (YOLO, Mask R-CNN , SAM, etc.) and I can obtain object masks, but I'm wondering : what exactly do You do with those segmented objects afterward? That is, once I have the Mask of an object (Say , a car , a person, a tree) what kind of logic or algorithms are applied to that segmented region? Is it only for visualization, or is there deeper processing involved? I'm interested in learning about real world use cases where segmentation is the first step in a more complex pipeline. What comes after segmentation? Thanks for your thoughts and experiences! Examples plis. I'm Lost. Thanks","['The choice of model (object detection, segmentation, classification) is always driven by business requirements. \n\nProblem statements where object masks can be part of a viable solution can be things like medical imaging diagnosis (identify and classify suspect regions in scans) where not much comes after the modeling.\n\nSlightly more complex pipelines could come in the form of cell counting, where one might need to count the number of cells and keep track of their sizes (hence the usefulness of masks). \n\nAt the higher end of complexity, object masks can provide the base layer for augmented reality applications, like creating and applying face filters.', 'We use segmentation of car body panels to localize damages in images. This is useful, for example, in car insurance.', 'I have used instance-segmentation models primarily to help me precisely locate where an object is. One specific application was picking up small components with a robotic arm. In that application a bounding box wasn’t precise enough for me to pick up the parts due to their shapes, so I used the segmentation mask to give me a more accurate estimate for the center of mass of the object.', 'I mostly do hole filling after getting the mask. And based on the project, I also have to generate contours and remove small contour and re-construct the mask. And recently, I had to do perspective transforms and many other post processing steps.', 'I have used those masks to measure the size of that object and also classify them based on size.', 'You usually only do segmentation if you have a need already identified.\xa0\n\nIn my case I want to measure distances to objects and I’m using segmentation models to identify the exact position of the objects in the 2D image, then I use a monocular depth estimation model to get the distance to those pixels.\xa0\n\nDoing that with only bounding boxes would make it harder because the box would contain a lot of background pixels.\xa0', 'Could You please give me a repository link or something so I can apply it and understand it better? For example ""I have a segmented Apple"" what can I do with that Mask to create a real-life application? Please help me. I\'m Lost I don\'t know what to do.']","['The choice of model (object detection, segmentation, classification) is always driven by business requirements. \n\nProblem statements where object masks can be part of a viable solution can be things like medical imaging diagnosis (identify and classify suspect regions in scans) where not much comes after the modeling.\n\nSlightly more complex pipelines could come in the form of cell counting, where one might need to count the number of cells and keep track of their sizes (hence the usefulness of masks). \n\nAt the higher end of complexity, object masks can provide the base layer for augmented reality applications, like creating and applying face filters.', 'We use segmentation of car body panels to localize damages in images. This is useful, for example, in car insurance.', 'I have used instance-segmentation models primarily to help me precisely locate where an object is. One specific application was picking up small components with a robotic arm. In that application a bounding box wasn’t precise enough for me to pick up the parts due to their shapes, so I used the segmentation mask to give me a more accurate estimate for the center of mass of the object.', 'I mostly do hole filling after getting the mask. And based on the project, I also have to generate contours and remove small contour and re-construct the mask. And recently, I had to do perspective transforms and many other post processing steps.', 'I have used those masks to measure the size of that object and also classify them based on size.']",1,10,1.0,Discussion ,1750037597.0
1lc9t70,computervision,should I learn C to understand what Python code does under the hood?,I am a computer science master student in the US and am currently looking for a ml engineer internship.,"['No', ""You shouldn't learn C for that reason, you should learn it for a big amount of different and better reasons. Anyway for a ml engineer internship, knowing C it likely won't help much for the selection but it can definitely help you in some cases you may never encounter during your job."", 'You’ll have to know C, but this article is a staple that really rips apart the language in terms of how and why it’s built the ways it’s built. Great read — grab a few cups of tea. \n\nhttps://realpython.com/cpython-source-code-guide/', ""You would want to learn c/c++ whenever you work with embedded/edge devices. While C won't help you get a better chance at securing ml engineer position, it can help you better understand your PC hardware as you will realize of how much stuff is taken for granted when coding in Python. If you really want to get to bare-metal, might as well check out assembly languages to know what is under the hood. Again, it depends what you want to pursue later and adjust your skills accordingly."", ""No, just learn to write good code. If you have a good mindset for writing high quality code, languages become much less of a barrier. Get good at writing Python, touch on Cython, numba.njit, etc. if you come across a reason to accelerate a function or two that can't be sensibly vectorised. Raising your skill ceiling is a much better investment than doing a few tutorials on C++"", 'Maybe. Depends on you. Python is a great prototyping language. It lets you very easy accomplish a lot of stuff quickly. But for some reason they were like “you know what is gonna kind of suck? Loops. Who needs ‘em, right?” If I was picking up programming today, I think I would probably lean towards learning Rust over C++ as it seems to be getting pretty wide adoption. Ultimately though, learning more is always better than knowing less.', 'Knowing how a VM works is definitely useful but getting into the details of the Python VM is not really necessary.  Learning C in general is just a good idea because it’s not that hard, you will probably run into C code at some point in your programming career, and it does have some useful concepts that higher level languages don’t that you should know.', ""To get a high level idea of how it works under the hood, probably not needed. But to really understand it in detail I think you'd need to. For ML this would only be useful in some jobs, though. Maybe those that would require C extensions, possibly also CUDA. Or some kind of low level language tooling."", ""C/C++ programmer here.\n\n\nI don't see what value it would provide you.\xa0\n\n\nHappy to answer questions.\xa0""]","['No', ""You shouldn't learn C for that reason, you should learn it for a big amount of different and better reasons. Anyway for a ml engineer internship, knowing C it likely won't help much for the selection but it can definitely help you in some cases you may never encounter during your job."", 'You’ll have to know C, but this article is a staple that really rips apart the language in terms of how and why it’s built the ways it’s built. Great read — grab a few cups of tea. \n\nhttps://realpython.com/cpython-source-code-guide/', ""You would want to learn c/c++ whenever you work with embedded/edge devices. While C won't help you get a better chance at securing ml engineer position, it can help you better understand your PC hardware as you will realize of how much stuff is taken for granted when coding in Python. If you really want to get to bare-metal, might as well check out assembly languages to know what is under the hood. Again, it depends what you want to pursue later and adjust your skills accordingly."", ""No, just learn to write good code. If you have a good mindset for writing high quality code, languages become much less of a barrier. Get good at writing Python, touch on Cython, numba.njit, etc. if you come across a reason to accelerate a function or two that can't be sensibly vectorised. Raising your skill ceiling is a much better investment than doing a few tutorials on C++""]",13,51,0.78,Discussion ,1750018352.0
1lc17bf,computervision,Maths needed to understand Szeliski,"Hi all hope you're well!


I recently had a play with some openCV stuff to recreate the nuke code document scanner from Mission Impossible which was super fun. Turned out to be far more complex than expected but after a bit of hacking and a very hamfisted implementation of tesseract OCR I got it working over the weekend which is pretty cool!


I'm a fairly experienced FE dev so I'm comfortable with programming but I haven't really done much maths in the last decade or so. I really enjoyed playing comp vision so want to dig deeper and looking around Szeliski's book ""Computer Vision: Algorithms and Applications"" seems to be the go to for doing that.


So my question is what level of maths do I need to understand the book. Having a scan through it seems to be quite heavy on matrixes with some snazzy Greek letters that mean nothing to me. What is the best way to learn this stuff? I started getting back into maths about 3 months back but stalled around pre-calc. Would up to calc 2 cover it?


Thanks.","['It’s all Linear algebra all the way down\n\nYou don’t need calculus to learn linear algebra but it’s a big step up in mathematical maturity and abstraction from what you’re used to so far so: try to learn it, but if you find it hard don’t be discouraged.\n\nThe most important content in cal 2 is numerical optimization, with Newton’s method for example, and finding extrema with derivatives. CV is almost always “just” fitting a model to data, which is essentially always “just” adding linear algebra on top of that. (See levenberg-marquardt.)\n\nYou can definitely try learning vector algebra first (like dot products, cross products, line intersections, etc) but that’s a very limited perspective on linearity. Linearity is far far more general and a bit more abstract and you’ll want that greater perspective so don’t trick yourself into thinking you get it too early.']","['It’s all Linear algebra all the way down\n\nYou don’t need calculus to learn linear algebra but it’s a big step up in mathematical maturity and abstraction from what you’re used to so far so: try to learn it, but if you find it hard don’t be discouraged.\n\nThe most important content in cal 2 is numerical optimization, with Newton’s method for example, and finding extrema with derivatives. CV is almost always “just” fitting a model to data, which is essentially always “just” adding linear algebra on top of that. (See levenberg-marquardt.)\n\nYou can definitely try learning vector algebra first (like dot products, cross products, line intersections, etc) but that’s a very limited perspective on linearity. Linearity is far far more general and a bit more abstract and you’ll want that greater perspective so don’t trick yourself into thinking you get it too early.']",9,2,0.8,Help: Theory ,1749996535.0
1lbx3is,computervision,"Do multimodal LLMs (like Chatgpt, Gemini, Claude) use OCR under the hood to read text in images?","SOTA multimodal LLMs can read text from images (e.g. signs, screenshots, book pages) really well - almost better than OCR.

Are they actually using an internal OCR system (like Tesseract or Azure Vision), or do they learn to ""read"" purely through pretraining (like contrastive learning on image-text pairs)?","['Usually multimodal LLMs are trained using a pair of image and text representing the image using contrastive learning.\n\nYou can learn more about it here:\nhttps://huggingface.co/blog/vlms-2025', 'They’re multimodal. You can download a vision capable LLM yourself with LM Studio to see there is no side OCR system.', 'Its important to first point out that the interfaces you interact are chatbots, not LLMs. And there should definitely be huge underlying systems which consists various processing functions, services and models. Which of those are used depends on the contents of your queries and images.\n\n\nNow, back to your question. I believe they most likely use a combination of both. For instance, if your text query, you explicitly tell the LLM that this is a license plate, then a simple OCR model maybe invoked. But you only command, ""Get me the text,"" without providing anymore information, then first a VLM has to be invoked to describe the scene, a detector to localize the potential objects with text, and then finally the the OCR model to get the license plate numbers.\n\n\nAnd thats only an accuracy-oriented naive solution. Balancing the accuracy and the cost is definitely requires a lot more research and engineering. The point is foundation models are only a part of the equation.', ""I don't think their training methods are open so it's hard to say. But I for one would be a bit surprised if some form of OCR module and textual ground truth is not involved. If not during inference, then it could be a differentiable module that is pretrained and fine tuned along with the main vision head. Totally guessing though."", 'RemindMe! 3 days', 'Qwen 2.5 vl which you can run locally does not', ' Most of them use a pretrained vision language model to extract the visual patterns and translate them into the text embedding using an adapter. The pretrain could be extended to also ‘ocr’ images , in this case the caption will be the ocr.\nI think that in agentic flows the agent could ‘decide’ to ocr an image']","['Usually multimodal LLMs are trained using a pair of image and text representing the image using contrastive learning.\n\nYou can learn more about it here:\nhttps://huggingface.co/blog/vlms-2025', 'They’re multimodal. You can download a vision capable LLM yourself with LM Studio to see there is no side OCR system.', 'Its important to first point out that the interfaces you interact are chatbots, not LLMs. And there should definitely be huge underlying systems which consists various processing functions, services and models. Which of those are used depends on the contents of your queries and images.\n\n\nNow, back to your question. I believe they most likely use a combination of both. For instance, if your text query, you explicitly tell the LLM that this is a license plate, then a simple OCR model maybe invoked. But you only command, ""Get me the text,"" without providing anymore information, then first a VLM has to be invoked to describe the scene, a detector to localize the potential objects with text, and then finally the the OCR model to get the license plate numbers.\n\n\nAnd thats only an accuracy-oriented naive solution. Balancing the accuracy and the cost is definitely requires a lot more research and engineering. The point is foundation models are only a part of the equation.', ""I don't think their training methods are open so it's hard to say. But I for one would be a bit surprised if some form of OCR module and textual ground truth is not involved. If not during inference, then it could be a differentiable module that is pretrained and fine tuned along with the main vision head. Totally guessing though."", 'RemindMe! 3 days']",30,12,0.9,Discussion ,1749982973.0
1lbti5o,computervision,Ball and human following robot help,Im new to computer vision and i have an assignment to use computer vision in a robot that can follow objects. Is it possible to track both humans and object such as a ball in the same time? and what model is the best to use? is open cv capable of doing all of it? thank you in advance for the help,"[""It is very much possible. You can use any model trained on coco dataset. In the COCO dataset there are classes for peson and sports-ball. You will only need to filter your détections based on those two classes.\nI'll advice you to look at an object detection tutorial and go from there""]","[""It is very much possible. You can use any model trained on coco dataset. In the COCO dataset there are classes for peson and sports-ball. You will only need to filter your détections based on those two classes.\nI'll advice you to look at an object detection tutorial and go from there""]",1,6,0.6,Help: Project,1749968374.0
1lbmx96,computervision,"Help, hit and run license plate","Is there any way to see the license plate number on this video. He broke my rear view mirror and sped off.
https://www.dropbox.com/scl/fi/b0rbra02hbtzuhslwpadc/Untitled-video-Made-with-Clipchamp.mp4?rlkey=5esh52p4op0ynr0mv2fbszfus&e=1&st=sbvisb26&dl=0","[""Did you email CSI first? They're good at zoom & enhance.""]","[""Did you email CSI first? They're good at zoom & enhance.""]",0,4,0.18,Help: Project,1749945965.0
1lbhqs9,computervision,Please suggest cheap GPU server providers,"Hi I want to run a ML model online which requires very basic GPU to operate online. Can you suggest some cheaper and good option available? Also, which is comparatively easier to integrate. If it can be less than 30$ per month It can work. ","['Maybe Runpod?', 'Maybe Vast.ai?', 'simplepod.ai works well too but you’re gonna have to stomach the latency between typing and seeing it on the terminal screen. \n\nIn return, the gpu rates are competitive (0.35/hr on 4090 24GB) compared to runpod’s (0.70/hr', 'The cheapest I know is https://salad.com/ since it runs on idle gaming machines. It is not very reliable, though.\n\nOurs start from $0.36 per month for 4090 which is at the lowest end of the market for something hosted in a Tier 3 datacenter. It is probably the minimum you need if you don’t want implement any failover mechanisms: https://www.cloudrift.ai', 'Runpod all the way', 'if you just wana test some stuff or even training models i can provide my own local one rtx 4070']","['Maybe Runpod?', 'Maybe Vast.ai?', 'simplepod.ai works well too but you’re gonna have to stomach the latency between typing and seeing it on the terminal screen. \n\nIn return, the gpu rates are competitive (0.35/hr on 4090 24GB) compared to runpod’s (0.70/hr', 'The cheapest I know is https://salad.com/ since it runs on idle gaming machines. It is not very reliable, though.\n\nOurs start from $0.36 per month for 4090 which is at the lowest end of the market for something hosted in a Tier 3 datacenter. It is probably the minimum you need if you don’t want implement any failover mechanisms: https://www.cloudrift.ai', 'Runpod all the way']",3,16,0.71,Help: Theory ,1749931457.0
1lb9thf,computervision,SDXL images vs. Alchemist,"Somebody told me about image fine-tuning with Alchemist. Looked into it. According to the makers, this SFT dataset bolsters aesthetics, while staying true to the prompts.

Before and after on SDXL (prompt: “A white towel”):

https://preview.redd.it/d54g9e8wgw6f1.png?width=1260&format=png&auto=webp&s=3e042381716eedc9fb9bfd44b2592737b5814f3a

https://preview.redd.it/yuvb4md0hw6f1.png?width=1260&format=png&auto=webp&s=1a97300d0fa4c7afebd5242256e618e2338b7bc6



The images look promising to me, but I remain somewhat skeptical. Would be great to hear from someone who’s actually tested it firsthand!

",[],[],0,0,0.17,Discussion ,1749910810.0
1lb8zg1,computervision,Video object classification (Noisy),"Hello everyone!
I would love to hear your recommendations on this matter.



Imagine I want to classify objects present in video data. First I'm doing detection and tracking, so I have the crops of the object through a sequence. In some of these frames the object might be blurry or noisy (doesn't have valuable info for the classifier) what is the best approach/method/architecture to use so I can train a classifier that kinda ignores the blurry/noisy crops and focus more on the clear crops?



to give you an idea, some approaches might be: 1- extracting features from each crop and then voting, 2- using a FC to give an score to features extracted from crops of each frame and based on that doing weighted average and etc. I would really appreciate your opinion and recommendations.


thank you in advance.","[""I would say the voting procedure would work well, basically classify each crop and take the most common prediction.   \n  \nDetecting noise is a bit subjective, you can consider doing some operations such as a laplacian filter to try to measure blurriness, or you can also train a classifier to predict the frame's quality.\n\nOne other approach is that since all crops belong to the same object (if your tracker is good), then the extracted features should be quite similar, unless the crop is noisy. So maybe you can keep track of a mean feature tensor and if a new feature tensor extracted from a crop is too different from it, then this crop is possibly of low image quality. When you measure distance between features, try out different metrics such as cosine similarity for example.""]","[""I would say the voting procedure would work well, basically classify each crop and take the most common prediction.   \n  \nDetecting noise is a bit subjective, you can consider doing some operations such as a laplacian filter to try to measure blurriness, or you can also train a classifier to predict the frame's quality.\n\nOne other approach is that since all crops belong to the same object (if your tracker is good), then the extracted features should be quite similar, unless the crop is noisy. So maybe you can keep track of a mean feature tensor and if a new feature tensor extracted from a crop is too different from it, then this crop is possibly of low image quality. When you measure distance between features, try out different metrics such as cosine similarity for example.""]",0,1,0.5,Help: Theory ,1749908494.0
1lb7f7p,computervision,Need Help with Image Stitching for Vehicle Undercarriage Inspection - Can't Get Stitching to Work,"Hi r/computervision,

I'm working on an under-vehicle inspection system (UVIS) where I need to stitch frames from a single camera into one high-resolution image of a vehicle's undercarriage for defect detection with YOLO. I'm struggling to make the stitching work reliably and need advice or help on how to do it properly.

**Setup**:

* Single fixed camera captures frames as the vehicle moves over it.
* Python pipeline: frame\_selector.py ensures frame overlap, image\_stitcher.py uses SIFT for feature matching and homography, YOLO for defect detection.
* Challenges: Small vehicle portion per frame, variable vehicle speed causing motion blur, too many frames, changing lighting (day/night), and dynamic background (e.g., sky, not always black).

**Problem**:

* Stitching fails due to poor feature matching. SIFT struggles with small overlap, motion blur, and reflective surfaces.
* The stitched image is either misaligned, has gaps, or is completely wrong.
* Tried histogram equalization, but it doesn't fix the stitching issues.
* Found a paper using RoMa, LoFTR, YOLOv8, SAM, and MAGSAC++ for stitching, but it’s complex, and I’m unsure how to implement it or if it’ll solve my issues.

**Questions**:

1. How can I make image stitching work for this setup? What’s the best approach for small overlap and motion blur?
2. Should I switch to RoMa or LoFTR instead of SIFT? How do I implement them for stitching?
3. Any tips for handling motion blur during stitching? Should I use deblurring (e.g., DeblurGAN)?
4. How do I separate the vehicle from a dynamic background to improve stitching?
5. Any simple code examples or libraries for robust stitching in similar scenarios?


Please share any advice, code snippets, or resources on how to make stitching work. I’m stuck and need help figuring out the right way to do this. Thanks!

**Edit**: Vehicle moves horizontally, frames have some overlap, and I’m aiming for a single clear stitched image.","['You can\'t stitch images taken from a translating camera, unless what you are trying to stitch is a planar object.\n\nA car going over fixed camera = translating camera over fixed object.\n\nThe reason you can\'t stitch with homographies is that varying depth under the car induce parallax, where displacement depends on depth, making stitching impossible (or at least much harder).\n\nIf your camera is fast enough, you might try simulating a line camera... Take a single line from each image and join them into a single image.\n\nAnother approach is to go full stereo and recover depth from pairs of images. Then you get a depth map of the underside of the car, which can them be textured.\n\nTo get a better grasp of the problem, check out ""Mosaicing with Parallax using Time Warping"" by Shmuel peleg, or similar papers. Good reads.', ""This is really difficult. I think motion blur is going to make it near impossible. How about using a flash to get a non-motion blur image? Also that might make your lighting more consistent?\n\nOne thing I've done before on difficult stitching problems (mostly in microscopy/macroscopy for me) is to use a canny edge transformation, and then stitch the edge-image. Of course then apply that to the original image. That has worked for me stitching large insect specimens that had reflective surfaces mucking things up. Good luck!"", ""That's what I wanted to suggest. Sounds like adding a light projector might solve some of his problems. Something else you could do is decrease the exposure time and increase the camera's analog gain to try and get rid of the motion blur."", ""To begin with, if the image is motion blurred, you're not going to get a high quality stitched image out of it.  Secondly, you need to make sure you're narrowing the field down to not include anything outside of the car that's static, because that's going to mess with anything.  Since the camera is fixed, you should already know what portions of the camera this.  Finally, good luck.  I don't think this is going to work, but I guess there's a small chance it does.  You're better off just checking everything frame by frame instead of trying to stitch it and check."", 'When i have this sort of problem, my starting point is always ""can i do this manually"" in photoshop-esque tools.\n\nHopefully that will start making it clear where problems lie (as per other comments, perspective change, lens distortion, blur etc)', 'You are definitely going to have parallax issues, but you could try and NCC based aligner rather than a feature based aligner since you have no rotational motion to the camera. if you have small overlap, you should be able to align without needing an unrealistically large search space for the NCC.\xa0', 'I wonder if drone mapping software or concepts are relevant? Or how about photogrametry? That works by stitching photos taken from many different angles. \xa0\n\nI imagine having a high frame rate would help so you can limit each photo to just a narrow slice and still get full coverage. That’ll reduce parallax issues. Or just drive the cars really slow.\xa0\n\nYou could even get 3D information by using slices taken from an oblique angle. The math is beyond me though…but it seems like it could works that way you can see behind objects at least a little bit.\xa0', 'You can solve this without machine learning. Try using image registration calculations on the overlapping pixels\n\nCalculating this in Fourier space is also computationally efficient: it’s how panoramic images are created on your phone']","['You can\'t stitch images taken from a translating camera, unless what you are trying to stitch is a planar object.\n\nA car going over fixed camera = translating camera over fixed object.\n\nThe reason you can\'t stitch with homographies is that varying depth under the car induce parallax, where displacement depends on depth, making stitching impossible (or at least much harder).\n\nIf your camera is fast enough, you might try simulating a line camera... Take a single line from each image and join them into a single image.\n\nAnother approach is to go full stereo and recover depth from pairs of images. Then you get a depth map of the underside of the car, which can them be textured.\n\nTo get a better grasp of the problem, check out ""Mosaicing with Parallax using Time Warping"" by Shmuel peleg, or similar papers. Good reads.', ""This is really difficult. I think motion blur is going to make it near impossible. How about using a flash to get a non-motion blur image? Also that might make your lighting more consistent?\n\nOne thing I've done before on difficult stitching problems (mostly in microscopy/macroscopy for me) is to use a canny edge transformation, and then stitch the edge-image. Of course then apply that to the original image. That has worked for me stitching large insect specimens that had reflective surfaces mucking things up. Good luck!"", ""That's what I wanted to suggest. Sounds like adding a light projector might solve some of his problems. Something else you could do is decrease the exposure time and increase the camera's analog gain to try and get rid of the motion blur."", ""To begin with, if the image is motion blurred, you're not going to get a high quality stitched image out of it.  Secondly, you need to make sure you're narrowing the field down to not include anything outside of the car that's static, because that's going to mess with anything.  Since the camera is fixed, you should already know what portions of the camera this.  Finally, good luck.  I don't think this is going to work, but I guess there's a small chance it does.  You're better off just checking everything frame by frame instead of trying to stitch it and check."", 'When i have this sort of problem, my starting point is always ""can i do this manually"" in photoshop-esque tools.\n\nHopefully that will start making it clear where problems lie (as per other comments, perspective change, lens distortion, blur etc)']",2,16,0.75,Help: Project,1749903699.0
1lb5pre,computervision,Help : Yolov8n continual training,"I have custom trained a yolov8n model on some data and I want to train it on more data but a different one but I am facing the issue of catastrophic forgetting and I am just stuck there like I am training it to detect vehicles and people but if I train it on vehicles it won't detect people which is obvious but when I use a combined dataset of both vehicle and people the it won't recognize vehicles I am just so tired of searching for methods please help me , I am just a beginner trying to get into this.","['Changing your training dataset will change the output head, which in essence means that prior output predictions will not exist anymore. If your target is people and vehicles, you must always include both as an output option, even if your dataset only includes images of one.', ""Are you adding both classes to your class data? Them being in the same dataset isn't enough."", 'You need to store all the precious training data and add the new stuff to it.\n\nEvery image needs to be labeled for all of the classes (“Null” is a label, a very important one). In your example you have a model that already recognizes people and you want to add cars, well you will need to label any people in the new car images. You could do that automatically using the model, but it’s best to do it manually.\xa0', ""Continual training is hard. If you're just doing this to learn, then reading and implementing lots of different methods is a great thing to do to build intuition. If this is for a larger project, then telling us those details may help the sub help you identify a different path forward.\n\nAlso, please use punctuation in writing."", 'Yes this is expected. You can fine-tune on a pretrained model but if you want to retain the pretrained classes then you need to train on both (old and new) datasets simultaneously.']","['Changing your training dataset will change the output head, which in essence means that prior output predictions will not exist anymore. If your target is people and vehicles, you must always include both as an output option, even if your dataset only includes images of one.', ""Are you adding both classes to your class data? Them being in the same dataset isn't enough."", 'You need to store all the precious training data and add the new stuff to it.\n\nEvery image needs to be labeled for all of the classes (“Null” is a label, a very important one). In your example you have a model that already recognizes people and you want to add cars, well you will need to label any people in the new car images. You could do that automatically using the model, but it’s best to do it manually.\xa0', ""Continual training is hard. If you're just doing this to learn, then reading and implementing lots of different methods is a great thing to do to build intuition. If this is for a larger project, then telling us those details may help the sub help you identify a different path forward.\n\nAlso, please use punctuation in writing."", 'Yes this is expected. You can fine-tune on a pretrained model but if you want to retain the pretrained classes then you need to train on both (old and new) datasets simultaneously.']",0,7,0.5,Help: Project,1749897619.0
1lawyk4,computervision,Teaching Line of Best Fit with a Hand Tracking Reflex Game,"Last week I was teaching a lesson on quadratic equations and lines of best fit. I got the question I think every math teacher dreads: ""But sir, when are we actually going to use this in real life?""

Instead of pulling up another projectile motion problem (which I already did), I remembered seeing a viral video of FC Barcelona's keeper, Marc-André ter Stegen, using a light up reflex game on a tablet. I had also followed a tutorial a while back to build a similar hand tracking game. A lightbulb went off. This was the perfect way to show them a real, cool application (again).

**The Setup: From Math Theory to Athlete Tech**

I told my students I wanted to show them a project. I fired up this hand tracking game where you have to ""hit"" randomly appearing targets on the screen with your hand. I also showed the the video of  Marc-André ter Stegen using something similar. They were immediately intrigued.

**The ""Aha!"" Moment: Connecting Data to the Game**

This is where the math lesson came full circle. I showed them the raw data collected:

*x is the raw distance between two hand keypoints the camera sees (in pixels)*

>x = \[300, 245, 200, 170, 145, 130, 112, 103, 93, 87, 80, 75, 70, 67, 62, 59, 57\]

*y is the actual distance the hand is from the camera measured with a ruler (in cm)*

>y = \[20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100\]

(it was already measured from the tutorial but we re measured it just to get the students involved).

I explained that to make the game work, I needed a way to predict the distance in cm for any pixel distance the camera might see. And how do we do that? By finding a curve of best fit.

Then, I showed them the single line of Python code that makes it all work:


*This one line finds the best-fitting curve for our data*

>coefficients = np.polyfit(x, y, 2) 

*The result is our old friend, a quadratic equation: y = Ax**^(2)* *+ Bx + C*

**The Result**

Honestly, the reaction was better than I could have hoped for (instant class cred).

It was a powerful reminder that the ""how"" we teach is just as important as the ""what."" By connecting the curriculum to their interests, be it gaming, technology, or sports, we can make even complex topics feel relevant and exciting.

Sorry for the long read.

Repo: [https://github.com/donsolo-khalifa/HandDistanceGame](https://github.com/donsolo-khalifa/HandDistanceGame)

Leave a star if you like the project

","['i got here from the Barca post and what you did is so cool, as a machine learning student i really appreciate your work man, keep it up❤️', 'Crazy']","['i got here from the Barca post and what you did is so cool, as a machine learning student i really appreciate your work man, keep it up❤️', 'Crazy']",40,4,0.98,Showcase,1749865267.0
1lajky1,computervision,Ultralytics YOLO,"Hi, has anybody successfully implemented a deformable convolution layer in the ultralytics module, I have been trying for a week and facing all kinds of error from shape mismatch to segmentation fault.",[],[],0,0,0.5,Help: Project,1749830316.0
1lajgnu,computervision,Training on real data and testing on synthetic data,"Hi everyone, i have trained my model on real aerial data that includes drones, planes, and birds. However, when I test it on simulated data, the performance drops noticeably. Would it make sense to include synthetic data in the training set to improve generalization?

If so, how can I avoid overfitting to the synthetic scenes specially if there's a risk of the model memorizing specific visuals that it will later be tested on?

Also, my dataset is quite imbalanced: around 90% of the samples are drones, and only 10% are other objects. Do you have any training recommendations to address this imbalance effectively?

Thanks in advance!
","['You should NEVER do this.\n\nTrain on a mix of both if you have to, and validate on real only.', 'First of all, what is the requirement and end use? What is the use of this model?', ""I would include at least some synthetic data with the training data, use the same annotatio process you do for the real data. I find the pixilation and lighting with synthetic data can make it fairly significantly different in training results depending on the material you're using."", 'If I were you, I’d split the real dataset into training and test sets, and use synthetic data to augment the training set. You can also use it to balance the classes.  \nPlease keep in mind, your mileage may vary depending on the quality and variability of your synthetic data.', ""it doesn't  sound a good approach, why don't you make synthetic data, you mix to the original data and then split everything in training, test and validation?"", 'If the synthetic data does not vary a lot, and you want to explore possibilities where you wouldn\'t need to label the synthetic data (atleast not 100%), then consider trying Unsupervised Domain Adaptation (UDA). \n\nAnd for working with UDA checkout \'\'TL lib"" on GitHub.']","['You should NEVER do this.\n\nTrain on a mix of both if you have to, and validate on real only.', 'First of all, what is the requirement and end use? What is the use of this model?', ""I would include at least some synthetic data with the training data, use the same annotatio process you do for the real data. I find the pixilation and lighting with synthetic data can make it fairly significantly different in training results depending on the material you're using."", 'If I were you, I’d split the real dataset into training and test sets, and use synthetic data to augment the training set. You can also use it to balance the classes.  \nPlease keep in mind, your mileage may vary depending on the quality and variability of your synthetic data.', ""it doesn't  sound a good approach, why don't you make synthetic data, you mix to the original data and then split everything in training, test and validation?""]",0,7,0.5,Discussion ,1749830017.0
1laionb,computervision,How to find where 2 videos from different camera feeds overlap,"Hi guys,

I am working on a project where I have pairs of videos (query, reference), taken from different camera perspectives (different angles of a car intersection) and I want to find where is the frame X of the reference video that corresponds to frame 0 of the query video.

Do you know how I could approach this problem? Thanks in advance!","['Can you provide two sample frames where you add example annotations of what you would expect to be found? Or rephrase you question, please?\n\nWhat do you mean with ""overlap"", like multiple images put together to a panoramic image?', 'Maybe something like SuperPoint plus SuperGlue? Look for a spike in the number of “close” matches - that’s when the two videos start to overlap.\xa0\n\nIt may also be possible to just compare overall embeddings of each image. A model like dinov2 can generate useful embeddings that will be more similar between images that overlap. Measure the cosine distance or some other vector distance metric.\xa0', 'I think a simpler description of the problem you are facing is ""given an image in perspective A, which images from this dataset in perspective B is the closest"". I would assume there are features such as cars and pedestrian that can be used to do the matching. If this is the case, a VLM could extract the info of those specific key objects, and thats what could be compared', 'Cool problem! Do you have any other info like camera extrinsic/intrinsic matrices so we can transform one camera image to the other?']","['Can you provide two sample frames where you add example annotations of what you would expect to be found? Or rephrase you question, please?\n\nWhat do you mean with ""overlap"", like multiple images put together to a panoramic image?', 'Maybe something like SuperPoint plus SuperGlue? Look for a spike in the number of “close” matches - that’s when the two videos start to overlap.\xa0\n\nIt may also be possible to just compare overall embeddings of each image. A model like dinov2 can generate useful embeddings that will be more similar between images that overlap. Measure the cosine distance or some other vector distance metric.\xa0', 'I think a simpler description of the problem you are facing is ""given an image in perspective A, which images from this dataset in perspective B is the closest"". I would assume there are features such as cars and pedestrian that can be used to do the matching. If this is the case, a VLM could extract the info of those specific key objects, and thats what could be compared', 'Cool problem! Do you have any other info like camera extrinsic/intrinsic matrices so we can transform one camera image to the other?']",2,6,0.75,Help: Project,1749828111.0
1lahyhm,computervision,"LightlyTrain x DINOv2: Smarter Self-Supervised Pretraining, Faster",,['Nice to see that distillation also works well. That means I can take the huge pretrained dinov2 models and distill them to my custom model architecture. Very nice!'],['Nice to see that distillation also works well. That means I can take the huge pretrained dinov2 models and distill them to my custom model architecture. Very nice!'],11,3,0.92,Showcase,1749826398.0
1laggb1,computervision,Stuck: Detecting symbols from engineering floor plan (vector PDF → DWG/SVG/DXF or CV?),"Hey everyone,

I’m building a Python tool to extract symbols & wall patterns from floor plans. The idea is to detect symbols from the *legend section*, then find & count them across the actual plan.

**The input:**

* I get vectorized PDFs (exported from AutoCAD or similar).
* I can convert to DWG / DXF / SVG.
* Symbols in the legend have text descriptions, and the same symbols repeat across the plan.

**The problem:**

* Symbols aren’t stored as blocks/inserts — they’re broken down into low-level geometry: polylines, polygons, etc.
* I tried converting to high-res PNG and applying CV (masking, template matching, feature matching) — but it’s been very unstable:
   * Background clutter overlaps symbols.
   * Many false positives & missed detections.
   * Matching scores are unreliable.

**My question:**

* Should I shift focus to the vector formats? (e.g. directly parse DWG/SVG geometry?)
* Or is there a more stable CV approach for symbol detection in this context?

Been spending lots more time than I planned on this one, so any advice, experiences, or even partial thoughts would be super helpful 🙏","['Any samples to share?', 'Can you get the original dxf/ dwg files, there normally symbols and legends would be on separate layers, this would greatly simplify your symbol classification.\n\nYou might also consider cnn for symbol classification, need to augment the training data with all orientations of the symbols, and need to know the predefined symbol set.', 'Depending on your tolerance for errors I would use a VLM to get an image of each icon, then use an object detector or even just a template matcher to find them on the rest of the page.\n\nI imagine most VLMs can do a decent job of locating the legend and picking out the icons.\xa0\n\nHow much Dara do you have for development and how fast do you need this to run?']","['Any samples to share?', 'Can you get the original dxf/ dwg files, there normally symbols and legends would be on separate layers, this would greatly simplify your symbol classification.\n\nYou might also consider cnn for symbol classification, need to augment the training data with all orientations of the symbols, and need to know the predefined symbol set.', 'Depending on your tolerance for errors I would use a VLM to get an image of each icon, then use an object detector or even just a template matcher to find them on the rest of the page.\n\nI imagine most VLMs can do a decent job of locating the legend and picking out the icons.\xa0\n\nHow much Dara do you have for development and how fast do you need this to run?']",1,3,1.0,Help: Project,1749822687.0
1lafaei,computervision,Looking for an Accurate 3D Color Point Cloud SLAM Algorithms for High-Precision Mapping,"I’m working on a project that requires super accurate 3D color point cloud SLAM for both localization and mapping, and I’d love your insights on the best algorithms out there.
I have currently used fast-lio( not accurate enough), fast-livo2(really accurate, but requires hard-synchronization)

My Setup:
•  LiDAR: Ouster OS1-128 and Livox Mid360
•  Camera: Intel RealSense D456

Requirements
•  Localization: ~ 10 cm error over a 100-meter trajectory .
•  Object Measurement Accuracy:10 precision. For example, if I have a 10 cm box in the point cloud, it should measure ~10 cm in the map, not 15 cm or something
•  3D Color Point Clouds: Need RGB-textured point clouds for detailed visualization and mapping.

I’m looking for open-source SLAM algorithms that can leverage my LiDARs and RealSense camera to hit these specs. I’ve got the hardware to generate dense point clouds, but I need guidance on which algorithms are the most accurate for this use case.

I’m open to experimenting with different frameworks (ROS/ROS2, Python, C++, etc.) and tweaking parameters to get the best results. If you’ve got sample configs, tutorials , please share!

Thanks in advance for any advice or pointers
","['What is the accuracy of your LIDAR and camera system? How are you calibrating your camera to LIDAR? The question here should be that is your setup capable of the accuracy you are asking for? At a distance of 20m if you want to measure a box of 15cm with that is kind of accuracy, you have to buy a LIDAR with much much higher angular precision.', ""you should look at the papers at this year's CVPR, there is some really impressive work in that direction\n\nbut im not sure about with that specific sensor setup. you may be able to do better than you think with just vslam""]","['What is the accuracy of your LIDAR and camera system? How are you calibrating your camera to LIDAR? The question here should be that is your setup capable of the accuracy you are asking for? At a distance of 20m if you want to measure a box of 15cm with that is kind of accuracy, you have to buy a LIDAR with much much higher angular precision.', ""you should look at the papers at this year's CVPR, there is some really impressive work in that direction\n\nbut im not sure about with that specific sensor setup. you may be able to do better than you think with just vslam""]",6,5,1.0,Help: Project,1749819544.0
1lacmjq,computervision,question: getting mit licensed yolov9 to work,"Hello, has anyone ever implemented the MIT licensed version of YOLO by MultimediaTechLab and gotten it to work.  I have attempted to do this on colab, on my ide, but it just won´t. After a lot of changing configuration it just crashes and I don´t know what to change so it uses GPU. If anyone has done this and knows how please share.thank you","['Can you provide more details, please? Which code-base, which repo, which code have you used? Instead of us using search engines we might end up using a different code-base than you.\n\nWhat are its dependencies, in which versions?\n\n""got it to work"", do you mean training, faintuning, quantization - or do you mean inference?', 'Have you verified that torch itself can use the GPU in the specific environment you’re using for the model?', 'Ive had better luck with d-fine and rt-detr2. Rb-detr is probably also worth checking out.']","['Can you provide more details, please? Which code-base, which repo, which code have you used? Instead of us using search engines we might end up using a different code-base than you.\n\nWhat are its dependencies, in which versions?\n\n""got it to work"", do you mean training, faintuning, quantization - or do you mean inference?', 'Have you verified that torch itself can use the GPU in the specific environment you’re using for the model?', 'Ive had better luck with d-fine and rt-detr2. Rb-detr is probably also worth checking out.']",1,5,1.0,Help: Project,1749811008.0
1labawt,computervision,Is micro-particle detection feasible in real time?,"Hello,
I'm currently working on a project where I need to track microparticles in real time.

These microparticles appear as fiber-like black lines.
They can rotate in any direction, and their shapes vary in both length and width.

https://preview.redd.it/hnhnckghsn6f1.png?width=528&format=png&auto=webp&s=8064e1a3948a7c4e305606b8e5871142b60aa41b

[Example of the camera live feed](https://preview.redd.it/u00zgddiun6f1.png?width=884&format=png&auto=webp&s=ed1bbb3124c12f6a4f39ad6605e7624ca4f6dd94)


Is it possible to accurately track at least a small cluster of these fibers in real time?

I’ve followed some YouTube tutorials to train a YOLOv8 model on a small dataset (500 images), but the results are quite poor. The model struggles to detect the fibers accurately.

Have a good day,
(text corrected by CHATGPT just in case the system flags it as an AI generated post)","[""Yes it's feasible and you probably don't need yolov8. It seems a first glance like a simple thresholding and connected components problem."", 'Are these magnetic nanoparticles?   What are your goals?\n  \nBased on these images, I agree with the other person that it’s a simple threshold + mathematical morphology + connected component situation. Assuming they’re magnetic particles (so they should all be going in the same direction), it would be easy to fit an ellipse or rectangle to your masked blobs of that helps you assign a more standardized length/direction metric.  \n  \nAlso check out medial axis and skeletonization algos (skimage skeletonization is fast) if you want to convert your blob chains into single-pixel-width connected lines.', ""Aim: your use of terminology is confusing. Are you doing just detection, or detection+tracking?\n\nDataset size: 500 images is not a lot. Did you do any augmentation? For example, you mentioned the particles can rotate, so you can rotate your training images, thus significantly expanding your dataset.\n\nDataset representation: how did you select those 500 samples for training? Poor model performance may be because your dataset isn't diverse enough so it can't extrapolate to unseen data. Proper data selection is crucial.\n\nAlternative solution: you can try using classical image processing to segment pixels of particles. From your example image it seems you're dealing with grayscale data with 3 parts: white background, some noise/smaller particles, and your larger particles of interest. You can try building a pipeline which divides the image into foreground (particles) and background (everything else). You won't even need deep learning or manual annotations."", 'Also, optical flow (sparse) would be the simplest way to track these, if the goal was to measure their speed or something.', ""optical flow etc... I've worked with microscopy images where faint fiber like particles had to be detected.  \nThese kind of tasks are hard for convolution based solutions by their nature."", 'if only need to detect and track use sahi with yolo tracker']","[""Yes it's feasible and you probably don't need yolov8. It seems a first glance like a simple thresholding and connected components problem."", 'Are these magnetic nanoparticles?   What are your goals?\n  \nBased on these images, I agree with the other person that it’s a simple threshold + mathematical morphology + connected component situation. Assuming they’re magnetic particles (so they should all be going in the same direction), it would be easy to fit an ellipse or rectangle to your masked blobs of that helps you assign a more standardized length/direction metric.  \n  \nAlso check out medial axis and skeletonization algos (skimage skeletonization is fast) if you want to convert your blob chains into single-pixel-width connected lines.', ""Aim: your use of terminology is confusing. Are you doing just detection, or detection+tracking?\n\nDataset size: 500 images is not a lot. Did you do any augmentation? For example, you mentioned the particles can rotate, so you can rotate your training images, thus significantly expanding your dataset.\n\nDataset representation: how did you select those 500 samples for training? Poor model performance may be because your dataset isn't diverse enough so it can't extrapolate to unseen data. Proper data selection is crucial.\n\nAlternative solution: you can try using classical image processing to segment pixels of particles. From your example image it seems you're dealing with grayscale data with 3 parts: white background, some noise/smaller particles, and your larger particles of interest. You can try building a pipeline which divides the image into foreground (particles) and background (everything else). You won't even need deep learning or manual annotations."", 'Also, optical flow (sparse) would be the simplest way to track these, if the goal was to measure their speed or something.', ""optical flow etc... I've worked with microscopy images where faint fiber like particles had to be detected.  \nThese kind of tasks are hard for convolution based solutions by their nature.""]",22,11,0.97,Help: Project,1749805714.0
1la9g9f,computervision,An Important Interview | Any suggestion would help.,"I am fresh graduate and I have got an on-site interview offer from a company. They usually don't hire fresh grads. The HR sent me the mail in which he mentioned the content of interview :

\-> Domain deep dive - Computer Vision & Model development

 I am already familiar with some concepts of computer vision - not a pro though. I have three days. How do I prepare best. Any resources or suggestion would be highly appreciated.

Regards","['Following', 'I had a similar thing for my company. I just went over my projects I did at uni to keep them fresh in memory. I also checked out some videos of Georgia Tech on things I was no longer fully certain about.\n\nThe biggest thing is to know the connecting parts of the course, not one implementation or something like that. Just be able to understand existing problems, why they are annoying to deal with and what ways we have to solve them or maybe even side-step the problem.\n\nI know this is not much, but this is what I did and I got the job, so 😅', 'Soft skills are wayyy important than hard skills', 'Wear glasses like bill gates']","['Following', 'I had a similar thing for my company. I just went over my projects I did at uni to keep them fresh in memory. I also checked out some videos of Georgia Tech on things I was no longer fully certain about.\n\nThe biggest thing is to know the connecting parts of the course, not one implementation or something like that. Just be able to understand existing problems, why they are annoying to deal with and what ways we have to solve them or maybe even side-step the problem.\n\nI know this is not much, but this is what I did and I got the job, so 😅', 'Soft skills are wayyy important than hard skills', 'Wear glasses like bill gates']",2,8,0.75,Help: Theory ,1749798009.0
1la6et7,computervision,ResNet-50 on CIFAR-100: modest accuracy increase from quantization + knowledge distillation (with code),"
Hi everyone,
I wanted to share some hands-on results from a practical experiment in compressing image classifiers for faster deployment. The project applied Quantization-Aware Training (QAT) and two variants of knowledge distillation (KD) to a ResNet-50 trained on CIFAR-100.

**What I did:**

* Started with a standard FP32 ResNet-50 as a baseline image classifier.
* Used QAT to train an INT8 version, yielding \~2x faster CPU inference and a small accuracy boost.
* Added KD (teacher-student setup), then tried a simple tweak: adapting the distillation temperature based on the teacher’s confidence (measured by output entropy), so the student follows the teacher more when the teacher is confident.
* Tested CutMix augmentation for both baseline and quantized models.

**Results (CIFAR-100):**

* FP32 baseline: 72.05%
* FP32 + CutMix: 76.69%
* QAT INT8: 73.67%
* QAT + KD: 73.90%
* QAT + KD with entropy-based temperature: 74.78%
* QAT + KD with entropy-based temperature + CutMix: 78.40% (All INT8 models run \~2× faster per batch on CPU)

**Takeaways:**

* With careful training, INT8 models can *modestly but measurably* beat FP32 accuracy for image classification, while being much faster and lighter.
* The entropy-based KD tweak was easy to add and gave a small, consistent improvement.
* Augmentations like CutMix benefit quantized models just as much (or more) than full-precision ones.
* Not SOTA—just a practical exploration for real-world deployment.

**Repo:** [https://github.com/CharvakaSynapse/Quantization](https://github.com/CharvakaSynapse/Quantization)

**Looking for advice:**
If anyone has feedback on further improving INT8 model accuracy, or experience scaling these tricks to bigger datasets or edge deployment, I’d really appreciate your thoughts!","['Nice learning setup! If you can train a bigger teacher model than resnet50 that would get a better accuracy, that would help the quantized resnet50 student model to reach a better accuracy.', ""Great experiment! I don't gdt one thing, which model is student and which one teacher.\nWhen you type int8 + KD it means distillation from FP32 with CutMix or without?""]","['Nice learning setup! If you can train a bigger teacher model than resnet50 that would get a better accuracy, that would help the quantized resnet50 student model to reach a better accuracy.', ""Great experiment! I don't gdt one thing, which model is student and which one teacher.\nWhen you type int8 + KD it means distillation from FP32 with CutMix or without?""]",15,7,0.95,Help: Project,1749786733.0
1la5w1d,computervision,Best Standalone Outdoor Camera with Battery & Connectivity for vehicle tracking,"Hi all,
Looking for a standalone outdoor camera (60+ FPS, battery-powered, weatherproof) that can upload video to the cloud for computer vision tasks,any recommendations?",['an old iphone in a waterproof box! (and some custom software :)'],['an old iphone in a waterproof box! (and some custom software :)'],1,1,1.0,Help: Project,1749784987.0
