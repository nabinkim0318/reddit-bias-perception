id,subreddit,title,selftext,comments,top_comments,score,num_comments,upvote_ratio,flair,created_utc
1lsvtlj,StableDiffusion,Best Illustrious (anime) Model?,"What is currently the best illustrious (anime) model in your opinion and why? I feel like the ranking on Civit is not accurate, frankly the most popular illustrious models right now are not the best. Current highest rated model monthly:


1. JANKUv4: It's alright but I don't like the shiny sheen that it has.
2. Prefect Illustrious: Nothing special, tends to favor overly curvy females.
3. ilustmix: A very good semi-realistic model, 50/50 realism/anime mix.
4. Nova Anime: Has good colors, more saturated, more contrast.
5. One obsession: Best model out of the top 5 imo, better color and more balanced lighting.

And actually I think WAI is still very good even though it doesn't rank high anymore. I have tried less popular models that are clearly better than the current top 5. ",[],[],1,0,1.0,Discussion,1751788569.0
1lsx9h6,StableDiffusion,"Generation comes out as over saturated, grainy, and deep fried","So I’m trying to use model: Obsession (Illustrious XL) v-pred_v1.1, but everytime I try and use it my generations would come out this way. What am I doing wrong here? I’m still a noob at this but I want to understand why this keeps on happening with this ONE base model.

My generation settings:
- Sampling Method: Eurla A
- Upscaler: R-ESRGAN 4x+ Anime6B
- Schedule Type: Align Your Steps
- Sampling Steps: 20
- Hires Steps: 15
- Denoising Strength 0.7
- 832 Width
- 1216 Height
- CGF Scale: 1

Please someone intelligent tell me what I can do to fix this.",[],[],1,0,1.0,Question - Help,1751794521.0
1lt36v2,StableDiffusion,Has anyone been able to install Phidias diffusion text to 3D?,"I've been trying to get Phidias Diffusion to work, but it always fails when attempting to install diff-gaussian-rasterization. Is there anyone who knows how to run this properly?

[https://github.com/3DTopia/Phidias-Diffusion](https://github.com/3DTopia/Phidias-Diffusion)

",[],[],1,0,1.0,Question - Help,1751813753.0
1lszgcc,StableDiffusion,can someone help a complete newbie w/hardware choices?,"hi all

as per subject, i'm very new to this and have spent a few weeks researching the various approaches, ui's and models etc. i'm just a bit unsure on hardware.

i currently have a mac mini m4, but have been wanting to go back to windows for a while.

i'd like to build a budget system. system will be mostly used for music production, stable diffusion, and a small amount of gaming.

i'm torn between going for a used 3060 12gb (around £180 on ebay) or an arc b580 (around £250)

can anyone give me some advice?","[""Get Nvidia's GPU with as much v-ram as you can."", 'If you are a newb, NVidia is your only real option. While you *can* use AI on other cards, just getting it working properly is a hobbyist project of its own. The 3060 12 GB sounds like a reasonable budget choice, but getting 16 GB would be much more helpful.']","[""Get Nvidia's GPU with as much v-ram as you can."", 'If you are a newb, NVidia is your only real option. While you *can* use AI on other cards, just getting it working properly is a hobbyist project of its own. The 3060 12 GB sounds like a reasonable budget choice, but getting 16 GB would be much more helpful.']",1,2,1.0,Question - Help,1751802996.0
1lt2rtb,StableDiffusion,I documented every Fooocus 2.5.5 setting for 6 months. Then taught AI to configure them perfectly.,"Look at this portrait. First generation, 40 seconds. Zero post-processing.

Getting here took 6 months of documenting every Fooocus setting, model comparison, and LoRA weight that actually matters.

Then I built HyperFocus-AI - a meta-prompt that turns any LLM into a Fooocus 2.5.5 configuration expert.

**What HyperFocus-AI does:**

**My simple input:** ""A young woman with pink hair and freckles""

**What it returned:**

Enhanced Prompt: ""A young woman with pastel pink hair and soft freckles across her cheeks and nose, facing the camera with a calm, serene expression, hyper-detailed skin texture with visible pores and fine peach fuzz, soft window light from the side, smooth bokeh background, natural makeup, delicate eyelashes, subtle catchlights in the eyes, captured with an 85mm lens at f/1.4, ultra high-resolution portrait photography style""

**Complete Technical Configuration:**

* Base Model: RealVisXL\_V5.0
* LoRA Stack: Add\_Details\_v1.2.safetensors (0.5), PerfectEyesXL.safetensors (0.3), warm\_light\_style\_v2 (0.5)
* CFG Scale: 7.5
* Image Sharpness: 5.0
* Sampler: dpmpp 2m sde gpu
* Selected Styles: \[""Fooocus V2"", ""Fooocus Photograph"", ""Fooocus Enhance""\]

**Plus:** Carefully crafted negative prompt to eliminate common artifacts

First generation using these exact settings. This level of photorealism used to take me 50+ attempts.

**Why this changes everything:**

The prompt transformed my basic idea into:

* Professional photography language
* Specific technical details (85mm f/1.4)
* Skin texture optimization
* Lighting direction
* Every parameter calculated

No more:

* ""Which model for portraits?""
* ""What CFG scale?""
* Plastic-looking skin
* Wasted generations

# The Meta-Prompt:

\[[Fooocus 2.5.5 Optimizer](https://github.com/Prestigious-Fan118/Fooocus-2.5.5-Optimizer/blob/main/Fooocus%202.5.5%20Optimizer)\]

# How to use:

1. Copy HyperFocus-AI into ChatGPT
2. Give it any image idea
3. Copy the complete configuration into Fooocus 2.5.5
4. Generate photorealistic perfection

**Pro Tip:** For optimal results, tell HyperFocus-AI which base models and LoRAs you have installed. It will adapt its recommendations to YOUR available resources rather than suggesting models you don't have.

Not selling anything. No course. Just sharing 6 months of research.

P.S. - From ""pink hair and freckles"" to museum-quality portrait settings. This is what systematic optimization looks like.

P.P.S. - Yes, I'm using AI to optimize AI. Welcome to 2025.","[""all this but you still don't understand sdxl doesn't support natural language prompts like you're using here\n\nalso, almost every single SDXL finetune has a different recommended sampler and CFG, which you'll have to manually use anyway for best results\n\njust learn to use your tools man, you don't have to over complicate things"", ""Amusingly, you also had ChatGPT write this post, didn't you?"", 'This is nothing special dude', 'Glad it works for you. I’m out of the loop for a few more months until this all becomes commonplace.', 'Impressive quality for straight text to image. This only works for photo realism?']","[""all this but you still don't understand sdxl doesn't support natural language prompts like you're using here\n\nalso, almost every single SDXL finetune has a different recommended sampler and CFG, which you'll have to manually use anyway for best results\n\njust learn to use your tools man, you don't have to over complicate things"", ""Amusingly, you also had ChatGPT write this post, didn't you?"", 'This is nothing special dude', 'Glad it works for you. I’m out of the loop for a few more months until this all becomes commonplace.', 'Impressive quality for straight text to image. This only works for photo realism?']",12,7,0.71,Tutorial - Guide,1751812661.0
1lt0vnd,StableDiffusion,Question. I have a image of a bartender behind a bar next to a line of beer taps. If I create a video from the image asking for him to pour a beer from the taps will it work?,,"['Yeah, if you have the hardware to render a long enough sequence.  But you will have to fight the usual AI stuff: the glass magically appearing in his hand, etc.  Might have to generate an end sequence first for use as a keyframe for it to be a practical goal.']","['Yeah, if you have the hardware to render a long enough sequence.  But you will have to fight the usual AI stuff: the glass magically appearing in his hand, etc.  Might have to generate an end sequence first for use as a keyframe for it to be a practical goal.']",0,1,0.5,Question - Help,1751807447.0
1lt0d3n,StableDiffusion,Speeding up WAN VACE,"I don't think SageAttention or TeaCache works with WAN. I've already lowered my resolution and set my input to a lower FPS.

Is there anything else I can do to speed up the inference?","['[https://huggingface.co/Kijai/WanVideo\\_comfy/blob/main/Wan21\\_T2V\\_14B\\_lightx2v\\_cfg\\_step\\_distill\\_lora\\_rank32.safetensors](https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_T2V_14B_lightx2v_cfg_step_distill_lora_rank32.safetensors)\n\nPlug this in as a LoRA at a strength of 1.\n\nReduce your steps to 4 and CFG to 1.\n\nEnjoy :D', 'causvid', 'CauseVid lora works.\n\nI found though having a strength of 0.5 with no SLG gets me speed and quality.\n\nAh also I do about 8 steps', 'You can try wan 2.1 VACE fusion x fine tune 8 steps model.', 'Both Sageattention and teacache definitely work.']","['[https://huggingface.co/Kijai/WanVideo\\_comfy/blob/main/Wan21\\_T2V\\_14B\\_lightx2v\\_cfg\\_step\\_distill\\_lora\\_rank32.safetensors](https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_T2V_14B_lightx2v_cfg_step_distill_lora_rank32.safetensors)\n\nPlug this in as a LoRA at a strength of 1.\n\nReduce your steps to 4 and CFG to 1.\n\nEnjoy :D', 'causvid', 'CauseVid lora works.\n\nI found though having a strength of 0.5 with no SLG gets me speed and quality.\n\nAh also I do about 8 steps', 'You can try wan 2.1 VACE fusion x fine tune 8 steps model.', 'Both Sageattention and teacache definitely work.']",0,6,0.5,Question - Help,1751805945.0
1lszkso,StableDiffusion,"Can anyone tell me the best AI voiceover generator that has various characters like Peter Griffin, Goku, Homer Simpson, etc?",,['Why not make your own character voices and come up with your own ideas?'],['Why not make your own character voices and come up with your own ideas?'],0,1,0.17,Question - Help,1751803409.0
1lsy14m,StableDiffusion,Which local ai can generate image and factual text output? I did these with an chatgpt type ai but is there a way to do them locally?,,"['> factual \n\nI’ll stop you right there', 'You can easily load an LLM in ComfyUI and then have that create a prompt for an image model all in one workflow. It’s the same approach commercial services use as well, there isn’t a unified GPT that does text and image, it’s a pipeline of different systems working together.', ""I don't think there's anything that rivals chatGPT / Sora when it comes to this."", 'Not possible with current tech. Your best bet is just using Photoshop. Maybe next year.', 'The models by Black Forest Labs, who developed, among others, ""Flux.1"" and ""Flux.1 Kontext"" have incredibly good performance, and BFL also offers `dev` versions of its models...smaller distills of their larger ""Pro"" models, designed to run on consumer hardware:\n\nhttps://bfl.ai/models/flux-kontext']","['> factual \n\nI’ll stop you right there', 'You can easily load an LLM in ComfyUI and then have that create a prompt for an image model all in one workflow. It’s the same approach commercial services use as well, there isn’t a unified GPT that does text and image, it’s a pipeline of different systems working together.', ""I don't think there's anything that rivals chatGPT / Sora when it comes to this."", 'Not possible with current tech. Your best bet is just using Photoshop. Maybe next year.', 'The models by Black Forest Labs, who developed, among others, ""Flux.1"" and ""Flux.1 Kontext"" have incredibly good performance, and BFL also offers `dev` versions of its models...smaller distills of their larger ""Pro"" models, designed to run on consumer hardware:\n\nhttps://bfl.ai/models/flux-kontext']",0,10,0.39,Question - Help,1751797622.0
1lsxu36,StableDiffusion,Anything I can do to improve generation speed with Chroma?,"Hey, i have just only 8gb vram and I know it's probably not realistic to strive for faster generation but it takes me about 5mins for a single image. Just wondering if there's anything I can do about it? Thanks in advance. ","['There is a low step new model posted https://civitai.com/models/1330309/chroma', 'You can use 8 step lora to speed up some', 'Hyper chroma low step lora - I use it with 10 steps (so render time 2x faster compared to the default 20 steps), and it improves quality of the images too, makes hands and details better.\n\nIt fixes the smudged backgrounds and reduces the distortion of details too.\n\n[https://huggingface.co/silveroxides/Chroma-LoRA-Experiments/blob/main/Hyper-Chroma-low-step-LoRA.safetensors](https://huggingface.co/silveroxides/Chroma-LoRA-Experiments/blob/main/Hyper-Chroma-low-step-LoRA.safetensors)', 'Try using a smaller gguf version like a Q4.', 'Set the CFG to 1 after 50% of steps are done and it’ll cut the time down by about 25%', 'TorchCompile and sageattn help', ""Are you using the [FluxMod KSampler](https://github.com/lodestone-rock/ComfyUI_FluxMod)? If you set activation\\_casting to fp16 in the FluxMod KSampler, you should be able to render a typical 1024x1024 image in about 2 minutes, unless you're using one of the slower samplers like heun or dpmpp\\_2s\\_ancestral.  \nI also have 8 GB of VRAM for reference."", ""I have a 12GB card and a typical 1024x1024, 26 step eurler simple image takes about three minutes. I had to reinstall my entire ComfyUI folder so I'm not sure if that's the expected performance. I'm on a 3060.""]","['There is a low step new model posted https://civitai.com/models/1330309/chroma', 'You can use 8 step lora to speed up some', 'Hyper chroma low step lora - I use it with 10 steps (so render time 2x faster compared to the default 20 steps), and it improves quality of the images too, makes hands and details better.\n\nIt fixes the smudged backgrounds and reduces the distortion of details too.\n\n[https://huggingface.co/silveroxides/Chroma-LoRA-Experiments/blob/main/Hyper-Chroma-low-step-LoRA.safetensors](https://huggingface.co/silveroxides/Chroma-LoRA-Experiments/blob/main/Hyper-Chroma-low-step-LoRA.safetensors)', 'Try using a smaller gguf version like a Q4.', 'Set the CFG to 1 after 50% of steps are done and it’ll cut the time down by about 25%']",0,23,0.44,Question - Help,1751796850.0
1lsxhl0,StableDiffusion,V2V workflow for improving quality?,"Hi there, I hope you can help me.
TLDR: I have a video of different clips stitched together. The fact that they are different clips make the actors in the clips move in a weird way. Is there a way to give a V2V the clip and make it have more coherent movements, while preserving the likeness and outfit of the character, possibly improving the overall quality too?

Lately with Kontext I started experimenting with I2V with first and last frame guidance, it is great!
I can upload an image of my DnD warrior to Kontext and create another image of him surprised if front of a dragon, then create an animation from those key frames. I noticed that unfortunately if the two images are too different the model does not understand the request well, so I have to create many 2 seconds long videos with different key frames.
Doing so, though, makes the character move in short bursts of movement, and the final result is weird to watch.
Is there a way to feed the final video to a Video to Video model (WAN, HY, anything is fine, I don't care if it is censored or not) and have it recreate the scene with more coherent movements? Also, if I manage to create such a video, would it be possible to enhance the quality / resolution?

Thanks in advance :)","[""You would usually go the other way: provide a video of smooth movements and have your consistent scenes follow the movements.  \n\n> I started experimenting with I2V with first and last frame guidance\n\nThis is helpful information.  It helps frame your request and your overall goal much better.\n\nI think you should probably aim to start with the video.  There are examples of using VACE to animate against openpose animations, though that level of rigging isn't strictly necessary.""]","[""You would usually go the other way: provide a video of smooth movements and have your consistent scenes follow the movements.  \n\n> I started experimenting with I2V with first and last frame guidance\n\nThis is helpful information.  It helps frame your request and your overall goal much better.\n\nI think you should probably aim to start with the video.  There are examples of using VACE to animate against openpose animations, though that level of rigging isn't strictly necessary.""]",1,1,0.67,Question - Help,1751795486.0
1lsxf65,StableDiffusion,"Chroma V41 low steps RL is out! 12 steps, double speed."," 12 steps, double speed, try it out

https://civitai.com/models/1330309/chroma

I recommend deis sgm_uniform for artsy stuff, maybe euler beta for photography ( double pass).","['That also means losing negative prompts and defaulting to 5.0 CFG, though, right?', 'Where did you got official information about this version? Is the 12-steps, no negative prompts recommended setting?', 'Does it work well without negative prompts? previously chroma needed standard negative prompts to not produce garbage\n\nI will need to try it out.', 'Sorry, a noob here, what\'s the difference between this model to the others?  In the huggingface repo there is the ""normal"" version, the ""detailed-calibrated"" one (as far as i understood, trained on higher resolutions?) and now this one. I am trying to understand the difference..  i am using Chroma\'s default workflow, so would it work for this model like the others?\n\nAlso i am just curious (as a non-researcher) how is the creator training ""multiple versions"" simultaneously? I thought the ""computing power/resources"" is quite high, does this literally cost triple for him? does he need to be consistent with the training method between versions? (because previous versions didn\'t have ""low steps"" version).\n\nP.S he already published v42 (but as of now, not with this version).', ""Does [NAG](https://github.com/ChenDarYen/ComfyUI-NAG) work with this version, and what settings if it does?\n\nAlso does anyone know what's better for Chroma between t5xxl and the flan one? If it's flan then what file is it that I download? I'm currently using fp16 t5xxl on my 3060 12gb."", 'Can we also get 4 steps one day — like how original schenell version was. Need more speed', ""For those who might need it, [there's an FP8 version here](https://huggingface.co/Clybius/Chroma-fp8-scaled/blob/main/v41/chroma-unlocked-v41-few-steps-rl_float8_e4m3fn_scaled_learned.safetensors)."", ""I'd really love it they can improve skin detail... it doesn't look realistic at all to me. At least on the samples of civitai.  It's very schnell. And hands. That would be all I ask for."", ""I just downloaded this, and I quite like this model. I mainly generate anime images, and many of the results I'm getting are amazing."", ""You're telling me this is distilled ? I don't even care about the step count if it happened to be distilled, from my personal experience in any model is that more steps end up giving some unique results.  There is also a rescale CFG lora released on silveroxiders repo which allows you to run any of the existing models on CFG 1. \n\n[https://huggingface.co/silveroxides/Chroma-LoRA-Experiments/blob/main/chroma-unlocked-rescaled\\_cfg\\_LoRA-rank\\_16-fp32.safetensors](https://huggingface.co/silveroxides/Chroma-LoRA-Experiments/blob/main/chroma-unlocked-rescaled_cfg_LoRA-rank_16-fp32.safetensors)""]","['That also means losing negative prompts and defaulting to 5.0 CFG, though, right?', 'Where did you got official information about this version? Is the 12-steps, no negative prompts recommended setting?', 'Does it work well without negative prompts? previously chroma needed standard negative prompts to not produce garbage\n\nI will need to try it out.', 'Sorry, a noob here, what\'s the difference between this model to the others?  In the huggingface repo there is the ""normal"" version, the ""detailed-calibrated"" one (as far as i understood, trained on higher resolutions?) and now this one. I am trying to understand the difference..  i am using Chroma\'s default workflow, so would it work for this model like the others?\n\nAlso i am just curious (as a non-researcher) how is the creator training ""multiple versions"" simultaneously? I thought the ""computing power/resources"" is quite high, does this literally cost triple for him? does he need to be consistent with the training method between versions? (because previous versions didn\'t have ""low steps"" version).\n\nP.S he already published v42 (but as of now, not with this version).', ""Does [NAG](https://github.com/ChenDarYen/ComfyUI-NAG) work with this version, and what settings if it does?\n\nAlso does anyone know what's better for Chroma between t5xxl and the flan one? If it's flan then what file is it that I download? I'm currently using fp16 t5xxl on my 3060 12gb.""]",105,33,0.93,News,1751795201.0
1lsx9p7,StableDiffusion,2DN NAI - highly detailed NoobAI v-pred model,"I thought I’d share my new model, which consistently produces really detailed images.

After spending over a month coaxing NoobAI v-pred v1 into producing more coherent results+ I used my learnings to make a more semi-realistic version of my 2DN model

CivitAI link: https://civitai.com/models/520661

Noteworthy is that all of the preview images on CivitAI use the same settings and seed! So I didn’t even cherry pick from successive random attempts. I did reject some prompts for being boring or too samey to the other gens, that’s all.

I hope people find this model useful, it really does a variety of stuff, without being pigeonholed into one look. It uses all of the knowledge of NoobAI’s insane training but with more details, realism and coherency. It can be painful to first use a v-pred model, but they do way richer colours and wider tonality. Personally I use reForge after trying just about everything.

+ note: this is the result of that month’s work https://civitai.com/models/99619?modelVersionId=1965505","[""I'll try this one when I get home, love your 2DN series of models"", 'Someone spoonfeed me, is NoobAI related to Illustrious? I made a bunch of Loras for a Illustrious model, so they could possibly work still right? I will try now.', ""Another vpred fix. Cool, I'll try it later today.\nCan you share the idea behind it? I've made a similar one, would be interesting to compare them\nhttps://www.reddit.com/r/StableDiffusion/s/ihjV3oxesr"", 'does the model require the control lora?', 'Cool!', 'Any comyui workflows?', 'Just tried it, this is amazing. Thank you!']","[""I'll try this one when I get home, love your 2DN series of models"", 'Someone spoonfeed me, is NoobAI related to Illustrious? I made a bunch of Loras for a Illustrious model, so they could possibly work still right? I will try now.', ""Another vpred fix. Cool, I'll try it later today.\nCan you share the idea behind it? I've made a similar one, would be interesting to compare them\nhttps://www.reddit.com/r/StableDiffusion/s/ihjV3oxesr"", 'does the model require the control lora?', 'Cool!']",115,11,0.97,Resource - Update,1751794546.0
1lswqok,StableDiffusion,SDXL - Cannot train model,"I was able to train SDXL model a few weeks back and now it wont let me because it freaks out about the images folder not having an underscore. Yet it DOES. I tried updating kohya-ss . What options do I have here?

[https://youtu.be/92vkuCT8s\_U](https://youtu.be/92vkuCT8s_U)

I want to just edit the .py file to tell it to stop checking the underscore. Any advice?","['I personally use OneTrainer it doesn’t have these weird idiosyncrasies', ""Rename folder 'control_100' to '100_control'.""]","['I personally use OneTrainer it doesn’t have these weird idiosyncrasies', ""Rename folder 'control_100' to '100_control'.""]",0,2,0.14,Question - Help,1751792365.0
1lswqig,StableDiffusion,SDXL - Cannot train model,"I was able to train SDXL model a few weeks back and now it wont let me because it freaks out about the images folder not having an underscore. Yet it DOES. I tried updating kohya-ss . What options do I have here?

[https://youtu.be/92vkuCT8s\_U](https://youtu.be/92vkuCT8s_U)

I want to just edit the .py file to tell it to stop checking the underscore. Any advice?",[],[],0,0,0.13,Question - Help,1751792343.0
1lswpla,StableDiffusion,SDXL - Cannot train model,"I was able to train SDXL model a few weeks back and now it wont let me because it freaks out about the images folder not having an underscore. Yet it DOES. I tried updating kohya-ss . What options do I have here?

[https://youtu.be/92vkuCT8s\_U](https://youtu.be/92vkuCT8s_U)

I want to just edit the .py file to tell it to stop checking the underscore. Any advice?",[],[],0,0,0.13,Question - Help,1751792240.0
1lswhnr,StableDiffusion,The Fat Rat - Myself & I - AI Music Video,"a video I've made for a uni assignment Decided to make another music video this time about a song from ""The Fat Rat"" it does basically include almost all of the new stuff that came out in the last 3 or 4 months, up until the day FusionX got released i've used:

* Flux distilled with some loras,
* Wan T2V, I2V, Diffusion Forcing, VACE Start End Frame, Fun Style Transfer, Camera Loras,
* Adiff with AudioReact,

",[],[],3,0,0.64,Animation - Video,1751791330.0
1lsvyfs,StableDiffusion,How to create portable version of Web-UI?,"Hello there!

I've been trying to make a portable version of A1111, Fooocus, and ForgeUI... But whenever I clean-install a new version of Windows, while all the Web-UIs are on another drive... It always tries to re-download the same requirements that are needed to launch the Web-UI...

Is there any way to make the requirements also portable?

Thanks in-advance!",[],[],0,0,0.5,Question - Help,1751789093.0
1lsvnwa,StableDiffusion,"I'm trying to pass an image through a LORA to make it look like a painting, the more I increase denoise, the better the image looks but at the cost of the initial composition, but when i decrease the denoise, the quality of the output decreases significantly and doesn't look like a painting anymore",,"['this is the wf im using \n\nhttps://preview.redd.it/m1bkq08xk7bf1.png?width=1497&format=png&auto=webp&s=09e6214fe16d91ac79fb515fd074988eb626b2b4', 'Either use Flux Kontext or a tile controlnet.\n\nBut the real answer for good quality is to use at least q4\\_k\\_s.\n\nq2 is too small to produce artefact free results. The model has been compressed too far to make clean images.\n\nBut SDXL with a tile controlnet will give you better results. Even if you run SDXL at fp8.', 'Try Flux kontext or sdxl with controlnets (I recommend juggernaut for stylized stuff)', 'looks like an upscaler problem visually , but i m not expert i dont use comfi', ""If you tell scan.con to hurry up and ship me my replacement GPU I can finally boot my workstation and release a model, nodes and workflow exactly for this (turning photos or art into other art styles). It's more coherent than Context and it uses SD1.5 so it's reasonably fast. It needs a V2 to include more anime data but v1 nails painterly brushstrokes making it appear very close to digital or tradition art styles.\n\nLiterally been waiting over 5 months now"", ""One thing you can try is getting it to do a slight change and then passing the image through again. A few iterations add up and will get more of the effect while the larger picture changes less.\n\nYou can also add upscaling / downscaling while you're doing it, it allows for more change while not losing what made the original image how it was."", 'https://preview.redd.it/l5avl3yd99bf1.png?width=1280&format=png&auto=webp&s=36020cf9a9b025da1eaa1092cf9580b7680ee4ce', 'Do you mean like this? \n\nhttps://preview.redd.it/i23yvcm4p7bf1.png?width=1051&format=png&auto=webp&s=b721fdd2122c1db4a79c280d1ba064c679c2065b']","['this is the wf im using \n\nhttps://preview.redd.it/m1bkq08xk7bf1.png?width=1497&format=png&auto=webp&s=09e6214fe16d91ac79fb515fd074988eb626b2b4', 'Either use Flux Kontext or a tile controlnet.\n\nBut the real answer for good quality is to use at least q4\\_k\\_s.\n\nq2 is too small to produce artefact free results. The model has been compressed too far to make clean images.\n\nBut SDXL with a tile controlnet will give you better results. Even if you run SDXL at fp8.', 'Try Flux kontext or sdxl with controlnets (I recommend juggernaut for stylized stuff)', 'looks like an upscaler problem visually , but i m not expert i dont use comfi', ""If you tell scan.con to hurry up and ship me my replacement GPU I can finally boot my workstation and release a model, nodes and workflow exactly for this (turning photos or art into other art styles). It's more coherent than Context and it uses SD1.5 so it's reasonably fast. It needs a V2 to include more anime data but v1 nails painterly brushstrokes making it appear very close to digital or tradition art styles.\n\nLiterally been waiting over 5 months now""]",3,22,0.6,Question - Help,1751787912.0
1lsgbon,StableDiffusion,realistic baking (Chroma v42),"Chroma seems to be the next SOTA model for realism and prompt following .

**Prompt:**

woman naturalistic photograph of candid , Nurturing Radiance

In the warm glow of a sunlit kitchen, we find ourselves in the presence of a naturalistic photograph capturing an intimate moment. The subject is a caring blonde mother, her eyes radiant with maternal love as she gazes directly at us. She wears a flour-dusted apron over her casual attire while midway through baking cookies, evident by the mixing bowl and ingredients scattered on the counter behind her. Her gentle smile and relaxed posture exude an aura of comfort and nurturing.

The scene is framed to emphasize her warmth and connection with us as viewers. The soft natural light from a nearby window bathes her in a golden hue, highlighting the subtle texture of her apron and the slight wisps of flour on her skin. In the background, we can see glimpses of family life - a calendar marked with appointments, drawings stuck to the refrigerator door.


**Euler/beta/30 steps**



",['Realistically these women might need baking instructions. Clearly they are trying to shower in the flower as they pour it in the bowl. The only time I have seen this much flower on someone baking was watching a guy with tourettes try and bake.'],['Realistically these women might need baking instructions. Clearly they are trying to shower in the flower as they pour it in the bowl. The only time I have seen this much flower on someone baking was watching a guy with tourettes try and bake.'],7,1,0.73,Discussion,1751739446.0
1lstfsw,StableDiffusion,"I have finally started publishing my AI webcomic project, made with Krita AI Diffusion","Hello! I would like to share with you the art project I have been working on for some time now. It's a fantasy webcomic, involving characters from popular fairy tales and mythology, taking place in the world of Midgard in the edge of collapse.

Most of the work is done using Krita AI Diffusion (it's AI assisted art), and it counts with a anime concept opening made with Wan 2.1 (I know artstyle is inconsistent but by the time I worked in some of the scenes VACE was only available for Wan 1.3B)

It's amateurish since I'm no expert in making comics, and I'm still exploring and experimenting with different tools to improve my workflows (and I'm constantly learning about the new tools and models that show up), you should expect a bit of artstyle fluctuation in these first chapters. In order to keep character consistency for the prologue, I had to manually tweak the generations and use a lot of inpainting, and Lora training, since there was no models or tools for character consistency in the moment I worked on it. Hopefully with Flux Kontext these things can be improved.

As a note, I know there are some contrasting visual elements in the artstyle (i.e things that look like papercut or cartoon when the rest is anime), that some of you are probably not going to understand (most do but some people didn't get it). It's not an error, it's an artistic choice and is intentional, those elements have a reason to look like that.

The original language of the comic is Spanish, as it's my native language. Most of you are probably going to read a translated version (by me).

You can check the comic (and animation) in the following site (also coded with the help of AI ✌️). Of course I'm open for feedback for either the comic itself, the translation or the website (and if you find a bug you can report as well). The comic of course is totally free to read and I plan to keep it like that til the end.

https://apo-tale-lypse.com/",[],[],0,0,0.42,No Workflow,1751779243.0
1lsrw7y,StableDiffusion,Comfy,Is Kontext better than flux?? Any clear comparisons?? ,"['For t2i? I feels it is better with styles. Yet you still need to write an essay to get something really cool and styles are not that consistent.', 'There are completely different models with different purposes:  \nFlux.1 Dev/Schnell are text-to-image (t2i) models. you enter natural language prompt and get your image  \nFlux Kontext Dev is image-to-image allowing to edit the source image or two (like Omnigen or Bagel). you enter an image and command how to modify it.  \nso it is hard to compare grapes and oranges, though both are fruit.  \nthey both are Flux from Black Forest Labs, but these are entirely different models.']","['For t2i? I feels it is better with styles. Yet you still need to write an essay to get something really cool and styles are not that consistent.', 'There are completely different models with different purposes:  \nFlux.1 Dev/Schnell are text-to-image (t2i) models. you enter natural language prompt and get your image  \nFlux Kontext Dev is image-to-image allowing to edit the source image or two (like Omnigen or Bagel). you enter an image and command how to modify it.  \nso it is hard to compare grapes and oranges, though both are fruit.  \nthey both are Flux from Black Forest Labs, but these are entirely different models.']",0,12,0.29,Question - Help,1751773680.0
1lsm83q,StableDiffusion,Why don’t my Flux LoRAs train the subject’s teeth correctly?,"Recently I have been training with AI-Toolkit, and my last Flux training was fantastic. The fidelity of the subject is nearly perfect. The only hiccup is that the teeth don’t train properly. My subject has veneers, which are very straight, white, etc. For some reason, even though her teeth appear in several of the training images, and generations where the image features visible teeth, the subject’s teeth are nowhere to be found and replaced by jagged, not straight teeth.

I’m wondering if there is a trick to getting the teeth correct. Any tricks anybody can offer?","['Are the teeth only on a very small part of the image? Then you are most likely running into problems with the resolution of the latent space.\n\nTry to train images where the teeth get enough resolution (i.e. at least portraits, perhaps even some close-ups) and then during image generation use tools like ADetailler to let the model generate the face in full resolution']","['Are the teeth only on a very small part of the image? Then you are most likely running into problems with the resolution of the latent space.\n\nTry to train images where the teeth get enough resolution (i.e. at least portraits, perhaps even some close-ups) and then during image generation use tools like ADetailler to let the model generate the face in full resolution']",1,1,1.0,Question - Help,1751755447.0
1lsvku7,StableDiffusion,Mongolian women,,"['No, this is just run of the mill standard AI slop.\n\nAt least spend [10 seconds googling](https://www.google.com/search?newwindow=1&client=firefox-b-d&sca_esv=87161b55b482f184&sxsrf=AE3TifNck8qXDErM_UiNRyn3ys5V6GkzGQ:1751787805833&q=mongolian+women&udm=2&fbs=AIIjpHw2KGh6wpocn18KLjPMw8n5Yp8-1M0n6BD6JoVBP_K3fXXvA3S3XGyupmJLMg20um-g802xpvK_P6JR85uWJcIA4RlyV9ClBA3imj_j00IU7QH5B2l5wwOVkjgubwdJUKJYyAtDblAxB1ZXPCQNlVp_7QheRvetSIOoPf-cAwZHC9Rl05XrksV_xoLqK4Rx2jPq9vDvM3aDDIq77hqeUQr1rMd1VQ&sa=X&ved=2ahUKEwjk042P3qeOAxXfQ_EDHVcUGS4QtKgLKAF6BAgQEAE&biw=1833&bih=987&dpr=1.25) before making claims like that', 'Lol, what?', 'Bot post.']","['No, this is just run of the mill standard AI slop.\n\nAt least spend [10 seconds googling](https://www.google.com/search?newwindow=1&client=firefox-b-d&sca_esv=87161b55b482f184&sxsrf=AE3TifNck8qXDErM_UiNRyn3ys5V6GkzGQ:1751787805833&q=mongolian+women&udm=2&fbs=AIIjpHw2KGh6wpocn18KLjPMw8n5Yp8-1M0n6BD6JoVBP_K3fXXvA3S3XGyupmJLMg20um-g802xpvK_P6JR85uWJcIA4RlyV9ClBA3imj_j00IU7QH5B2l5wwOVkjgubwdJUKJYyAtDblAxB1ZXPCQNlVp_7QheRvetSIOoPf-cAwZHC9Rl05XrksV_xoLqK4Rx2jPq9vDvM3aDDIq77hqeUQr1rMd1VQ&sa=X&ved=2ahUKEwjk042P3qeOAxXfQ_EDHVcUGS4QtKgLKAF6BAgQEAE&biw=1833&bih=987&dpr=1.25) before making claims like that', 'Lol, what?', 'Bot post.']",0,3,0.2,No Workflow,1751787564.0
1lsuyzu,StableDiffusion,Where you get your workflows for comfy ui,"Whenever i bring workflow  for comfy ui from civitai, i end up with many fucken costom nodes with their conflict and other probleme, some say bad workflow may have mallusous codes, dangerous ones, is there a place for simple trusted workflows that need minimum amount of custom nodes. ","[""I make them myself. That or from official sources like [https://comfyanonymous.github.io/ComfyUI\\_examples/](https://comfyanonymous.github.io/ComfyUI_examples/) if I don't know how the specific thing works. ComfyUI also has built-in workflows inside the UI, which also can contain example workflows from the custom nodes:\n\nhttps://preview.redd.it/8k0tzgxye7bf1.png?width=3413&format=png&auto=webp&s=26710dc32810802f38e0b599de316bb81cd0e087\n\nThe only reason I would look at another user's workflow is to just see the implementation that I can copy in my own workflow."", 'From comfyui offical - [https://comfyui-wiki.com/en/tutorial/basic](https://comfyui-wiki.com/en/tutorial/basic)\n\nEvery tuturial has workflows with native nodes and optional for gguf versions.', 'I build them myself. And I study other people’s nodes to learn how they did it.', ""I mostly reverse engineer someone’s workflow to fit my needs and setup. From there, I build my own custom flow, there's a valuable learning opportunity in that process. If you want to excel in ComfyUI, don’t just blindly use a workflow you get from someone else. Take the time to understand how each node works, what it does, and why it's used.""]","[""I make them myself. That or from official sources like [https://comfyanonymous.github.io/ComfyUI\\_examples/](https://comfyanonymous.github.io/ComfyUI_examples/) if I don't know how the specific thing works. ComfyUI also has built-in workflows inside the UI, which also can contain example workflows from the custom nodes:\n\nhttps://preview.redd.it/8k0tzgxye7bf1.png?width=3413&format=png&auto=webp&s=26710dc32810802f38e0b599de316bb81cd0e087\n\nThe only reason I would look at another user's workflow is to just see the implementation that I can copy in my own workflow."", 'From comfyui offical - [https://comfyui-wiki.com/en/tutorial/basic](https://comfyui-wiki.com/en/tutorial/basic)\n\nEvery tuturial has workflows with native nodes and optional for gguf versions.', 'I build them myself. And I study other people’s nodes to learn how they did it.', ""I mostly reverse engineer someone’s workflow to fit my needs and setup. From there, I build my own custom flow, there's a valuable learning opportunity in that process. If you want to excel in ComfyUI, don’t just blindly use a workflow you get from someone else. Take the time to understand how each node works, what it does, and why it's used.""]",0,8,0.36,Question - Help,1751785128.0
1lsu2sx,StableDiffusion,Wan 2.1 Puppetry!,Fun part of this one was generating clips non stop for about two days then finding what remotely fit the lipsync. No magic there but it worked out in a fun way! ,[],[],5,0,0.63,Animation - Video,1751781640.0
1lstg37,StableDiffusion,Ayuda buscando página gratis!,"Hay alguna página que te permitan hacer videos de 10 a 20 segundos con una imagen gratis o que te den créditos gratis por día, y que no tengan problemas con el nfsw ?","['no hay ninguna o al menos no he encontrado, ya es dificil que te de creditos gratis, mas aun que no tengas problemas con el nfsw, te sugiero frame pack studio, esta en pinokio, pero es local']","['no hay ninguna o al menos no he encontrado, ya es dificil que te de creditos gratis, mas aun que no tengas problemas con el nfsw, te sugiero frame pack studio, esta en pinokio, pero es local']",0,1,0.08,Question - Help,1751779272.0
1lssvuv,StableDiffusion,No Model Populating - Text to Video,"Hi all,

I am using Stability Matrix (local). I have Foocus with ComfyAI. Whenever I attempt a text to project video, there is never a model that is available on the dropdown. I have downloaded several with the WAN in the description (even some NSF ones to try and see if that would alleviate the issue) and nothing works. Can anyone tell me what's causing this?","['Wrong path of model files. If they are safe tensor put in unet.', 'I discovered this after many hours last night. I had to install cogstudio. Even then, it only semi-worked using the browser UI.']","['Wrong path of model files. If they are safe tensor put in unet.', 'I discovered this after many hours last night. I had to install cogstudio. Even then, it only semi-worked using the browser UI.']",0,4,0.3,Question - Help,1751777165.0
1lsqiwu,StableDiffusion,Why am I having this problep when trying to run Image to image for sdxl,I want to use photon for ilage to image and I have this error,"['That CLIP text encode node is only for SDXL. The ""g"" refers to SDXL\'s CLIP-G ViT text encoder, which SD 1.5 models don\'t have.']","['That CLIP text encode node is only for SDXL. The ""g"" refers to SDXL\'s CLIP-G ViT text encoder, which SD 1.5 models don\'t have.']",2,1,0.62,Question - Help,1751768943.0
1lso67i,StableDiffusion,These are some of my achievements in exploring virtual reality. I really like these environments and characters that do not actually exist. I hope to hear criticism and suggestions so I can continue to improve,"    Being barefoot is not because I'm a pervert, but because I want to observe the stability of flux-generated feet



    Although I can't share WF directly (but I have shared it with friends who have helped me, I just am not ready to make it public yet, so please don't blame me :-) )
    But I am willing to share every detail:
    1. Using flux-dev/chroma + amater V6 lora as the base, chroma is somewhat unstable, but occasionally it can produce amazing results
    2. Generated with 2pass, no inpaint
    3. 99% of the effect comes from RES4LYF nodes (https://github.com/ClownsharkBatwing/RES4LYF)


https://preview.redd.it/wi1xasssc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=22e89bad1a588b102f93f55b82a1969a4a69da90

https://preview.redd.it/wezfx1vtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=4c7f92051c2eb7fa77a92332dec10550a79ec09f

https://preview.redd.it/24nem6vtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=f4ab431393f7b25bf98bd686296bdc866d53d322

https://preview.redd.it/4pyy48vtc5bf1.png?width=3072&format=png&auto=webp&s=75bc5e544d54c2dd34ed3a52e34df19cbbaad632

https://preview.redd.it/w2ql4avtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=473c80a67072f2275eb2496ed64495f45d4f6910

https://preview.redd.it/sgkhxuvtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=c74ea07029d021eaa38019d91190ca4902ee3e95

https://preview.redd.it/7pd8dgvtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=ef57bfcc7c9df51e5e1fbed2adfa0a35c2136b8f

https://preview.redd.it/to6reivtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=25dc15becaaa9acc658ba6c2f84b9e0e77830138

https://preview.redd.it/3zpzcpvtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=25410238638ad68b6763993c462a145a54211959

https://preview.redd.it/ihan57wtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=90273bacc903e0d79956547243e5a1431c85736c

https://preview.redd.it/31fzv3w7d5bf1.jpg?width=3072&format=pjpg&auto=webp&s=df944d69571d6517e86f3328c4f8c4bd9fff4e20

",[],[],0,0,0.43,No Workflow,1751761284.0
1lsn181,StableDiffusion,"Weekend drop, Flux WF - will share it in the comments in a bit. These colors, dead easy to eat too much of this stuff, all the zest just evaporates like a cloud of smoke...",,['WF: [https://openart.ai/workflows/Zz7DKx49CRNnoL3yyXVf](https://openart.ai/workflows/Zz7DKx49CRNnoL3yyXVf)'],['WF: [https://openart.ai/workflows/Zz7DKx49CRNnoL3yyXVf](https://openart.ai/workflows/Zz7DKx49CRNnoL3yyXVf)'],2,1,0.6,Workflow Included,1751757827.0
1lsmwsh,StableDiffusion,I’d like to create videos with characters but use my own backgrounds. Any advice on a suitable platform? Thanks,,"['You can use of [Character forge by Stoira](https://getcharacterforge.stoira.com)', 'If you want free open source way to do it, use Wan 2.1 and VACE.  Supply the background you want as the VACE reference image and it will perfect integrate your character (assume via a LoRA) into the background.']","['You can use of [Character forge by Stoira](https://getcharacterforge.stoira.com)', 'If you want free open source way to do it, use Wan 2.1 and VACE.  Supply the background you want as the VACE reference image and it will perfect integrate your character (assume via a LoRA) into the background.']",1,2,0.6,Question - Help,1751757459.0
1lsme5s,StableDiffusion,Why is flux dev so bad with painting texture ? Any way to create a painting that looks like a painting?,Even loras trained in styles like van gogh have a strange AI feel,"['Have you tried prompts like rough painting, rough strokes, impasto, thick paint, impressionism ?\n\nBTW The guy at the table looks like Captain Flint from Black Sails. :)', 'Try some of my artistic LoRAs: [https://civitai.com/user/NobodyButMeow/models](https://civitai.com/user/NobodyButMeow/models)\n\nDepending on the style, my LoRAs can be anywhere between 60-80% faithful to the artist\'s style.\n\nI do tend to overtrain a bit to achieve the desired style, so if you find that a LoRA is not flexible enough, you can try either a lower weight or use an earlier epoch.\n\nAlso, with some of my LoRAs, try using a lower Guidance (1.5-2.5), which may bring out more of the ""painterly texture"".  That is true for Flux-Dev in general as well.', '""Even loras trained in styles like van gogh have a strange AI feel""\n\ndon\'t forget, not all loras are created equally. a lot of the ones you find easily, even the first results on the bigger sites like civit and tensor are lazily done, it takes some digging to find the quality ones. a good test is to run the same seed with and without the lora to see how transformative it is. also watch out for loras that came out around the same time as flux--i avoid those because those are the \'excited about the shiny new toy and didnt take their time\' loras. look for ones that have newer updates and more than 1 version', ""Have you tried reducing Flux Guidance (aka distilled CFG)? The default is 3.5, but images that mimic traditional art benefit from setting it to a lower value, something like 1.4 to 2.4 (you'll need to experiment a bit)."", '[Pixel Wave Flux](https://civitai.com/models/141592?modelVersionId=992642) is way better at art styles.', 'No need for a Lora, just use chroma.\n\n2025-07-06\\_03-23-06 RL\\_v42\\_2xRL\\_batch dpmpp\\_2m + sigmoid\\_offset, CFG=1 1440x1440\n\nloaded completely 20338.526642227174 16975.371215820312 True\n\n100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 \\[00:34<00:00,  2.90s/it\\]\n\nPrompt executed in 36.44 seconds\n\n""High resolution detailed  professional van gogh style oil painting of a man and a woman in 1920 sitting on a table in a restaurant""\n\nhttps://preview.redd.it/myatp0mtn8bf1.png?width=1440&format=png&auto=webp&s=b1fee5ff40defdddb90f1828ec8f4c9885228c03', 'Feels to me like Flux (Kontext) has trouble with high resolution detail in general.\n\nWether it is paint brush strokes, skin texture or material detail, it always is either washed out or one ""plain"" color without any detail at all, resulting in this ""AI feel"".', ""Prompting is key. Btw kontext feels waaay better with styles than base flux. I did a post about styles half a year ago https://www.reddit.com/r/StableDiffusion/s/UnvE0zCHBQ\nSince then there was also some resource with list of styles it can effectively handle.\nFor getting hires textures you should upscale. Don't forget that all actual calculations are done in latent and it is really small. And then it gets to desired resolution via vae. But yeah flux was always kinda bad with styles, sd was way better at that"", 'Eldrich paint sketch Lora is the way', ""The pixelwave dev fine tune is way better with drawings, paintings and such.\n\nI'm so waiting for the nunchaku team to release a quantize tool for dummies like myself to do a SVDQuant of it.""]","['Have you tried prompts like rough painting, rough strokes, impasto, thick paint, impressionism ?\n\nBTW The guy at the table looks like Captain Flint from Black Sails. :)', 'Try some of my artistic LoRAs: [https://civitai.com/user/NobodyButMeow/models](https://civitai.com/user/NobodyButMeow/models)\n\nDepending on the style, my LoRAs can be anywhere between 60-80% faithful to the artist\'s style.\n\nI do tend to overtrain a bit to achieve the desired style, so if you find that a LoRA is not flexible enough, you can try either a lower weight or use an earlier epoch.\n\nAlso, with some of my LoRAs, try using a lower Guidance (1.5-2.5), which may bring out more of the ""painterly texture"".  That is true for Flux-Dev in general as well.', '""Even loras trained in styles like van gogh have a strange AI feel""\n\ndon\'t forget, not all loras are created equally. a lot of the ones you find easily, even the first results on the bigger sites like civit and tensor are lazily done, it takes some digging to find the quality ones. a good test is to run the same seed with and without the lora to see how transformative it is. also watch out for loras that came out around the same time as flux--i avoid those because those are the \'excited about the shiny new toy and didnt take their time\' loras. look for ones that have newer updates and more than 1 version', ""Have you tried reducing Flux Guidance (aka distilled CFG)? The default is 3.5, but images that mimic traditional art benefit from setting it to a lower value, something like 1.4 to 2.4 (you'll need to experiment a bit)."", '[Pixel Wave Flux](https://civitai.com/models/141592?modelVersionId=992642) is way better at art styles.']",37,30,0.79,Discussion,1751755931.0
1lsm47h,StableDiffusion,what you guys are using to edit binary masks (black and white)?,"https://preview.redd.it/5nohenysu4bf1.png?width=1235&format=png&auto=webp&s=fe7d658924ff342814102b01c2c1de034fc98f0f

I need to edit 179 masks, just to refine some small details. Photoshop takes a long time because I need to open the image, overlay the mask, edit, change transparency, etc... Is there a specific tool that opens the image and the mask together and after editing saves the mask directly?

the print is from an application I'm making",['For rectangular masks I have written a PR for taggui: [https://github.com/jhc13/taggui/pull/353](https://github.com/jhc13/taggui/pull/353)'],['For rectangular masks I have written a PR for taggui: [https://github.com/jhc13/taggui/pull/353](https://github.com/jhc13/taggui/pull/353)'],0,1,0.4,Question - Help,1751755133.0
1lslihm,StableDiffusion,Can't find this problem listed anywhere online and need help! SD FORGE,"have been running into this issue since last week. I've deleted and redownloaded forge so many times and still having this issue on all clean installs.

Anyone run into this or know how to fix it?","['I have been having the same problem for some time now and I coulden\'t finda a way to fix it ether. So I did the next best thing and replaced it whit the Stable-Diffusion-Webui-Civitai-Helper. It does the exect same thing, except that it works. \n\nSo if you do get civitai heler, go in to settigns,to:\n1. Enter your CivitAi API\n2. turn ""Blocks images that are more NSFW than the chosen rating"" to XXX.  \n3. to make sure that ""Download Example Images Locally"" is off\n\nFell free to experiment whit other settings.']","['I have been having the same problem for some time now and I coulden\'t finda a way to fix it ether. So I did the next best thing and replaced it whit the Stable-Diffusion-Webui-Civitai-Helper. It does the exect same thing, except that it works. \n\nSo if you do get civitai heler, go in to settigns,to:\n1. Enter your CivitAi API\n2. turn ""Blocks images that are more NSFW than the chosen rating"" to XXX.  \n3. to make sure that ""Download Example Images Locally"" is off\n\nFell free to experiment whit other settings.']",1,3,0.6,Question - Help,1751753415.0
1lskq4d,StableDiffusion,How come there isn’t a popular peer-to-peer sharing community  to download models as opposed to Huggingface and Civitai?,"Is there a technical reason why the approach to hoarding and sharing models hasn’t gone the p2p route? That seems to be the best way to protect the history of these models and get around all the censorship concerns.

Or does this exist already and it’s just not popular yet?","['Torrents would work. But there needs to be a fancy front end with model previews, much like how Civitai looks.', ""I think part of the issue is a standardized naming scheme. It's basically the wild west in regards to how people name things, so it would be extremely difficult to find particular things in a p2p format. Add on top of that, without a way to look at how good the lora/models are, you're downloading them blind. Good lora's are difficult to find and a lot of them are just over-trained trash. \n\nIt would take a big change in the community and how things are done for it to be a good option. At the very least we'd need a website for people to host the images and explanations like the rule 34 website so people knew what lora's to get."", ""I don't think p2p is as popular as it once was. I feel like you would just end up with a lot of dead torrents with zero seeders. Most people are leechers, after all.\n\n\nI think there's something called a seedbox but that sounds equivalent to it being hosted on a random computer. Will anyone bother?\n\n\nI guess people would rather ask for a mega link or something when they find someone on Discord who has what they want. Idk though, I'm old and the Internet has changed."", 'Should bring back Limewire for this.', 'I think the community is just too small so no one is really doing that so far. I don’t think there is a technical issue in these. This can also be done as a private tracker so users must upload sufficiently to obtain points to download new models. All the discussion/showcaes can be kept, maybe not the inference part for test.', 'Thats a great question. AI models are perfect for p2p!', 'I tried that civitaibay site earlier. First glance everything (before scrolling further down) said it had 1 seeder, tried to download one through the magnet link and it wouldnt get the metadata of the torrent so I had to get it from the actual civitai site that was linked within the torrent info on the site', 'Build it and they will come.', ""There is! But it isn't popular yet: https://civitasbay.org\n  \nI just found out about it yesterday too.."", 'There are some alternatives, but not a lot yet. If you know of more feel free to let us know: https://datahoarding.org/archives.html#CivitAI']","['Torrents would work. But there needs to be a fancy front end with model previews, much like how Civitai looks.', ""I think part of the issue is a standardized naming scheme. It's basically the wild west in regards to how people name things, so it would be extremely difficult to find particular things in a p2p format. Add on top of that, without a way to look at how good the lora/models are, you're downloading them blind. Good lora's are difficult to find and a lot of them are just over-trained trash. \n\nIt would take a big change in the community and how things are done for it to be a good option. At the very least we'd need a website for people to host the images and explanations like the rule 34 website so people knew what lora's to get."", ""I don't think p2p is as popular as it once was. I feel like you would just end up with a lot of dead torrents with zero seeders. Most people are leechers, after all.\n\n\nI think there's something called a seedbox but that sounds equivalent to it being hosted on a random computer. Will anyone bother?\n\n\nI guess people would rather ask for a mega link or something when they find someone on Discord who has what they want. Idk though, I'm old and the Internet has changed."", 'Should bring back Limewire for this.', 'I think the community is just too small so no one is really doing that so far. I don’t think there is a technical issue in these. This can also be done as a private tracker so users must upload sufficiently to obtain points to download new models. All the discussion/showcaes can be kept, maybe not the inference part for test.']",82,67,0.86,Discussion,1751751205.0
1lsk8s6,StableDiffusion,Multiple T5 clip models. Which one should I keep?,"For some reason I have 3 T5 clip models:

* t5xxl\_fp16 (\~9.6GB)
* umt5\_xxl\_fp8\_e4m3fn\_scaled (\~6.6GB)
* t5xxl\_fp8\_e4m3fn\_scaled (\~5.0GB)

The first two are located at 'models\\clip' and the last one at 'models\\text\_encoders'.

What's the different between the two fp8 models? Is there a reason to keep them if I have the fp16 one?
I have a 3090, if that matters.","['you may have collected different ones for running different models and workflows in the past. Sometimes you will delete one and realize you still needed it for a different workflow from like 2 months ago.', '1 TB and still low on space because of comfyui \n\nit keeps getting bigger every day', 'umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled is for WAN  \nthe other t5 models are for Flux\n\nGenerally, FP16 is higher quality, but \\~FP8 is twice as fast. Pick your poison. But for a text encoder, I would *always* choose the highest quality you can fit in system RAM.', ""That's the result of people using so many variations of these. I heard about FLAN T5 as well which supposedly works better when using Chroma."", 'There is also \nt5_xxl_flan_new_alt_fp8_e4m3fn and \nflan-t5-xxl-fp16', 'In my limited hardware environment, I needed all of them for professional-level work. However, I delete them when I’m not using them.', ""Umt5xl is for one video model if I'm not wrong, if you're not using one currently, just delete it, is not compatible with most of the T2I."", 'I bought a 4tb Nvme drive to start looking into this so-called AI that all the cool kids seem to be doing, and 6 months later I have 33Mb of empty space!']","['1 TB and still low on space because of comfyui \n\nit keeps getting bigger every day', 'you may have collected different ones for running different models and workflows in the past. Sometimes you will delete one and realize you still needed it for a different workflow from like 2 months ago.', 'umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled is for WAN  \nthe other t5 models are for Flux\n\nGenerally, FP16 is higher quality, but \\~FP8 is twice as fast. Pick your poison. But for a text encoder, I would *always* choose the highest quality you can fit in system RAM.', ""That's the result of people using so many variations of these. I heard about FLAN T5 as well which supposedly works better when using Chroma."", 'There is also \nt5_xxl_flan_new_alt_fp8_e4m3fn and \nflan-t5-xxl-fp16']",8,18,0.73,Question - Help,1751749897.0
1lsjcaa,StableDiffusion,No humans needed: AI generates and labels its own training data,"We’ve been exploring how to train AI without the painful step of manual labeling—by letting the system generate its own perfectly labeled images.

The idea: start with a 3D mesh of a human body, render it photorealistically, and automatically extract all the labels (like body points, segmentation masks, depth, etc.) directly from the 3D data. No hand-labeling, no guesswork—just pixel-perfect ground truth every time.

Here’s a short video showing how it works.

Let me know what you think—or how you might use this kind of labeled synthetic data.","['AI : third leg ?! what do we have here', ""I had the same idea for months now since ai am also a 3D artist. I always thought, why can't we just train models to predict data related to 3D objects?"", ""Yeah this hasn't worked well so far"", 'Helps to avoid privacy issues with real human images, and the limited nature of human datasets. Able to generate varying clothing, environments, poses, gender, etc.', ""This is awesome. What is this geared up for or is it gonna be it's own repository? I am a 3d artist as well I sell my pose sets for Daz models and would love to be able to train AI sets for image generations."", 'Interesting concept. Please keep us updated on the progress in the future.', 'I think the real deal will be when we will be able to do this while keeping multiview consistency.', 'A little confused as to what you are doing, but I (may) have done something similar; I rendered several thousand images using DAZ Studio with known poses, known backgrounds, known scene composition in a highly procedural way such that a simple script could accurate create the appropriate prompt for each image in the sequence. It worked reasonably well. \n\nOne trick I used was to label every 3D image with the token ""3d render"", then simply not using the token would result in a photo graphically realistic person instead of a 3d render being generated. I also trained with several thousand photo\'s with the token ""photo"", but adding the token ""photo"" was less useful then simply not using ""3d render"" in the prompt.']","['AI : third leg ?! what do we have here', ""I had the same idea for months now since ai am also a 3D artist. I always thought, why can't we just train models to predict data related to 3D objects?"", ""Yeah this hasn't worked well so far"", 'Helps to avoid privacy issues with real human images, and the limited nature of human datasets. Able to generate varying clothing, environments, poses, gender, etc.', ""This is awesome. What is this geared up for or is it gonna be it's own repository? I am a 3d artist as well I sell my pose sets for Daz models and would love to be able to train AI sets for image generations.""]",64,10,0.88,Resource - Update,1751747470.0
1lsj7hq,StableDiffusion,"Can a anime/cartoon focused t2v/i2v model do ""more"" than a realistic one?","Im a noob btw I just had this random thought and wanted to ask.

But can a video model trained for only anime output do more on local machines than something like veo3 or Wan2.1? My thought was if its trained on anime/cartoons and no (or minimal for whats needed) realistic data, wouldn't it fit more due to anime styles being generally simpler than real images? Or does it still use the same number of parameters despite things being simpler in the training data?

I ask because I am REALLY hoping we get some anime video models at some point and thats what they specialize in rather than them all trying for super impressive realistic outputs (which are still cool). Like an Illustrious t2v or something.

I mean how much further could we get with 32gb VRAM if we just did cartoons or anime? Would love to see what the community content would look like for this if it let creators output more with less hardware.

Take it a step further and what if the models were broken down by style choice rather than shoving them all into 1 mega model? What sort of benefits might that have if any?

Anyway thats my random thought, am I crazy or might this be a realistic goal someday?","['pony video wen', ""What you are thinking of is possible. It would just be very expensive to train. The best choice for finetuning an illustration and anime text-to-video model we have is Hunyuan Video, as it already has the necessary data, is flexible to train on, and is uncensored for the most part (anime styles are known by Hunyuan, but they're limited). But as you mentioned, anime videos aren't as prominent as the real-life footage Hunyuan was trained on, which is presumably most of its training dataset.\n\nWAN could have a finetune for its img2vid version for exclusive use with anime, illustrations, and 3d styles, making it even better with those styles in particular.""]","['pony video wen', ""What you are thinking of is possible. It would just be very expensive to train. The best choice for finetuning an illustration and anime text-to-video model we have is Hunyuan Video, as it already has the necessary data, is flexible to train on, and is uncensored for the most part (anime styles are known by Hunyuan, but they're limited). But as you mentioned, anime videos aren't as prominent as the real-life footage Hunyuan was trained on, which is presumably most of its training dataset.\n\nWAN could have a finetune for its img2vid version for exclusive use with anime, illustrations, and 3d styles, making it even better with those styles in particular.""]",1,2,0.57,Discussion,1751747120.0
1lsi0rj,StableDiffusion,"Is there a ""bad"" way to prompt with natural language prompts?","Just trying to learn a little coming from more tag-based models.

Are there any notable bad ways of writing a prompt in natural language that might give bad results? or just give it a few sentences of whatever you want and thats generally correct?

like would the following be okay or might it result in problems?
""a man walking down a rainy road in a city. blue shirt, with an umbrella, he has short hair""

so its going from natural to tag a little bit but would that still work most the time?","['Many people still use Booru style tags or word tag salad and expect good responses.  The number of Flux images I see with ""1girl,,clothes,big breasts,uniform"", or ""dog,car,chasing, .noon"" astounds me.  That and the negatives... It\'s been almost three  years since I did the day one beta tests for SD and people are still using ""bad hands, missing fingers"", etc in the negatives, which are not generally applicable to Flux. \n\nBut, the NL engine tries its best to determine what you want, which is why it works ""most"" time and other times it fails miserably and then people blame the engine. For me personally, I think of natural language prompting as if you\'re describing a scene to a visually impaired person.  \n\n\n\nAs u/Dezordan says, your example is fine., but could also be:\n\n""A man who has short hair and is wearing a blue shirt is carrying an umbrella while walking down a rainy road in a city""  \n\nBTW, here is a test. Clockwise from **top left**: your prompt, my prompt, Booru style tagging, SD tagging. \n\nSD: man, short hair, blue shirt, umbrella, walking, rainy street, city, day, realistic, photo style\n\nBooru: 1boy, short hair, blue shirt, holding umbrella, walking, city street, rain, solo, outdoors, daytime, city\n\nhttps://preview.redd.it/ybmh8mlp94bf1.png?width=695&format=png&auto=webp&s=041d6eeb62ce18df5cef825ac0fa9d95c513a869', ""There is no bad prompt, but there might be non optimal prompt. When a prompt is doing what you want it's fine, and you can't break anything (only waste computation time).\n\nPersonally I also think the SDXL way of a short sentence and then some tags to round it off is a very efficient way. And writing prose for Flux is far more tedious.\n\nBut you don't have to do that (yourself). Just use an LLM to translate your style of prompt to one or few precise sentences."", ""It's better to avoid purple prose. So, your example is okay.  \nYou can use some simple ways to describe mood and other things, but avoid being too esoteric."", 'Here\'s my learnings from using flux\n\n\\- Using normal sentences with periods is better.  When you use comma separated lists the model will still try to make sense of what you\'re saying, but it\'s as if someone started speaking to you in single words without connecting words.\n\n\\- Pronouns are good.  ""Her hair is..."",  ""His stance is..."".  It ties the request to a specific entity within the image.  But again, if you don\'t the model will still contextually figure it out... it just might get it wrong every now and then.  For example ""blue shirt"" by itself could be interpreted as ""any shirt that appears in the image should be blue"" where ""His shirt is blue."" is more precise.\n\n\\- If the model isn\'t adhering, there\'s a strong chance that either you\'re using the wrong lexical term for it or the model just don\'t know what that is.  Sometimes the concepts that a model doesn\'t know (wasn\'t trained on) about feels very random.\n\nSo the best case for your prompt would be: \n\n""A short haired man wearing a blue shirt and holding an umbrella is walking down a rainy road.""', 'Keep it clear and simple...\n\nI love the colorful clothes she wears, and the way the sunlight plays upon her hair.', 'Use language that can sent matching goal level you get garbage.  \nLook up primegen and r1 with Devin.  \n\n7 prompts and it could no longer complete basic git commands after being told it’s a monkey without being told directly']","['Many people still use Booru style tags or word tag salad and expect good responses.  The number of Flux images I see with ""1girl,,clothes,big breasts,uniform"", or ""dog,car,chasing, .noon"" astounds me.  That and the negatives... It\'s been almost three  years since I did the day one beta tests for SD and people are still using ""bad hands, missing fingers"", etc in the negatives, which are not generally applicable to Flux. \n\nBut, the NL engine tries its best to determine what you want, which is why it works ""most"" time and other times it fails miserably and then people blame the engine. For me personally, I think of natural language prompting as if you\'re describing a scene to a visually impaired person.  \n\n\n\nAs u/Dezordan says, your example is fine., but could also be:\n\n""A man who has short hair and is wearing a blue shirt is carrying an umbrella while walking down a rainy road in a city""  \n\nBTW, here is a test. Clockwise from **top left**: your prompt, my prompt, Booru style tagging, SD tagging. \n\nSD: man, short hair, blue shirt, umbrella, walking, rainy street, city, day, realistic, photo style\n\nBooru: 1boy, short hair, blue shirt, holding umbrella, walking, city street, rain, solo, outdoors, daytime, city\n\nhttps://preview.redd.it/ybmh8mlp94bf1.png?width=695&format=png&auto=webp&s=041d6eeb62ce18df5cef825ac0fa9d95c513a869', ""There is no bad prompt, but there might be non optimal prompt. When a prompt is doing what you want it's fine, and you can't break anything (only waste computation time).\n\nPersonally I also think the SDXL way of a short sentence and then some tags to round it off is a very efficient way. And writing prose for Flux is far more tedious.\n\nBut you don't have to do that (yourself). Just use an LLM to translate your style of prompt to one or few precise sentences."", ""It's better to avoid purple prose. So, your example is okay.  \nYou can use some simple ways to describe mood and other things, but avoid being too esoteric."", 'Here\'s my learnings from using flux\n\n\\- Using normal sentences with periods is better.  When you use comma separated lists the model will still try to make sense of what you\'re saying, but it\'s as if someone started speaking to you in single words without connecting words.\n\n\\- Pronouns are good.  ""Her hair is..."",  ""His stance is..."".  It ties the request to a specific entity within the image.  But again, if you don\'t the model will still contextually figure it out... it just might get it wrong every now and then.  For example ""blue shirt"" by itself could be interpreted as ""any shirt that appears in the image should be blue"" where ""His shirt is blue."" is more precise.\n\n\\- If the model isn\'t adhering, there\'s a strong chance that either you\'re using the wrong lexical term for it or the model just don\'t know what that is.  Sometimes the concepts that a model doesn\'t know (wasn\'t trained on) about feels very random.\n\nSo the best case for your prompt would be: \n\n""A short haired man wearing a blue shirt and holding an umbrella is walking down a rainy road.""', 'Keep it clear and simple...\n\nI love the colorful clothes she wears, and the way the sunlight plays upon her hair.']",2,7,0.6,Question - Help,1751743894.0
1lsgl1z,StableDiffusion,"Need help fixing my kohya SS settings, if anyone can pitch in?","I have successfully created lora's before, but today i trained a concept and a character and when i tested them it was like it didn't exist. I tried my old lora's and they work perfectly well.

I use the same folder strategy database > \[steps\]\_\[name\] \[type\]

settings I changed from defaul

model > illustreous xl > SDXL

safetensors - fp16 precision.

10 epochs - max train steps 6000

LR scheduler - cosine\_with restarts - 0.0005 learning rate, LR cycles = 3, text encoder LR = 0.00005, Unet LR = 0.0005

no half VAE, network dimension/alph 32/16, clip skip = 2, shuffle caption and flip augmentation are activated, full fp16 training, min gamma = 5, noise offset = 0.1.


and that's it for the settings i remember usually using, but for some reason today i trained a concept and a character and when tested i didn't even get a person, i got a floor a bench, random objects. It was like nothing was trained.

 ","['If you still have artifacts from your old training, find and compare the config for the differences.']","['If you still have artifacts from your old training, find and compare the config for the differences.']",2,3,1.0,Question - Help,1751740118.0
1lsgk19,StableDiffusion,Beyond the Peak: A Follow-Up on CivitAI’s Creative Decline (With Graphs!),,"[""It makes sense that CivitAI's activity is declining. Censorship issues aside, the main NSFW model remains SDXL and the site is mainly NSFW content. SDXL is almost two years old now, newer models don't have nearly as many finetunes and they take way longer to train. It's possible that people are just getting bored of it and unsubscribing just like in online games."", 'The decline in activity actually isn\'t as bad as I thought it would be. Even if they never allow celebrity content again, they need to fix their broke ass filtering ""solution."" It\'s literally one of the worst UX I\'ve ever experienced.', 'I notice loras of even anime or cartoon characters getting nuked randomly with pages leading to 404 not found. Not a fan of going on panic downloading spree but it hurts  losing models and loras. Weird how the samekosaba loras were all forced into the pg category all at once. Have a feeling vtuber loras are getting axed soon.', 'This is entirely how you interpret the data. If you look at this chart you can see that June 2025 was lower than average for 2025, but higher than every month in 2024 (included in the chart). If we looked at this data in early March we could conclude that January was the peak, and that we would see a continued decline. A single month of lower generations is not enough to conclude that the site as a whole is on a downward trend.\n\nhttps://preview.redd.it/w9gr19j444bf1.jpeg?width=800&format=pjpg&auto=webp&s=77210805bdc3f2ccdf1f4e50cb77924f2388dfae', ""It's certainly leveling off, but I think you need a few more days in July to get representative data. We're in the midst of a long weekend and holiday in the US, and one for which it's common to take extra days off.\n\nAlso purely anecdotal, but I'm noticing fewer downtimes and quicker gen times compared to a few month ago. You know, when everyone was complaining about that instead. Perhaps the reduced traffic is helping here."", ""Good lord! It's civitai.com really dead?"", 'Hey dude the article is cool but can you write it yourself rather than use chatgpt? It euphemises everything and makes it extremely cringe. I would much prefer reading an article with a few grammar errors that convey what you want it to rather than having it wrapped in euphemistic slop.']","[""It makes sense that CivitAI's activity is declining. Censorship issues aside, the main NSFW model remains SDXL and the site is mainly NSFW content. SDXL is almost two years old now, newer models don't have nearly as many finetunes and they take way longer to train. It's possible that people are just getting bored of it and unsubscribing just like in online games."", 'The decline in activity actually isn\'t as bad as I thought it would be. Even if they never allow celebrity content again, they need to fix their broke ass filtering ""solution."" It\'s literally one of the worst UX I\'ve ever experienced.', 'This is entirely how you interpret the data. If you look at this chart you can see that June 2025 was lower than average for 2025, but higher than every month in 2024 (included in the chart). If we looked at this data in early March we could conclude that January was the peak, and that we would see a continued decline. A single month of lower generations is not enough to conclude that the site as a whole is on a downward trend.\n\nhttps://preview.redd.it/w9gr19j444bf1.jpeg?width=800&format=pjpg&auto=webp&s=77210805bdc3f2ccdf1f4e50cb77924f2388dfae', 'I notice loras of even anime or cartoon characters getting nuked randomly with pages leading to 404 not found. Not a fan of going on panic downloading spree but it hurts  losing models and loras. Weird how the samekosaba loras were all forced into the pg category all at once. Have a feeling vtuber loras are getting axed soon.', ""It's certainly leveling off, but I think you need a few more days in July to get representative data. We're in the midst of a long weekend and holiday in the US, and one for which it's common to take extra days off.\n\nAlso purely anecdotal, but I'm noticing fewer downtimes and quicker gen times compared to a few month ago. You know, when everyone was complaining about that instead. Perhaps the reduced traffic is helping here.""]",33,29,0.76,News,1751740047.0
1lsfrfw,StableDiffusion,Wan 2.1 - Extend a video with a loop,"Hey👋🏼

I was playing around with Wan2.1 and i got some amazing results with it. Unfortunately I am limited to a 5sec clips because of my gpu.

Is there a way to loop the 5sec video 2 times, so the video would be saved as 15sec video?

I would like to implement this in the workflow so I don’thave to do manual editing.

Has anyone found a way to do it?","['See my last post with the creepy clown, I put in a link to a loop workflow that works very well. Most of the last few posts I did use it too if you want examples.', ""I suppose you could use a custom node if there isn't already one.  But it's easy enough to use imagemagick on the command-line: ```convert input.webp -loop 0 output.webp``` to loop forever.  You can also do things like ```convert input.webp -coalesce -duplicate 1,-2-1 -loop 0 output.gif``` to convert your animated webp to a gif that goes forward and then backward through your images so you don't have rough cuts at the seams. \n\nImagemagick is STRONG and worth playing around with.  You can do montages and all kinds of other image editing with concise commands along with conversion between formats, etc."", ""I think the Merge Images node from the VideoHelperSuite extension can do this. You feed your 5 second video into both inputs of the node, which repeats the video twice, and then feed that output and the original 5 second video into a second Merge Images node, which will append another copy of the 5 second video.\xa0\n\n\nIt's possible that the RepeatImageBatch node with 3 repeats also does this, depending on how it handles an input with multiple images.\n\n\nNote that with the Skyreels Diffusion Forcing models (Wan-based) you can get longer videos for the same VRAM it takes you to generate 5 second videos. It gens an initial video, then uses the end portion and generates a continuation.\n\n\nHere is a workflow, but it needs the ComfyUI-WanVideoWrapper extension, I don't know if there is a native equivalent - https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_skyreels_diffusion_forcing_extension_example_01.json"", ""Maybe try 'ping-pong' option from sampler (change false to true) your video will have 9sec. You will have half video forward and half backward.""]","['See my last post with the creepy clown, I put in a link to a loop workflow that works very well. Most of the last few posts I did use it too if you want examples.', ""I suppose you could use a custom node if there isn't already one.  But it's easy enough to use imagemagick on the command-line: ```convert input.webp -loop 0 output.webp``` to loop forever.  You can also do things like ```convert input.webp -coalesce -duplicate 1,-2-1 -loop 0 output.gif``` to convert your animated webp to a gif that goes forward and then backward through your images so you don't have rough cuts at the seams. \n\nImagemagick is STRONG and worth playing around with.  You can do montages and all kinds of other image editing with concise commands along with conversion between formats, etc."", ""I think the Merge Images node from the VideoHelperSuite extension can do this. You feed your 5 second video into both inputs of the node, which repeats the video twice, and then feed that output and the original 5 second video into a second Merge Images node, which will append another copy of the 5 second video.\xa0\n\n\nIt's possible that the RepeatImageBatch node with 3 repeats also does this, depending on how it handles an input with multiple images.\n\n\nNote that with the Skyreels Diffusion Forcing models (Wan-based) you can get longer videos for the same VRAM it takes you to generate 5 second videos. It gens an initial video, then uses the end portion and generates a continuation.\n\n\nHere is a workflow, but it needs the ComfyUI-WanVideoWrapper extension, I don't know if there is a native equivalent - https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_skyreels_diffusion_forcing_extension_example_01.json"", ""Maybe try 'ping-pong' option from sampler (change false to true) your video will have 9sec. You will have half video forward and half backward.""]",0,4,0.4,Question - Help,1751737968.0
1lse6g1,StableDiffusion,How Well Are AI Model Creators Keeping Up With Aesthetic Terminology and Visual Vocabulary?,"I've been thinking about something that's been bugging me about AI image generation, and I'm curious what others think.

**The Core Issue: AI Models Need a Shared Visual Language**

Every AI model relies on what's essentially a lingua franca a shared vocabulary that connects concepts to generate what we're asking for. When we prompt these models, we're constantly trying to figure out which combination of words will unlock the specific aesthetic or visual element we want. But here's the thing: how current and comprehensive is that vocabulary?

**New Aesthetics Are Constantly Emerging**

Today I learned about ""Angelpunk"" a term describing the early 90s fascination with Judeo-Christian iconography in art and media (think Evangelion, Trigun). This got me wondering: are model creators actively updating their training data to include emerging aesthetic movements and terminology?

I stumbled across an entire aesthetics wiki that's basically a rabbit hole of visual categories I never knew existed. Did you know there's a distinction between ""techware"" and ""warware""? Both have similar vibes but completely different visual signatures. Same with synthwave, vaporwave, and outrun they all use synthetic music aesthetics but are distinctly different movements.

**The Specificity Problem**

Here's where it gets interesting: we often lack precise language for specific visual elements. Take superhero masks as an example. Most people default to ""superhero mask"" and get a domino mask, but there are actually several distinct types:
- Domino masks (classic Batman/Robin style)
- Bridge-up masks (covering from nose bridge upward)
- Hair-out variants (full coverage but hair exposed)
- Reverse masks (Winter Soldier style, covering nose down)

**The Real Question**

How well are these nuanced aesthetic categories and precise visual descriptors actually built into current AI models? Are we limited by:
1. Training data that doesn't include emerging aesthetic movements?
2. Lack of precise terminology for specific visual elements?
3. Model creators not keeping pace with evolving visual culture?

When I try to generate something specific, I often feel like I'm playing a guessing game with the model's vocabulary. Sometimes it nails exactly what I want; other times it seems like certain aesthetic concepts just don't exist in its training.

**Discussion Points:**
- Have you noticed gaps in AI models' understanding of specific aesthetics?
- Are there visual styles or elements you can't seem to get models to understand?
- How do you think model creators should approach updating aesthetic vocabulary?

I'm genuinely curious whether this is a training data issue, a terminology gap, or if I'm just not finding the right prompt combinations. What's your experience been?
","['It is due to the captioned training data. If it doesn’t know a word or term you won’t be able to prompt for it, but other techniques could reproduce the content if guided with IPAdapter, Flux Redux, Flux Kontext etc. You can likely prompt an outfit type through a detailed description of it, but something specific like a type of mask requires terminology to be prompted easily. Open models tend to have a broader corpus of training data rather than very specific styles or trends like some of the closed models have (because they scraped anything and everything and sometimes optimise for these things as a feature). This is why there is a huge ecosystem around custom LoRA and models, introducing missing or niche knowledge.', 'This is why synthetic data will be huge when it becomes viable (dependable); this is how models will be able to train themselves, to fill in the immense amount of gaps that our current linguistic landscape misses-- \n\nHow many ways could I have written that sentence? And how would each way maximize its ability to convey its meaning and to whom? And yet, each image has ""one caption""? Only maximum in effect to the person who saw the image and described it and who they are to describe it! \n\nOn the other end of this; it is amazing how the process eventually inverts-- Starting with a pen or photoshop, suddenly having the computer do all the work! and now, yet again attempting to get the precision of pen and photoshop--', ""I use SDXL-based models like Pony and Illustrious, which use booru tags. If the boorus they were trained on have enough tagged examples of that aesthetic, the it'll work. If not, it's LoRA training time.\n\n\nAlso, for my purposes, I use artist styles rather than the aesthetic terms you are focussing on."", ""Base model training data is captioned with whatever automatic captioning method is the best at the time.\n\nCurrently, it's Flux's embraced excessive AI essays. Instead of naming concepts or iconic visuals, it just describes them in extreme detail.\n\nTo introduce tag-style concepts, you will have to train models yourself, adding manual captions/tags where necessary, or wait until AI captioning improves/changes direction (which might take years)."", 'I told Kontext to “increase bust size, make boobs bigger” but got nothing']","['It is due to the captioned training data. If it doesn’t know a word or term you won’t be able to prompt for it, but other techniques could reproduce the content if guided with IPAdapter, Flux Redux, Flux Kontext etc. You can likely prompt an outfit type through a detailed description of it, but something specific like a type of mask requires terminology to be prompted easily. Open models tend to have a broader corpus of training data rather than very specific styles or trends like some of the closed models have (because they scraped anything and everything and sometimes optimise for these things as a feature). This is why there is a huge ecosystem around custom LoRA and models, introducing missing or niche knowledge.', 'This is why synthetic data will be huge when it becomes viable (dependable); this is how models will be able to train themselves, to fill in the immense amount of gaps that our current linguistic landscape misses-- \n\nHow many ways could I have written that sentence? And how would each way maximize its ability to convey its meaning and to whom? And yet, each image has ""one caption""? Only maximum in effect to the person who saw the image and described it and who they are to describe it! \n\nOn the other end of this; it is amazing how the process eventually inverts-- Starting with a pen or photoshop, suddenly having the computer do all the work! and now, yet again attempting to get the precision of pen and photoshop--', ""I use SDXL-based models like Pony and Illustrious, which use booru tags. If the boorus they were trained on have enough tagged examples of that aesthetic, the it'll work. If not, it's LoRA training time.\n\n\nAlso, for my purposes, I use artist styles rather than the aesthetic terms you are focussing on."", ""Base model training data is captioned with whatever automatic captioning method is the best at the time.\n\nCurrently, it's Flux's embraced excessive AI essays. Instead of naming concepts or iconic visuals, it just describes them in extreme detail.\n\nTo introduce tag-style concepts, you will have to train models yourself, adding manual captions/tags where necessary, or wait until AI captioning improves/changes direction (which might take years)."", 'I told Kontext to “increase bust size, make boobs bigger” but got nothing']",3,6,0.62,Discussion,1751733831.0
1lsdjyl,StableDiffusion,What's up with Pony 7?,"The lack of any news over the past few months can't help but give rise to unpleasant conclusions. In the official Discord channel, everyone who comes to inquire about the situation and the release date gets a stupid joke about ""two weeks"" in response. Compare this with Chroma, where the creator is always in touch, and everyone sees a clear and uninterrupted roadmap.

I think that Pony 7 was most likely a failure and AstraliteHeart simply does not want to admit it. The situation is similar to Virt-A-Mate 2.0, where after a certain time, people were also fed vague dates and the release was delayed under various formulations, and in the end, something disappointing came out, barely even pulling for alpha.

It could easily happen that when Pony comes out, it will be outdated and no one needs it.","[""I think astra is in the sunk-cost fallacy. I hate to be in their shoes, but choosing auraflow might not have been the best course. I would cut the project short and start from scratch on top of something more accessible but it's easy for me say."", ""It's incredibly hard to catch lightning in a bottle twice.\n\nSince Pony v7 started training, Illustrious, Noob, Flux, Chroma, etc. all have come out so other notable models have advanced or further pushed SDXL or new architectures.\n\nI'm sure it'll be a competent model, but I don't know that it'll have the same impact as V6 pony."", ""\\> Compare this with Chroma, where the creator is always in touch, and everyone sees a clear and uninterrupted roadmap.\n\nChroma is great, that's why we are sponsoring it!\n\n\\> I think that Pony 7 was most likely a failure and AstraliteHeart simply does not want to admit it.\n\nIt's a strong model with some issues that I want to fix before release. It takes time to fix such issues. More time than I anticipated or wanted but I would rather have a decent model than release something sad. The bar is very high because of V6 and all the models that came after it.\n\nIt is also \\*\\*very expensive\\*\\* to train models of this size (in addition to knowing how to train them), hence you only see a few models coming out now (literally, Chroma is the only one really large finetune aside V7 I am aware that is not SDXL). So we have to take care of the financial part of the process too, and it takes time.\n\n  \n\\> It could easily happen that when Pony comes out, it will be outdated and no one needs it.\n\nThe best I can do is to build cool models that bring new tech to the table, this worked ok for the last \\~8 model I've released."", ""Chroma is gaining a lot of traction, and most of the tooling for it already exists. Since P7 is likely to be similar in size but using a different architecture, I think it will not become as popular as V6. LodeStones is also releasing a new checkpoint every four days on average, so people can get a taste of how the development is going. The aim is 50 epochs, and he's nearing epoch 42.\n\nIf P7 comes as a lighter model, people will use it."", ""They may have swung big, missed big.\n\nPony being on SD 1.5 and then XL, was smaller and more agile to work on. So when XL launched they were able to pivot to that and ride it to being a success. A problem with the post XL world though is we still may not have the proper replacement. Newer models have been slower, larger, worse licensing and not training very well.\n\nChroma was able to ride the community Flux tinkering and come out on top of a schnell tune with their own tweaks. That may be the future, assuming the final tune doesn't blow apart, distills make it smaller/faster, and it trains well for loras. But it's also possible next week a new 3B param base model comes out that's good, open licensed, and easy to train and we all move to that. That would kill Chroma, though the team could possibly just pivot to the new model and start training on top of that.\n\nThe scene moves way too fast for 6 month plus projects to be viable."", 'AstraliteHeart was in a tough position when he started on Pony 7, deciding the base model to be XL again or Flux with bad license, they compromised and now better alternatives exist. But its too late to start over', 'Illustrious is pony 7', 'It was doomed the moment auraflow was chosen (nobody was using that model) and ramped up the censorship. Probably wont ever come out tbh. AI just developed too fast to keep up given whats already invested.', 'Remember the name of this sub, models rise and fall all the time, there will always be a better model.', 'He has been kinda sanguine about the whole thing, pointing out that he supports Chroma and if that model ends up eating the attention given to his, that’s still a win (somehow).']","[""I think astra is in the sunk-cost fallacy. I hate to be in their shoes, but choosing auraflow might not have been the best course. I would cut the project short and start from scratch on top of something more accessible but it's easy for me say."", ""It's incredibly hard to catch lightning in a bottle twice.\n\nSince Pony v7 started training, Illustrious, Noob, Flux, Chroma, etc. all have come out so other notable models have advanced or further pushed SDXL or new architectures.\n\nI'm sure it'll be a competent model, but I don't know that it'll have the same impact as V6 pony."", ""\\> Compare this with Chroma, where the creator is always in touch, and everyone sees a clear and uninterrupted roadmap.\n\nChroma is great, that's why we are sponsoring it!\n\n\\> I think that Pony 7 was most likely a failure and AstraliteHeart simply does not want to admit it.\n\nIt's a strong model with some issues that I want to fix before release. It takes time to fix such issues. More time than I anticipated or wanted but I would rather have a decent model than release something sad. The bar is very high because of V6 and all the models that came after it.\n\nIt is also \\*\\*very expensive\\*\\* to train models of this size (in addition to knowing how to train them), hence you only see a few models coming out now (literally, Chroma is the only one really large finetune aside V7 I am aware that is not SDXL). So we have to take care of the financial part of the process too, and it takes time.\n\n  \n\\> It could easily happen that when Pony comes out, it will be outdated and no one needs it.\n\nThe best I can do is to build cool models that bring new tech to the table, this worked ok for the last \\~8 model I've released."", ""Chroma is gaining a lot of traction, and most of the tooling for it already exists. Since P7 is likely to be similar in size but using a different architecture, I think it will not become as popular as V6. LodeStones is also releasing a new checkpoint every four days on average, so people can get a taste of how the development is going. The aim is 50 epochs, and he's nearing epoch 42.\n\nIf P7 comes as a lighter model, people will use it."", ""They may have swung big, missed big.\n\nPony being on SD 1.5 and then XL, was smaller and more agile to work on. So when XL launched they were able to pivot to that and ride it to being a success. A problem with the post XL world though is we still may not have the proper replacement. Newer models have been slower, larger, worse licensing and not training very well.\n\nChroma was able to ride the community Flux tinkering and come out on top of a schnell tune with their own tweaks. That may be the future, assuming the final tune doesn't blow apart, distills make it smaller/faster, and it trains well for loras. But it's also possible next week a new 3B param base model comes out that's good, open licensed, and easy to train and we all move to that. That would kill Chroma, though the team could possibly just pivot to the new model and start training on top of that.\n\nThe scene moves way too fast for 6 month plus projects to be viable.""]",138,113,0.87,Discussion,1751732171.0
1lsd52s,StableDiffusion,Some question about LoRA training,"Hello everyone!

I want to train a LoRA for Flux inspired by classic sword and sorcery imagery from the 1980s. Think of Larry Elmore, Keith Parkinson, Jeff Easley, Clyde Caldwell, Frank Frazetta, or Boris Vallejo, for example. I don't want the LoRA to perfectly replicate each of these styles, but rather to create a new one that works well as an amalgamation of all of them.



Well, I have three questions.

First, I would like the LoRA to recognize certain elements and learn to replicate them perfectly:

\- The character's class or stereotype: barbarian, warrior, sorcereress, wizard, thief, cleric, paladin, ranger, etc.

\- The character's race: human, elf, drow, dwarf, halfling, etc.

\- Classic creatures and monsters: orcs, goblins, skeleton warriors, vampires, dragons, beholders, mind flayers, centaurs, griffins, phoenixes, etc.

\- Specific poses: arms akimbo, arms crossed, kneeling with a weapon held, standing over corpses or ruins with a raised sword and chest expanded, etc.

\- Female breasts: I don't want to make a pornographic LoRA, but I do think it's important that it knows how to draw topless women in an anatomically correct way.

So, my first question is this: how many images of each type (dwarves, orcs, breasts, etc.) do I need to give the LoRA for it to learn how to replicate them, and from how many different angles?



Secondly, since the faces of the characters in these types of images tend to be quite neutral, to give the user more control in the future when choosing the type of faces they want, I've come up with the idea of ​​generating multiple images of facial expressions (anger, fear, sadness, surprise, etc.) using the Arthemy Comix Flux base model + Larry Elmore's LoRA + a LoRA of facial expressions. So, my question is: is it acceptable to use these AI-generated images as part of the LoRA training data? Will it cause me problems?



Thirdly, I want to know if I'm correctly describing the images for the LoRA training. For this image here ([https://www.this-is-cool.co.uk/wp-content/uploads/2019/07/the-art-of-clyde-caldwell.jpg](https://www.this-is-cool.co.uk/wp-content/uploads/2019/07/the-art-of-clyde-caldwell.jpg)), I wrote this description by hand. Please tell me if it's suitable and how I can improve it:

*Character left: white dragon, green eyes, standing on two legs, frontal view slightly turned to the right, looking at the sorceress; character center: female elf, elven woman, sorceress, white skin, long pointed ears, beautiful face, large breasts, short brown spiky hair, green dress, thin shoulder straps, deep v-neck showing cleavage, bare arms, bare legs, long front and back panels, large golden earrings, golden necklace, golden upper arm cuff bracelet on her left arm, golden forearm cuff bracelets on both her arms, golden jewel belt with a large embedded ruby, standing, frontal view slighty turned to the right, looking at the chest, arms raised at chest level, white magical beams projecting from hands towards a locked chest; full body shot; interior scene, treasure chamber inside a tower, glasseless large window at the background, a city with tall tower can be seen from the window, stone walls, wooden beams on the ceiling, chains hanging from the ceiling, an open treasure chest full with gold coins at the bottom left, a small round wooden table at the bottom right, an ornate small golden chest on the table; natural lighting from the window on the background, artificial lighting from the magic beams of the spell; the sorceress is casting a spell to open a locked chest; oil painting, vertical composition; sword and sorcery, medieval fantasy, old-school fantasy; Clyde Caldwell style, signature at the bottom right*


Thanks in advance for the answers!","[""You want to do a big multiconcept training. I did something similar here: [https://civitai.com/models/1434675/ancient-roman-clothing](https://civitai.com/models/1434675/ancient-roman-clothing)\n\nBut it was much smaller in number of concepts. For my training I had used about 700 images, so expect to need quite a few more.\n\nThen I guess you'll also need to train a LoKR and not a LoRA to be able to store all concepts. You'd also should aim to raise the number of images that go into one step (high batch size, gradient accumulation).\n\nFor the faces I did mask all of them. When they are required, due to the style, expect to need even more images to prevent the model learning specific faces.\n\nThe example prompt is problematic: you need to prompt in exactly the same way you'd use the model later on. I don't think anybody will write such a long prompt. And for the few thousand images you'll need I don't think you want to caption all manually. Especially as it's helping to use multicaptioning, i.e. have more than one caption per image.  \nFor this I could use Gemini to caption all my images: I created a large prompt explaining all my trigger words and then let Gemini detect them and caption them.\n\nIt'll be much work. But when it's working it's really satisfying! :)""]","[""You want to do a big multiconcept training. I did something similar here: [https://civitai.com/models/1434675/ancient-roman-clothing](https://civitai.com/models/1434675/ancient-roman-clothing)\n\nBut it was much smaller in number of concepts. For my training I had used about 700 images, so expect to need quite a few more.\n\nThen I guess you'll also need to train a LoKR and not a LoRA to be able to store all concepts. You'd also should aim to raise the number of images that go into one step (high batch size, gradient accumulation).\n\nFor the faces I did mask all of them. When they are required, due to the style, expect to need even more images to prevent the model learning specific faces.\n\nThe example prompt is problematic: you need to prompt in exactly the same way you'd use the model later on. I don't think anybody will write such a long prompt. And for the few thousand images you'll need I don't think you want to caption all manually. Especially as it's helping to use multicaptioning, i.e. have more than one caption per image.  \nFor this I could use Gemini to caption all my images: I created a large prompt explaining all my trigger words and then let Gemini detect them and caption them.\n\nIt'll be much work. But when it's working it's really satisfying! :)""]",2,1,1.0,Question - Help,1751731112.0
1lscvor,StableDiffusion,"Am I Missing Something? No One Ever Talks About F5-TTS, and it's 100% Free + Local and > Chatterbox","I see Chatterbox is the new/latest TTS tool people are enjoying, however F5-TTS has been out for awhile now and I still think it sounds better and more accurate with one-shot voice cloning, yet people rarely bring it up? You can also do faux podcast style outputs with multiple voices if you generate a script with an LLM (or type one up yourself). Chatterbox sounds like an exaggerated voice actor version of the voice you are trying to replicate yet people are all excited about it, I don't get what's so great about it","['F5 while having a decent reading, it is unstable and hallucinates mid-sentences. Or fails to speak hyphenated words. Or suddenly changes the pace of the read.', ""I'm in the same boat, I don't really like chatterbox over F5 but it does seem F5 isn't going anywhere so the community ignores it (in a similar situation with Ace Step for local music gen. It's one of the few ones we have and it can do lora training but it's dead in the water it seems). I also think the other problem is there seems to be a general consensus that for top quality local voice cloning you use xtts v2 which has been out for a while now.\n\nIt's the double edged sword of open source. Community support can work miracles but if their is no unity behind any one model then things just quickly die (nvidia being a bunch of greedy shit heels and not giving us more vram at a reasonable price isn't helping either)."", ""I made the mistake of using ElevenLabs *first*. If you've never used TTS before, F5 is impressive. If you've used state of the art TTS (ElevenLabs), then F5, Chatterbox, SoVITS, and all the other local models sound like ass."", 'Obviously most people disagree with you, which is why chatterbox is so popular. I didn’t find F5 usable, but I do chatterbox. Also, chatterbox voice cloning is better imo. I can use TTS inside silly tavern no problems. My only issue with cb is that it pronounces some words wrong, and it doesn’t do accents well, but hopefully that will be resolved in future versions. But I think it’s solid enough to use for my use case, which is an AI personal assistant.', 'F5 has a more restrictive license I believe.', 'In case anyone is interested, I haven\'t been using them myself recently but last I heard F5 is still the best. \n\nEdit: probably time to update this with Wan and lipsync, and local music gen, \n\nAnyway:\n\nThere are so many models! https://artificialanalysis.ai/text-to-speech/arena\n\nMar2025 https://github.com/SparkAudio/Spark-TTS\n\nDec2024\n\nhttps://huggingface.co/geneing/Kokoro\n\nNewest, October 2024:\n\nF5-TTS and E2-TTS [https://www.youtube.com/watch?v=FTqAQvARMEg](https://www.youtube.com/watch?v=FTqAQvARMEg)   \nGithub Page: [https://github.com/SWivid/F5-TTS](https://github.com/SWivid/F5-TTS)   \nCode: [https://swivid.github.io/F5-TTS/](https://swivid.github.io/F5-TTS/)   \nAI Model : [https://huggingface.co/SWivid/F5-TTS](https://huggingface.co/SWivid/F5-TTS)\n\n\nu/perfect-campaign9551 says F5 tts sucks, it doesn\'t read naturally. Xttsv2 is still the king yet\n\n...\n\nYou want to hang out in r/AIVoiceMemes\n\nCoqui is fast but the voices are bad.\n\nTortoise is slow and unreliable but the voices are often great.\n\nStyleTTS2 is meant to be great and fast, but I could never figure out how to run it.\n\nThe key difference between Style and Coqui is that, I believe (things change), that you can train StyleTTS2.\n\nRVC does voice to voice, if you\'re struggling to get the \\*\\*\\*precise\\*\\*\\* pacing then you should speak into a mic and voice clone it with RVC.\n\nYou will want to seek podcasts and audiobooks on YouTube to download for audio sources.\n\nYou will want to use UVR5 to separate vocals from instrumentals if that becomes a thing.\n\nYou will eventually want to try lip syncing video, for that you will use EasyWav2Lip or possibly Face Fusion.\n\nIf you\'re having difficulty with install, there are Pinokio installs of a lot of TTS that can be easier to use, but are more limited.\n\nCheck out Jarod\'s Journey for all of the advice, especially about Tortoise: [https://www.youtube.com/@Jarods\\_Journey](https://www.youtube.com/@Jarods_Journey)\n\nCheck out P3tro for the only good installation tutorial about RVC: [https://www.youtube.com/watch?v=qZ12-Vm2ryc&t=58s&ab\\_channel=p3tro](https://www.youtube.com/watch?v=qZ12-Vm2ryc&t=58s&ab_channel=p3tro)\n\nEdit: Jarod made a gui for StyleTTS2. Also, try alltalk?\n\nEdit: u/a_beautifil_rhind \n\nstyletts has a better model called vokan.\nhttps://huggingface.co/ShoukanLabs/Vokan/tree/main/Model\n\nThere\'s also fish-audio now in addition to xtts. Also voicecraft.\n\nEdit: u/tavirabon\n\nCoqui (XTTS) can be finetuned https://github.com/daswer123/xtts-finetune-webui\n\nAlso https://github.com/RVC-Boss/GPT-SoVITS which is a step up from other zero-shot TTS and most few-shot TTS (>1 minute of clear natural speech) finetuning\n\nEdit: u/battlerepulsiveO\n\nYou can use the huggingface model of XTTS V2 because there are people who have finetuned XTTS V2 before. It\'s really simple to train with different methods like one that has automated for you where you just drop in the audio files. Or you can personally create a dataset and a csv file with the name of the audio file and the transcription, and all the wav files should be stored inside a wav folder. It all depends on the notebook you\'re using.\n\nEdit: u/dumpimel\n\nhave you tried alltalk? it\'s based on coqui\n\nhttps://github.com/erew123/alltalk_tts\n\nyou drop a 20s .wav in the ""voices"" folder and it\'s pretty decent at reproducing the voice\n\nthey also say you can finetune it further', ""It works pretty good.  But, it's not maintained, as I understand.\n\nI've used it and created a batcher for it."", 'I think we are missing the new player which is playDiffusion. I tried it and it is a big improvement over f5. it is basically from play.ht whose voice models were sometimes better if not at par with cloning compared to elvenlabs', 'My issue with chatterbox is, in the latest comfyui it just does not generate an output (I can only use it on my older backup) and the stand alone chatterbox just crashes all the time.']","['F5 while having a decent reading, it is unstable and hallucinates mid-sentences. Or fails to speak hyphenated words. Or suddenly changes the pace of the read.', ""I'm in the same boat, I don't really like chatterbox over F5 but it does seem F5 isn't going anywhere so the community ignores it (in a similar situation with Ace Step for local music gen. It's one of the few ones we have and it can do lora training but it's dead in the water it seems). I also think the other problem is there seems to be a general consensus that for top quality local voice cloning you use xtts v2 which has been out for a while now.\n\nIt's the double edged sword of open source. Community support can work miracles but if their is no unity behind any one model then things just quickly die (nvidia being a bunch of greedy shit heels and not giving us more vram at a reasonable price isn't helping either)."", ""I made the mistake of using ElevenLabs *first*. If you've never used TTS before, F5 is impressive. If you've used state of the art TTS (ElevenLabs), then F5, Chatterbox, SoVITS, and all the other local models sound like ass."", 'Obviously most people disagree with you, which is why chatterbox is so popular. I didn’t find F5 usable, but I do chatterbox. Also, chatterbox voice cloning is better imo. I can use TTS inside silly tavern no problems. My only issue with cb is that it pronounces some words wrong, and it doesn’t do accents well, but hopefully that will be resolved in future versions. But I think it’s solid enough to use for my use case, which is an AI personal assistant.', 'F5 has a more restrictive license I believe.']",36,39,0.88,Discussion,1751730410.0
1lscnvj,StableDiffusion,Whats the best AI short story generator currently?,"Hey. I am looking for the best AI short story generator out there, mostly for understanding them from a technical perspective. I am seeing lots of AI short story content on social media and wonder if they all do it with their own workflow or if there is a SOTA public platform for this. As a reference, I am talking about content like this, for example - https://x.com/TrungTPhan/status/1940905807908110711 (I am not affiliated with that post)","['Try asking /r/localllama', 'chatgpt', ""I don't think there is an AI to do that. It's simply making a lot of short videos and putting them together to make a story."", ""It's probably all about creative prompting and possibly invoking styles of good authors."", 'I mean AI short story generators with Video capabilities. Not text only btw']","['Try asking /r/localllama', 'chatgpt', ""I don't think there is an AI to do that. It's simply making a lot of short videos and putting them together to make a story."", ""It's probably all about creative prompting and possibly invoking styles of good authors."", 'I mean AI short story generators with Video capabilities. Not text only btw']",0,5,0.3,Question - Help,1751729835.0
1lscbye,StableDiffusion,Character consistency for cartoon styles,"Anybody know where I can read more about generating better prompts for consistent characters? I know about using Loras etc. but I just want to get started with simple stickfigure characters.

For example, I tried using these prompts:


>In a minimalist cartoon illustration featuring a flat pastel-yellow background (#F7E36B), the scene is set in a 16:9 landscape aspect ratio, maintaining a vivid, playful aesthetic. The foreground showcases a stick figure with an oversized round white head, a thick black outline (\~8px), dot eyes, expressive eyebrows, and a comically surprised mouth. The character has thin, exaggerated stick arms and legs, with mitten-like hands and flat feet, wearing only plain white shorts. The expression suggests a realization or shock, contributing to the theme of self-sabotage.

>In the middle ground, another stick figure, similarly designed, strikes a pondering pose, featuring one hand on their head and the other hand outstretched as if questioning. Both figures maintain a playful interaction while adhering to the cartoon's bold, expressive visual style.

>In the background, a simplistic setting of a single, thick-outlined black table with a minimal white vase on it adds context without clutter. The environment features essential elements only, each with super-thick black outlines and solid white interiors, ensuring a clean, humorous visual narrative.

>A speech bubble (with a thick black outline) emerges from the first character's mouth, capturing the sentence, ""Did you know you're probably sabotaging your own happiness right now and don’t even realize it?"" in a playful, hand-drawn font. The text inside the bubble is pure black, enhancing readability and fitting seamlessly into the cartoon world.

>The composition leaves a clean, 5% margin around the edges, maintaining focus and clarity, ensuring that all visual elements and messages are easily recognizable at a glance. This image instantly conveys the theme of subconscious actions affecting one's well-being in a humorous, relatable manner.

https://preview.redd.it/6s13t17go2bf1.png?width=1024&format=png&auto=webp&s=b682b3619b281e2175df68bb3104f0fd42747735

>In the foreground, a cartoon stick figure with two arms and two legs sits at the edge of a white bed, its oversized bald white head hung low, reflecting a sense of melancholy. The figure's expressive brows are furrowed and mouth slightly downturned, amplifying the feeling of dissatisfaction. Beside it, a simple white alarm clock rings loudly, indicating the start of another monotonous day.

>In the middle ground, an ascending staircase fades into an indistinct hallway, symbolizing the path of routine and the unknown longing for something more. The stick figure's eyes glance longingly toward this direction, hinting at a deep, unfulfilled desire for change.

>In the background, behind the staircase, a minimalist cityscape silhouette can be seen through a large window with thick black outlines. The sky is an indistinct blank white, devoid of vibrant color, reinforcing the sense of a life lacking excitement.

>The color palette is uniform and monochrome with solid white fills and black outlines consistent across all elements, ensuring clarity in the depiction of an autopilot existence. The tone of the image is somber yet gently comedic due to the exaggerated proportions and expressive features, maintaining the playful integrity of the cartoon style.

>Overall, the image portrays a simplistic yet profound moment of introspection, making the viewer contemplate the feeling of living on autopilot while expressing it through a coherent, stylized cartoon world.

https://preview.redd.it/j1iy3v4oo2bf1.png?width=1024&format=png&auto=webp&s=ca21ab90c63901f6f01925e6662d719f3f8212fc

So it was unable to keep the same style. Should I pass in less information as the prompt? I am using FLUX-Schnell to generate the images. I have been playing around with hundreds of prompts, reading tutorials to get the best prompts etc. but nothing seems to allow me to keep a consistent style across the images... I am hoping to create a comic book style workflow.","[""I think the purple prose like that is less effective than you'd think. You most likely wouldn't be able to maintain style through prompt alone. Something like Flux Kontext would have a better chance of doing it, at least it can maintain the character.""]","[""I think the purple prose like that is less effective than you'd think. You most likely wouldn't be able to maintain style through prompt alone. Something like Flux Kontext would have a better chance of doing it, at least it can maintain the character.""]",0,2,0.2,Question - Help,1751728932.0
1lsb708,StableDiffusion,Best model to use as base for SDXL Lora?,"Hey guys, what's the model to use as base? I know at some point SDXL itself was the way to go but doing it on some finetunes can give better results, so wondering if anyone got any idea. I haven't trained a realistic lora in a while so out of date.","['If it is for personal use, train it on the model you will use for inference. If you plan to share it, training it on base SDXL will allow it to be used on the broadest range of finetunes at the cost of some fidelity.']","['If it is for personal use, train it on the model you will use for inference. If you plan to share it, training it on base SDXL will allow it to be used on the broadest range of finetunes at the cost of some fidelity.']",0,1,0.29,Question - Help,1751725846.0
1lsap7b,StableDiffusion,7800xt to 5070ti,"I am looking to upgrade my video card as I am done messing with amd when it comes to generative ai. I am hoping to hear from anyone with 50xx series cards and how they are performing for them, particularly with wan 2.1. a little input would be great to help me make my decision. Right now I'm looking at the 5070ti cards.

Thanks!","['wait for the 5070ti 24gb', ""The RTX 5070 Ti is a good choice for AI tasks but also look at RTX 5060 Ti 16GB - this offers a more budget-friendly option with same amount of memory, albeit with slower performance but I am sure the price difference is more compared to the performance difference.   \n\n\nIf you're looking for a significant upgrade then wait and keep an eye for the 5070 Ti Super 24GB, which could be the best upgrade even for 4k games."", 'A change to NVIDIA makes sense especially for AI regardless which GPU. More memory is more important than (slightly) more performance. I upgraded from a 2080ti to a 5090 and it was definitely the right choice.', 'Just built a PC with these two components for a generative only machine.\n\n5070ti runs quite well IF you are on a good driver. Every other driver causes issues.', ""What's your use case? What GenAI tech stack are you using?""]","['wait for the 5070ti 24gb', ""The RTX 5070 Ti is a good choice for AI tasks but also look at RTX 5060 Ti 16GB - this offers a more budget-friendly option with same amount of memory, albeit with slower performance but I am sure the price difference is more compared to the performance difference.   \n\n\nIf you're looking for a significant upgrade then wait and keep an eye for the 5070 Ti Super 24GB, which could be the best upgrade even for 4k games."", 'A change to NVIDIA makes sense especially for AI regardless which GPU. More memory is more important than (slightly) more performance. I upgraded from a 2080ti to a 5090 and it was definitely the right choice.', 'Just built a PC with these two components for a generative only machine.\n\n5070ti runs quite well IF you are on a good driver. Every other driver causes issues.', ""What's your use case? What GenAI tech stack are you using?""]",1,11,0.67,Question - Help,1751724466.0
1lsals9,StableDiffusion,Opensource Image2image generator,"Hi all,

Alpha-VLLM’s Lumina-mGPT-2.0 was going to release an Image to Image generator to take on chatGPT’s one but nothing seems to have been released. Anyone know of another both diffusion or autoregression image 2 image generator that is open source?",['https://github.com/VectorSpaceLab/OmniGen2'],['https://github.com/VectorSpaceLab/OmniGen2'],0,1,0.33,Question - Help,1751724198.0
1lsa9so,StableDiffusion,Alternative to RVC for real time?,RVC is pretty dated at this point. Many new ones have released but they're TTS instead of voice conversion. I'm pretty left behind in the voice section. What's a good newer alternative?,"[""I don't think there is any. Most current papers only use TTS unfortunately, as you mentioned.\n\nI actually theorycrafted a newer realtime V2V architecture based on newer papers, but it's a hassle and training time is huge with a good dataset on a consumer gpu as mine. Also having to train submodules like the vocoder from scratch adds to that time even more"", 'RVC does real time though...', 'I’m also on the lookout for one, I’ve looked but unable to find a newer alternative .', 'For what purpose? Voice cloning? There are a few good TTS alternatives. If you want voice to voice, you can use STT and then TTS with a reference voice. Some newer alternatives have really good zero shot voice cloning.', ""I used it throughout this [narrated noir video](https://www.youtube.com/watch?v=mIDSYRsuSFM) and a lot of people picked up on it. If people are noticing things then thats a disctraction. So yea, I am looking for improved alternatives too.  \n  \nRVC is good and it allows us to add our own dramatic inflection to the voice, but the process is laborious and also has a crackle and drop outs that can take a bit of work to fix sometimes.  In other ways it is too good, my best trained actors I can't use because its too obvious who they are.\n\nI personally dont like many TTS they sound like AI or just inflict in the wrong places. So following this thread in hopes of seeing new stuff but I doubt many are working on it.""]","[""I don't think there is any. Most current papers only use TTS unfortunately, as you mentioned.\n\nI actually theorycrafted a newer realtime V2V architecture based on newer papers, but it's a hassle and training time is huge with a good dataset on a consumer gpu as mine. Also having to train submodules like the vocoder from scratch adds to that time even more"", 'RVC does real time though...', 'I’m also on the lookout for one, I’ve looked but unable to find a newer alternative .', 'For what purpose? Voice cloning? There are a few good TTS alternatives. If you want voice to voice, you can use STT and then TTS with a reference voice. Some newer alternatives have really good zero shot voice cloning.', ""I used it throughout this [narrated noir video](https://www.youtube.com/watch?v=mIDSYRsuSFM) and a lot of people picked up on it. If people are noticing things then thats a disctraction. So yea, I am looking for improved alternatives too.  \n  \nRVC is good and it allows us to add our own dramatic inflection to the voice, but the process is laborious and also has a crackle and drop outs that can take a bit of work to fix sometimes.  In other ways it is too good, my best trained actors I can't use because its too obvious who they are.\n\nI personally dont like many TTS they sound like AI or just inflict in the wrong places. So following this thread in hopes of seeing new stuff but I doubt many are working on it.""]",18,8,0.87,Question - Help,1751723218.0
1lrphxy,StableDiffusion,Face fusion 3.2.0 10 second content filter?,"Hi!. Does anybody know how to stop this from happening? No matter how long the video is, it’ll cut it as a 10 second video and then save it as “temp” which doesn’t do with the other (sfw) videos Im pretty sure. I’m also using Pinokio version 3.9.0. (It’s my body with an AI generated face). Any help would be really appreciated ",[],[],1,0,1.0,Question - Help,1751652797.0
1ls0mnl,StableDiffusion,How to train a lora for pony?,Is there any equivalent of flux gym but for the pony model? I try use kohya but is so confusing and only allows loras based on SDXL. I don't know if it's compatible. I'm a newbie in this. I have only used fluxgym in the past. Preferible I prefer using a cloud service like Mimic Pc since my own PC is not that good. Thanks ,"[""Pony is literally SDXL model. If you don't want to use Kohya, use OneTrainer or something.""]","[""Pony is literally SDXL model. If you don't want to use Kohya, use OneTrainer or something.""]",1,1,1.0,Question - Help,1751686641.0
1ls2q7u,StableDiffusion,i can download 100K+ LoRA and organize from civitai,"desktop app - [https://github.com/rajeevbarde/civit-lora-download](https://github.com/rajeevbarde/civit-lora-download)


it does lot of things .... all details in README.

this was vibe coded in 14 days Cursor trial plan.... bugs expected",[],[],6,0,0.71,Tutorial - Guide,1751694536.0
1ls8ftk,StableDiffusion,Looking for the BEST faceswap tool without a gpu or cpu only,"The title says it all.  I'm looking for a face-swap tool that can reproduce the original expression of the person on original image or video, while looking realistic. I tried Roop(?) or something like that but the person never looked like the source material or the skin texture was weird even though I used high quality images and videos.

If there's a paid tool (Paid as in an online service where I can buy credits or rent a gpu and do the swap there) it would be good too.


Are there any tools that I can create a model (like a Lora) for a person's face? I feel like source images aren't really a good way to be consistent if there's makeup etc. involved.


Thanks for the help!","['roop, facefusion or anything with inswapper 128. isn\'t really cut it for perfect one to one swap. but that also the only ""zero train"" face swapper.  \n  \nIf you want fullface simmilarity you have to go deepfacelab, and it can do CPU, but it needs to be trained first', 'New facefusion 3.3 is dope it has new hyperswap model. Free if you ok with basic command prompt (ask ChatGPT he will guide you) to install or pay 20 bucks for quick installer and support devs\xa0']","['roop, facefusion or anything with inswapper 128. isn\'t really cut it for perfect one to one swap. but that also the only ""zero train"" face swapper.  \n  \nIf you want fullface simmilarity you have to go deepfacelab, and it can do CPU, but it needs to be trained first', 'New facefusion 3.3 is dope it has new hyperswap model. Free if you ok with basic command prompt (ask ChatGPT he will guide you) to install or pay 20 bucks for quick installer and support devs\xa0']",0,4,0.25,Question - Help,1751717429.0
1ls9phn,StableDiffusion,Can you animate characters on a green screen?,"I've tried with img2vid, with a picture of a character on a green screen already, but it always ended up changing background color or shade at some point in the animation. ","[""A good first step might be applying a reference only control net and tweaking its strength until you get a nice balance between consistency and flexibility.\n\nBut, honestly, that you're having these problems suggests you're using older models.  Newer ones are much better at applying your prompts.  There are models and workflows that can stamp out all your keyframes on a single image and others than can take your single starting image and generate entire videos from it.""]","[""A good first step might be applying a reference only control net and tweaking its strength until you get a nice balance between consistency and flexibility.\n\nBut, honestly, that you're having these problems suggests you're using older models.  Newer ones are much better at applying your prompts.  There are models and workflows that can stamp out all your keyframes on a single image and others than can take your single starting image and generate entire videos from it.""]",0,9,0.33,Question - Help,1751721515.0
1ls8z68,StableDiffusion,"Is it worth training loras for FluxFill? I don't see anyone doing that. Is it better to use other methods like ACE++, Redux, Alimama controlnet ?","I'm not sure, but I've read some people saying that loras trained with flux fill are not good","['You can try to train flux kontext loras, it the same if not better', ""I don't know but you can try one alternative that was found during flux1 kontext release\n\nsomeone was loading flux dev loras via flux dev and then merging it with flux kontext and then removing the difference (or something like that) and they said it worked nice\n\nit is worth trying the same approach but with flux fill instead of kontext :)""]","['You can try to train flux kontext loras, it the same if not better', ""I don't know but you can try one alternative that was found during flux1 kontext release\n\nsomeone was loading flux dev loras via flux dev and then merging it with flux kontext and then removing the difference (or something like that) and they said it worked nice\n\nit is worth trying the same approach but with flux fill instead of kontext :)""]",2,2,0.62,Discussion,1751719235.0
1ls8a4q,StableDiffusion,Audio dubbing locally,I used eleven labs but now I am looking for a local solution to upload an audio and then be able to translate it or change accent to another language without changing the voice. any suggestion with the model and workflow ? ,[],[],1,0,1.0,Question - Help,1751716897.0
1ls6rh0,StableDiffusion,"Why can't I find any 5090 on Vast.ai? Even resetting filters (or setting them to maximum) won't help. I rented 5090s before, so it definitely worked before. Did they banned them? Is the website buggy? Am I just stupid and disabled something? What's going on?","Also, a while ago someone posted a website where we could see the number of different GPUs on vast.ai. Could someone check if there are any 5090s there?","[""You have an older template that's excluding it because it's not compatible. Use the CUDA 12.8 one."", ""I checked and I see plenty of 5090's. Don't know how to help you but just letting you know the problem is something on your end. Maybe ask in the chat feature of the Vast.ai website?""]","[""You have an older template that's excluding it because it's not compatible. Use the CUDA 12.8 one."", ""I checked and I see plenty of 5090's. Don't know how to help you but just letting you know the problem is something on your end. Maybe ask in the chat feature of the Vast.ai website?""]",0,4,0.5,Question - Help,1751711105.0
1ls6qj7,StableDiffusion,Igorr's ADHD - How did they do it?,"Not sure this is the right sub, but anyway, hoping it is: I'm trying to wrap my head around at how Meatdept could achive such outstanding results with this video using ""proprietary and open-source"" tools.

From the video caption, they state: ""we explored the possibilities of AI for this new Igorrr music video: ""ADHD"". We embraced almost all existing tools, both proprietary and open source, diverting and mixing them with our 3D tools"".

I tried the combination Flux + Wan2.1, but the results were nowhere close to this. Veo 3 is way too fresh IMO for a work that probably took a month or two at the very least. And a major detail: the consistency is unbelievable, the characters, the style and the photography stay pretty much the same throughout all the countless scenes/shots. Any ideas what they could've used?","['So there\'s a lot going on here, but most of it is ""a lot of work"". Notice that this is built out of very short segments, 3 to 5 seconds . . . in this almost 5 minute film, there\'s maybe 60-80 segments, an astonishing number (some are repeats though). The very short shot length does suggest AI generation \n\nWhat I think I\'m seeing is a staggering amount of planning, followed by a heckuva lot of work in generating, and then a ton of work editing it all together. You could do a \\_lot\\_ of this in easy tools, not hard to get a five second clip with this kind of theme, but what\'s really hard is to get dozens of these clips which add up to a coherent (and funny !) narrative.\n\nPeople often miss how important editing is to film. Understanding where the beats are in a scene, how to cut from A to B to C . . . that\'s a art.  When I look at any one segment of this, I think to myself ""yeah, I know how I\'d get that clip\'. Where my jaw drops is how they\'ve pulled this all together in this coordinated, coherent way. Doing that is a matter not just of a lot of work, but also of an artistic vision. So yeah, most folks can learn to do a 3 to 5 second clip from some still source material that looks like this, but the magic is in the coherence of dozens of clips that make sense and tell a great story. \n\nIf you\'re looking for image quality that\'s at this level -- its going to be image based video for sure. That is you\'re going to want to render a bunch of reference frames, of the highest quality, and then use them for image to video. Not sure which application it would be . . . quality wise, a lot of things can produce stuff that looks this snappy. If I were guessing, I might have guessed Kling video, given the time that this was created (back in April) that limits some of what it could be (eg could be Midjourney for some of the still sources, but there was no MJ video at that pointP   . . . but I bet with the right tweaks it could have been done in a lot of other things. The only thing I\'m pretty certain of is that this isn\'t text to video, there have to be image references here . . . that\'s \\[part of\\] what creates the consistency. Indeed, as they describe what they put together, this looks like it was storyboarded and planned with a lot of tools including 3D', ""I'm amazed that not even AI can reach the level of absurdity and mindfuckery that Chris Cunningham works do. It really reminds me how much of a genius that man is.   \nIf someone don't know him yet, check [Rubber Johnny](https://www.youtube.com/watch?v=9-gyf23k26I&list=RD9-gyf23k26I&start_radio=1) to get an idea."", 'Just read what you quoted. They used many other techniques than AI generative models. They could use for example Blender 3D for stuff like tracking, masking, and adding objects.', 'thank you for reminding me to listen more to Igorrr. Sorry for not providing any useful info about how to do it, how they made it :3', 'Looks like Flux with a Ben Stein and maybe like a Clark Gable LoRA, mixed with 1950s mid-century modern or atomic-age/retro-futurism LoRA, then fed into whatever I2V tools they used. Maybe WAN or proprietary ones (they said they used some closed-source tools in the video description). They also mentioned they used 3D tools, maybe to generate the more abstract 3D rendered clips.\n\nAnd then a lot of editing.', 'This is what people who say things like ‘in a few years all movies and tv shows will be done with AI, just type in what you want and it’ll be generated for you’ miss. Yes you can do it with AI, but for it to be any good takes a ton of hard work, planning and talent. Same as with anything else.\xa0', '> How did they do it ?\n\nDrugs. Essentially.', 'awesome video. The early part was Veo I think - the beginning pace was slower and more realistic. I suspect they worked on it for a while and Veo came out and they had to use it. I think the way it warms up into the surrealism is very effective.\n\nA small thing I noticed how they must have made 5 second clips of the main character pulling facial expressions and then sped that up x5 so his movements synced with the beat. So much work must have gone into this.  Music and video creation is blending more and more.']","['So there\'s a lot going on here, but most of it is ""a lot of work"". Notice that this is built out of very short segments, 3 to 5 seconds . . . in this almost 5 minute film, there\'s maybe 60-80 segments, an astonishing number (some are repeats though). The very short shot length does suggest AI generation \n\nWhat I think I\'m seeing is a staggering amount of planning, followed by a heckuva lot of work in generating, and then a ton of work editing it all together. You could do a \\_lot\\_ of this in easy tools, not hard to get a five second clip with this kind of theme, but what\'s really hard is to get dozens of these clips which add up to a coherent (and funny !) narrative.\n\nPeople often miss how important editing is to film. Understanding where the beats are in a scene, how to cut from A to B to C . . . that\'s a art.  When I look at any one segment of this, I think to myself ""yeah, I know how I\'d get that clip\'. Where my jaw drops is how they\'ve pulled this all together in this coordinated, coherent way. Doing that is a matter not just of a lot of work, but also of an artistic vision. So yeah, most folks can learn to do a 3 to 5 second clip from some still source material that looks like this, but the magic is in the coherence of dozens of clips that make sense and tell a great story. \n\nIf you\'re looking for image quality that\'s at this level -- its going to be image based video for sure. That is you\'re going to want to render a bunch of reference frames, of the highest quality, and then use them for image to video. Not sure which application it would be . . . quality wise, a lot of things can produce stuff that looks this snappy. If I were guessing, I might have guessed Kling video, given the time that this was created (back in April) that limits some of what it could be (eg could be Midjourney for some of the still sources, but there was no MJ video at that pointP   . . . but I bet with the right tweaks it could have been done in a lot of other things. The only thing I\'m pretty certain of is that this isn\'t text to video, there have to be image references here . . . that\'s \\[part of\\] what creates the consistency. Indeed, as they describe what they put together, this looks like it was storyboarded and planned with a lot of tools including 3D', ""I'm amazed that not even AI can reach the level of absurdity and mindfuckery that Chris Cunningham works do. It really reminds me how much of a genius that man is.   \nIf someone don't know him yet, check [Rubber Johnny](https://www.youtube.com/watch?v=9-gyf23k26I&list=RD9-gyf23k26I&start_radio=1) to get an idea."", 'Just read what you quoted. They used many other techniques than AI generative models. They could use for example Blender 3D for stuff like tracking, masking, and adding objects.', 'thank you for reminding me to listen more to Igorrr. Sorry for not providing any useful info about how to do it, how they made it :3', 'Looks like Flux with a Ben Stein and maybe like a Clark Gable LoRA, mixed with 1950s mid-century modern or atomic-age/retro-futurism LoRA, then fed into whatever I2V tools they used. Maybe WAN or proprietary ones (they said they used some closed-source tools in the video description). They also mentioned they used 3D tools, maybe to generate the more abstract 3D rendered clips.\n\nAnd then a lot of editing.']",18,10,0.67,Question - Help,1751710996.0
1ls65i0,StableDiffusion,"New to Stable Diffusion, Need Help Getting Started","Hey everyone, I’m new to Stable Diffusion and image generation in general. I’m really interested in using it to generate consistent images for my projects.

Can anyone guide me on how to set it up and use it properly? Do I need to pay for anything to get started?

Also, if there’s a YouTube video or tutorial that explains everything for beginners, I’d really appreciate it if you could drop the link!

Lastly, is there any AI tool similar to Stable Diffusion but for video generation? I’d love to explore that too.

Thanks in advance!","['SwarmUI is simple to use, and uses ComfyUI.  Swarm has a built-in model downloader that will maintain metadata from civitai.  \n\n[https://github.com/mcmonkeyprojects/SwarmUI](https://github.com/mcmonkeyprojects/SwarmUI)', 'If you prefer simple then try simple interface like ""Fooocus"" for stable diffusion XL modes:\xa0[YouTube - Fooocus installation](https://youtu.be/3tAaL57rhoU?si=ArrTPhPQJkbfW7ZN)\n\nThis\xa0[playlist - YouTube](https://youtube.com/playlist?list=PLPFN04WspxqsslRSpiLmwGR8QTpDYNv7z&si=-QOo5_980RbiniKn)\xa0is for beginners which covers topic like prompt, models, lora, weights, in-paint, out-paint, image to image, canny, refiners, open pose, consistent character, training a LoRA.\n\nAfter this if you wish to go further then look at Comfy UI', ""If you don't have the GPU, you can use online services for free or at very low cost: [Free Flux/SDXL Online Generators](https://new.reddit.com/r/StableDiffusion/comments/18h7r2h/free_online_sdxl_generators/)\n\nHere are some ELi5 guides that I wrote a while back which may or may not help:\n\n[ELi5: Absolute beginner's guide to getting started in A.I. Image generation](https://new.reddit.com/r/StableDiffusion/comments/1b2mhjv/eli5_absolute_beginners_guide_to_getting_started/)\n\n[ELi5: What are SD models, and where to find them](https://new.reddit.com/r/StableDiffusion/comments/11s6485/eli5_what_are_sd_models_and_where_to_find_them/)"", 'What GPU do you have?\n\nDownload [Stability Matrix](https://github.com/LykosAI/StabilityMatrix). That program is an installer/manager for the various UIs. This is all free by the way.\n\nOn Matrix download Forge or reForge as they will be the easiest to start with.\n\nGo to [civit.ai](http://civit.ai) and make an account. Download checkpoints that you like.\n\nFor video look at Wan 2.1 and you can run this on ComfyUI, one of the UIs you can download through Stability Matrix. You might struggle to run this depending on how much VRAM you have.']","['SwarmUI is simple to use, and uses ComfyUI.  Swarm has a built-in model downloader that will maintain metadata from civitai.  \n\n[https://github.com/mcmonkeyprojects/SwarmUI](https://github.com/mcmonkeyprojects/SwarmUI)', 'If you prefer simple then try simple interface like ""Fooocus"" for stable diffusion XL modes:\xa0[YouTube - Fooocus installation](https://youtu.be/3tAaL57rhoU?si=ArrTPhPQJkbfW7ZN)\n\nThis\xa0[playlist - YouTube](https://youtube.com/playlist?list=PLPFN04WspxqsslRSpiLmwGR8QTpDYNv7z&si=-QOo5_980RbiniKn)\xa0is for beginners which covers topic like prompt, models, lora, weights, in-paint, out-paint, image to image, canny, refiners, open pose, consistent character, training a LoRA.\n\nAfter this if you wish to go further then look at Comfy UI', ""If you don't have the GPU, you can use online services for free or at very low cost: [Free Flux/SDXL Online Generators](https://new.reddit.com/r/StableDiffusion/comments/18h7r2h/free_online_sdxl_generators/)\n\nHere are some ELi5 guides that I wrote a while back which may or may not help:\n\n[ELi5: Absolute beginner's guide to getting started in A.I. Image generation](https://new.reddit.com/r/StableDiffusion/comments/1b2mhjv/eli5_absolute_beginners_guide_to_getting_started/)\n\n[ELi5: What are SD models, and where to find them](https://new.reddit.com/r/StableDiffusion/comments/11s6485/eli5_what_are_sd_models_and_where_to_find_them/)"", 'What GPU do you have?\n\nDownload [Stability Matrix](https://github.com/LykosAI/StabilityMatrix). That program is an installer/manager for the various UIs. This is all free by the way.\n\nOn Matrix download Forge or reForge as they will be the easiest to start with.\n\nGo to [civit.ai](http://civit.ai) and make an account. Download checkpoints that you like.\n\nFor video look at Wan 2.1 and you can run this on ComfyUI, one of the UIs you can download through Stability Matrix. You might struggle to run this depending on how much VRAM you have.']",0,8,0.44,Question - Help,1751708555.0
1ls639w,StableDiffusion,What the hell is even happening.... I'm using the default flux kontext workflow from the documentations docs.comfy.org/tutorials/flux/flux-1-kontext-dev (don't ask about the naked guy i'm using i randomly found him on facebook),,"['oh I like the high heels', ""\\>while preserving the girl's facial features\n\nYeah, tell me about it"", ' we dont see a lot of naked guys on this sub OP\n\nyou found a naked guy on facebook in heels , lol\n\nuse latenet refrance , you can search it , you seem good at searching ( wink)', 'what are you cooking bro', ""Flux Kontext Dev support for multiple images is very janky. It's really only trained on single reference edits."", 'Try latent stitch, not image', 'Someone still got the bottom part of the cloth remover prompt active', 'Prompt guide … read it', 'Bro, why’re you always making things so complicated? I’ve seen you ask for help like, three times already. Even if you never check the official instructions, why not just use an AI assistant to help you with prompts… or switch to a better workflow? Kontext ain’t as tough as you’re making it out to be!']","['oh I like the high heels', ""\\>while preserving the girl's facial features\n\nYeah, tell me about it"", ' we dont see a lot of naked guys on this sub OP\n\nyou found a naked guy on facebook in heels , lol\n\nuse latenet refrance , you can search it , you seem good at searching ( wink)', 'what are you cooking bro', ""Flux Kontext Dev support for multiple images is very janky. It's really only trained on single reference edits.""]",0,16,0.39,Question - Help,1751708284.0
1ls5jqq,StableDiffusion,BeltOut: An open source pitch-perfect (SINGING!@#$) voice-to-voice timbre transfer model based on ChatterboxVC,"***For everyone returning to this post for a second time, I've updated the Tips and Examples section with important information on usage, as well as another example. Please take a look at them for me! They are marked in square brackets with [EDIT] and [NEW] so that you can quickly pinpoint and read the new parts.***

Hello! My name is Shiko Kudo, I'm currently an undergraduate at National Taiwan University. I've been around the sub for a long while, but... today is a bit special. I've been working all this morning and then afternoon with bated breath, finalizing everything with a project I've been doing so that I can finally get it into a place ready for making public. It's been a couple of days of this, and so I've decided to push through and get it out today on a beautiful weekend. AHH, can't wait anymore, here it is!!:

They say timbre is the only thing you can't change about your voice... well, not anymore.

**BeltOut** ([HF](https://huggingface.co/Bill13579/beltout), [GH](https://github.com/bill13579/beltout)) is **the world's first *pitch-perfect*, zero-shot, voice-to-voice timbre transfer model with *a generalized understanding of timbre and how it affects delivery of performances***. It is based on ChatterboxVC. As far as I know it is the first of its kind, being able to deliver eye-watering results for timbres it has never *ever* seen before (all included examples are of this sort) on many singing and other extreme vocal recordings.

**[NEW] To first give an overhead view of what this model does:**

First, it is important to establish a key idea about why your voice sounds the way it does. There are two parts to voice, the part you *can* control, and the part you *can't*.

For example, I can play around with my voice. I can make it sound *deeper*, more resonant by speaking from my chest, make it sound boomy and *lower*. I can also make the pitch go a lot higher and tighten my throat to make it sound sharper, more *piercing* like a cartoon character. With training, you can do a lot with your voice.

What you cannot do, no matter what, though, is change your ***timbre***. **Timbre** is the reason why different musical instruments playing the same note sounds different, and you can tell if it's coming from a violin or a flute or a saxophone. It is *also* why we can identify each other's voices.

It can't be changed because it is *dictated by your head shape, throat shape, shape of your nose, and more.* With a bunch of training you can alter pretty much everything about your voice, but someone with a mid-heavy face might always be louder and have a distinct ""shouty"" quality to their voice, while others might always have a rumbling low tone.

The model's job, and its *only* job, is to change *this* part. *Everything else is left to the original performance.* This is different from most models you might have come across before, where the model is allowed to freely change everything about an original performance, subtly adding an intonation here, subtly increasing the sharpness of a word there, subtly sneak in a breath here, to fit the timbre. This model does not do that, disciplining itself to strictly change only the timbre part.

So the way the model operates, is that it takes 192 numbers representing a unique voice/timbre, and also a random voice recording, and produces a new voice recording with that timbre applied, and *only* that timbre applied, leaving the rest of the performance entirely to the user.

**Now for the original, slightly more technical explanation of the model:**

It is explicitly different from existing voice-to-voice Voice Cloning models, in the way that it is not just entirely unconcerned with modifying anything other than timbre, but is even more importantly *entirely unconcerned with the specific timbre to map into*. The goal of the model is to learn *how* differences in vocal cords and head shape and all of those factors that contribute to the immutable timbre of a voice affects delivery of vocal intent in general, so that it can guess how the same performance will sound out of such a different base physical timbre.

This model represents timbre as just a list of 192 numbers, the **x-vector**. Taking this in along with your audio recording, the model creates a new recording, guessing how the same vocal sounds and intended effect would have sounded coming out of a different vocal cord.

In essence, instead of the usual `Performance -> Timbre Stripper -> Timbre ""Painter"" for a Specific Cloned Voice`, the model is a timbre shifter. It does `Performance -> Universal Timbre Shifter -> Performance with Desired Timbre`.

This allows for unprecedented control in singing, because as they say, timbre is the only thing you truly cannot hope to change without literally changing how your head is shaped; everything else can be controlled by you with practice, and this model gives you the freedom to do so while also giving you a way to change that last, immutable part.

# Some Points

* Small, running comfortably on my 6gb laptop 3060
* *Extremely* expressive emotional preservation, translating feel across timbres
* Preserves singing details like precise fine-grained vibrato, shouting notes, intonation with ease
* Adapts the original audio signal's timbre-reliant performance details, such as the ability to hit higher notes, very well to otherwise difficult timbres where such things are harder
* Incredibly powerful, doing all of this with just a single x-vector and the source audio file. No need for any reference audio files; in fact you can just generate a random 192 dimensional vector and it will generate a result that sounds like a completely new timbre
* Architecturally, only 335 out of all training samples in the 84,924 audio files large dataset was actually ""singing with words"", with an additional 3500 or so being scale runs from the VocalSet dataset. Singing with words is emergent and entirely learned by the model itself, learning singing despite mostly seeing SER data
* Make sure to read the [technical report](https://github.com/Bill13579/beltout/blob/main/TECHNICAL_REPORT.md)!! Trust me, it's a fun ride with twists and turns, ups and downs, and so much more.

Join the Discord https://discord.gg/MJzxacYQ!!!!! It's less about anything and more about I wanna hear what amazing things you do with it.

# Examples and Tips

The x-vectors, and the source audio recordings are both available on the repositories under the `examples` folder for reproduction.

**[EDIT] Important note on generating x-vectors from sample target speaker voice recordings: Make sure to get as much as possible.** It is highly recommended you let the analyzer take a look at at least 2 minutes of the target speaker's voice. More can be incredibly helpful. If analyzing the entire file at once is not possible, you might need to let the analyzer operate in chunks and then average the vector out. In such a case, after dragging the audio file in, wait for the `Chunk Size (s)` slider to appear beneath the `Weight` slider, and then set it to a value other than `0`. A value of around 40 to 50 seconds works great in my experience.

`sd-01*.wav` on the repo, [https://youtu.be/5EwvLR8XOts](https://youtu.be/5EwvLR8XOts) (output) / [https://youtu.be/wNTfxwtg3pU](https://youtu.be/wNTfxwtg3pU) (input, yours truly)

`sd-02*.wav` on the repo, [https://youtu.be/KodmJ2HkWeg](https://youtu.be/KodmJ2HkWeg) (output) / [https://youtu.be/H9xkWPKtVN0](https://youtu.be/H9xkWPKtVN0) (input)

**[NEW]2** [https://youtu.be/E4r2vdrCXME](https://youtu.be/E4r2vdrCXME) (output) / [https://youtu.be/9mmmFv7H8AU](https://youtu.be/9mmmFv7H8AU) (input) (Note that although the input *sounds* like it was recorded willy-nilly, this input is actually after **more than a dozen takes**. The input is not random, if you listen closely you'll realize that if you do not look at the timbre, the rhythm, the pitch contour, and the intonations are all carefully controlled. The laid back nature of the source recording is intentional as well. Thus, only because *everything other than timbre is managed carefully*, when the model applies the timbre on top, it can sound realistic.)

Note that a very important thing to know about this model is that it is a *vocal timbre* transfer model. The details on how this is the case is inside the technical reports, but the result is that, unlike voice-to-voice models that try to help you out by fixing performance details that might be hard to do in the target timbre, and thus simultaneously either destroy certain parts of the original performance or make it ""better"", so to say, but removing control from you, this model will not do any of the heavy-lifting of making the performance match that timbre for you!! In fact, it was **actively designed to restrain itself from doing so, since the model might otherwise find that changing performance details is the easier to way move towards its learning objective.**

So you'll need to do that part.

Thus, when recording with the purpose of converting with the model later, you'll need to be mindful and perform accordingly. For example, listen to this clip of a recording I did of Falco Lombardi from `0:00` to `0:30`: [https://youtu.be/o5pu7fjr9Rs](https://youtu.be/o5pu7fjr9Rs)

Pause at `0:30`. This performance would be adequate for many characters, but for this specific timbre, the result is unsatisfying. Listen from `0:30` to `1:00` to hear the result.

To fix this, the performance has to change accordingly. Listen from `1:00` to `1:30` for the new performance, also from yours truly ('s completely dead throat after around 50 takes).

Then, listen to the result from `1:30` to `2:00`. It is a marked improvement.

Sometimes however, with certain timbres like Falco here, the model still doesn't get it exactly right. I've decided to include such an example instead of sweeping it under the rug. In this case, I've found that a trick can be utilized to help the model sort of ""exaggerate"" its application of the x-vector in order to have it more confidently apply the new timbre and its learned nuances. It is very simple: we simply make the magnitude of the x-vector bigger. In this case by 2 times. You can imagine that doubling it will cause the network to essentially double whatever processing it used to do, thereby making deeper changes. There is a small drop in fidelity, but the increase in the final performance is well worth it. Listen from `2:00` to `2:30`.

**[EDIT] You can do this trick in the Gradio interface. Simply set the `Weight` slider to beyond 1.0. In my experience, values up to 2.5 can be interesting for certain voice vectors. In fact, for some voices this is necessary!** For example, the third example of Johnny Silverhand from above has a weight of 1.7 applied to it after getting the regular vector from analyzing Phantom Liberty voice lines (the `npy` file in the repository already has this weighting factor baked into it, so if you are recreating the example output, you should keep the weight at 1.0, but it is important to keep this in mind while creating your own x-vectors).

**[EDIT]** The degradation in quality due to such weight values vary wildly based on the x-vector in question, and for some it is not present, like in the aforementioned example. You can try a couple values out and see which values gives you the most emotive performance. When this happens it is an indicator that the model was perhaps a bit too conservative in its guess, and we can increse the vector magnitude manually to give it the push to make deeper timbre-specific choices.

Another tip is that in the Gradio interface, you can calculate a statistical average of the x-vectors of massive sample audio files; make sure to utilize it, and play around with the Chunk Size as well. I've found that the larger the chunk you can fit into VRAM, the better the resulting vectors, so a chunk size of 40s sounds better than 10s for me; however, this is subjective and your mileage may vary. Trust your ears!

# Supported Lanugage

The model was trained on a variety of languages, and not just speech. Shouts, belting, rasping, head voice, ...

As a baseline, I have tested Japanese, and it worked pretty well.

In general, the aim with this model was to get it to learn how different sounds created by human voices would've sounded produced out of a different physical vocal cord. This was done using various techniques while training, detailed in the technical sections. Thus, the supported types of vocalizations is vastly higher than TTS models or even other voice-to-voice models.

However, since the model's job is *only* to make sure your voice has a new timbre, the result will only sound natural if you give a performance matching (or compatible in some way) with that timbre. For example, asking the model to apply a low, deep timbre to a soprano opera voice recording will probably result in something bad.

Try it out, let me know how it handles what you throw at it!

# Socials

There's a [Discord](https://discord.gg/MJzxacYQ) where people gather; hop on, share your singing or voice acting or machine learning or anything! It might not be exactly what you expect, although I have a feeling you'll like it. ;)

My personal socials: [Github](https://github.com/Bill13579), [Huggingface](https://huggingface.co/Bill13579), [LinkedIn](https://www.linkedin.com/in/shiko-kudo-a44b86339/), [BlueSky](https://bsky.app/profile/kudoshiko.bsky.social), [X/Twitter](https://x.com/kudoshiko),

# Closing

This ain't the closing, you kidding!?? I'm so incredibly excited to finally get this out I'm going to be around for days weeks months hearing people experience the joy of getting to suddenly play around with a infinite amount of new timbres from the one they had up to then, and hearing their performances. I know I felt that same way...

I'm sure that a new model will come eventually to displace all this, but, speaking of which...

# Call to train

If you read through the technical report, you might be surprised to learn among other things just how incredibly quickly this model was trained.

It wasn't without difficulties; each problem solved in that report was days spent gruelling over a solution. However, I was surprised myself even that in the end, with the right considerations, optimizations, and head-strong persistence, many many problems ended up with extremely elegant solutions that would have frankly never come up without the restrictions.

And this just proves more that people doing training locally isn't just feasible, isn't just interesting and fun (although that's what I'd argue is the most important part to never lose sight of), but incredibly important.

So please, train a model, share it with all of us. Share it on as many places as you possibly can so that it will be there always. This is how local AI goes round, right? I'll be waiting, always, and hungry for more.

\- Shiko","[""I don't understand what this is"", ""That's interesting, I wonder if it can do voiceovers for films?"", 'Is it possible to improve the sound quality in this way?\n\nFor example, I have several acapelas from Udio, I like the tone of the intonation, but the blurred transitions and the compressed spectrum are completely depressing. Is it possible to improve with this tool, bring the sound closer to the real sound of the voice? Even if the timbre changes a little?', ""Sounds interesting. But i'm not tech savvy so I'll wait for someone to make a installer with an interface."", 'Oh we gotta get this in front of T-pain! \n\nAwesome work.', 'Great work and share', 'How can I help (contribute) your project? Train or something?', 'edit: all is well!\n\n\n\nold text: not relevant anymore:\n\n---\n\nWhen you install this app the server opens a connection and exposes your PC to the internet. It can bypass firewalls and such as it uses the websurfing port. anyone with the link that appears can use the app on your computer accessing your hardware.\n\nThis is not a virus or something.. just a bad setting and since only you can see the link its not likely to happen but some people might not want that. Also there are reports of scanner bots that look for open servers like this.\n\nChange the last line of run.py so that it looks like this(the last word ""False"" is the important one:\n\n\n    demo.queue().launch(debug=True, share=False)\n\nthen all is well\n\ndisclaimer: i am sure this is not malicious on OP and this is just a mistake and the functionality a normal setting in gradio.\n\n\n/u/bill1357 i am sure it was a mistake pls change it.\n\nalso when you generate a voice for the second time the preview stays unchanged withthe first generation. only the downloadable file gets updated', ""Thanks for sharing your work with the community!\n\nI feel I'm missing some clarity, in particular because the delta between the inputs and outputs in your examples is very small so it's hard to tell what is happening. If I were to guess it sounds like audio to audio where you passed a recording of your voice through a model of your voice? My assumption and understanding from your description is that you've essentially created a version of RVC that runs under a different architecture and generates results better aligned to the voice clone, did I understand that correctly?\n\nIf so I have more questions for you :)\nWhat are areas where it does better or worse than RVC?\nCan your system handle inputs such as sighs, whispers, laughs? What about vocal fry? These are areas where RVC really struggles.\nAre you familiar with other architectures other than RVC?\nIs there a way to fine tune your base checkpoint? For example suppose instead of training on one voice, I place samples from 30 voices to further improve the base model before fine tuning for a specific voice. Is that how these work?\n\nAlso apologies if I'm entirely missing the point, AI technical jargon is still a bit beyond me and I mostly manage terms that I've learned in the past year or so just as a practitioner using and barely fine tuning image and video models rather than programming them."", ""you exceeded yout github quota and its now blocked for downloads:\n\n\n    git clone https://github.com/Bill13579/beltout\n    Cloning into 'beltout'...\n    remote: Enumerating objects: 138, done.\n    remote: Counting objects: 100% (138/138), done.\n    remote: Compressing objects: 100% (123/123), done.\n    remote: Total 138 (delta 16), reused 133 (delta 11), pack-reused 0     (   from 0)\n    Receiving objects: 100% (138/138), 139.31 KiB | 7.33 MiB/s, done.\n    Resolving deltas: 100% (16/16), done.\n    Downloading checkpoints/cfm_step_0.safetensors (285 MB)\n    Error downloading object: checkpoints/cfm_step_0.safetensors (530d737): Smudge error: Error downloading checkpoints/cfm_step_0.safetensors         (530d737209605985419c5d511084f2ca213a84691d3a1ca0fd0a4d2224ba3539): batch response: This repository exceeded its LFS budget. The account responsible for the budget should increase it to restore access.\n\nremove all binaries from github and put them in hugging face. :)""]","[""I don't understand what this is"", ""That's interesting, I wonder if it can do voiceovers for films?"", 'Is it possible to improve the sound quality in this way?\n\nFor example, I have several acapelas from Udio, I like the tone of the intonation, but the blurred transitions and the compressed spectrum are completely depressing. Is it possible to improve with this tool, bring the sound closer to the real sound of the voice? Even if the timbre changes a little?', ""Sounds interesting. But i'm not tech savvy so I'll wait for someone to make a installer with an interface."", 'Oh we gotta get this in front of T-pain! \n\nAwesome work.']",255,89,0.98,Resource - Update,1751706007.0
1ls544m,StableDiffusion,What's the highest quality i2v possible on 5090?,"So I've been in to AI from the start, I have printed canvases going back to SD 1.5 in my living room, I love everything about it and an reasonably comfortable with comfy.

Regarding ai video experience, I've pretty much tested each be model and it came out for fun and to see where we are in terms of real usability for real world cases.

Well a real world cases has come up to get a friend out of a hole. I work as a photographer and was shooting a wedding with a videographer, he's been at it 20 years and is very experienced but has somehow lost the shot of the bride and groom walking back down the aisle, this could be disastrous for him as one tricky bride can ruin a reputation and I know he's distraught.

At this point she is unaware, and I happen to have a good few high quality/resolution stills which I attempted to animate. They look pretty amazing from a distance of you squint, but these are real people and their faces just aren't detailed enough and loose consistency. I spent a few hours at it last night trying flf and single image i2v.

Speed is not a concern as it usually has been in my messing around with my own video generations, so I'm not sure really what the absolute optimumum quality can be pulled out to help with this situation from my new 5090, assuming that most speed ups degrade quality.

Is there a way to use a reference image to swap faces throughout such as reactor after the initial generation considering there are two faces? I only tried reactor once a good while ago. I'm considering splitting the generated video in half from 1024*720 to 512*720 as they couple are centrally framed so could potentially have only one of them in each video and then rejoin but that feels convoluted.

Maybe there is a great technique/workflow/solution I'm missing. I've increased steps which helped a bit, gone higher than 720p which also helped but hasn't got it to the stage that I think it would pass for a few seconds in a 5 minute video, I mean it might at first watch but after a rewatch they'll probably notice.

I'm using wan 720p i2v, I know 2.2 is coming so maybe that might be a further improvement, but he needs to get there fun delivered in the coming week. Just to note he doesn't need much of them walking down the aisle, it's just a few seconds as long as it's there, my tests have been on 81 frames, interpolated and I'll upscale properly once I've got a decent result.

Any other suggestions or workflows for this would be greatly appreciated!","['My advice is to use paid services with first - last frame options. Existing open source aren’t there yet.\xa0\n\nAn even better option would be animate one of your photos of her walking down isle without last frame and forget about last frame. You only need 2 seconds before switching to another shot.\xa0\n\nKling 2.1, VEO3 (with first frame options), Midjourney video (With upscaler later), and new Seedance Pro can all do the job pretty well.\xa0', 'This will get him, and you, into legal problems.']","['My advice is to use paid services with first - last frame options. Existing open source aren’t there yet.\xa0\n\nAn even better option would be animate one of your photos of her walking down isle without last frame and forget about last frame. You only need 2 seconds before switching to another shot.\xa0\n\nKling 2.1, VEO3 (with first frame options), Midjourney video (With upscaler later), and new Seedance Pro can all do the job pretty well.\xa0', 'This will get him, and you, into legal problems.']",0,17,0.45,Question - Help,1751704166.0
1ls3r31,StableDiffusion,Create Concept (v1.0_Flux1D) [WORK IN PROGRESS],"https://preview.redd.it/7qhx0kse60bf1.png?width=864&format=png&auto=webp&s=ebd011161d210204025196ebd1ddb237fb40a0c5

https://preview.redd.it/ns8g51ii60bf1.png?width=896&format=png&auto=webp&s=313dfbf00c4c1bbc4f86699b10fca17317e6d845

https://preview.redd.it/gcz9g0ii60bf1.png?width=896&format=png&auto=webp&s=ff613f8948221f8803feb45c55d2a442942987a8

https://preview.redd.it/bx7kzyhi60bf1.png?width=896&format=png&auto=webp&s=d08e9fc40d57ff62ebf8087cc2d40360372fb565

https://preview.redd.it/lrkqyzji60bf1.png?width=896&format=png&auto=webp&s=ad6e8330ba9d89873751e501588c7bc2439711fa

https://preview.redd.it/c64b80ii60bf1.png?width=896&format=png&auto=webp&s=b76872df66a517075cebfc1e8351549ab6a794b9

https://preview.redd.it/25hdh2ii60bf1.png?width=896&format=png&auto=webp&s=9702e54c21ff1a682592a3e1a43d4225a5dbb6c0

https://preview.redd.it/el8cezhi60bf1.png?width=896&format=png&auto=webp&s=3404c207a4ca82cda69fd30bfab461f24b304dee

https://preview.redd.it/jihbzzhi60bf1.png?width=896&format=png&auto=webp&s=d2e7f577c7e761904072462dedfea2302fb42a81

https://preview.redd.it/rux6ayhi60bf1.png?width=864&format=png&auto=webp&s=1a141267be77594eedfeb78bd2e68d7d273ef8a5

https://preview.redd.it/4c7ssuhi60bf1.png?width=864&format=png&auto=webp&s=116be8f45058e5b66467ce3a7ac2cf337d11c835

[https://civitai.com/models/1324671](https://civitai.com/models/1324671)
",[],[],1,0,0.57,Resource - Update,1751698550.0
1ls3r1g,StableDiffusion,Need help,"I have installed wan 2.1 but there is no option for text to image to image to video, it is missing. what to do. please guide me.

https://preview.redd.it/uzwh1a0e70bf1.png?width=680&format=png&auto=webp&s=0f9107875899c2c30a93b660a49769fbf6c9a35d




thanks","['You are inside Pinokio interface, start the program what uses Wan2.1 models: [https://www.youtube.com/watch?v=Ls8QOgkSm4w&t=4m20s](https://www.youtube.com/watch?v=Ls8QOgkSm4w&t=4m20s)']","['You are inside Pinokio interface, start the program what uses Wan2.1 models: [https://www.youtube.com/watch?v=Ls8QOgkSm4w&t=4m20s](https://www.youtube.com/watch?v=Ls8QOgkSm4w&t=4m20s)']",0,1,0.4,Question - Help,1751698545.0
1ls26js,StableDiffusion,How to find the original links of downloaded LORAs?,"I downloaded dozens of LORAs and I have no idea what most of them do, I forgot to save their CivitAI link and now I can't see a preview of what their generations look like.

Is it possible to find the civitai link I originally downloaded them from?","[""I made a couple of related scripts some time ago:\n\nhttps://github.com/RupertAvery/civitai-scripts\n\n`download_your_model_metadata.py` will download the LORAs metadata as a .JSON file based on it's checksum.\n\nIt basically:\n\n* computes the LORAs checksum\n* uses the Civitai API to search for the model based on the checksum\n* downloads the metadata\n\nThe metadata should contain the url of the model and the trigger words and the author notes.\n\nFrom the README\n\n## Prerequisites\n\n* Python 3.10+\n* requests package\n\n```\npip install requests\n```\n\n## Download Metadata for your models\n\n`download_your_model_metadata.py` downloads metadata based on your models. \n\nJust point the script at the folder or network folder where your models are.\n\nThis computes the SHA-256 of your model and uses it to lookup the model in Civitai. Useful when you don't know the actual name of the model.\n\nThe data downloaded is the raw api response in json format. It will be downloaded next to your model with the same filename and .json extension.\n\nWorks with LORAs and Checkpoints. .pt should work too if you change the extension in the script.\n\nIf you are already using Stability Matrix or something similar to download metadata from Civitai, then this isn't really useful.\n\nThe purpose of this is to quickly grab the metadata for all your LORAs, which includes any trigger words.\n\nUsage:\n\n```\npython download_your_model_metadata.py <path to safetensors>\n```"", 'They are extensions to save that kind of info for auto1111 or forge.\n\nI use this one : [https://github.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper](https://github.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper)', 'I open sourced it few hours ago.... it can fetch metadata from civitai, download\n\n[https://github.com/rajeevbarde/civit-lora-download](https://github.com/rajeevbarde/civit-lora-download)\n\nhttps://preview.redd.it/nl63gckwtzaf1.png?width=2460&format=png&auto=webp&s=e4954e810bd36b6707f0505bc8ecaaa4f9bbfff3', ""Which of the following is the easiest way to find out what trigger words a given, already downloaded LORA uses? The best would be a very short workflow, or an easy-to-use table, because I don't want to add nodes one by one to my countless - for me complicated - existing workflows. What do you recommend?"", 'If you downloaded from civitai while logged in you can click on your profile > download history.', 'Seems like all you want is the trigger words. Just make it good practice to grab a screenshot of the CivitAi page, along with saving an encapsulated .MHTML of the same page. Give both the same name as the LoRA. Store both alongside the downloaded LoRA file.', 'I use this:\n\n[https://github.com/Xypher7/lora-metadata-viewer](https://github.com/Xypher7/lora-metadata-viewer)\n\n* it can sometimes correctly guess the trigger words ...\n* can list what words were used in the training images, thus make prompt suggestions ...\n* can sometimes find again where on Civitai the LoRA was originally posted ...\n\nOnline demo, fully functional:\n\n[https://xypher7.github.io/lora-metadata-viewer](https://xypher7.github.io/lora-metadata-viewer)', 'You can import them into Stability Matrix, it will find the Civitai link via file hash and import the metadata into local storage.', 'Surely it is pretty obvious from the file name most of the time?', ""You only need this node\n\n[https://github.com/willmiao/ComfyUI-Lora-Manager](https://github.com/willmiao/ComfyUI-Lora-Manager)\n\nyou will have a new button in the top that will launch a lora manager interface + explorer\n\nyou will have to add the lora loader ( lora manager ) node and connect it to your workflow  and you can send any  Lora to that node from that interface. You don't need to copy trigger words anymore,\n\nit will sync with civitai and show the example images\n\nit is amazing node""]","[""I made a couple of related scripts some time ago:\n\nhttps://github.com/RupertAvery/civitai-scripts\n\n`download_your_model_metadata.py` will download the LORAs metadata as a .JSON file based on it's checksum.\n\nIt basically:\n\n* computes the LORAs checksum\n* uses the Civitai API to search for the model based on the checksum\n* downloads the metadata\n\nThe metadata should contain the url of the model and the trigger words and the author notes.\n\nFrom the README\n\n## Prerequisites\n\n* Python 3.10+\n* requests package\n\n```\npip install requests\n```\n\n## Download Metadata for your models\n\n`download_your_model_metadata.py` downloads metadata based on your models. \n\nJust point the script at the folder or network folder where your models are.\n\nThis computes the SHA-256 of your model and uses it to lookup the model in Civitai. Useful when you don't know the actual name of the model.\n\nThe data downloaded is the raw api response in json format. It will be downloaded next to your model with the same filename and .json extension.\n\nWorks with LORAs and Checkpoints. .pt should work too if you change the extension in the script.\n\nIf you are already using Stability Matrix or something similar to download metadata from Civitai, then this isn't really useful.\n\nThe purpose of this is to quickly grab the metadata for all your LORAs, which includes any trigger words.\n\nUsage:\n\n```\npython download_your_model_metadata.py <path to safetensors>\n```"", 'They are extensions to save that kind of info for auto1111 or forge.\n\nI use this one : [https://github.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper](https://github.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper)', 'I open sourced it few hours ago.... it can fetch metadata from civitai, download\n\n[https://github.com/rajeevbarde/civit-lora-download](https://github.com/rajeevbarde/civit-lora-download)\n\nhttps://preview.redd.it/nl63gckwtzaf1.png?width=2460&format=png&auto=webp&s=e4954e810bd36b6707f0505bc8ecaaa4f9bbfff3', ""Which of the following is the easiest way to find out what trigger words a given, already downloaded LORA uses? The best would be a very short workflow, or an easy-to-use table, because I don't want to add nodes one by one to my countless - for me complicated - existing workflows. What do you recommend?"", 'If you downloaded from civitai while logged in you can click on your profile > download history.']",4,14,0.64,Question - Help,1751692418.0
1lrzgf5,StableDiffusion,Does how powerful my GPU affect the quality of the final image output or does it only affect how fast I get my output?,,"[""The only way in which it affects quality is if it limits the model you can run. So, for example, with Flux Dev, the full fp 16 model is 20+ Gigabytes. You may not be able to run that on your GPU (basically it only runs comfortably on a GPU with 24 or 32 Gigabytes of VRAM, eg a 3090, 4090, or 50900\n\nBut on cards with less memory many folks will run the fp 8 model, which is usually 11 Gigabytes or so.\n\nThere's some quality loss there vs FP 16. Not a lot, you might not even notice, but some and more noticeable in some scenarios than other. It show up more in image 2 image, for example.\n\nSimilarly, if you look at the text encoder, you might choose the T5XXL FP8 vs the T5XXL FP 16 to save on VRAM, personally I find this effect is greater than with the checkpoint.\n\nSo no - a better GPU doesn't make better images, IFF you can run the same models. But sometimes the difference between a 4090 -- where you can easily run the FP 16 model and encoder in the 24 GB of VRAM, vs a 4060 where the 8 GB of VRAM mean you're going to have to choose less capable models . . . that will mean lower quality."", 'It should only affect the speed as long as everything else software-wise is the same.', 'We have a few variations based on internal architectures but your GPU will one affect speed.', ""Not in theory, but definitely in practice.  \n\n/u/amp1212 robustly answered the most common way: VRAM optimizations sacrificing quality.  But you can also lose out by simply not having access to the same hardware features.  A good but trivial example is the way some tools once used gpu rng when available even though it didn't have any particular advantage over using standardized algorithms on the CPU.  You'd get differences because of your hardware even though it was not a capability issue and they would usually look worse because most images put on display are selected for quality."", 'You can do it in your head with pen and paper and quality will be the same']","[""The only way in which it affects quality is if it limits the model you can run. So, for example, with Flux Dev, the full fp 16 model is 20+ Gigabytes. You may not be able to run that on your GPU (basically it only runs comfortably on a GPU with 24 or 32 Gigabytes of VRAM, eg a 3090, 4090, or 50900\n\nBut on cards with less memory many folks will run the fp 8 model, which is usually 11 Gigabytes or so.\n\nThere's some quality loss there vs FP 16. Not a lot, you might not even notice, but some and more noticeable in some scenarios than other. It show up more in image 2 image, for example.\n\nSimilarly, if you look at the text encoder, you might choose the T5XXL FP8 vs the T5XXL FP 16 to save on VRAM, personally I find this effect is greater than with the checkpoint.\n\nSo no - a better GPU doesn't make better images, IFF you can run the same models. But sometimes the difference between a 4090 -- where you can easily run the FP 16 model and encoder in the 24 GB of VRAM, vs a 4060 where the 8 GB of VRAM mean you're going to have to choose less capable models . . . that will mean lower quality."", 'It should only affect the speed as long as everything else software-wise is the same.', 'We have a few variations based on internal architectures but your GPU will one affect speed.', ""Not in theory, but definitely in practice.  \n\n/u/amp1212 robustly answered the most common way: VRAM optimizations sacrificing quality.  But you can also lose out by simply not having access to the same hardware features.  A good but trivial example is the way some tools once used gpu rng when available even though it didn't have any particular advantage over using standardized algorithms on the CPU.  You'd get differences because of your hardware even though it was not a capability issue and they would usually look worse because most images put on display are selected for quality."", 'You can do it in your head with pen and paper and quality will be the same']",0,7,0.38,Question - Help,1751682374.0
1lryd72,StableDiffusion,Morbid Art Styles,What's your thoughts and opinions on this? ,"[""It's ok."", 'Glad it’s not all boobs', 'Love these!!', 'Just wanted to say I love this style. Carry on.', 'You call that morbid???', 'Love it, I definitely prefer the ones that less fully red, the darker ones with just a hint of color are dope', '12 is 🔥', 'Feels like Mignola', 'kind of reminds me of the cursed lora i found, https://imgur.com/a/DNdskl1 (i call it cursed because when i used the trigger word,it spat out the images in the gallery, vs the girl it was supposed to generate.)', 'Reminds me of the old Samurai Jack on cartoon network']","[""It's ok."", 'Glad it’s not all boobs', 'Love these!!', 'You call that morbid???', 'Love it, I definitely prefer the ones that less fully red, the darker ones with just a hint of color are dope']",137,13,0.84,Discussion,1751678480.0
1lry8jn,StableDiffusion,CREEPY JOINTS,"JOINTS.

Using a workflow that was orginally made to loop videos, by simply adding a second different video input, you can join two similar clips seemlessly. The method creates and blends in a new 5 second join between clips that follows the motion and context almost perfectly. Here are 4 joined clips.


Wan 2.1, MMAudio, Stable Audio, Kontext



The original worflow for looping is here: [https://www.reddit.com/r/StableDiffusion/comments/1ktljys/loop\_anything\_with\_wan21\_vace/](https://www.reddit.com/r/StableDiffusion/comments/1ktljys/loop_anything_with_wan21_vace/)","['![gif](giphy|7krK2aL5IEUTK)', 'Interesting. How did you manage to reduce the color issues between clips? Post? If not, may you elaborate a little bit more your steps, please? Thanks, and nice video!']","['![gif](giphy|7krK2aL5IEUTK)', 'Interesting. How did you manage to reduce the color issues between clips? Post? If not, may you elaborate a little bit more your steps, please? Thanks, and nice video!']",14,7,0.77,Animation - Video,1751678032.0
1lrxzzv,StableDiffusion,YuzuUI: A New Frontend for SD WebUI,"I was frustrated with the UX of SD WebUI, so I built a separate frontend UI app: [https://github.com/crstp/sd-yuzu-ui](https://github.com/crstp/sd-yuzu-ui)

https://preview.redd.it/nzje645qcyaf1.jpg?width=1640&format=pjpg&auto=webp&s=13124ed74bbd16305b4d32485a3d570430785fa2



Features:

* Saves tab states and restores generated images after restarting the app
* Applies batch setting changes across multiple tabs (e.g., replace prompts across tabs)
* Significantly reduces memory usage, even with many open tabs
* Offers more advanced autocompletion

It's focused on txt2img. The UI looks pretty much like the original WebUI, but the extra features make it way easier to work with lots of prompts.

If you often generate lots of txt2img images across multiple tabs in WebUI, this might be useful for you.","['No inpaint?', 'What is on the backend?']","['No inpaint?', 'What is on the backend?']",38,7,0.85,News,1751677176.0
1lrxzc8,StableDiffusion,Lora x controlnet,"Does anyone know the best way to use a lora in a controlnet, I mean copying the photo and just changing the face. Sorry for the translation errors, I'm 🇧🇷",[],[],0,0,0.25,Question - Help,1751677110.0
1lrww1r,StableDiffusion,Is there anything out there to make the skin look more realistic?,,"['I recommend posting what are using right now so people can recommend you more of a ""tweaks approach"" rather than total redo. \n\nIt\'s Flux, right? What CFG? Sampler? Scheduler? Any LORAs? Prompt? Seed?', 'https://imgur.com/a/RasoAep\n\nThis good enough skin for you?\n\nCreated using my FLUX LoRa + my recommended workflow (linked in the model description), using a ChatGPT prompt of your image.\n\nLink: https://civitai.com/models/970862/amateur-snapshot-photo-style-lora-flux', 'SDXL or Chroma with low denoise', 'https://preview.redd.it/lcizbrwo8yaf1.png?width=2048&format=png&auto=webp&s=a8127f431ee951e56430d753537b9a12e38a866d\n\nany difference ?', 'Some asians have skin like this', 'A trick that I use is to plug KSampler Advanced with the exact same seed and run it on the output from a regular KSampler, repeating the last few steps. Works on pretty much everything and amplifies details quite a lot', 'I have an upscale that solves this', 'Porcelain like texture....smooth, prompt matters', 'I don\'t use Flux but, for trying to reproduce the feeling of real pictures, I prefer low resolution images to HD. Usually, something not higher than 1440p. Higher than that, skin starts looking ""fake"" and characters less ""realistic"".\n\nFor upscaling, I only use two things: HiResFix for txt2img, and ADetailer for img2img. (ADetailer can be use for upscaling if you set it right. Not sure if this is common knowledge or not, it\'s something I discovered by accident.)\n\nI usually prefer images that are taken with low environmental lighting, or with strong flash light over a dark background.', 'Use “multiply sigmas” node set to 0.99 rather than 1. Or use detail daemon. Or both.']","['I recommend posting what are using right now so people can recommend you more of a ""tweaks approach"" rather than total redo. \n\nIt\'s Flux, right? What CFG? Sampler? Scheduler? Any LORAs? Prompt? Seed?', 'https://imgur.com/a/RasoAep\n\nThis good enough skin for you?\n\nCreated using my FLUX LoRa + my recommended workflow (linked in the model description), using a ChatGPT prompt of your image.\n\nLink: https://civitai.com/models/970862/amateur-snapshot-photo-style-lora-flux', 'SDXL or Chroma with low denoise', 'https://preview.redd.it/lcizbrwo8yaf1.png?width=2048&format=png&auto=webp&s=a8127f431ee951e56430d753537b9a12e38a866d\n\nany difference ?', 'Some asians have skin like this']",89,49,0.83,Question - Help,1751673498.0
1lrw55p,StableDiffusion,Flux Kontext Image to Tattoo,"Flux Kontext keeps on amazing me. Using the below Tattoo Flux dev Lora you can transfer any image to a tattoo style on human skin as shown in the examples.

https://civitai.com/models/867846/tattoo-flux-lora",[],[],17,0,0.79,Tutorial - Guide,1751671166.0
1lrv41n,StableDiffusion,Flux kontext dev tested prompts,"For some time now there is a Flux kontext dev model among the users, can you share here the best prompts that do exactly the image editing (inpaint) they should ? There are many pages and videos with basic examples, but nowhere are all the capabilities of this model described. Which working prompts have surprised you with their ability ? Write their wording and share with others the possibilities of this great editing model. Thank you. I'm happy to learn new possibilities. I apologize for my English :)","['https://docs.bfl.ai/guides/prompting_guide_kontext_i2i\n\nThis is the official prompting guide', 'remove watermark\n\nmaintain scale, proportions\n\nupscale this image, make it crisp, add details. Maintain style, background an lighting']","['https://docs.bfl.ai/guides/prompting_guide_kontext_i2i\n\nThis is the official prompting guide', 'remove watermark\n\nmaintain scale, proportions\n\nupscale this image, make it crisp, add details. Maintain style, background an lighting']",1,4,0.57,Discussion,1751668070.0
1lrudoy,StableDiffusion,Remastered FF.,,"['looks like a bad photoshop for a comedy podcast thumbnail. But I like anything retro.', 'So lazy.they look nothing like in game\nDo more effort on the face.its like lazy img2img']","['looks like a bad photoshop for a comedy podcast thumbnail. But I like anything retro.', 'So lazy.they look nothing like in game\nDo more effort on the face.its like lazy img2img']",0,2,0.36,Comparison,1751665956.0
1lru62m,StableDiffusion,"""Forgotten Models"" Series: PixArt Sigma + SD 3.5 L Turbo as Refiner.",,"['This is a cool series, thanks.', ""I see why they're forgotten."", 'Prompts: [https://civitai.com/posts/19150556](https://civitai.com/posts/19150556)', 'These look nice and are very interesting, but ultimately more of a tool for inspiration than actual targeted use.\n\nBecause should you want to recreate most of the images purposefully, you wouldnt be able to. At least not without the exact settings and prompt used.\n\nThese are like results from a slot machine. At some point its going to be right and spit out something pretty, but if you have a concrete goal and vision in mind its very hard to create that.\n\nImage one:\n\nThere are essentially just two tags with direct information on what should be created.\n\nI will maybe give it ""biomechanical alien"", though those lips and the chin are too human like to be alien. But ""eye contact"" is completely missing.\n\nSo you either confused the model with the flood of ""quality"" tags or it has shit prompt adherence that it didnt do the two things it was told to do.\n\nImage two:\n\nIt is a blocky spaceship, but it has neither round nor red thrusters engines. I would also debate the support spacecraft flying around it with red streaks but maybe its some absurdly large spaceship and the support crafts are just tiny and thus just streaks.\n\nImage three:\n\n""long claws and six arms"" No claws and neither does it have six appendages in total even. ""stripe texture of body"" no stripes at all. ""on the background is a wall full of bodies and corpses and carcass of dead aliens"" the walls are filled with pipes not corpses. Did the ""illuminating space marine"" get eaten?\n\n\\-------------\n\nThis might be a useful tool to create hundreds of inspiration images where you can get ideas from, but if the model doesnt actually show what you want it to show, its not that useful. No matter how pretty it is.', ""Those are the ai-est looking ai pics I've ever seen"", 'ngl, those look amazing. Would it be possible to share a comfy workflow? I would appreciate it!']","['This is a cool series, thanks.', ""I see why they're forgotten."", 'Prompts: [https://civitai.com/posts/19150556](https://civitai.com/posts/19150556)', 'These look nice and are very interesting, but ultimately more of a tool for inspiration than actual targeted use.\n\nBecause should you want to recreate most of the images purposefully, you wouldnt be able to. At least not without the exact settings and prompt used.\n\nThese are like results from a slot machine. At some point its going to be right and spit out something pretty, but if you have a concrete goal and vision in mind its very hard to create that.\n\nImage one:\n\nThere are essentially just two tags with direct information on what should be created.\n\nI will maybe give it ""biomechanical alien"", though those lips and the chin are too human like to be alien. But ""eye contact"" is completely missing.\n\nSo you either confused the model with the flood of ""quality"" tags or it has shit prompt adherence that it didnt do the two things it was told to do.\n\nImage two:\n\nIt is a blocky spaceship, but it has neither round nor red thrusters engines. I would also debate the support spacecraft flying around it with red streaks but maybe its some absurdly large spaceship and the support crafts are just tiny and thus just streaks.\n\nImage three:\n\n""long claws and six arms"" No claws and neither does it have six appendages in total even. ""stripe texture of body"" no stripes at all. ""on the background is a wall full of bodies and corpses and carcass of dead aliens"" the walls are filled with pipes not corpses. Did the ""illuminating space marine"" get eaten?\n\n\\-------------\n\nThis might be a useful tool to create hundreds of inspiration images where you can get ideas from, but if the model doesnt actually show what you want it to show, its not that useful. No matter how pretty it is.', ""Those are the ai-est looking ai pics I've ever seen""]",52,12,0.86,Workflow Included,1751665358.0
1lru0mu,StableDiffusion,Use Kontext on Apple Silicon (MFLUX updated),,[],[],5,0,0.78,Resource - Update,1751664921.0
1lrtnxk,StableDiffusion,VLM multi-turn captioning app,,"['Github: https://github.com/victorchall/vlm-caption\n\nTLDR app: use multi-turn prompts with a vision model to create captions, with some extra bells and whistles.  \n\nTLDR install: Install LM Studio to self host the VLM model of your choice, edit a caption.yaml file to setup various things, then double click the exe to run.\n\nLatest release download:\n\nhttps://github.com/victorchall/vlm-caption/actions/runs/16079650854/artifacts/3467782671', ""So how's this differ from\xa0https://github.com/jhc13/taggui\n\n\nThe VLMs produce better caption outputs?"", 'Does this support joycaption gguf?']","['Github: https://github.com/victorchall/vlm-caption\n\nTLDR app: use multi-turn prompts with a vision model to create captions, with some extra bells and whistles.  \n\nTLDR install: Install LM Studio to self host the VLM model of your choice, edit a caption.yaml file to setup various things, then double click the exe to run.\n\nLatest release download:\n\nhttps://github.com/victorchall/vlm-caption/actions/runs/16079650854/artifacts/3467782671', ""So how's this differ from\xa0https://github.com/jhc13/taggui\n\n\nThe VLMs produce better caption outputs?"", 'Does this support joycaption gguf?']",3,8,0.71,Resource - Update,1751663921.0
1lrt2si,StableDiffusion,Can flux Kontext fix Body horror?,"I haven't gotten around to make space for flux Kontext to try it, but maybe someone already did this: when using too many or incompatible Loras for flux, bodies start to get mixed with the environment in truly horrible ways. Sometimes the arm of one person seamlessly extends into the leg of another person and stuff like that. Can you use Kontext to untangle that mess to still safe the image by saying something like ""fix the body autonomy"" or similar?","['I doubt it.  If you cannot fix it via img2img with some high denoise with Flux-Dev, then Flux-Kontext probably won\'t do it either.\n\nFlux-Kontext is mainly an image editor, used to replace say the background, style or clothing.  I somehow doubt that any kind of ""fix the body anatomy"" training went into it.  I suppose somebody can make a LoRA for it, but base Kontext most likely won\'t have it.\n\nBut you can try, I could be wrong.  Try to modify the image by say changing to a different background (from indoor to outdoor, for example) and see what happens.\n\nBTW, it does not do NSFW 😁.\n\nIf you can post a SFW example here, I can try it for you, or you can try it online for free yourself at tensor. art', 'Generally not. Its not impossible, but it generally helps to start with a hand edit in a photoeditor to simplify your Thing. Basically, twisted nightmare fuel abominations pose challenges for genAI tools in figuring out ""what is this supposed to be"". Its much easier to fill in a missing limb than it is to sort out just which of six limbs are supposed to be the right ones.\n\nAs a practical matter, if you find you\'re generating abominations, and you\'d like something a little less Island of Dr Moreau, you\'d be better off regenerating with a better checkpoint, lora and prompt', 'Not really as of right now. HOWEVER! Kontext LoRA training allows for input-image to output-image training, meaning theoretically you can gather a dataset of your malformed generations that need fixing, use this a your Start images and manually fix your dataset with inpainting. Use the fixed images as end images, use a prompt like „fix the subjects hands“ and train. The lora should then be able to achieve this.\n\nEdit: I genuinely think a LoRA like this could be a really useful tool for the community, so if anyone wants to make it happen hmu']","['I doubt it.  If you cannot fix it via img2img with some high denoise with Flux-Dev, then Flux-Kontext probably won\'t do it either.\n\nFlux-Kontext is mainly an image editor, used to replace say the background, style or clothing.  I somehow doubt that any kind of ""fix the body anatomy"" training went into it.  I suppose somebody can make a LoRA for it, but base Kontext most likely won\'t have it.\n\nBut you can try, I could be wrong.  Try to modify the image by say changing to a different background (from indoor to outdoor, for example) and see what happens.\n\nBTW, it does not do NSFW 😁.\n\nIf you can post a SFW example here, I can try it for you, or you can try it online for free yourself at tensor. art', 'Generally not. Its not impossible, but it generally helps to start with a hand edit in a photoeditor to simplify your Thing. Basically, twisted nightmare fuel abominations pose challenges for genAI tools in figuring out ""what is this supposed to be"". Its much easier to fill in a missing limb than it is to sort out just which of six limbs are supposed to be the right ones.\n\nAs a practical matter, if you find you\'re generating abominations, and you\'d like something a little less Island of Dr Moreau, you\'d be better off regenerating with a better checkpoint, lora and prompt', 'Not really as of right now. HOWEVER! Kontext LoRA training allows for input-image to output-image training, meaning theoretically you can gather a dataset of your malformed generations that need fixing, use this a your Start images and manually fix your dataset with inpainting. Use the fixed images as end images, use a prompt like „fix the subjects hands“ and train. The lora should then be able to achieve this.\n\nEdit: I genuinely think a LoRA like this could be a really useful tool for the community, so if anyone wants to make it happen hmu']",1,6,0.56,Question - Help,1751662298.0
1lrrrdp,StableDiffusion,Ovis-U1-3B small yet capable all to all free model,"1 input
Prompt :Make the same place Abandent deserted ruined old destroyed , realistic photo.
2 result

3 input
Prompt:Use white marble pillars to hold pergulla
4 result","['Model download\n\n\nhttps://huggingface.co/AIDC-AI/Ovis-U1-3B\n\n\nThe project is released under Apache License 2.0\n\n\nI am not the owner of it', 'Try it here\n\nhttps://huggingface.co/spaces/AIDC-AI/Ovis-U1-3B', '>Building on the foundation of the Ovis series, Ovis-U1 is a 3-billion-parameter unified model that seamlessly integrates\xa0**multimodal understanding**,\xa0**text-to-image generation**, and\xa0**image editing**\xa0within a single powerful framework.\n\nhttps://preview.redd.it/uww4kg5g6zaf1.png?width=2782&format=png&auto=webp&s=bc7735f0b4500b9e958de038c8ab66357fe6feb3\n\nThis could be interesting. I am definitely looking forward playing with it when it gets integrated into ComfyUI.', 'Hay Mr Kijai  save us', ""I wish it didn't use the SDXL VAE..."", 'Can it be used in Comfyui?', '[deleted]']","['Model download\n\n\nhttps://huggingface.co/AIDC-AI/Ovis-U1-3B\n\n\nThe project is released under Apache License 2.0\n\n\nI am not the owner of it', 'Try it here\n\nhttps://huggingface.co/spaces/AIDC-AI/Ovis-U1-3B', '>Building on the foundation of the Ovis series, Ovis-U1 is a 3-billion-parameter unified model that seamlessly integrates\xa0**multimodal understanding**,\xa0**text-to-image generation**, and\xa0**image editing**\xa0within a single powerful framework.\n\nhttps://preview.redd.it/uww4kg5g6zaf1.png?width=2782&format=png&auto=webp&s=bc7735f0b4500b9e958de038c8ab66357fe6feb3\n\nThis could be interesting. I am definitely looking forward playing with it when it gets integrated into ComfyUI.', 'Hay Mr Kijai  save us', ""I wish it didn't use the SDXL VAE...""]",86,16,0.91,Resource - Update,1751658696.0
1lrrdyi,StableDiffusion,Can we take a moment to appreciate how insane Flux Kontext dev is?,"Just wanted to drop some thoughts because I’ve been seeing some people throwing shade at Flux Kontext dev and honestly… I don’t get it.

I’ve been messing with AI models and image gen since late 2022. Back then, everything already felt like magic, but it was kinda hard to actually gen/edit images the way I wanted. You’d spend a lot of time inpainting, doing weird workarounds, or just Photoshopping it by hand.

And now… we can literally prompt edits. Like, “oh, I want to change this part” and boom, the model can just do it (most of the time lol). Sure, sometimes you still need to do some manual touch-ups, upscaling, or extra passes, but man, the fact we can even *do this* locally on our PCs is just surreal to me.

I get that nothing’s perfect, but some posts I see like “ehh, Kontext dev kinda sucks” really make me stop and go: bro… this is crazy tech. We’re living in a timeline where this stuff is just available to us.

Anyway, I’m super grateful for the devs behind Flux Kontext. It’s an incredible tool and it’s made image gen and editing even more fun!","[""Kontext can do impressive things - but fails impressively on other tasks where you'd think it should be easy doable.\n\nE.g. watermark removal is surprising good (except that the whole image got soft).\n\nAnd face swap isn't working.\n\nI'm pretty sure that LoRAs will appear to fix these things."", ""It works fine for low res images but when you try a higher res image it just  lowers it so for me it's pointless,flux dev fill works better"", ""Sorry mate, it can't even change a face with another random face. Not really impressed with the current state of things, inpaint and control nets are still king. Sure Kontext is a convenient package but if it works 20% of the time, what's the point?"", 'To me this all feels like the 90s and the computer and internet world.  Everything was moving fast and was surprisingly hard to get things working a lot of the time and there was all sorts of space and energy for hobbyist creators to make sites and tools.  So much overlap of competing effort and so many things that didn\'t quite play well together and so it often felt like a maze.\n\nI remember scouring through magazines, computer stores, and the wholly inadequate online sites to find things like new tools I could plug into Visual Basic for building end user interfaces.  Things like calendar tools or text display controls with extra functionality than the base ones.\n\nWith AI images and video it has a lot of the same feel but now we have all sorts of social media/collaborative tools to get info, like here on Reddit or Huggingface.  However, that just means that the info and option availiblity is 100x the size and the learning curve for newer people like myself can be pretty steep.  When I open some workflows in Comfy my jaw drops, and the vaunted ""don\'t worry it will download the nodes for you!"" almost never works because things get out-of-date or moved so fast.\n\nFun times.  Enjoy this kind of exciting, creative energy because in time it will become much better...but also much more commercial and corporate and with locked down access and censorship.', 'How can i appreciate anything censored???', 'I tried it for a bit and it was neat.  Mostly it seemed to basically just replace the background and keep the subject the same but lower resolution - OR - it would change the subject to not the subject. I’m sure I’m using it wrong but I don’t get the hype.', 'So far im dissapointed. I just get lots of blur with higher quality images. The stitching and combining feels like a gambling game and and sometimes it just does nothing I guess when you trigger its filter', ""I agree. It's so powerful. I think some people are giving up on it too soon. Sometimes you can simply rerun with different seed and get exactly what you were looking for."", 'https://preview.redd.it/7pt5adkvxwaf1.jpeg?width=1200&format=pjpg&auto=webp&s=7ddc57d6aa319aba066fdf939efced0a4a038956\n\nLooks like a million years ago.', 'Sometimes, it is a matter of expectations.\n\nSome people read the original announcements and posts (which are based on Kontext Max/Pro) and got all excited about it.  When Kontext-Dev does not live up to their expectations, they quickly soured on it.\n\nKontext-Dev is finicky and does not always work, I admit that.  But I also see a lot of potential, specially the ability to train custom editing LoRAs for it.  My main problem is my own lack of imagination/creativity to come up with such idea.  Some of the most obvious and useful ideas such as watermark removal are built in.  NSFW ideas such as cloth removal is obvious and is in fact quite trivial to do (i.e., generating the training set is very easy).\n\nBut at least I will be training some artistic LoRAs for it, once tensor support Kontext training.']","[""Kontext can do impressive things - but fails impressively on other tasks where you'd think it should be easy doable.\n\nE.g. watermark removal is surprising good (except that the whole image got soft).\n\nAnd face swap isn't working.\n\nI'm pretty sure that LoRAs will appear to fix these things."", ""It works fine for low res images but when you try a higher res image it just  lowers it so for me it's pointless,flux dev fill works better"", ""Sorry mate, it can't even change a face with another random face. Not really impressed with the current state of things, inpaint and control nets are still king. Sure Kontext is a convenient package but if it works 20% of the time, what's the point?"", 'How can i appreciate anything censored???', 'To me this all feels like the 90s and the computer and internet world.  Everything was moving fast and was surprisingly hard to get things working a lot of the time and there was all sorts of space and energy for hobbyist creators to make sites and tools.  So much overlap of competing effort and so many things that didn\'t quite play well together and so it often felt like a maze.\n\nI remember scouring through magazines, computer stores, and the wholly inadequate online sites to find things like new tools I could plug into Visual Basic for building end user interfaces.  Things like calendar tools or text display controls with extra functionality than the base ones.\n\nWith AI images and video it has a lot of the same feel but now we have all sorts of social media/collaborative tools to get info, like here on Reddit or Huggingface.  However, that just means that the info and option availiblity is 100x the size and the learning curve for newer people like myself can be pretty steep.  When I open some workflows in Comfy my jaw drops, and the vaunted ""don\'t worry it will download the nodes for you!"" almost never works because things get out-of-date or moved so fast.\n\nFun times.  Enjoy this kind of exciting, creative energy because in time it will become much better...but also much more commercial and corporate and with locked down access and censorship.']",221,91,0.85,Discussion,1751657696.0
1lrqa6g,StableDiffusion,How come openpose always generates black images for me?,,"[""I'm trying to recreate this pose from https://www.pixiv.net/en/artworks/122806305 but I can't get controlnet to generate the pose"", 'Have you tried with AIO Aux preprocessor node instead? You have multiple controlnet models there, including openpose', ""Sometimes that's just how it is. It doesn't detect it. Perhaps it considers it all a background or something.""]","[""I'm trying to recreate this pose from https://www.pixiv.net/en/artworks/122806305 but I can't get controlnet to generate the pose"", 'Have you tried with AIO Aux preprocessor node instead? You have multiple controlnet models there, including openpose', ""Sometimes that's just how it is. It doesn't detect it. Perhaps it considers it all a background or something.""]",0,10,0.5,Question - Help,1751654810.0
1lrplle,StableDiffusion,How Can I Swap My Face Into an AI Image Accurately?,"I generated an AI image and I’d like to replace the face in it with my own, while keeping my facial features accurate and realistic. Is there an easy way to do this? I’ve tried using GPT’s image generation, but it struggles to get my face right and keep everything consistent","[""The best way is to create or pay someone to create a LORA of you from 20 or so photos of you. You can then inpaint with that LORA to replace the character's head in the AI image with your own and it will produce accurate and realistic results from all angles.\n\nIf you don't have a powerful PC to do it yourself you can use online services like CivitAI to have the LORA made and to do the face swaps with it on there.\n\nA less accurate option is to use faceswappers (either online or locally like FaceFusion) but this won't get the headshape and hairline etc right for you, which is important in getting a likeness accurate."", 'Train a LoRA with 15 - 20 images: [https://youtu.be/-L9tP7\\_9ejI?si=4A9EabhLvHVnPX9T](https://youtu.be/-L9tP7_9ejI?si=4A9EabhLvHVnPX9T)', 'https://github.com/Gourieff/ComfyUI-ReActor', 'Dm me I can help you']","[""The best way is to create or pay someone to create a LORA of you from 20 or so photos of you. You can then inpaint with that LORA to replace the character's head in the AI image with your own and it will produce accurate and realistic results from all angles.\n\nIf you don't have a powerful PC to do it yourself you can use online services like CivitAI to have the LORA made and to do the face swaps with it on there.\n\nA less accurate option is to use faceswappers (either online or locally like FaceFusion) but this won't get the headshape and hairline etc right for you, which is important in getting a likeness accurate."", 'Train a LoRA with 15 - 20 images: [https://youtu.be/-L9tP7\\_9ejI?si=4A9EabhLvHVnPX9T](https://youtu.be/-L9tP7_9ejI?si=4A9EabhLvHVnPX9T)', 'https://github.com/Gourieff/ComfyUI-ReActor', 'Dm me I can help you']",0,5,0.42,Question - Help,1751653045.0
1lrowce,StableDiffusion,My first try at making an autoregressive colorizer model,"Hi everyone,
This is my first try ever on making an autoregressive (sort of) AI model that can colorize any 2D lineart image.

For now, it has only trained for a small amount of time and only works on \~4 specific images I have. Maybe when I have time and money, I'll try to expand it with a larger dataset (and see if it'll work).","['Never test your model on dataset used to train them, it means literally nothing.', 'First test with 500 image pairs, validate your results without using training images. And then you can think of scaling it.', 'Update: On new tested images, it gives me a full flat purple image.', 'Man, if that thing can colorize all the manga it will be a beast', '90s internet vibes.', 'Can you explain more? How do you do that, and what kind of model you are using?', ""i don't understand what you wrote, but i like what i see"", 'This is really cool, expand your data set and keep going!', ""I feel like a lot of the basic models like this pick up on subtleties left by the linework conversion algorithm, and completely fail when given a human drawn sketch. \n\nBut it's still a nice project, and valuable learning, just mentioning might be interesting to test like that too"", ""New Update as of 05/07/25: Now images not in dataset starts colorizing but it show too much artifacts like blurriness.\n\nI think I'm missing a lot here because the training doesn't improve that much despite feeding the model with 100 new pairs.""]","['Never test your model on dataset used to train them, it means literally nothing.', 'First test with 500 image pairs, validate your results without using training images. And then you can think of scaling it.', 'Update: On new tested images, it gives me a full flat purple image.', 'Man, if that thing can colorize all the manga it will be a beast', '90s internet vibes.']",458,31,0.96,Discussion,1751651330.0
1lros7q,StableDiffusion,Simpletuner creator is reporting N S F W loras on huggingface and they are being removed. The community needs to look elsewhere to post controversial loras,"There is a Flux Fill link to remove clothes that was on the site several months ago. And today it disappeared.

Until recently it was not common for hugginface to remove anything","['bro must be fun at parties', 'Open source fine tuning tool creator against NSFW how does that even make sense?', ""This guy has blocked access to his discord it seems.\n\nBut not only that, He edited my message on github.\n\nhttps://preview.redd.it/a13f3qhkcxaf1.png?width=606&format=png&auto=webp&s=ae60913142718ccc6182de3874672cfe5b61742c\n\nI didn't say thank you."", 'He has a long history of being that type of concern troll: https://github.com/comfyanonymous/ComfyUI/issues/949', ""I really don't understand why he is wasting his time on this. Does he work for hugginface or black forest labs?"", 'Some people have serious mental problems.', 'We need torrents for this kind of stuff, in other cases it will be removed sooner or later.', ""This is why we can't have nice things"", 'Wth is his problem?? Thankfully I got them all saved up but it sucks for other people...\n\nFor those who want it: [https://mega.nz/file/HAlkmY4C](https://mega.nz/file/HAlkmY4C)\n\ndecryption key: jRHFJriy5x\\_9aNfYELcDSFS-ezatiBNcG5jvwvtIdU0', 'Explain yourself u/terminusresearchorg\n\nEdit: they blocked me']","['bro must be fun at parties', 'Open source fine tuning tool creator against NSFW how does that even make sense?', ""This guy has blocked access to his discord it seems.\n\nBut not only that, He edited my message on github.\n\nhttps://preview.redd.it/a13f3qhkcxaf1.png?width=606&format=png&auto=webp&s=ae60913142718ccc6182de3874672cfe5b61742c\n\nI didn't say thank you."", 'He has a long history of being that type of concern troll: https://github.com/comfyanonymous/ComfyUI/issues/949', ""I really don't understand why he is wasting his time on this. Does he work for hugginface or black forest labs?""]",555,274,0.94,Discussion,1751651041.0
1lrop5e,StableDiffusion,How to use ai to add to a video?,"Hello all,

I am wondering if there is a way to use stable diffusion to add items to a video that i recorded? Specifically, i am trying to add shades rolling down in front of my house window in a couple different colours to see a general idea for what they would look like. If this is possible, where would i start? I do have a beefy gpu and have knowledge of coding if that makes a difference.

Thanks in advance for your replies!","[""Try searching for video to video or v2v workflows, I guess.  Though for your example, you can probably get away with a still image or a start/stop image and an i2v workflow.\n\n> I do have a beefy gpu\n\nYou're on the bleeding edge to do 5 secs of 720p on a 5090.""]","[""Try searching for video to video or v2v workflows, I guess.  Though for your example, you can probably get away with a still image or a start/stop image and an i2v workflow.\n\n> I do have a beefy gpu\n\nYou're on the bleeding edge to do 5 secs of 720p on a 5090.""]",0,1,0.33,Question - Help,1751650833.0
1lrobrd,StableDiffusion,What is her name?,"https://preview.redd.it/qx664pgo6waf1.png?width=768&format=png&auto=webp&s=9fea32925ba0c1e30de19563163a47986e77cde0

","['1girl, blonde, floral dress, no legs.', 'AND HER NAME IS JOOOOOHN CEEEEENAAAA !!!!!!', 'darude sandstorm\n\nseriously though, posts like these should get you culled from society', 'never goon', 'Hi, my name is, what? My name is, who?\n\nMy name is, chka-chka, Slim Shady\n\nHi, my name is, huh? My name is, what?\n\nMy name is, chka-chka, Slim Shady']","['1girl, blonde, floral dress, no legs.', 'AND HER NAME IS JOOOOOHN CEEEEENAAAA !!!!!!', 'darude sandstorm\n\nseriously though, posts like these should get you culled from society', 'never goon', 'Hi, my name is, what? My name is, who?\n\nMy name is, chka-chka, Slim Shady\n\nHi, my name is, huh? My name is, what?\n\nMy name is, chka-chka, Slim Shady']",0,8,0.05,Discussion,1751649895.0
1lrnt9h,StableDiffusion,Flux Kontext is not working for this image,"https://preview.redd.it/hrkmn5hk2waf1.png?width=1637&format=png&auto=webp&s=30beb56448c54664c9341085e538477f987d0b4b

How can i solve it? ","['https://preview.redd.it/ne1wb9q29waf1.jpeg?width=1542&format=pjpg&auto=webp&s=c9b71da9c154ed1786ee11068eb51834c667c079\n\nKontext is absolutely unpredictable. This prompt works: ""remove the person from the left.""', ""Kontext is not something predictable, you can't know what would work with a certain image without testing it first, probably inpainting is the best solution, it is also not possible to understand the changes that were made in this workflow, images outside the standard size never help."", 'Try: Remove the person on the left from the photo while maintaining the original composition']","['https://preview.redd.it/ne1wb9q29waf1.jpeg?width=1542&format=pjpg&auto=webp&s=c9b71da9c154ed1786ee11068eb51834c667c079\n\nKontext is absolutely unpredictable. This prompt works: ""remove the person from the left.""', ""Kontext is not something predictable, you can't know what would work with a certain image without testing it first, probably inpainting is the best solution, it is also not possible to understand the changes that were made in this workflow, images outside the standard size never help."", 'Try: Remove the person on the left from the photo while maintaining the original composition']",0,5,0.13,Question - Help,1751648607.0
1lrno3b,StableDiffusion,Krita AI results looks too similar,"i liked the idea of local AI as it would mean I could set it to generate 1000 results then go to bed and see the results in the morning

However I was dissapointed that every generation is so similar. for example if I prompt ""medieval fantasy city at night"" I basically get the exact same hue of green/blue, and the same type of buildings and identical vibe

is there a way to tell it to randomize the results more?","['Write longer prompts. Flowmatching models like flux converge to pretty much the same result if you use short ones or use something like chroma with a low CFG.', ""You could use wildcards and similar to randomize some details about prompts. Otherwise models would generate similar images. In case of Krita AI you'd need to create a custom workflow for this."", 'https://preview.redd.it/j1r78q8wq1bf1.png?width=2188&format=png&auto=webp&s=af955e90f40f874679002154d7d07acc183c090e\n\nPlug a loop into the denoise. This will vary the image... By a lot.']","['Write longer prompts. Flowmatching models like flux converge to pretty much the same result if you use short ones or use something like chroma with a low CFG.', ""You could use wildcards and similar to randomize some details about prompts. Otherwise models would generate similar images. In case of Krita AI you'd need to create a custom workflow for this."", 'https://preview.redd.it/j1r78q8wq1bf1.png?width=2188&format=png&auto=webp&s=af955e90f40f874679002154d7d07acc183c090e\n\nPlug a loop into the denoise. This will vary the image... By a lot.']",0,8,0.2,Question - Help,1751648266.0
1lrngyp,StableDiffusion,Trying to use an upscaling workflow using a nunchaku based FLUX model (Works great on low vram and it outputs 4K images + Workflow included),"The workflow : https://drive.google.com/file/d/1Vc00eMbB3xZXO6PLnc6a6_yDM4ebszgp/view?usp=sharing

The model : https://civitai.com/models/1545303?modelVersionId=1861654

The upscaler : https://huggingface.co/uwg/upscaler/blob/main/ESRGAN/4x_NMKD-Siax_200k.pth","['the chin ass seems to be intensified as well, thanks for the workflow by the way.', 'oof she got hit in the chin by flux HARD \n\nhttps://preview.redd.it/r97ttj970xaf1.png?width=200&format=png&auto=webp&s=5022d000c5f94629e88bcb91c90ead0e85409431', '""Johny Silverarm""', 'Does the ultimate upscale work on nunchaku yet?', 'What do you mean by LOW RAM?  \nCause I have only 4 GB VRAM and it might interest me.', 'I want a normal upscale to like 20k px and above, what do you recommend?', 'isnt SVD a video model?']","['the chin ass seems to be intensified as well, thanks for the workflow by the way.', 'oof she got hit in the chin by flux HARD \n\nhttps://preview.redd.it/r97ttj970xaf1.png?width=200&format=png&auto=webp&s=5022d000c5f94629e88bcb91c90ead0e85409431', '""Johny Silverarm""', 'Does the ultimate upscale work on nunchaku yet?', 'What do you mean by LOW RAM?  \nCause I have only 4 GB VRAM and it might interest me.']",43,13,0.83,Tutorial - Guide,1751647770.0
1lrn7ta,StableDiffusion,Any one having trouble wit Kontext using SWARMUI,"I have SWARMui installed with the comfi backend. As far as i know, it all works fine, i can generate images etc. When it comes to Kontext, though, neither the FP8 or DEV16 version pay attention to my prompts. Load initial image, etc, type prompt just not getting the great results i see on the sub What settings are everyone using inside swarm? ","['Did you set the metadata of the model to Flux Kontext? It has to be set manually in swarmUI, in the small drop-down menu that appears next to the model (and edit metadata)']","['Did you set the metadata of the model to Flux Kontext? It has to be set manually in swarmUI, in the small drop-down menu that appears next to the model (and edit metadata)']",0,2,0.5,Question - Help,1751647131.0
1lrmnfj,StableDiffusion,"I built a GUI tool for FLUX LoRA manipulation - advanced layer merging, face and style pre-sets, subtraction, layer zeroing, metadata editing and more.  Tried to build what I wanted, something easy.","Hey everyone,

I've been working on a tool called LoRA the Explorer - it's a GUI for advanced FLUX LoRA manipulation. Got tired of CLI-only options and wanted something more accessible.

**What it does:**

* Layer-based merging (take face from one LoRA, style from another)
* LoRA subtraction (remove unwanted influences)
* Layer targeting (mute specific layers)
* Works with LoRAs from any training tool

**Real use cases:**

* Take facial features from a character LoRA and merge with an art style LoRA
* Remove face changes from style LoRAs to make them character-neutral
* Extract costumes/clothing without the associated face (Gandalf robes, no Ian McKellen)
* Fix overtrained LoRAs by replacing problematic layers with clean ones
* Create hybrid concepts by mixing layers from differnt sources

The demo image shows what's possible with layer merging - taking specific layers from different LoRAs to create someting new.

It's free and open source. Built on top of kohya-ss's sd-scripts.

GitHub: [github.com/shootthesound/lora-the-explorer](https://github.com/shootthesound/lora-the-explorer)

Happy to answer questions or take feedback. Already got some ideas for v1.5 but wanted to get this out there first.

Notes: I've put a lot of work into edge cases! Some early flux trainers were not great on metadata accuracy, I've implemented loads of behind the scenes fixes when this occurs (most often in the Merge tab).  If a merge fails, I suggest trying concat mode (tickbox on the gui).

The merge failures are FAR less likely on the Layer merging tab, as this technique extracts layers and inserts into a new lora in a different way, making it all the more robust.  I may for version 1.5, harness an adaption of this technique for the regular merge tool.  But for now I need sleep and wanted to get this out!","[""Thank you SO much for explaining a bit of block and layer architecture in your GitHub post. I've always known what block/layer targeting could do for LoRAs and generations, but never understood which layers needed to be targeted to achieve a given goal. \n\nI don't know why, but reading through your post made it click for me. Thank you greatly for your contributions. I'm looking forward to trying this out today!"", 'That sounds great! Is it possible to do the same for Illustrious or Pony? If possible, could you make a video explaining and showing how things will work? That would help us understand it better.', ""Quick demo clip, - the Loras in the video are linked in app and on the GitHub.  I'll do a better video when I'm not in look after the kids mode!\n\n[https://www.dropbox.com/scl/fi/h3cclw6houcdwrp48knvw/LoraTheExplorer.webm?rlkey=llevswi4osrspln80pykzf270&st=mb4jhh42&dl=0](https://www.dropbox.com/scl/fi/h3cclw6houcdwrp48knvw/LoraTheExplorer.webm?rlkey=llevswi4osrspln80pykzf270&st=mb4jhh42&dl=0)"", ""Ran into an issue using the bat for installation, and I can't find the particular function in the code either:\n> ❌ Installation failed: 'charmap' codec can't encode character '\\U0001f9ed' in position 16: character maps to <undefined>\n\nThe odd part is that it occurred after getting a success message that lora-the-explorer was successfully installed. Running the bat fails for security reasons on W11 (with no way to override in the popup dialogue) but running the python launch command in terminal notifies me that sd-scripts was not installed. \n\nI should have a version of that on my computer from a kohya install, so what's my options here? Is there a fix to be made, a manual install, or can I link my existing sd-scripts location?"", 'Gj! Does it work with loras trained with AI-Toolkit?', ""does this mean you can combine character lora and nsfw lora and make img without distortion like few nsfw loras that don't affect face?"", ""So help me understand the Layer Targeting tool. With the presets guidance, I was working under the assumption that Facial Layers preset would preserve (not remove) facial details while discarding the rest of the information. The sidebar seems to have a conflicting opinion on this, citing that the facial layers *removes* the face while keeping style/costume and yet is ideal for character loras. \n\nTrying it, it seems to have been a description snafu because the facial layers preset does indeed retain training detail on the face. Or maybe I've encountered a placebo effect somehow in testing the output. \n\nWhen selecting the layers on this page and generating a lora result, what am I looking at? A lora with those selected layers removed or a lora with *only* those layers?"", 'Did you do more tests with the person / face specific layers? I remember a thread that demonstrated 7/12/16/20 but I am quite certain that those were not ALL layers if you want to keep the identity absolutely intact.', 'A lot of bug fixes overnight, including adding an update.bat file etc.  Either do a manual git pull or reinstall in a fresh folder to make updating easy in future.', 'Can you imagine how a bird man would be actually dumb and pecking at everything shiny lol']","[""Thank you SO much for explaining a bit of block and layer architecture in your GitHub post. I've always known what block/layer targeting could do for LoRAs and generations, but never understood which layers needed to be targeted to achieve a given goal. \n\nI don't know why, but reading through your post made it click for me. Thank you greatly for your contributions. I'm looking forward to trying this out today!"", 'That sounds great! Is it possible to do the same for Illustrious or Pony? If possible, could you make a video explaining and showing how things will work? That would help us understand it better.', ""Quick demo clip, - the Loras in the video are linked in app and on the GitHub.  I'll do a better video when I'm not in look after the kids mode!\n\n[https://www.dropbox.com/scl/fi/h3cclw6houcdwrp48knvw/LoraTheExplorer.webm?rlkey=llevswi4osrspln80pykzf270&st=mb4jhh42&dl=0](https://www.dropbox.com/scl/fi/h3cclw6houcdwrp48knvw/LoraTheExplorer.webm?rlkey=llevswi4osrspln80pykzf270&st=mb4jhh42&dl=0)"", ""Ran into an issue using the bat for installation, and I can't find the particular function in the code either:\n> ❌ Installation failed: 'charmap' codec can't encode character '\\U0001f9ed' in position 16: character maps to <undefined>\n\nThe odd part is that it occurred after getting a success message that lora-the-explorer was successfully installed. Running the bat fails for security reasons on W11 (with no way to override in the popup dialogue) but running the python launch command in terminal notifies me that sd-scripts was not installed. \n\nI should have a version of that on my computer from a kohya install, so what's my options here? Is there a fix to be made, a manual install, or can I link my existing sd-scripts location?"", 'Gj! Does it work with loras trained with AI-Toolkit?']",56,33,0.92,Resource - Update,1751645678.0
1lrl7ht,StableDiffusion,Ignorant question: how Flix Kontext Lora are trained ?,I never trained a Lora but AFAIK you need to collect a dataset of image-prompt pairs. How is the story is different in kontext ? What I mean is the dataset contains a tuple of three component (input image + prompt + result) instead of a pair?,"['Control image and target image with a prompt', 'from the maker of ai toolkit himself\nhttps://www.youtube.com/watch?v=WSWubJ4eFqI', '[First test using Kontext Dev Lora trainer : r/FluxAI](https://www.reddit.com/r/FluxAI/comments/1lmgcov/first_test_using_kontext_dev_lora_trainer/)', 'you dont need to do before/after training for kontext. you can also just train normal like you would any normal style or outfit.']","['Control image and target image with a prompt', 'from the maker of ai toolkit himself\nhttps://www.youtube.com/watch?v=WSWubJ4eFqI', '[First test using Kontext Dev Lora trainer : r/FluxAI](https://www.reddit.com/r/FluxAI/comments/1lmgcov/first_test_using_kontext_dev_lora_trainer/)', 'you dont need to do before/after training for kontext. you can also just train normal like you would any normal style or outfit.']",0,6,0.22,Discussion,1751642097.0
1lrjjnh,StableDiffusion,REQUESTED TUTORIAL FOR AI STORY SHORTS FOR FREE,"Recently i have uploaded a video on these community [https://www.reddit.com/r/aivideo/comments/1lpua34/ai\_story\_for\_kids\_suggestions/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/aivideo/comments/1lpua34/ai_story_for_kids_suggestions/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)

Where people asked me what is the procedure of it so these is the best tutorial for it [https://youtu.be/Od-Y4Mt2Ung](https://youtu.be/Od-Y4Mt2Ung)   ",[],[],0,0,0.22,Discussion,1751637858.0
1lrixrr,StableDiffusion,How to make that graphics?,,"['Chatgpt image generator\xa0', 'just use Flux Kontext in comfy, give it the start image and tell it what you want the next image to be.']","['Chatgpt image generator\xa0', 'just use Flux Kontext in comfy, give it the start image and tell it what you want the next image to be.']",0,9,0.21,Question - Help,1751636220.0
1lria24,StableDiffusion,Anything better than Lustify for naughties?,Lustify is decent wondered if anyone has other recommendations for adult stuff?,"[""Wait for the new version of Lustify, I'm working on it currently. A lot of reworks are being done"", ""Chroma is quickly becoming my Lustify replacement.\n\nIt can do anything Lustify does but output higher visual aesthetic with creative variety and excellent prompt adherence.\n\nDownside is the model is still in training so you get weird seeds (every 5 generations one horror result) and it's quite slow because it uses Flux with negative prompt (2x slower than normal flux)\n\nYou can however speed it up with sage attention, torch compile"", 'My  [Jib Mix Illustrious ](https://civitai.com/models/1255024/jib-mix-illustrious-realistic)model defaults to NSWF. but my [Jib Mix Realistic SDXL](https://civitai.com/models/194768/jib-mix-realistic-xl) has better details but can also do NSFW but is not as good at complex NSFW poses as Illustrious.\n\nhttps://preview.redd.it/n337gcfh8vaf1.png?width=2249&format=png&auto=webp&s=b3815a5a6e0dea037fe62b610340ef16291de57c', 'Big Love and it has tons of custom loras', 'Started using Chroma and GGUF Q8 in forge UI - it is good: [https://youtu.be/-H5WXCQiVjk?si=tpPyT-aOJP1TnVXm](https://youtu.be/-H5WXCQiVjk?si=tpPyT-aOJP1TnVXm)', ""Wow thanks for the replies, just tried Chroma only got an 8GB card followed that Youtube Nosleep posted very usefull! Its very slow but really nice quality, one day I'll bite the bullet and get a 16gb card but my trusty 2070 super keeps chugging away lol."", ""Lustify is great but frankly I wouldn''t rely on just one model, they all have different flavours.\n\n  \nMust have's in terms of realism including Lustify are BigASP and P\\*\\*nmaster Pro/Amateur models by iamddtla on civitai. BigLove is good at prompt adherence and very specific things but lacks on Realism.\n\nAll are superb but will give you different flavours."", 'BigLust is my number followed by my custom merge Pyros + BigAsp2', 'You could try [https://civitai.com/models/277058/epicrealism-xl](https://civitai.com/models/277058/epicrealism-xl)\n\nI use it for most of my stuff.', 'Pony with flux inpainting. Thank me later']","[""Wait for the new version of Lustify, I'm working on it currently. A lot of reworks are being done"", ""Chroma is quickly becoming my Lustify replacement.\n\nIt can do anything Lustify does but output higher visual aesthetic with creative variety and excellent prompt adherence.\n\nDownside is the model is still in training so you get weird seeds (every 5 generations one horror result) and it's quite slow because it uses Flux with negative prompt (2x slower than normal flux)\n\nYou can however speed it up with sage attention, torch compile"", 'My  [Jib Mix Illustrious ](https://civitai.com/models/1255024/jib-mix-illustrious-realistic)model defaults to NSWF. but my [Jib Mix Realistic SDXL](https://civitai.com/models/194768/jib-mix-realistic-xl) has better details but can also do NSFW but is not as good at complex NSFW poses as Illustrious.\n\nhttps://preview.redd.it/n337gcfh8vaf1.png?width=2249&format=png&auto=webp&s=b3815a5a6e0dea037fe62b610340ef16291de57c', 'Big Love and it has tons of custom loras', 'BigLust is my number followed by my custom merge Pyros + BigAsp2']",80,106,0.86,Question - Help,1751634335.0
1lrgs0m,StableDiffusion,"Omni Avatar looking pretty good - However, this took 26 minutes on an H100","This looks very good imo for open source, this is using the Wan 14B model with 30 steps and 720P resolution.
","['What a fantastic freeze‑frame backdrop we’ve got here!', 'lol stop complaining guys.. every new thing kinda sucks at first. And this one at least seems to add sound!', 'Jesus people are so god damned picky all of a sudden. I thought it was quite realistic for open source.', 'It looks great. Sure, the background is static, but nobody would notice that if they saw a 4s clip once.\n\nWhat kills it for me is the voice. It sounds reasonably human. But like a human voice actress, not like a human speaking to me or like a human making influencer content', 'I urgently need skilled sound engineers; all the voices currently sound overly polished, as if they were created in a studio—no background sounds, no echo, nothing natural.', 'People complaining about the background, just rotoscope her out and put the overlay woman over a background video from real life or generated separately\n\nThe bigger issue is the shit speed', '26 minutes on an H100…', 'Yeah, this is very good quality, but the price is too high. What chances this will get more efficient?', 'Did you use any accelerators like in the github mentioned? [FusioniX](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/blob/main/FusionX_LoRa/Wan2.1_I2V_14B_FusionX_LoRA.safetensors) and [lightx2v](https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_T2V_14B_lightx2v_cfg_step_distill_lora_rank32.safetensors) LoRA acceleration? Tea Cache?', 'Reminds me of Hunyuan Video Avatar at the start, where she overly exaggerates the ""Hi!"" with her face. I don\'t know why they tend to do that, but they end up looking like chickens pecking at feed. Other than that, the facial animation isn\'t bad really.']","['What a fantastic freeze‑frame backdrop we’ve got here!', 'lol stop complaining guys.. every new thing kinda sucks at first. And this one at least seems to add sound!', 'Jesus people are so god damned picky all of a sudden. I thought it was quite realistic for open source.', 'It looks great. Sure, the background is static, but nobody would notice that if they saw a 4s clip once.\n\nWhat kills it for me is the voice. It sounds reasonably human. But like a human voice actress, not like a human speaking to me or like a human making influencer content', 'People complaining about the background, just rotoscope her out and put the overlay woman over a background video from real life or generated separately\n\nThe bigger issue is the shit speed']",214,85,0.81,Discussion,1751629695.0
1lrgre3,StableDiffusion,"In 4k video restoration and upscale , open-source tech has now outperformed proprietary, paid alternatives.","In my free time, I usually try to replicate closed-source AI technology. Due to work requirements, I am currently researching video super-resolution and restoration. On the most difficult old TV series ""Journey to the West"" to super-resolution and restore, I tried 7 or 8 different methods, and finally found that the open source effect after fine-tuning is really good, and it is much better than the strongest topaz in character consistency, noise reduction, and image restoration.","['And which open source model did you use?', 'https://preview.redd.it/xlxe9h87evaf1.png?width=566&format=png&auto=webp&s=61e8414260dca17c94319e9a1c2e8abe3c805f21', 'Not revealing how you did that in an open source community is akin to marketing closed source alternatives. Hence, this will be downvoted.', '[deleted]', 'Is ""Open Source"" the brand name? Because I don\'t see anything open source in the post.', 'the open source one looks horrdenus, like early days of AI', 'Looks good!\nwe need more open source upscalers for video,I been using waifu2x, which is decent', 'https://i.redd.it/01eplzu7zwaf1.gif', 'Open source models on most tech can achieve great results because.. its open source and people pool their knowledge.  But it usually requires a lot of work. And when it comes down to it, no one wants to do the work.', ""And all three look bad, I'd take an old blurry video over some AI scaled uncanny valley crap""]","['And which open source model did you use?', 'https://preview.redd.it/xlxe9h87evaf1.png?width=566&format=png&auto=webp&s=61e8414260dca17c94319e9a1c2e8abe3c805f21', 'Not revealing how you did that in an open source community is akin to marketing closed source alternatives. Hence, this will be downvoted.', '[deleted]', 'Is ""Open Source"" the brand name? Because I don\'t see anything open source in the post.']",215,69,0.76,Discussion,1751629636.0
1lrggnf,StableDiffusion,Is Deepswap the best?,"Been playing around with AI face swap tools and while deepswap isn’t bad it kind of forces you into a subscription before you can really test the quality. Just looking for a more flexible alternative. something that’s decent for short, fun video edits with friends and ideally lets you upload your own clips without slapping on watermarks or crashing mid-edit. What are you all using lately?","['Anything that leverages inswapper128 backend is still the best local version - ideally with some pixel up scaling and control over feathering and options to change feature identification. Rope and others fit this requirement. Would pay tons to money to get inswapper256.\n\nWorth noting that this above statement hasn’t changed in two years, basically no major progress in free public models', 'I felt the same way about deepswap. decent results but the whole paywall-before-preview thing kinda killed it for me. I have been using VidMage AI lately just for casual edits and its perfect for meme style stuff with friends', 'Best faceswappers use inswapper128 as a base and gpen for enhancement. Roop and its successors as an example.\nThe problem with all of then is with obstacles and extreme angles, especially in videos (because you wont manualy deal with thousands of frames)\nThats why currently IMHO the best option is WAN(vace) with lora trained for specific face. But a drawback is only 5 seconds length or a bit more.', 'The new facefusion 3.3 is pretty decent with its hyperswap', 'I got some good results with [VisoMaster](https://github.com/visomaster/VisoMaster), but it works locally, so you need to have a decent GPU.']","['Anything that leverages inswapper128 backend is still the best local version - ideally with some pixel up scaling and control over feathering and options to change feature identification. Rope and others fit this requirement. Would pay tons to money to get inswapper256.\n\nWorth noting that this above statement hasn’t changed in two years, basically no major progress in free public models', 'I felt the same way about deepswap. decent results but the whole paywall-before-preview thing kinda killed it for me. I have been using VidMage AI lately just for casual edits and its perfect for meme style stuff with friends', 'Best faceswappers use inswapper128 as a base and gpen for enhancement. Roop and its successors as an example.\nThe problem with all of then is with obstacles and extreme angles, especially in videos (because you wont manualy deal with thousands of frames)\nThats why currently IMHO the best option is WAN(vace) with lora trained for specific face. But a drawback is only 5 seconds length or a bit more.', 'The new facefusion 3.3 is pretty decent with its hyperswap', 'I got some good results with [VisoMaster](https://github.com/visomaster/VisoMaster), but it works locally, so you need to have a decent GPU.']",2,14,0.56,Discussion,1751628596.0
1lrggl5,StableDiffusion,Making good Mecha?,"Does anyone have advice for how to make good Mecha with specific details? I've tried Hidream, chroma and Omnigen 2 but I can't seem to get decent results from any of them.

1. Hidream:
Ignores details and seems to just want to do the same one mecha.

2. Chroma:
Great variation but it struggles with details (laser cannon on left arm, flamethrower on right, etc). Fair amount of floating stuff depending on the prompt.

3. Omnigen 2:
Doesn't do details and they come out extremely basic.

There seem to be some flux Loras on civitai.

Overall, any advice?  My big goal is to make a few 6 second Wan clips with a Mecha that matches the targeted details.","['Use the model that works best for you to generate base images and then iterate upon them through inpainting and perhaps your own sketching on top of the image (laser cannon on left arm, flamethrower on right, etc). You can also use one model that generates the images in a way that is accurate to the prompt and another model to img2img it to add details.', 'https://preview.redd.it/xo0t7v14dwaf1.jpeg?width=1022&format=pjpg&auto=webp&s=f39780310f26ba7d4f2bee3867de7342c343b2a0\n\nFrom my experience, none of the mentioned models can produce truly fascinating and detailed complex objects like cyborgs, mecha, etc.  \nDALL·E 3 was my favorite for that kind of stuff.']","['Use the model that works best for you to generate base images and then iterate upon them through inpainting and perhaps your own sketching on top of the image (laser cannon on left arm, flamethrower on right, etc). You can also use one model that generates the images in a way that is accurate to the prompt and another model to img2img it to add details.', 'https://preview.redd.it/xo0t7v14dwaf1.jpeg?width=1022&format=pjpg&auto=webp&s=f39780310f26ba7d4f2bee3867de7342c343b2a0\n\nFrom my experience, none of the mentioned models can produce truly fascinating and detailed complex objects like cyborgs, mecha, etc.  \nDALL·E 3 was my favorite for that kind of stuff.']",1,2,0.6,Question - Help,1751628589.0
1lrgg0z,StableDiffusion,New Abstract Portrait Lora - EchoBlur & BlurShift,"Hey everyone!

I've just released a new lora call EchoBlur & BlurShift, trained on a curated dataset of abstract portrait photography. It's designed for Flux.

For download: [https://civitai.com/models/1743530/echoblur-blurshift-abstract-and-glitch-identity](https://civitai.com/models/1743530/echoblur-blurshift-abstract-and-glitch-identity)

Would love to hear your feedback or see what you make with it ! ","['https://i.redd.it/eazvi5zdvuaf1.gif', 'Looks awesome, will test it out!', ""pitch that at music people they'll love it for cover art."", 'Could you link to an IMG2VIDEO workflow that I can use to do this LORA? Thank you very much!']","['https://i.redd.it/eazvi5zdvuaf1.gif', 'Looks awesome, will test it out!', ""pitch that at music people they'll love it for cover art."", 'Could you link to an IMG2VIDEO workflow that I can use to do this LORA? Thank you very much!']",82,10,0.9,Resource - Update,1751628531.0
1lrgamn,StableDiffusion,Kontext + photoshop= magic,"Using photoshop to layer backgrounds and 2d characters. Then sending to FLUX KONTEXT Prompt:""Convert to Semi-realism, keep size proportions and pose unchanged"" ","['Do you really need Photoshop at all here? You can remove the background and layer in Comfy too.', 'upvoted because Rocko', 'lol. paid for photoshop so you could *not* use it, and use a less useful dev version instead. I am confused at this thinking.']","['Do you really need Photoshop at all here? You can remove the background and layer in Comfy too.', 'upvoted because Rocko', 'lol. paid for photoshop so you could *not* use it, and use a less useful dev version instead. I am confused at this thinking.']",64,7,0.88,News,1751628013.0
1lrfq0j,StableDiffusion,Is it possible I’m missing a config file?,"I keep getting a json decoding error about the persistent cache. I ran a search and found a lot of people having this error a few years ago and fixing it by removing some commas from the file. It’s the same file mine is having an error decoding but I’m having a hard time finding it and when I search my computer for it it’s not showing up there. Seems unlikely it would be gone but I have no clue. Can you tell I’m new? And have no clue what I’m doing? I can gen sometimes still but eventually get a sticking cursor or just. Black box instead of my image. Can anyone help me? Using a1111 sd 1.5 on windows ten with a lowly 1050 with 6vram. Have been generating fine up until this point. The only thing I’ve done lately is download and use a vae which I’d not done before. I did get a half vae error after that I fixed by turning odd token counting. Have since deleted the vae. Kind of at my wits end, please help ","['Getting the half vae error again now even tho the setting is still ticked that fixed it before.', 'Use comfyui, please . you guys need to make the jump']","['Getting the half vae error again now even tho the setting is still ticked that fixed it before.', 'Use comfyui, please . you guys need to make the jump']",0,2,0.33,Question - Help,1751625999.0
1lrev0n,StableDiffusion,Best vids to teach noobs?,"Hey all,

I need to teach non AI people the foundations of AI but specifically for image/video gen.

Like latent space, samplers , models, cannies etc.

What are the best digestible and accessible videos or YouTube channels out there can get the points across without overwhelming people?

Thanks ","['https://youtube.com/@pixaroma?si=KCfb0_0qRwlPYtV2 definitely check this channel out, hes got over 50 videos on teaching/optimizing comfy', 'Fellow noob here, I thought this video explained ComfyUI pretty well! https://youtu.be/23VkGD-4uwk?si=OkLvf74amqiOGI4C', 'Start with explaining that\xa0stable diffusion are just this are large files; then there are python script like Comfy UI, Forge UI, Fooocus - which makes the computer use stable diffusion and generate images. \n\nThen ask to understand this simple interface made by python ""Fooocus"":\xa0[YouTube - Fooocus installation](https://youtu.be/3tAaL57rhoU?si=ArrTPhPQJkbfW7ZN)\n\nThis\xa0[playlist - YouTube](https://youtube.com/playlist?list=PLPFN04WspxqsslRSpiLmwGR8QTpDYNv7z&si=-QOo5_980RbiniKn)\xa0is for beginners which covers topic like prompt, models, lora, weights, in-paint, out-paint, image to image, canny, refiners, open pose, consistent character, training a LoRA.\n\nAfter that if wish to go further in gen AI, than start with Comfy UI.', ""If you're giving them links to YouTube videos, it's not really you teaching them, is it?""]","['https://youtube.com/@pixaroma?si=KCfb0_0qRwlPYtV2 definitely check this channel out, hes got over 50 videos on teaching/optimizing comfy', 'Fellow noob here, I thought this video explained ComfyUI pretty well! https://youtu.be/23VkGD-4uwk?si=OkLvf74amqiOGI4C', 'Start with explaining that\xa0stable diffusion are just this are large files; then there are python script like Comfy UI, Forge UI, Fooocus - which makes the computer use stable diffusion and generate images. \n\nThen ask to understand this simple interface made by python ""Fooocus"":\xa0[YouTube - Fooocus installation](https://youtu.be/3tAaL57rhoU?si=ArrTPhPQJkbfW7ZN)\n\nThis\xa0[playlist - YouTube](https://youtube.com/playlist?list=PLPFN04WspxqsslRSpiLmwGR8QTpDYNv7z&si=-QOo5_980RbiniKn)\xa0is for beginners which covers topic like prompt, models, lora, weights, in-paint, out-paint, image to image, canny, refiners, open pose, consistent character, training a LoRA.\n\nAfter that if wish to go further in gen AI, than start with Comfy UI.', ""If you're giving them links to YouTube videos, it's not really you teaching them, is it?""]",5,6,0.67,Tutorial - Guide,1751622748.0
1lrenq8,StableDiffusion,[FusionX] Jiggly animation and urge to communicate,"Hey Guys,
I have the Problems with my Generation:

* Its Jiggly
* It speaks all the time

I tried promts with ""Silent"", ""closed mouth"" and more but nothing helps.

Here is an example:

https://reddit.com/link/1lrenq8/video/ejk56csiutaf1/player

https://preview.redd.it/yxw5kda1vtaf1.png?width=1690&format=png&auto=webp&s=2d6be996db6ec67a6f30613ea72f896e383f0f4e

https://preview.redd.it/fb4ry504vtaf1.png?width=1704&format=png&auto=webp&s=0617f03f08fdcf0b8a236af8eb7a365392dd60c7

Here's the complete Workflow:https://pastebin.com/KWQqsqus


Do you have any suggestions?","['Seek out the ""NAG"" node, be sure to get the one for the native workflow. This helps your negative prompt actually work, even on 1 cfg. Then try putting ""talking, open mouth, teeth, tongue"" etc into the negative prompt. You could also add ""Keeps his mouth closed"" in the positive. \n\n  \nAs for the shakiness, it just seems to be something Wan 2.1 does for really cartoony stuff. It handles realistic/2.5d really well, but the more cartoony you go the more shaky/limited animations tends to become. At least in my experience. Maybe someone can chip in more on that because I still struggle with that as well.', 'heh, I have a similar problem in LTXV - cant make all the characters shut up 🤐']","['Seek out the ""NAG"" node, be sure to get the one for the native workflow. This helps your negative prompt actually work, even on 1 cfg. Then try putting ""talking, open mouth, teeth, tongue"" etc into the negative prompt. You could also add ""Keeps his mouth closed"" in the positive. \n\n  \nAs for the shakiness, it just seems to be something Wan 2.1 does for really cartoony stuff. It handles realistic/2.5d really well, but the more cartoony you go the more shaky/limited animations tends to become. At least in my experience. Maybe someone can chip in more on that because I still struggle with that as well.', 'heh, I have a similar problem in LTXV - cant make all the characters shut up 🤐']",3,2,0.67,Question - Help,1751621955.0
1lreevz,StableDiffusion,Difference between FLUX Kontext dev and max is sadly very huge.,"So my prompt was ""Make the image in 2D Oil painting style"".

The first image is my original one.
The second one is made with FLUX Kontext.dev.
The last one is the one made with FLUX Kontext.max

This is very sad and annoying to still se that in 2025.","[""I bet I could make a better image with dev, it's more like the prompt issue, maybe their site enhanced the prompt somehow, I'll get back to it in the evening."", ""They're a business, if they give away the best stuff they've got for free, how are they going to pay for the devs and hardware?\n\nTo me, this is like an airline offering free flights, but in economy class. It'd be ridiculous to sit on the plane and complain that you can't have the first class legroom and catering when you're flying for free."", 'It is not a problem, it is an opportunity.', 'You cant expect to have all nice thing for free', ""Give it some time. It's just the base model, released a few days ago. You can already find LoRAs for different styles, and there will be more to come."", 'loras will overcome this bullshit. we did it with regular flux we can do it with kontext--and it will be even better', ""The last one doesn't look like an oil painting.\xa0"", 'i just added ""anime oil painting"". flux Kontext dev GGUF Q8\n\nhttps://preview.redd.it/04sce5ve7uaf1.png?width=1280&format=png&auto=webp&s=87e9e1152db78651a3f36ac38eb931b2c797617b', 'this is what i got with this prompt: ""apply an oil-painting look with rich brush texture without altering layout or subjects"" on flux kontext dev. not sure if its the look you are going for but it does look like an oil painting to me.\n\nhttps://preview.redd.it/wxwum7550uaf1.png?width=1024&format=png&auto=webp&s=db4e9f533beb53ad55f4811b95e8a1dc60adeb36', 'To be honest, the dev effect is very poor, and the image quality is also damaged. Just use it as a toy\\~\\~ You can throw it away after the novelty period\\~\\~ It is basically a model with enhanced controls. If you add lora or control or something, it will easily break down']","[""I bet I could make a better image with dev, it's more like the prompt issue, maybe their site enhanced the prompt somehow, I'll get back to it in the evening."", ""They're a business, if they give away the best stuff they've got for free, how are they going to pay for the devs and hardware?\n\nTo me, this is like an airline offering free flights, but in economy class. It'd be ridiculous to sit on the plane and complain that you can't have the first class legroom and catering when you're flying for free."", 'It is not a problem, it is an opportunity.', 'You cant expect to have all nice thing for free', ""Give it some time. It's just the base model, released a few days ago. You can already find LoRAs for different styles, and there will be more to come.""]",82,123,0.73,Discussion,1751620982.0
1lre7vj,StableDiffusion,At long last two new (style) FLUX LoRa's from me + updated the last model to my new standard + all my models are now also on Tensor + new improved recommended inference workflow that is also much more organized and neat looking now!,"After more than half a year of just updating my existing models to an ever better standard I have finally settled on a final standard for all my models and updated the last one to that standard. I have thus also finally created two new fresh style LoRa's of Avatar Legend of Korra and StarWars The Bad Batch.

I have also created a new and improved recommended inference workflow (you can find it in each model description) and updated all samples to that new standard. The new workflow also looks much more organized now with notes and a snap to grid and neat layout!

I have also finally imported all my models to Tensor. You can find them there under the same name.

From now on I will keep creating new models again.

I have also already created a Kontext version of all of my models (so 20 in total) that I first still need to do some testing on and sample generation before I can upload them.

Link to the two new style models:

https://civitai.com/models/1742627/avatar-the-legend-of-korra-style-lora-flux

https://civitai.com/models/1742651/starwars-the-bad-batch-style-lora-flux

Link to new inference workflow:

https://www.dropbox.com/scl/fi/twherl9631kmz6acart9g/recommended_FLUX-dev-Kontext_inference_workflow_by_-AI_Characters.json?rlkey=o6ryxdvsxrv6pj1smx3bz95kr&st=369d7pj9&dl=1

Were so back.","['PS: I basically didnt sleep the entire night and powered through to get this done before work still.\n\nNow I have to suffer at work sleep-deprived :(', 'Examples look awesome, thanks for sharing your hard work']","['PS: I basically didnt sleep the entire night and powered through to get this done before work still.\n\nNow I have to suffer at work sleep-deprived :(', 'Examples look awesome, thanks for sharing your hard work']",31,2,0.9,Resource - Update,1751620201.0
1lrdsg3,StableDiffusion,Help installing Stability-Fast-3D to A1111,"Hello! I am farely new to this and I've scoured the YouTubes but haven't found an answer to my question so I'm reaching out to the community for help.

I've got A1111 running and Stable Diffusion which I use and can generally install models for (drag and drop the safetensors into the Web UI > Stable Diffusion folder). That's basically the limit of my knowledge.

I would really really like to try out this LoRA:
[https://github.com/Stability-AI/stable-fast-3d](https://github.com/Stability-AI/stable-fast-3d)

[https://huggingface.co/stabilityai/stable-fast-3d](https://huggingface.co/stabilityai/stable-fast-3d)

But I am hella stuck! Please could anyone out there help me with installing this to A1111? I am a noobie, an enthusiast, a hobbyist and at the mercy of this great and hopefully forgiving community! :D

Thank you!","[""Automatic1111 is dead, it hasn't been updated for a year now. As such, it's not going to have support for something released six months ago. \n\nMigrate to [ComfyUI](https://www.comfy.org/) and import the example workflow that they provide on the github page into it."", ""Also, although that's not a fun answer, those 3D generation models are very low quality compared to the best free closed source models (free to use, but they're not local) like Hunyuan 2.5 or Sparc3D. (you can find them easily by googlin' them)""]","[""Automatic1111 is dead, it hasn't been updated for a year now. As such, it's not going to have support for something released six months ago. \n\nMigrate to [ComfyUI](https://www.comfy.org/) and import the example workflow that they provide on the github page into it."", ""Also, although that's not a fun answer, those 3D generation models are very low quality compared to the best free closed source models (free to use, but they're not local) like Hunyuan 2.5 or Sparc3D. (you can find them easily by googlin' them)""]",2,3,0.6,Question - Help,1751618472.0
1lrbyf2,StableDiffusion,"If you know, you know",,"['Cringe', 'Touch grass 🙏', 'I don’t, explain please.', 'eh', 'FYI guys I don’t condone this. Just making a joke lol', 'you have a sense of humor, high five 👋', ""There is also a subreddit just for ai pron... you don't need to be a cringe kid on the Internet.""]","['Cringe', 'Touch grass 🙏', 'I don’t, explain please.', 'eh', 'FYI guys I don’t condone this. Just making a joke lol']",0,18,0.41,Meme,1751611165.0
1lrbu97,StableDiffusion,"What does ""Module"" metadata mean in the generated image?","I've been looking through some example images on Civitai, and some of them have the following added to the end of usual metadata string:

>Module 1: NSFWFilter, Module 2: sdxlVAE\_sdxlVAE

What does it mean, and what are the ""Modules""? AUTO1111 that I use doesn't seem to add any ""modules"" metadata  in, even with  additional networks...",[],[],2,0,1.0,Question - Help,1751610714.0
1lrbtkf,StableDiffusion,Question on Wan 2.1 i2v generation optimization and quality,"Hi, I've been using Wan 2.1 lately and realized that the i2v 480p 14B model is also capable of generating 720p results as well (I don't know if there is a hidden upscaler or if it just generates directly).

But it seems that although the improved resolution, the video quality itself is poor compared to a 480p generation from the 480p model (in terms of facial consistency, lighting etc.). Is this something normal? Perhaps related to the fact that the Loras I am using is 480p based?

Also, there's always this weird gray-ish screen that appears around the first ~1 seconds before the video gets processed. I think a certain Lora is triggering this, since when I lower its strength it dissapears, then again I find that messing with teacache (increasing) solves this issue with some loss of quality. Also, I am using skip layer guidance, and turning this off also helps with quality improvement (although it says this improved quality originally..?)

I'm basically trying to find the optimum setting so that I can retain my Lora strength and get best results for i2v 480p. Any comments or helps available?
","['well because it is the same model, well architecturally speaking, even the patch embedding dimension is the same (this is what responsible for resolution, any ViT will have it)\n\nhttps://preview.redd.it/4y1zj0lh0taf1.png?width=1637&format=png&auto=webp&s=f8591eb872550d8fec86bc4611f8a4f4621556fa\n\nleft is 720 right is 480  \nI mean it is the same reason why T2V can both generate 480 and 720p, it just that it does not have image embed layer (responsible for image input)\n\n>But it seems that although the improved resolution, the video quality itself is poor compared to a 480p generation from the 480p model (in terms of facial consistency, lighting etc.).\n\ni mean, yeah? open their huggingface project. and see the dataset  sankey, you will know why. The patch embedding is the same, but the image embed layer and diffusion attention is trained on more 720p dataset', ""afaik both 14b models are the same in all ways except resolution used during training. so the same dataset and stuff, but targeting resolutions close to 480p or 720p only. they'll still understand other higher or lower resolutions, but both models have their sweet spot around 480p or 720p."", 'I am generating in 480p and upscale with RealESGRAN_2.pth with „uspcale Image (with model)“, you can check some clips here: https://civitai.com/user/tremolo28/posts']","['well because it is the same model, well architecturally speaking, even the patch embedding dimension is the same (this is what responsible for resolution, any ViT will have it)\n\nhttps://preview.redd.it/4y1zj0lh0taf1.png?width=1637&format=png&auto=webp&s=f8591eb872550d8fec86bc4611f8a4f4621556fa\n\nleft is 720 right is 480  \nI mean it is the same reason why T2V can both generate 480 and 720p, it just that it does not have image embed layer (responsible for image input)\n\n>But it seems that although the improved resolution, the video quality itself is poor compared to a 480p generation from the 480p model (in terms of facial consistency, lighting etc.).\n\ni mean, yeah? open their huggingface project. and see the dataset  sankey, you will know why. The patch embedding is the same, but the image embed layer and diffusion attention is trained on more 720p dataset', ""afaik both 14b models are the same in all ways except resolution used during training. so the same dataset and stuff, but targeting resolutions close to 480p or 720p only. they'll still understand other higher or lower resolutions, but both models have their sweet spot around 480p or 720p."", 'I am generating in 480p and upscale with RealESGRAN_2.pth with „uspcale Image (with model)“, you can check some clips here: https://civitai.com/user/tremolo28/posts']",0,3,0.36,Question - Help,1751610640.0
1lr98p8,StableDiffusion,Animatediff and related error within Google Colab notebook,"Hello,

I’m needing some assistance after encountering & resolving an endless cycle of errors when trying to code a workbook so I can convert images to MP4’s. I can go into limited detail based on my knowledge, but I have some progress logged.

These issues could not be resolved by two AI platforms (Y’all may laugh and say no kidding.) But the most recent error addressed broken links to realisticvision6.0, NumPy binary incompatibility, Hugging face errors. etc.

If the easiest thing is to screenshot the errors I’m more than happy to do that, if anyone would please take a look. I’m new to the sub and am not familiar with what is/isn’t allowed.

Thx for your time and have a Happy 4th!",[],[],0,0,0.25,Question - Help,1751601512.0
1lr88lc,StableDiffusion,V-PRED vs EPS - Tried using V-PRED but results are ugly...,"Title. I recently tried experimenting with V-PRED illustrious models (anime screenshot merge, solvent eclipse in particular)... with LORA use, generally V-PRED comes out slightly more accurate (but more blurry and not responsive to artist tags). Is this also everyone's experience?

Perhaps it is because the way the LORA is trained? Not sure...

EPS still takes the win for me.","[""You need to use the Euler CFG++ samplers alongside dynamic thresholding to get the best results with NoobAI-XL's V-Pred version. Any variation except for the ancestral one, as that particular sampler tends to oversaturate outputs.\n\nUse a low CFG scale with them. I recommend starting with 1.5 and adjusting from there to your preferences or until the outputs begin to burn from oversaturation. Set Dynamic Thresholding's mimic scale to 3.5, and set the mimic mode to Half-Cosine down to start. This will keep the outputs clean and free from oversaturation."", 'I usually just use Anime Screenshot Merge when I want an anime screenshot style, but here are my settings (Forge): Euler A / Align Your Steps / 30 Steps / CFG 3-4.  I also include the tags ""anime screenshot, anime screencap, anime coloring"" along with the usual Illustrious quality modifiers, and the results are usually pretty good IMO ([some sample images](https://civitai.com/posts/18447182)).\n\nIn a more general sense, I do find VPred to be a bit more exacting than EPS.  It\'s usually a bit more accurate, but this can be a double-edged sword since it means it\'s more dependent on you providing accurate prompts.  Also be careful with lighting tags as these can overwhelm the image. (You can also try using [this LoRA](https://civitai.com/models/1555532/noob-ai-xl-v-pred-stoopid-colorfix) to compensate.)\n\n> artist tags\n\nI hate using artist tags, so I can\'t comment on VPred vs EPS.  What I can say is that not all models respect artist tags equally - some are highly sensitive, others stick to their own style.  You\'d need a much larger sample size, but I suspect it\'s more those specific models than VPred vs EPS (especially given the popularity of the baseline [Noob VPred checkpoint](https://civitai.com/models/833294?modelVersionId=1190596))\n\n> with LORA use ... Perhaps it is because the way the LORA is trained?\n\nI\'m pretty sure it\'s a training issue.  I\'ve come across a few LoRAs that had both a VPred and EPS version, and they definitely worked better when used ""correctly"".  Unfortunately I can\'t recall them off the top of my head.', 'What interface do you use? Also vpred models in general seem to be ceey sensitiveto prompting. The more descriptive the better the output. I use noobai midnight merge periodically and its results are pretty alright.']","[""You need to use the Euler CFG++ samplers alongside dynamic thresholding to get the best results with NoobAI-XL's V-Pred version. Any variation except for the ancestral one, as that particular sampler tends to oversaturate outputs.\n\nUse a low CFG scale with them. I recommend starting with 1.5 and adjusting from there to your preferences or until the outputs begin to burn from oversaturation. Set Dynamic Thresholding's mimic scale to 3.5, and set the mimic mode to Half-Cosine down to start. This will keep the outputs clean and free from oversaturation."", 'I usually just use Anime Screenshot Merge when I want an anime screenshot style, but here are my settings (Forge): Euler A / Align Your Steps / 30 Steps / CFG 3-4.  I also include the tags ""anime screenshot, anime screencap, anime coloring"" along with the usual Illustrious quality modifiers, and the results are usually pretty good IMO ([some sample images](https://civitai.com/posts/18447182)).\n\nIn a more general sense, I do find VPred to be a bit more exacting than EPS.  It\'s usually a bit more accurate, but this can be a double-edged sword since it means it\'s more dependent on you providing accurate prompts.  Also be careful with lighting tags as these can overwhelm the image. (You can also try using [this LoRA](https://civitai.com/models/1555532/noob-ai-xl-v-pred-stoopid-colorfix) to compensate.)\n\n> artist tags\n\nI hate using artist tags, so I can\'t comment on VPred vs EPS.  What I can say is that not all models respect artist tags equally - some are highly sensitive, others stick to their own style.  You\'d need a much larger sample size, but I suspect it\'s more those specific models than VPred vs EPS (especially given the popularity of the baseline [Noob VPred checkpoint](https://civitai.com/models/833294?modelVersionId=1190596))\n\n> with LORA use ... Perhaps it is because the way the LORA is trained?\n\nI\'m pretty sure it\'s a training issue.  I\'ve come across a few LoRAs that had both a VPred and EPS version, and they definitely worked better when used ""correctly"".  Unfortunately I can\'t recall them off the top of my head.', 'What interface do you use? Also vpred models in general seem to be ceey sensitiveto prompting. The more descriptive the better the output. I use noobai midnight merge periodically and its results are pretty alright.']",0,13,0.46,Question - Help,1751598215.0
1lr7znb,StableDiffusion,Need advice from LORA trainers regarding a commission for an OC,"Hey everyone.
I recently had a commission for an OC. The prompt required to replicate her was extremely long, but I managed to get it done to the customer's satisfaction.

They commissioned me for a 2nd set of images, but now they want interaction between 2 different subjects. This is borderline impossible with pure prompting, because it tends to result in both characters having most or some of the characteristics from the prompt; they basically come out looking the same, but I need them to look very different.

My idea was to train a LORA on the OC so that I don't have to rely on a complex prompt, and possibly train a 2nd concept of a generic character in the same LORA.

Does anybody know if this will work? If I train 2 separate characters in 1 LORA, is it possible to make them interact without having the AI blend them together into similar looking characters?

I have some experience training LORA's on characters and concepts and I understand that it takes a decent amount of time to do. I don't want to waste my time attempting something that won't give me good results","[""You almost certainly want to do this with Flux Kontext. If you haven't looked into it yet, then this is the time to do so.\n\n[https://docs.bfl.ai/guides/prompting\\_guide\\_kontext\\_i2i#iterative-editing-with-prompts-while-keeping-character-consistency](https://docs.bfl.ai/guides/prompting_guide_kontext_i2i#iterative-editing-with-prompts-while-keeping-character-consistency) will give you some idea of what it can do."", ""Can someone tell me why this was downvoted? It's a sincere attempt to get advice, and I included all relevant information.  \nIf there is a way to make my post better, please let me know."", ""Why can't this be a composite image where you produce 2 separate images with the prompts for the two characters. Then take one character from one and put it in the other manually with a drawing program. The interaction can still be the same in both images. Like they are talking to their twin. But one will be manually replaced. Or am I missing something?"", 'This is ""easy"" with Adetailer.', 'regional prompting, inpainting, controlnet(s), adetailer, etc or some combination are usually necessary though some encoders and models make your prompts far more powerful than others.', ""Two characters in one lora is rarely worth it to me. Make the lora of just the OC. Generate an image of two characters using the OC lora - you'll get two copies of OC. Do an Img2Img of the result without the lora, using low guidance, or try a normal generation with a very specific prompt, or use a control net to lock the pose - basically you want the same image with different characters. Cut one character out, paste it over the character in your first image, and then run it through Img2Img with high guidance to clean it up.""]","[""You almost certainly want to do this with Flux Kontext. If you haven't looked into it yet, then this is the time to do so.\n\n[https://docs.bfl.ai/guides/prompting\\_guide\\_kontext\\_i2i#iterative-editing-with-prompts-while-keeping-character-consistency](https://docs.bfl.ai/guides/prompting_guide_kontext_i2i#iterative-editing-with-prompts-while-keeping-character-consistency) will give you some idea of what it can do."", ""Can someone tell me why this was downvoted? It's a sincere attempt to get advice, and I included all relevant information.  \nIf there is a way to make my post better, please let me know."", ""Why can't this be a composite image where you produce 2 separate images with the prompts for the two characters. Then take one character from one and put it in the other manually with a drawing program. The interaction can still be the same in both images. Like they are talking to their twin. But one will be manually replaced. Or am I missing something?"", 'This is ""easy"" with Adetailer.', 'regional prompting, inpainting, controlnet(s), adetailer, etc or some combination are usually necessary though some encoders and models make your prompts far more powerful than others.']",0,11,0.3,Question - Help,1751597400.0
1lr6u9y,StableDiffusion,"Hi, guys. I need help","Hi, I have these models and I am starting, I have already managed to make them work with Hire and I would like help to have specific styles for each model and not have to be changing. Can anyone help me? Also if you have info on the extensions and prompts I feel it's very confusing all that stuff. I want to know more info about styles archive.
","[""No idea what you're even asking. Like none of what you wrote makes a lick of sense."", 'The kind of help you need, this sub might not be able to provide.\n\njk no kink shaming. Goon away my friend! And share with the world!', ""The models have to be loaded on ram,.so unless you have a lot of ram, you won't have space for a lot of models anyway.\n\nYour best bet is to learn to use comfy so you have better control over which models are in memory ,or use a model that can handle more styles,like pony."", 'you can make template images with the model and Loras you want with a basic prompt, then save those images in a folder and rename them for better organization, then you can just drag and drop the image of the style you want to make into the PNG info section of Stable Difusion, and then transfer the prompt to the ""txt2img"" section, then you just have to change the model and re-write your prompt, the Lora and the rest of the configurations will be saved into the metadata of the image', ""Styles here can mean 2 things, but the answer is a no for both.\n\nIf you meant those styles:\n\nhttps://preview.redd.it/vuc05xspzvaf1.png?width=487&format=png&auto=webp&s=14cfab32bb4f0f5062ee863ff6a926c6674d4044\n\nThen I can say that you can't choose to preload them for specific models.\n\nIf you meant art styles, then the issue is that every model is pretty different from each other. Even models that are based on Illustrious would have have a different default style that influences all the other styles.  \nYou could use LoRA, but even that would still depend on the model."", 'I think you might be wanting to start with the standard ComfyUI workflow and create a separate workflow file for each model.\n\nBut I\'m not sure what you mean by other things. By ""styles archive"" do you mean CivitAI? Is ""Hire"" a brand name for some service?']","[""No idea what you're even asking. Like none of what you wrote makes a lick of sense."", 'The kind of help you need, this sub might not be able to provide.\n\njk no kink shaming. Goon away my friend! And share with the world!', ""The models have to be loaded on ram,.so unless you have a lot of ram, you won't have space for a lot of models anyway.\n\nYour best bet is to learn to use comfy so you have better control over which models are in memory ,or use a model that can handle more styles,like pony."", 'you can make template images with the model and Loras you want with a basic prompt, then save those images in a folder and rename them for better organization, then you can just drag and drop the image of the style you want to make into the PNG info section of Stable Difusion, and then transfer the prompt to the ""txt2img"" section, then you just have to change the model and re-write your prompt, the Lora and the rest of the configurations will be saved into the metadata of the image', ""Styles here can mean 2 things, but the answer is a no for both.\n\nIf you meant those styles:\n\nhttps://preview.redd.it/vuc05xspzvaf1.png?width=487&format=png&auto=webp&s=14cfab32bb4f0f5062ee863ff6a926c6674d4044\n\nThen I can say that you can't choose to preload them for specific models.\n\nIf you meant art styles, then the issue is that every model is pretty different from each other. Even models that are based on Illustrious would have have a different default style that influences all the other styles.  \nYou could use LoRA, but even that would still depend on the model.""]",0,7,0.19,Question - Help,1751593732.0
1lr6l3r,StableDiffusion,Made a guide for multi image references for Flux Kontext. Included prompt examples and workflow for what's worked for me so far,,"['great tutorial', ""these style changes are so inconsistent with kontext.\n\nIf the person is in odd poses or the outfit has complex design, It's hit and miss.""]","['great tutorial', ""these style changes are so inconsistent with kontext.\n\nIf the person is in odd poses or the outfit has complex design, It's hit and miss.""]",44,4,0.86,Tutorial - Guide,1751592939.0
1lr669z,StableDiffusion,Is there a tutorial for kindergartners?,"I am an absolute beginner to this and am interested in learning, but I have yet to find a decent tutorial aimed at a know-nothing audience. Sure, they show you how to collect the necessary pieces, but every tutorial I've found throws a million terms at you without explaining what each one means and especially not how they interconnect or build onto each other. It's like someone handing all the parts of an engine to a child and saying, ""Ok, go build a car now.""

Are there any tutorials that clearly state what every term/acronym they use means, what every button/slider/etc they click on does, and progresses through them in a logical order without assuming you know a million other things already?","['A lot of it is usually learned while practicing and by trial and error. There are also plenty of old and new articles. Although depending on UI, you don\'t even need to learn anything specific to how something works.\n\nIt sounds like you are talking about ComfyUI, so there is this still relevant [playlist ](https://youtube.com/playlist?list=PLcW1kbTO1uPhDecZWV_4TGNpys4ULv51D&si=bhYLaMakXctPxnYj)by Latent Vision. Despite its name of ""advanced understanding"", it starts off with explaining each node, basics, and what terms mean.', 'aistudio.google.com\n\nIt will walk you through everything. It is not 100%, but neither is the internet, and is way better than ChatGPT for this use IMHO.', 'In order to build a car, you need to understand the theory about how a car works, i.e., have a mental model with the right level of abstraction.  Same with using A.I. image generation and its tools.  Without a theory/mental model, you\'ll get lost very quickly.\n\nTo get you started on building this mental model, watch this youtube, which is about Diffusion model in general.  ""AI art, explained by Vox"". The actual explanation start at around 6:00 [https://youtu.be/SVcsDDABEkM?t=357](https://youtu.be/SVcsDDABEkM?t=357)\n\nI also wrote a couple old posts, which may or may not help you, but take a quick look if you want:\n\n[ELi5: What are SD models, and where to find them](https://new.reddit.com/r/StableDiffusion/comments/11s6485/eli5_what_are_sd_models_and_where_to_find_them/)\n\n[ELi5: Absolute beginner\'s guide to getting started in A.I. Image generation](https://new.reddit.com/r/StableDiffusion/comments/1b2mhjv/eli5_absolute_beginners_guide_to_getting_started/)', '- I wrote [this primer](https://civitai.com/articles/7492) a while back with the idea of giving people a ""jumping off"" point.\n- If you\'re interested in doing more manual editing with AI, the [Invoke Youtube channel](https://www.youtube.com/@invokeai) has some excellent design sessions.  Unfortunately, they\'re not structured into any sort of lesson plan so I\'d mostly consider them an intermediate level.  If you do decide to take a look:\n  - I\'d start with [this one](https://www.youtube.com/watch?v=SCqbx4r9NJM), as it\'s a fairly straightforward case that will give you a good overall idea of how Invoke works, even if you don\'t fully understand all the terms and tools yet.\n  - [Here\'s a video describing the fundamentals](https://www.youtube.com/watch?v=kzRL88ffv1o).\n  - [This is one of the best](https://www.youtube.com/watch?v=hRiz1rsESr0) IMO - it\'s long, but it does a good job of showing a beginning to end workflow.', 'NerdyRodent on YouTube has some fairly easy tutorials to follow.', ""Start with basic workflows,\n\n[https://comfyanonymous.github.io/ComfyUI\\_examples/](https://comfyanonymous.github.io/ComfyUI_examples/)\n\nTry to understand what's going on, and if you have any doubts, ask your trusted LLM. The basics are usually always the same: a text encoder, a VAE, a model, the prompt, an empty latent, and the k-sampler. The real theory behind what's going on can be really confusing. I mean, trying to understand the latent space usually gives me headaches. Don’t rush into animation, it's where everything comes together at once.\n\nThose of us who've been around for a while had the advantage of adapting to changes little by little, but also the downside of not having many resources to turn to when we had questions."", 'since you are a beginner, you need to start here: https://youtu.be/IIy3YwsXtTE?si=mufRrHmCRU1B3ZDw', ""You're a software engineer? Well I'm a chemical engineer (hello engineering comrade). I have been dabbling in this stuff for about 6 months. I have very basic programming skills and I struggled at first but chat gpt can be helpful in answering basic to intermediate questions.  You just have to dive head first into the deep end and piece your knowledge together as you go. Once you start generating and enjoy it you can't get enough of it. It scratches the technical side as well as the creative side which I love. The tools for AI generation are developing so incredibly fast that what you learned today will very likely be improved tomorrow."", 'You seem to have the background to follow [Coding Stable Diffusion from Scratch](https://www.youtube.com/watch?v=ZBKpAp_6TGI) by Umar Jamil. \n\nThis tutorial will help you to understand each involved part, how the text-to-mage, image-to-image, and in-painting pipelines work, and how to modularize and improve on it. It is not ComfyUI-specific, but it will enrich your knowledge on what, how and why the ComfyUI nodes are stitched together the way they are.\n\nThe first hour of the video explains the steps in the diffusion pipeline, which correlates to some of the ComfyUI nodes. The next couple of hours explains how to piece it step by step in Pytorch, which you could potentially just skip out on.', 'You\'ll probably appreciate [SwarmUI](http://github.com/mcmonkeyprojects/SwarmUI). You asked for clear info on `what every button/slider/etc they click on does` and Swarm literally has that - every single button and slider in the UI has a ""?"" button next to it with an explanation of what it is, examples of how to use it, and sometimes also links to further docs. It\'s designed to be so simple anyone can get into it, while still having the full feature set range you\'d expect of a pro toolkit, so that you can just stick with Swarm forever into the pro level.']","['A lot of it is usually learned while practicing and by trial and error. There are also plenty of old and new articles. Although depending on UI, you don\'t even need to learn anything specific to how something works.\n\nIt sounds like you are talking about ComfyUI, so there is this still relevant [playlist ](https://youtube.com/playlist?list=PLcW1kbTO1uPhDecZWV_4TGNpys4ULv51D&si=bhYLaMakXctPxnYj)by Latent Vision. Despite its name of ""advanced understanding"", it starts off with explaining each node, basics, and what terms mean.', 'aistudio.google.com\n\nIt will walk you through everything. It is not 100%, but neither is the internet, and is way better than ChatGPT for this use IMHO.', 'In order to build a car, you need to understand the theory about how a car works, i.e., have a mental model with the right level of abstraction.  Same with using A.I. image generation and its tools.  Without a theory/mental model, you\'ll get lost very quickly.\n\nTo get you started on building this mental model, watch this youtube, which is about Diffusion model in general.  ""AI art, explained by Vox"". The actual explanation start at around 6:00 [https://youtu.be/SVcsDDABEkM?t=357](https://youtu.be/SVcsDDABEkM?t=357)\n\nI also wrote a couple old posts, which may or may not help you, but take a quick look if you want:\n\n[ELi5: What are SD models, and where to find them](https://new.reddit.com/r/StableDiffusion/comments/11s6485/eli5_what_are_sd_models_and_where_to_find_them/)\n\n[ELi5: Absolute beginner\'s guide to getting started in A.I. Image generation](https://new.reddit.com/r/StableDiffusion/comments/1b2mhjv/eli5_absolute_beginners_guide_to_getting_started/)', '- I wrote [this primer](https://civitai.com/articles/7492) a while back with the idea of giving people a ""jumping off"" point.\n- If you\'re interested in doing more manual editing with AI, the [Invoke Youtube channel](https://www.youtube.com/@invokeai) has some excellent design sessions.  Unfortunately, they\'re not structured into any sort of lesson plan so I\'d mostly consider them an intermediate level.  If you do decide to take a look:\n  - I\'d start with [this one](https://www.youtube.com/watch?v=SCqbx4r9NJM), as it\'s a fairly straightforward case that will give you a good overall idea of how Invoke works, even if you don\'t fully understand all the terms and tools yet.\n  - [Here\'s a video describing the fundamentals](https://www.youtube.com/watch?v=kzRL88ffv1o).\n  - [This is one of the best](https://www.youtube.com/watch?v=hRiz1rsESr0) IMO - it\'s long, but it does a good job of showing a beginning to end workflow.', 'NerdyRodent on YouTube has some fairly easy tutorials to follow.']",2,49,0.54,Question - Help,1751591648.0
1lr5ynz,StableDiffusion,"Is it even possible to train character models with Flux Kontext, and if it is, could it behave like Flux Fill without needing inpainting masks?","If I trained a Kontext model using images of myself and separate images of just the background, could I then simply instruct Kontext to ‘Place ohwx man in the middle of this photo,’ and it would insert me directly without needing to deal with inpainting masks anymore? ","['Which software is used to train flux kontext? I need to add some concepts that are missing.', 'That might work. \n\nBut please stop using ""ohwx"", that\'s definitely not a rare token for Flux. And repeating it just makes people think it\'s important although it is not.', 'Honestly, you probably don\'t even need a lora.  Just input both images and say ""put this man in this scene"" or whatever.  Alternatively, crudely copy and paste the image yourself and describe the result you want.  This seems to give you a little more control over fine placement.', 'My first attempt is much worse than a dev Lora. I converted the end images to black and white pencil sketches and then trained it with the prompt “convert these sketches into realistic images”. The image quality is certainly better than flux dev, but the character is basically unusable.\xa0']","['Which software is used to train flux kontext? I need to add some concepts that are missing.', 'That might work. \n\nBut please stop using ""ohwx"", that\'s definitely not a rare token for Flux. And repeating it just makes people think it\'s important although it is not.', 'Honestly, you probably don\'t even need a lora.  Just input both images and say ""put this man in this scene"" or whatever.  Alternatively, crudely copy and paste the image yourself and describe the result you want.  This seems to give you a little more control over fine placement.', 'My first attempt is much worse than a dev Lora. I converted the end images to black and white pencil sketches and then trained it with the prompt “convert these sketches into realistic images”. The image quality is certainly better than flux dev, but the character is basically unusable.\xa0']",0,6,0.29,Question - Help,1751591004.0
1lr4g0h,StableDiffusion,Control so slow at 4060 Mobil,"It used to be fast But now I installed again and when I try to generate image with controlnet it generating forever also the same problem for batch count images
It rise up to 8 10 20 30Min for 2 image at the same time I tried every solution anyone can help me ?","[""Controlnets increases VRAM requirements and if you exceed the available VRAM, it'll fall back on system RAM, which is going to cause the whole process to slow down dramatically."", ""You probably had some sort of optimization scheme in place previously.  Most likely culprits would be --xformers or --lowvram / --medvram if you're using a webui.  Or it could be that you previously used inference settings that would be more performant (lower resolution followed by an upscaling pass, for example, or number of steps or whatever).\n\nThere are a million possibilities and some are verrrry subtle (like driver changes that would cause verrrrry slow generation now when they would've previously just failed with an out of memory error).\n\n> the same problem for batch count images It rise up to 8 10 20 30Min\n\nNot in front of a ui atm and can't quite recall the verbiage they use, but one of the options increases the number of batches and the other increases the number of images in each batch.  The latter option has a powerful impact on memory use and it sounds like your performance issues are already bad enough without straining them further.""]","[""Controlnets increases VRAM requirements and if you exceed the available VRAM, it'll fall back on system RAM, which is going to cause the whole process to slow down dramatically."", ""You probably had some sort of optimization scheme in place previously.  Most likely culprits would be --xformers or --lowvram / --medvram if you're using a webui.  Or it could be that you previously used inference settings that would be more performant (lower resolution followed by an upscaling pass, for example, or number of steps or whatever).\n\nThere are a million possibilities and some are verrrry subtle (like driver changes that would cause verrrrry slow generation now when they would've previously just failed with an out of memory error).\n\n> the same problem for batch count images It rise up to 8 10 20 30Min\n\nNot in front of a ui atm and can't quite recall the verbiage they use, but one of the options increases the number of batches and the other increases the number of images in each batch.  The latter option has a powerful impact on memory use and it sounds like your performance issues are already bad enough without straining them further.""]",0,4,0.4,Discussion,1751586381.0
1lr4eem,StableDiffusion,wan2.1 vace q3 gguf 3sec vid took 2430 sec on my 16gb vram,I use the workflow from pixaroma it took 2430 sec. Is this normal what should i do?,"[""It is convenient to specify your GPU.\n\nAnd that resolution is kinda demand, i think it is starting to enter in high end GPUs performance. I can tell you that I take around 10 min for a 480x832 with 53 frames in a RX6800 (ZLUDA) going to that resolution, I think I did once and was getting around 1 hour and 1 hour and an half ( did let it finished since it was only a test )\n\nHere is my recent one (testing torchcompile in my environment):\n\n    Detected model in_channels: 16\n    Model type: t2v, num_heads: 40, num_layers: 40\n    Model variant detected: 14B\n    model_type FLOW\n    Using accelerate to load and assign model weights to device...\n    Loading transformer parameters to cpu: 100%|█████████████████████████████████████| 1095/1095 [00:00<00:00, 1560.12it/s]\n    Loading LoRA: Wan\\Wan21_T2V_14B_lightx2v_cfg_step_distill_lora_rank32 with strength: 1.0\n    Loading LoRA: Wan\\BoobSlider_WAN14_V1 with strength: 0.30000000000000016\n    Loading LoRA: Wan\\BoobPhysics_WAN_v7 with strength: 0.8000000000000004\n    Loading LoRA: Wan\\nsfw_lora_wan_14b_e5 with strength: 1.0\n    Loading model and applying LoRA weights:: 100%|██████████████████████████████████████| 607/607 [02:11<00:00,  4.62it/s]\n    timesteps: tensor([999, 749, 499, 249], device='cuda:0')\n    Seq len: 21840\n    Swapping 20 transformer blocks\n    Initializing block swap: 100%|█████████████████████████████████████████████████████████| 40/40 [00:05<00:00,  7.35it/s]\n    ----------------------\n    Block swap memory summary:\n    Transformer blocks on cpu: 6704.63MB\n    Transformer blocks on cuda:0: 6704.63MB\n    Total memory used by transformer blocks: 13409.26MB\n    Non-blocking memory transfer: False\n    ----------------------\n    Sampling 53 frames at 480x832 with 4 steps\n    100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [10:24<00:00, 156.23s/it]\n    Allocated memory: memory=0.055 GB\n    Max allocated memory: max_memory=11.149 GB\n    Max reserved memory: max_reserved=12.830 GB\n    VAE decoding: 100%|████████████████████████████████████████████████████████████████████| 15/15 [01:03<00:00,  4.20s/it]\n    Prompt executed in 00:14:43""]","[""It is convenient to specify your GPU.\n\nAnd that resolution is kinda demand, i think it is starting to enter in high end GPUs performance. I can tell you that I take around 10 min for a 480x832 with 53 frames in a RX6800 (ZLUDA) going to that resolution, I think I did once and was getting around 1 hour and 1 hour and an half ( did let it finished since it was only a test )\n\nHere is my recent one (testing torchcompile in my environment):\n\n    Detected model in_channels: 16\n    Model type: t2v, num_heads: 40, num_layers: 40\n    Model variant detected: 14B\n    model_type FLOW\n    Using accelerate to load and assign model weights to device...\n    Loading transformer parameters to cpu: 100%|█████████████████████████████████████| 1095/1095 [00:00<00:00, 1560.12it/s]\n    Loading LoRA: Wan\\Wan21_T2V_14B_lightx2v_cfg_step_distill_lora_rank32 with strength: 1.0\n    Loading LoRA: Wan\\BoobSlider_WAN14_V1 with strength: 0.30000000000000016\n    Loading LoRA: Wan\\BoobPhysics_WAN_v7 with strength: 0.8000000000000004\n    Loading LoRA: Wan\\nsfw_lora_wan_14b_e5 with strength: 1.0\n    Loading model and applying LoRA weights:: 100%|██████████████████████████████████████| 607/607 [02:11<00:00,  4.62it/s]\n    timesteps: tensor([999, 749, 499, 249], device='cuda:0')\n    Seq len: 21840\n    Swapping 20 transformer blocks\n    Initializing block swap: 100%|█████████████████████████████████████████████████████████| 40/40 [00:05<00:00,  7.35it/s]\n    ----------------------\n    Block swap memory summary:\n    Transformer blocks on cpu: 6704.63MB\n    Transformer blocks on cuda:0: 6704.63MB\n    Total memory used by transformer blocks: 13409.26MB\n    Non-blocking memory transfer: False\n    ----------------------\n    Sampling 53 frames at 480x832 with 4 steps\n    100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [10:24<00:00, 156.23s/it]\n    Allocated memory: memory=0.055 GB\n    Max allocated memory: max_memory=11.149 GB\n    Max reserved memory: max_reserved=12.830 GB\n    VAE decoding: 100%|████████████████████████████████████████████████████████████████████| 15/15 [01:03<00:00,  4.20s/it]\n    Prompt executed in 00:14:43""]",1,1,1.0,Question - Help,1751586252.0
1lr46ug,StableDiffusion,Kontext. Do you think the model has potential ? Can Loras improve style transfer ? And the traditional problem of Flux plastic skin ?,,"[""You're not going to get better quality images with Kontext - it's a convenience play, it saves manually inpainting.  Loras will always be better but Kontext covers the use cases where there isn't a Lora or you want a quick fix."", 'Try the prompt ""add realistic skin details"" in kontext with a flux image. Thank me later', 'The Flux models and its censorship can go to hell!', ""off course LoRA can improve styles.\n\nhttps://preview.redd.it/j0313lse4taf1.png?width=1019&format=png&auto=webp&s=f60890504461da7b7c288160788ef7067b0954b6\n\nThe model has a huge editing potential and I'd say that is SOTA for local generation alongside Chroma."", '>Kontext. Do you think the model has potential ? \n\nYes, of course.  \n\n\n>Can Loras improve style transfer? \n\nYes, you will always get a better a style if you train a LoRA for the model.\n\n>And the traditional problem of Flux plastic skin ? \n\nYes', 'Nice style btw, how were those generated ? Lora or Kontext then ?', 'What was the prompt here? - ""Transform this image into a color pencil drawing of Rocky boxers with liberal THE SIMPSONS skin color applied.""\n\nWhy can\'t it do art transfers without making everyone orange?', 'Yes x3']","[""You're not going to get better quality images with Kontext - it's a convenience play, it saves manually inpainting.  Loras will always be better but Kontext covers the use cases where there isn't a Lora or you want a quick fix."", 'Try the prompt ""add realistic skin details"" in kontext with a flux image. Thank me later', 'The Flux models and its censorship can go to hell!', ""off course LoRA can improve styles.\n\nhttps://preview.redd.it/j0313lse4taf1.png?width=1019&format=png&auto=webp&s=f60890504461da7b7c288160788ef7067b0954b6\n\nThe model has a huge editing potential and I'd say that is SOTA for local generation alongside Chroma."", '>Kontext. Do you think the model has potential ? \n\nYes, of course.  \n\n\n>Can Loras improve style transfer? \n\nYes, you will always get a better a style if you train a LoRA for the model.\n\n>And the traditional problem of Flux plastic skin ? \n\nYes']",26,21,0.7,Discussion,1751585635.0
1lr3qkm,StableDiffusion,Jib Mix Realistic XL - v18.0 Skin Supreme - Showcase,"This version has better skin details and photorealism (while still being flexible with art styles)

For download/generation or to see more images or prompts: [https://civitai.com/models/194768/jib-mix-realistic-xl](https://civitai.com/models/194768/jib-mix-realistic-xl)","['  \nJib Mix SDXL V17 vs V18 skin comparison (no upscale used) \n\nhttps://preview.redd.it/chl0zne5tqaf1.png?width=2108&format=png&auto=webp&s=82f1150e657f0c2aa4c2610c4bba0a2816e5ef80\n\n[https://civitai.com/images/85837708](https://civitai.com/images/85837708)', 'https://i.redd.it/6qzeul9ahraf1.gif', 'JibMix is my favorite in all versions.\n- SDXL\n- Flux\n- Pony\n\nThank you for these great checkpoints and your continued work!', ' Good. What is the workflow to get those detailed results?', 'Hey can you post the workflow please', 'Great stuff as usual! Thanks, Jib!', 'Can i use this checkpoint on A1111?', 'Just checked some of prompts, jeeez man, you for sure love negative prompting.\n\nHave you ever try to.. not use it at all?', ""After JibMix v17, I found the new version (v18) to be burning my generations. Every other image comes out oversaturated and with excessive contrast; it looks like a too high CFG was applied, only I'm using my normal workflow that works just fine with all other models including Jib Mix v17. There is something wrong with v18 - maybe a DMD2 LoRA merged in or something like that. For now, I rolled back to v17, which does not exhibit that behavior.""]","['  \nJib Mix SDXL V17 vs V18 skin comparison (no upscale used) \n\nhttps://preview.redd.it/chl0zne5tqaf1.png?width=2108&format=png&auto=webp&s=82f1150e657f0c2aa4c2610c4bba0a2816e5ef80\n\n[https://civitai.com/images/85837708](https://civitai.com/images/85837708)', 'https://i.redd.it/6qzeul9ahraf1.gif', 'JibMix is my favorite in all versions.\n- SDXL\n- Flux\n- Pony\n\nThank you for these great checkpoints and your continued work!', ' Good. What is the workflow to get those detailed results?', 'Hey can you post the workflow please']",117,41,0.87,Resource - Update,1751584360.0
1lr27y1,StableDiffusion,Swarm UI not sticking to image input for video,I'm trying to get swarmui to stay close to the image input for video but I'm not getting anything similar to the actual image. I've tried wan2 and some others but always the same. What am I doing wrong?,"['Init image creativity needs to be 0', ""with the Wan Vace workflow I use it stays close to the image if the CFG is 1. It's not bad if it is 2, but basically starts to lose the likeness of the person at higher values, but then though doesn't adhere to the prompt so well. That is my experience anyway.\n\nYou can have a high CFG, get it to do what you want and then use that video as the control video with a low CFG to get the character looking more like the image.""]","['Init image creativity needs to be 0', ""with the Wan Vace workflow I use it stays close to the image if the CFG is 1. It's not bad if it is 2, but basically starts to lose the likeness of the person at higher values, but then though doesn't adhere to the prompt so well. That is my experience anyway.\n\nYou can have a high CFG, get it to do what you want and then use that video as the control video with a low CFG to get the character looking more like the image.""]",0,2,0.5,Question - Help,1751580198.0
1lr26n2,StableDiffusion,"what am I doing wrong... same LORA, same prompt, I'm using a pretty basic workflow but why is the difference so huge",,"[""That's life bro, there is always expectation vs reality.\n\nhttps://preview.redd.it/i50qn44t5raf1.png?width=1250&format=png&auto=webp&s=023d775ae3ba64725bac080874e61717ea9cff60"", 'Not gonna lie - the second picture made me chuckle. \n\nBut to provide any support we require some more information on workflow and nodes you used.', ""Do you really want to know what you're doing wrong without showing what you're doing or what you should be doing?"", 'Okay, that second picture is just hilariously bad. lol', 'Check sampler, scheduler, number of steps and checkpoint if there is any specific one to be used.  \nThe scheduler + sampler and number of steps may be a big difference.', ""Should've attached link to LoRA, maybe it has answers to your questions. Since there are just too many possibilities of why you have a different output."", 'Even same checkpoint?', 'haha bro, you made me laught with the comparision of the second picture']","[""That's life bro, there is always expectation vs reality.\n\nhttps://preview.redd.it/i50qn44t5raf1.png?width=1250&format=png&auto=webp&s=023d775ae3ba64725bac080874e61717ea9cff60"", 'Not gonna lie - the second picture made me chuckle. \n\nBut to provide any support we require some more information on workflow and nodes you used.', ""Do you really want to know what you're doing wrong without showing what you're doing or what you should be doing?"", 'Okay, that second picture is just hilariously bad. lol', 'Check sampler, scheduler, number of steps and checkpoint if there is any specific one to be used.  \nThe scheduler + sampler and number of steps may be a big difference.']",0,17,0.36,Question - Help,1751580098.0
1lr1pk7,StableDiffusion,Linux add icon?,"I’m trying to swap to Linux but it refuses to let me set an icon for any app that’s not coming from the mint store or installer packages that aren’t app images.

I’ve been trying all day. Ive followed all the advice I could find online and tried ChatGPT and Claude.

I made a shortcut and edited the .desktop file. I tried including the wm class in that file. I tried using AppImageLauncher.

Nothing works. The best luck has been with AppImageLauncher. It at least made an icon that I can search in my menu and pin to panel, but clicking it opens different window on my panel which I cannot pin to my panel.

This is driving me crazy. ","['\\- Alt+Drag executable to desktop => link here.\n\n\\- Right click desktop => Create launcher.\n\nProbably there are other ways too.', 'Post the contents of the desktop file.', ""You can make launchers in Linux mint on the desktop by making a text file with the right stuff in it. You need to talk to deepseek but that's how I have comfy set up with an icon I downloaded pointing to the folder and launching like it's in the folder. Deepseek knows how to make these and will walk you through it easy and tell you how to make it executable either right click properties check the box at the bottom or chmod +x filename.desktop\xa0\n\n\nDeepseek is your best friend in Linux it was trained on all the man pages and knows how to write bash for your scripts. You'll get the hang of it soon don't give up. I'm completely off windows now, even hacked windows software runs on Linux without worry. You just gotta learn wine next hah.\xa0""]","['\\- Alt+Drag executable to desktop => link here.\n\n\\- Right click desktop => Create launcher.\n\nProbably there are other ways too.', 'Post the contents of the desktop file.', ""You can make launchers in Linux mint on the desktop by making a text file with the right stuff in it. You need to talk to deepseek but that's how I have comfy set up with an icon I downloaded pointing to the folder and launching like it's in the folder. Deepseek knows how to make these and will walk you through it easy and tell you how to make it executable either right click properties check the box at the bottom or chmod +x filename.desktop\xa0\n\n\nDeepseek is your best friend in Linux it was trained on all the man pages and knows how to write bash for your scripts. You'll get the hang of it soon don't give up. I'm completely off windows now, even hacked windows software runs on Linux without worry. You just gotta learn wine next hah.\xa0""]",0,14,0.5,Question - Help,1751578854.0
1lr1e2i,StableDiffusion,Working on creating a fully automated AI instagram account using n8n.,"Mainly wondering what needs improvement? Also if anyone can point me into the right direction for gaining followers, using just automation that would be great!","['Why would anyone do this? What’s the purpose?', 'Boo this man!', 'Annying cat ears', 'your work has decent quality, but man are you being shadow-banned ? just 2 followers ?']","['Why would anyone do this? What’s the purpose?', 'Boo this man!', 'Annying cat ears', 'your work has decent quality, but man are you being shadow-banned ? just 2 followers ?']",0,11,0.27,Question - Help,1751578016.0
1lr0gsm,StableDiffusion,Voice cloning / TTS generation for other languages?,"Are there open source tools to clone a voice for languages besides English and French?

I’d be looking for German at the moment, but maybe there are more languages that can be done.

Thanks.","['I\'d check huggingface TTS model list filtered by language and sort by ""Trending"" / ""Most downloads"" / ""Recently updated"" to get overview of the models that may\'ve been missed:\n\nhttps://huggingface.co/models?pipeline_tag=text-to-speech&language=de&sort=trending\n\nAnd maybe try Kartoffel models from https://huggingface.co/SebastianBodza\n\nFor example Orpheus based one? https://huggingface.co/SebastianBodza/Kartoffel_Orpheus-3B_german_natural-v0.1\n\nAs I\'m not fluent in German myself I\'ve been looking for training new language / finetuning process, not for immediate output quality when I\'ve made this search.\n\nAlso, never looked at voice cloning so don\'t know how to quickly find models capable of that.', 'Coqui-TTS supports 17 languages  \n[https://coqui-tts.readthedocs.io/en/latest/models/xtts.html](https://coqui-tts.readthedocs.io/en/latest/models/xtts.html)  \nIt just needs a 6 second voice clip (with no background noise) to clone a voice', ""I'm also looking for non-english tts, I found this one for now https://huggingface.co/spaces/coqui/xtts - not great, but at least works.\n\nFollowing this post for more recommendations..."", 'There are two F5TTS models for German. Eines klingt garnicht schlecht.']","['I\'d check huggingface TTS model list filtered by language and sort by ""Trending"" / ""Most downloads"" / ""Recently updated"" to get overview of the models that may\'ve been missed:\n\nhttps://huggingface.co/models?pipeline_tag=text-to-speech&language=de&sort=trending\n\nAnd maybe try Kartoffel models from https://huggingface.co/SebastianBodza\n\nFor example Orpheus based one? https://huggingface.co/SebastianBodza/Kartoffel_Orpheus-3B_german_natural-v0.1\n\nAs I\'m not fluent in German myself I\'ve been looking for training new language / finetuning process, not for immediate output quality when I\'ve made this search.\n\nAlso, never looked at voice cloning so don\'t know how to quickly find models capable of that.', 'Coqui-TTS supports 17 languages  \n[https://coqui-tts.readthedocs.io/en/latest/models/xtts.html](https://coqui-tts.readthedocs.io/en/latest/models/xtts.html)  \nIt just needs a 6 second voice clip (with no background noise) to clone a voice', ""I'm also looking for non-english tts, I found this one for now https://huggingface.co/spaces/coqui/xtts - not great, but at least works.\n\nFollowing this post for more recommendations..."", 'There are two F5TTS models for German. Eines klingt garnicht schlecht.']",7,4,0.89,Question - Help,1751575694.0
1lqzvge,StableDiffusion,Help! Advice on character art development for book,"Not sure if I'm in the right place. I am an author of erotica and I need to create images of my characters. I don't want any actual nudity, but very suggestive. I want to develop the character images and be able to reuse the same people, but change outfits, poses, and backgrounds. What AI service should I be using? I've tried and paid for [Leanordo.ai](http://Leanordo.ai) and they are so dang strict. I have tried Civitai and I like the female character's look, but I can't figure out for the life of me how to make the changes I described above. ANY advice is appreciated. ","[""reusing characters or styles - you'd be needing to look into training Loras, and its a big learning curve if you havent used Comfyui a lot.\n\nThe recently released Flux Kontext model (dev on comfyui or pro if you pay for it somewhere) might be better where you can use the examples you pass in as reference but its got its issues too.""]","[""reusing characters or styles - you'd be needing to look into training Loras, and its a big learning curve if you havent used Comfyui a lot.\n\nThe recently released Flux Kontext model (dev on comfyui or pro if you pay for it somewhere) might be better where you can use the examples you pass in as reference but its got its issues too.""]",1,3,0.6,Question - Help,1751574204.0
1lqzora,StableDiffusion,Which programms can compress sdxl weights likes koboldcpp?,"Koboldcpp can supress weights shrinking a full 6.7gb safetensor from civitai into only 3.6 gb for 1024x1024 making the models run decently on my 6gb card and even on my steam deck.

For the most part the quality is 90%-95% of the original atleast when I compare it to the same settings and prompts on cívitai.

The problem is that koboldcpp is mainly focused on llm usage with sdxl being just a nice side feature and therefore limited in customization.

No high res fix, no upscaler no refiner.

So I am looking for another UI that has weight compression as a feature to safe vram.

I know you can use gguf in some of them but many of the popular models have only outdated gguf files online from much earlier versions and trying to compress them myself into gguf has failed me. (what do you use if you can't find the gguf version online)

Sadly I cannot seperatly safe the compressed model in koboldcpp.

Alternativly some other programm with which you could refine/upscale images in 6gb vram would be nice as well.

I currently have invoke,forge,krita and comfiui installed.

The refiner in forge is currently in maintenance and krita seems to just upscale the images.","['I do have this: https://github.com/Haoming02/sd-webui-compressor\n\nWhich converts the UNET component from **fp16** to **fp8**, saves ~2 GB for SDXL models.', 'I believe SD.Next offers several on-the-fly quantization methods', 'There are 0 problems with running SDXL on 6 Gb GPU with comfyui. Speed is decent too. Of course if you have nvidia.\n\nIf you **really** want it you can load model in fp8 (run comfy with --help and look for fp8 command-line options) and then save it reducing size by 50%. But why? Speed will be about the same but quality will be much worse.', 'I use stable-diffusion.cpp to generate GGUFs. [https://github.com/leejet/stable-diffusion.cpp/blob/master/docs/quantization\\_and\\_gguf.md](https://github.com/leejet/stable-diffusion.cpp/blob/master/docs/quantization_and_gguf.md)']","['I do have this: https://github.com/Haoming02/sd-webui-compressor\n\nWhich converts the UNET component from **fp16** to **fp8**, saves ~2 GB for SDXL models.', 'I believe SD.Next offers several on-the-fly quantization methods', 'There are 0 problems with running SDXL on 6 Gb GPU with comfyui. Speed is decent too. Of course if you have nvidia.\n\nIf you **really** want it you can load model in fp8 (run comfy with --help and look for fp8 command-line options) and then save it reducing size by 50%. But why? Speed will be about the same but quality will be much worse.', 'I use stable-diffusion.cpp to generate GGUFs. [https://github.com/leejet/stable-diffusion.cpp/blob/master/docs/quantization\\_and\\_gguf.md](https://github.com/leejet/stable-diffusion.cpp/blob/master/docs/quantization_and_gguf.md)']",0,8,0.5,Question - Help,1751573735.0
1lqzo0a,StableDiffusion,"""Forgotten Models"" Series: Cosmos 2 2b + SD 3.5 M Turbo as Refiner.",,"[""Using a refiner for illustrious, SDXL, Pony and NAI has worked well for me. I'll try the SD3.5 M Turbo as well. Cosmos P2 is really good. Really really good."", ""Didn't Cosmos predict2 launch like last month? Forgotten?"", ""It's worth noting that city96 offers GGUF versions of the 14b of this to suit any memory footprint needed. [https://huggingface.co/city96/Cosmos-Predict2-14B-Text2Image-gguf/tree/main](https://huggingface.co/city96/Cosmos-Predict2-14B-Text2Image-gguf/tree/main)"", 'Prompts: [https://civitai.com/posts/19105576](https://civitai.com/posts/19105576)', '6th and 12th one are really awesome !!', ""That's a wonderful image!\n\nI believe there are many combinations where even small models can produce great results, so I hope various models will be revisited and continue to evolve."", ""These look pretty great. I hadn't heard of cosmos before this point, gave it a try. Are the gens supposed to come out looking poorly upscaled and jpeg artifacty-lossy? I don't think I'm doing anything wrong (using basic comfyui workflow 35 steps, euler karras, though i've tried most combinations). They do look good passed through sdxl after. The gen time is brutal compared to sdxl and a fast lora though (1 minute for a 1024 on a 3060)."", 'sorry for been to late, but for what ComfyUi version that workflow was made? I have [**ComfyUI 0.3.43**](https://github.com/comfyanonymous/ComfyUI) and your workflow still says that\n\nUNETLoader\n\nERROR: Could not detect model type of: d:\\\\models\\\\unet\\\\Cosmos\\\\Cosmos-Predict2-2B-t2i.safetensors', ""where's the workflow?"", 'Cosmos blows and SD3.5 died of a shit license.']","[""Using a refiner for illustrious, SDXL, Pony and NAI has worked well for me. I'll try the SD3.5 M Turbo as well. Cosmos P2 is really good. Really really good."", ""Didn't Cosmos predict2 launch like last month? Forgotten?"", ""It's worth noting that city96 offers GGUF versions of the 14b of this to suit any memory footprint needed. [https://huggingface.co/city96/Cosmos-Predict2-14B-Text2Image-gguf/tree/main](https://huggingface.co/city96/Cosmos-Predict2-14B-Text2Image-gguf/tree/main)"", 'Prompts: [https://civitai.com/posts/19105576](https://civitai.com/posts/19105576)', '6th and 12th one are really awesome !!']",65,22,0.88,Workflow Included,1751573682.0
1lqymtd,StableDiffusion,_Cheyenne_2.4 ( hyper illustration ) update // SDXL model for Comics Lovers / Link in description,,"[""# About this version : [https://civitai.com/models/198051/cheyenne?modelVersionId=1969455](https://civitai.com/models/198051/cheyenne?modelVersionId=1969455)\n\n**IMPORTANT : Use Skip:-2 in ComfyUI because I merge a bit of a Illustrious model !! ( Thanks Marc for feedback!)**\n\nAfter more than 300 fusions, I wanted to propose a new paradigm for my *Cheyenne*SDXL model: hyper illustration. It's not perfect, but it gives me new horizons and the freedom to explore different styles... I hope you enjoy it. If you like it, please follow me on [**X**](https://x.com/AAurety) and/or [**KOFI**](https://ko-fi.com/aurety).. More versions to come."", ""I'm your biggest fan."", ""As someone who got into Stable Diffusion for its artistic possibilities, I can't thank you enough for your work. I personally don't care much for photographic realism, and even though manga/anime is of course an artistic style in itself, I get a little frustrated trying to detect if an Illustrious/Pony model has anything special or new to offer on that front.\n\nAs far as I'm concerned, Cheyenne is one of the few models I find actually fun when it comes to experimentation."", 'A new Cheyenne version! Great, I always recomended your models. Thank you!', 'Plz put it on tensor.rt  so we can try it online', 'Ooo this is nice! Can you upload this on Yodayo/Moescape too please? 🙏', 'https://preview.redd.it/4034q8q7gqaf1.jpeg?width=2688&format=pjpg&auto=webp&s=4d7b020e9e26c8026dfcf77efe4db9b41bba8f12\n\nPretty cool.', 'I like to mix artist does this model have more artists recognition than bases sdxl?']","[""# About this version : [https://civitai.com/models/198051/cheyenne?modelVersionId=1969455](https://civitai.com/models/198051/cheyenne?modelVersionId=1969455)\n\n**IMPORTANT : Use Skip:-2 in ComfyUI because I merge a bit of a Illustrious model !! ( Thanks Marc for feedback!)**\n\nAfter more than 300 fusions, I wanted to propose a new paradigm for my *Cheyenne*SDXL model: hyper illustration. It's not perfect, but it gives me new horizons and the freedom to explore different styles... I hope you enjoy it. If you like it, please follow me on [**X**](https://x.com/AAurety) and/or [**KOFI**](https://ko-fi.com/aurety).. More versions to come."", ""I'm your biggest fan."", ""As someone who got into Stable Diffusion for its artistic possibilities, I can't thank you enough for your work. I personally don't care much for photographic realism, and even though manga/anime is of course an artistic style in itself, I get a little frustrated trying to detect if an Illustrious/Pony model has anything special or new to offer on that front.\n\nAs far as I'm concerned, Cheyenne is one of the few models I find actually fun when it comes to experimentation."", 'A new Cheyenne version! Great, I always recomended your models. Thank you!', 'Plz put it on tensor.rt  so we can try it online']",80,12,0.9,Resource - Update,1751571145.0
1lqyd1a,StableDiffusion,"Kyutai TTS is here: Real-time, voice-cloning, ultra-low-latency TTS, Robust Longform generation","https://preview.redd.it/46c2vbkrkpaf1.png?width=680&format=png&auto=webp&s=a074dd8ac462e5276d42be14dd98e4b1700f67fd

Kyutai has open-sourced Kyutai TTS — a new real-time text-to-speech model that’s packed with features and ready to shake things up in the world of TTS.

It’s super fast, starting to generate audio in just \~220ms after getting the first bit of text. Unlike most “streaming” TTS models out there, it doesn’t need the whole text upfront — it works as you type or as an LLM generates text, making it perfect for live interactions.

They havent released the voice embedding model that can clone voices though.

And yes — it handles long sentences or paragraphs without breaking a sweat, going well beyond the usual 30-second limit most models struggle with.

Github: [https://github.com/kyutai-labs/delayed-streams-modeling/](https://github.com/kyutai-labs/delayed-streams-modeling/)
Huggingface: [https://huggingface.co/kyutai/tts-1.6b-en\_fr](https://huggingface.co/kyutai/tts-1.6b-en_fr)
[https://kyutai.org/next/tts](https://kyutai.org/next/tts)","['""You can also clone voices with just 10 seconds of audio."" no, you can\'t, because they kept that to themselves', ""> To ensure people's voices are only cloned consensually, we do not release the voice embedding model directly. Instead, we provide a repository of voices based on samples from datasets such as Expresso and VCTK. You can help us add more voices by anonymously donating your voice."", 'It’s here\n\nExcept it isn’t\n\nAnd you can’t use it how you want', 'Chatterbox lets you clone any voice you want.', 'Will give this a look later. Right now Chatterbox TTS Extended is my go to.', ""Their own page has unconsential clones. They're withholding it to sell it, obviously. The token open weight release is so people will promote their product."", ""I think ill passs without voice cloning its not use to me.  I like to make audiobooks with my favorite narrators, and I can't do that with this."", 'Why is this even here? Without voice cloning, this is effectively a useless toy. No one cares about the default voices, get this shit out of here.', 'After reading this thread I realized there are TTS like kokoro, chatterbox, big fish, Dia, etc. Can anyone who used them tell the pros and cons of each please?', 'Another one private model']","['""You can also clone voices with just 10 seconds of audio."" no, you can\'t, because they kept that to themselves', ""> To ensure people's voices are only cloned consensually, we do not release the voice embedding model directly. Instead, we provide a repository of voices based on samples from datasets such as Expresso and VCTK. You can help us add more voices by anonymously donating your voice."", 'It’s here\n\nExcept it isn’t\n\nAnd you can’t use it how you want', 'Chatterbox lets you clone any voice you want.', 'Will give this a look later. Right now Chatterbox TTS Extended is my go to.']",273,96,0.8,News,1751570482.0
1lqyavx,StableDiffusion,"Trying to generate LoRA on my local system, but I have some questions?","Hi everyone, I'm trying to get into doing my own training for LoRA with my computer. I have a few questions, but I will try to keep things streamlined. First, I'll include my system specifications as they might be related to any recommendations that you have. Do you think that this is good enough for training SDXL LoRAs?

* GPU: 4070ti Super 16GB VRAM
* RAM: 48GB DDR4 3200mhz
* Non-Volatile Storage: 2TB NVMe SSD
* CPU: i7-12700F

My experience thus far has been mostly with the CivitAI generator, though I have some degree of success during local training. I had a few questions about the training process in general, partly because it is clear that I have a misunderstanding somewhere along the line.

**If there's a good community that you can link me to for getting into local LoRA training, I would appreciate it!** Thus far, I have mainly been referencing GitHub documentation/wikis. I have some experience coding, but I have no experience in data science or the mathematical theories/formulas that are generally used. I feel like that is making it difficult for me to understand some of the terminology and concepts surrounding how Stable Diffusion works. Do you feel like a deeper knowledge of data science is necessary for making good LoRAs?

Aside from that, here's a few questions that I have if you would like to read. These are probably trivial questions that I can find/derive an answer for if I read more into the documentation or concepts.

1) I did LoRA training with the CivitAI generator, and it produced a good style LoRA. I copied its training parameters (Kohya engine) to OneTrainer and used the same dataset. It didn't seem to get anywhere as close in fidelity or quality with the same number of steps/repeats/Epochs. I had to almost quadruple the total step count (716 -> 2600) to get anywhere close to replicating the style, but even then it looked poor.

2) I also copied CivitAI's training parameters over to Kohya, with the same dataset, but I immediately got an OOM either because I set it up incorrectly or my system just can't handle it. I am going to guess it is mostly the latter but also a bit of the former. Some recommendations that I found said reducing batch size, changing latent caching mode, and decreasing resolution might help? I'm just surprised since I can do a batch size of 4 in OneTrainer without issue; my guess here is that I borked something in the setup phase.

3) How does one determine things like the number of steps, repeats, Epochs, network rank, and network alpha? In my research, everything seems to indicate that these are going to relate to the type of LoRA (style, concept, character, etc.) as well as the size/quality of your dataset. Most tutorials that I find about LoRA training usually say ""just use a preconfigured config and have a good dataset"". I think that only takes you so far, and that's why I am asking. Is most of this just tedious guesswork, or is there are smarter/more streamlined approach to finding good parameters?

Thanks for any answers that you are willing to provide, and I apologize as I am sure that these questions are common knowledge to the community.",[],[],1,0,1.0,Question - Help,1751570334.0
1lqx3qt,StableDiffusion,Where am I going wrong with Flux Kontext?,"I'm getting mixed results with Kontext (FP8 scaled). I'm inputting two images and trying to put Keanu Reeves on the surf board.

Kontext only accurately recreates the Keanu's face when the resolution is 1024 x 1024 but then the rest of the image is changed. Using similar or exact resolutions as the surfboard image yields poorer results but the composition of the background is better.

Also the water looks terrible and looks cartoonish and has a lot of pixelated-like artifacts. This tends to happen a lot in general with images I create, but it's particular bad with water.

Hopefully someone will notice something noobish I'm doing wrong.","['READ THE PROMPT GUIDE, and honestly youll likely need to inpaint the face to get it better, as theres not enough latent data for that small area of the image most likely.', 'I think initial image and obhect on it is too small. Don forget that neural magic is made in latent space. Used official workflow, kontext fp16, t5xxl\\_encoonly. Changed steps to 30, sampler to dpmp\\_2m scheduler to simple. Changed latent size to 1184x800. Prompt is ""Replace woman on surf in red box on second image with man from first image. make him shirtless. Maintain his facial features and hairstyle. Maintain composition and scale of second image. High quality action photo of a man from first image riding the wave on surf.""\n\nIt would probably be better to crop and upscale masked, then manipulate with kontext, then stitch back\n\nhttps://preview.redd.it/vc2w1ow1pqaf1.png?width=1184&format=png&auto=webp&s=1beb0653d31f756bca952278a98dff80a88f9294', 'To my great regret, **Flux kontext** is still too raw of a model and cannot perform such tasks well yet. It needs time for the tools required for this solution to emerge.', 'Idk what it is wirh flux kontext. From far the generations look good but they are always very... kinda pixelated. Not as clear and sharp as i am used to.', 'Upon further inspection (didnt swap persons before) it seems not just hard, it seems that kontext actually freeses unet after starting to change character. Background just dont change if you look closely to latent preview, which results in degraded image quality. This is either internal mechanism or we do not know specific prompt and model gets ""lost"". In case of remove watermark it clearly reproduces the image. And in this case output is hillariously bad.', 'Better quality versions of the images here:  \n[https://imgur.com/a/BusZ0Qu](https://imgur.com/a/BusZ0Qu)']","['READ THE PROMPT GUIDE, and honestly youll likely need to inpaint the face to get it better, as theres not enough latent data for that small area of the image most likely.', 'I think initial image and obhect on it is too small. Don forget that neural magic is made in latent space. Used official workflow, kontext fp16, t5xxl\\_encoonly. Changed steps to 30, sampler to dpmp\\_2m scheduler to simple. Changed latent size to 1184x800. Prompt is ""Replace woman on surf in red box on second image with man from first image. make him shirtless. Maintain his facial features and hairstyle. Maintain composition and scale of second image. High quality action photo of a man from first image riding the wave on surf.""\n\nIt would probably be better to crop and upscale masked, then manipulate with kontext, then stitch back\n\nhttps://preview.redd.it/vc2w1ow1pqaf1.png?width=1184&format=png&auto=webp&s=1beb0653d31f756bca952278a98dff80a88f9294', 'To my great regret, **Flux kontext** is still too raw of a model and cannot perform such tasks well yet. It needs time for the tools required for this solution to emerge.', 'Idk what it is wirh flux kontext. From far the generations look good but they are always very... kinda pixelated. Not as clear and sharp as i am used to.', 'Upon further inspection (didnt swap persons before) it seems not just hard, it seems that kontext actually freeses unet after starting to change character. Background just dont change if you look closely to latent preview, which results in degraded image quality. This is either internal mechanism or we do not know specific prompt and model gets ""lost"". In case of remove watermark it clearly reproduces the image. And in this case output is hillariously bad.']",0,7,0.33,Question - Help,1751567467.0
1lqwvdf,StableDiffusion,Help a newbie integrate stable diffusion into his lineart process?,"Hi Reddit, I'm a digital artist looking to experiment with integrating AI tools into my current process. I really enjoy the process of creating digital art, with one exception: whenever I work on a piece that requires lineart, I absolutely HATE doing the lineart. It takes so long, I can never get it to look right (partially due to my hands being shaky and uncoordinated), and it's no fun.

I was wondering if there's some kind of tool available that would let me draw a sketch, plug it into a workflow, and generate lineart that I can use as a starting point without having to draw it all myself? Does something like this exist?

Currently using Krita's AI plugin, but know very little about how it works.","['I gave it a go with Illustrious\n\n\n\nhttps://preview.redd.it/kjyupanmsraf1.png?width=1024&format=png&auto=webp&s=0ed179ec470042c88f581772fb8b4519a846d830\n\n  \nDetails below', 'Try Flux-Kontext:\n\n[https://docs.bfl.ai/guides/prompting\\_guide\\_kontext\\_i2i](https://docs.bfl.ai/guides/prompting_guide_kontext_i2i)\n\n[https://comfyui-wiki.com/en/tutorial/advanced/image/flux/flux-1-kontext](https://comfyui-wiki.com/en/tutorial/advanced/image/flux/flux-1-kontext)\n\nPlease note that Flux-Kontext-Dev (the version that can be run locally) is more finicky and will require more tweaking and detailed editing prompt to work compared to the Pro/Max (paid online version), so the examples in the guide may not work with a local setup.', ""So you want to draw the general shape and the AI would generate the lines? Maybe Flux Tools models could do this. You would have to look into them. Or look up controlnets.\n\nYou will need to install some kind of software to run those models, like ComfyUI (this one is a bit complicated to learn, but you can copy paste other people's workflows)."", 'This video should help you:\xa0[https://youtu.be/PPQnL0IRv8g](https://youtu.be/PPQnL0IRv8g)\n\nYou can watch the playlist on Krita AI diffusion:\xa0[https://www.youtube.com/playlist?list=PLPFN04WspxqvFhJDIXvIDZ3yveShMvgss](https://www.youtube.com/playlist?list=PLPFN04WspxqvFhJDIXvIDZ3yveShMvgss)', ""I feel like you can get close with control nets and tuning their strength according to how broad your strokes are.  Then probably some model or lora that uses that line style you're going for.  I cant test for you right now, but there are many you can choose from and it may be that you can get close with a soft edge or anime linestyle.  \n\nWould be worth exploring models and loras that are trained for the style you're after, as well.""]","['I gave it a go with Illustrious\n\n\n\nhttps://preview.redd.it/kjyupanmsraf1.png?width=1024&format=png&auto=webp&s=0ed179ec470042c88f581772fb8b4519a846d830\n\n  \nDetails below', 'Try Flux-Kontext:\n\n[https://docs.bfl.ai/guides/prompting\\_guide\\_kontext\\_i2i](https://docs.bfl.ai/guides/prompting_guide_kontext_i2i)\n\n[https://comfyui-wiki.com/en/tutorial/advanced/image/flux/flux-1-kontext](https://comfyui-wiki.com/en/tutorial/advanced/image/flux/flux-1-kontext)\n\nPlease note that Flux-Kontext-Dev (the version that can be run locally) is more finicky and will require more tweaking and detailed editing prompt to work compared to the Pro/Max (paid online version), so the examples in the guide may not work with a local setup.', ""So you want to draw the general shape and the AI would generate the lines? Maybe Flux Tools models could do this. You would have to look into them. Or look up controlnets.\n\nYou will need to install some kind of software to run those models, like ComfyUI (this one is a bit complicated to learn, but you can copy paste other people's workflows)."", 'This video should help you:\xa0[https://youtu.be/PPQnL0IRv8g](https://youtu.be/PPQnL0IRv8g)\n\nYou can watch the playlist on Krita AI diffusion:\xa0[https://www.youtube.com/playlist?list=PLPFN04WspxqvFhJDIXvIDZ3yveShMvgss](https://www.youtube.com/playlist?list=PLPFN04WspxqvFhJDIXvIDZ3yveShMvgss)', ""I feel like you can get close with control nets and tuning their strength according to how broad your strokes are.  Then probably some model or lora that uses that line style you're going for.  I cant test for you right now, but there are many you can choose from and it may be that you can get close with a soft edge or anime linestyle.  \n\nWould be worth exploring models and loras that are trained for the style you're after, as well.""]",5,17,0.78,Question - Help,1751566908.0
1lqwtnz,StableDiffusion,How can I create celebrity then and now videos?,I’ve seen many celebrity then and now age progression videos and was wondering if someone knows how it’s done. I’ve tried Wan 2.1 InP with poor results and used Chatgpt to create a prompt. Can this be done via Stable Diffusion or would I need a LoRA that’s been trained on a credit based website such as Pixverse or Lumafusion? ,"['It’s mostly start frame to end frame interpolation. You generate keyframes for each of the main “time periods” you want to hit and then generate the video from frame to frame.\n\nMore importantly though; don’t jump on these stupid trends. This is easy to produce slop that people will get tired of really quickly.', 'You git gud at a bunch of different things. AI creation, video editing, etc.', 'Not talking about the techniques used or whatever but making a video like this of a person where we DO have photos of different ages and not using them I thought nk is very disrespectful. CR7 in his teens looked NOTHING like what this vid shows.']","['It’s mostly start frame to end frame interpolation. You generate keyframes for each of the main “time periods” you want to hit and then generate the video from frame to frame.\n\nMore importantly though; don’t jump on these stupid trends. This is easy to produce slop that people will get tired of really quickly.', 'You git gud at a bunch of different things. AI creation, video editing, etc.', 'Not talking about the techniques used or whatever but making a video like this of a person where we DO have photos of different ages and not using them I thought nk is very disrespectful. CR7 in his teens looked NOTHING like what this vid shows.']",0,8,0.14,Question - Help,1751566793.0
1lqwsq2,StableDiffusion,It's information overload,,"['One major problem with the ComfyUI ecosystem in particular is that most workflows you’ll find online are full of unnecessary custom nodes that nobody ever explains the use of. The core logic of getting a model, conditioning and a latent into a KSampler is actually easy. If you’re a beginner; try to create a “fewest nodes possible” KSampler setup and ignore the billion custom nodes that advanced users use in their workflows.', ""https://preview.redd.it/hls3f99rbpaf1.jpeg?width=2688&format=pjpg&auto=webp&s=708d8fbe426d419f7ac3197eed823b539cab44d0\n\nIt's worth it in the end. Midjourney's text to image has been very lacking for a long time and other than good looking style, v7 didn't make it any better. Now that local Chroma v41 is really looking better and more coherent than ever, and the fact that Midjourney flat out refuses to do this prompt because of their censorship, I'm really only using them for the fast video generation now. The payoff for learning the local is worth it."", '\\*Laughs in ""1girl, standing, masterpiece, best quality""\\*', ""If you are serious about A.I. image generation, either professionally or as a serious hobby, learning to do it using open weight models and tools is well worth it in the long run.  \n\nThere will be no arbitrary limitations as to what you can accomplish.  For example, missing a style?  Train your own LoRA.  Want to make fan art of a character that is no in the model?  Again, just train one yourself.  You can even recreate your favorite MJ styles if you want. \n\nComfyUI may appear complicated, but if you understand the pipeline, and what those custom nodes are for, it will make more sense and less intimidating (I have to admit that those noodles are complicated, but you can hide them under SwarmUI). You can then build custom pipelines that fit your needs and automate things.\n\nIf you are in a hurry to get started, or if your GPU is not good enough, you can also use most of these models and LoRAs online for free or at low cost on places likes [civitai.com](http://civitai.com) or tensor. art\n\nThere's also the option of running ComfyUI online by renting cloud GPUs.  That would be fast and flexible but also more expensive."", 'ONE OF US! ONE OF US!\n\nYes, it can be a very deep hobby once you get into local generation (and training) , but there are loads of useful YouTube videos and you can always ask here or ask ChatGPT/(Other LLM) to explain something if you get stuck.', ""I love using Midjourney but it kinda sucks for fanart (specifically for media that's either too recent or niche). So I'm trying to learn how to use local models."", 'Me coming from SUPER EASY to use  gradio interfaces to comfy UI', ""My problem is that I don't have a GPU strong enough to run local..."", 'Welcome to the party. It’s way more fun in here.', 'The new comfyui installer and templates makes local stuff sooooo much easier than it used to be...']","['One major problem with the ComfyUI ecosystem in particular is that most workflows you’ll find online are full of unnecessary custom nodes that nobody ever explains the use of. The core logic of getting a model, conditioning and a latent into a KSampler is actually easy. If you’re a beginner; try to create a “fewest nodes possible” KSampler setup and ignore the billion custom nodes that advanced users use in their workflows.', ""https://preview.redd.it/hls3f99rbpaf1.jpeg?width=2688&format=pjpg&auto=webp&s=708d8fbe426d419f7ac3197eed823b539cab44d0\n\nIt's worth it in the end. Midjourney's text to image has been very lacking for a long time and other than good looking style, v7 didn't make it any better. Now that local Chroma v41 is really looking better and more coherent than ever, and the fact that Midjourney flat out refuses to do this prompt because of their censorship, I'm really only using them for the fast video generation now. The payoff for learning the local is worth it."", '\\*Laughs in ""1girl, standing, masterpiece, best quality""\\*', ""If you are serious about A.I. image generation, either professionally or as a serious hobby, learning to do it using open weight models and tools is well worth it in the long run.  \n\nThere will be no arbitrary limitations as to what you can accomplish.  For example, missing a style?  Train your own LoRA.  Want to make fan art of a character that is no in the model?  Again, just train one yourself.  You can even recreate your favorite MJ styles if you want. \n\nComfyUI may appear complicated, but if you understand the pipeline, and what those custom nodes are for, it will make more sense and less intimidating (I have to admit that those noodles are complicated, but you can hide them under SwarmUI). You can then build custom pipelines that fit your needs and automate things.\n\nIf you are in a hurry to get started, or if your GPU is not good enough, you can also use most of these models and LoRAs online for free or at low cost on places likes [civitai.com](http://civitai.com) or tensor. art\n\nThere's also the option of running ComfyUI online by renting cloud GPUs.  That would be fast and flexible but also more expensive."", 'ONE OF US! ONE OF US!\n\nYes, it can be a very deep hobby once you get into local generation (and training) , but there are loads of useful YouTube videos and you can always ask here or ask ChatGPT/(Other LLM) to explain something if you get stuck.']",311,91,0.89,Meme,1751566729.0
1lqwo9s,StableDiffusion,Can anyone help me find the “adv settings” node?,"https://preview.redd.it/avyl3i17apaf1.png?width=262&format=png&auto=webp&s=bcce5e4896259fec22fa482fccec72697a0eb899





Can anyone help me find the adv settings node?



How do I download and install it? I tried searching in the manager but couldn’t locate it. Any help would be appreciated!



Thanks!","['Looks like they made a group node that was renamed to ""Adv Settings"".']","['Looks like they made a group node that was renamed to ""Adv Settings"".']",0,3,0.33,Question - Help,1751566430.0
1lqwj40,StableDiffusion,"CLIPTextEncode - ERROR: Clip input is invalid: None //// I tried ""load clip"" node but there is no ""flux"" type what do i do",,"['Not familiar with that specific model, does it have the clip and t5 included? If not, you need to load the clip and t5 models separately, have a look at the example workflow for flux. Another thing is that flux workflows rarely use a negative prompt unless they are specially designed for it. this looks like a sdxl workflow which does not work for flux in most cases.', ""Use workflow from here: [https://comfyanonymous.github.io/ComfyUI\\_examples/flux/](https://comfyanonymous.github.io/ComfyUI_examples/flux/)  \nAlthough, I think ComfyUI has a built-in Flux workflow too. You could also copy workflow from the preview images of the model itself, you can see there that it uses a separate DualCLIP Loader.  \n\n\nYour workflow here has too many issues. CFG should be 1.0 and there is no negative - it's because Flux is a distilled model. Also, do not use the karras scheduler with DiT models like Flux."", 'I built my own workflow for my model https://civitai.com/models/1662914/workflow-for-fluxdev-by-danrisi-txt2img-img2img-upscale-and-some-qol-stuff']","['Not familiar with that specific model, does it have the clip and t5 included? If not, you need to load the clip and t5 models separately, have a look at the example workflow for flux. Another thing is that flux workflows rarely use a negative prompt unless they are specially designed for it. this looks like a sdxl workflow which does not work for flux in most cases.', ""Use workflow from here: [https://comfyanonymous.github.io/ComfyUI\\_examples/flux/](https://comfyanonymous.github.io/ComfyUI_examples/flux/)  \nAlthough, I think ComfyUI has a built-in Flux workflow too. You could also copy workflow from the preview images of the model itself, you can see there that it uses a separate DualCLIP Loader.  \n\n\nYour workflow here has too many issues. CFG should be 1.0 and there is no negative - it's because Flux is a distilled model. Also, do not use the karras scheduler with DiT models like Flux."", 'I built my own workflow for my model https://civitai.com/models/1662914/workflow-for-fluxdev-by-danrisi-txt2img-img2img-upscale-and-some-qol-stuff']",1,3,0.57,Question - Help,1751566082.0
1lquzqe,StableDiffusion,"Which is the best ai face swap ai tool or app for lipsing, face expression",I,[],[],1,0,1.0,Question - Help,1751562473.0
1lqulf9,StableDiffusion,Need advice to run SD on a 32Gb RAM 3060 6Gb Notebook.,"What would be the best configuration to run SD on my notebook? I used to run SD on it like 1.5 years ago with decent results, but then fell out of the loop and it seems things have changed drastically.","['If you want easy mode, install [pinokio.co](http://pinokio.co) and there is optymized Forge (you will have there flux, sdxl, inpainting etc). Pinokio will install everything automatic for you. /another similar option Stability matrix and there are also Forge or InvokeAI/\n\nWant more? Search youtube for ComfyUi and very low vram.', 'You should be able to run most things atm, I have a similar setup and I can run almost everything. Forge or Reforge can run SDXL and Flux models without issues.\nComfyUI can run basically everything (video, images, voices, etc), so depends on what you want to do.\nNow getting the best results in ComfyUI will depend on the settings of the workflow, but you can get pretty good results if you are willing to take some shortcuts.\nThe main issue you might face is the time it will take to generate things and some Out of Memory situations, but those can be fixed by retrying or lowering the settings of the generation.\nSo now you just need to select and look at tutorials to learn.', 'I have same setup. Using—lowvram args in comfyui helps to avoid oom errors.', '\\* Install linux (mint is safe choice). Optional but saves you a lot of windows headache and its signature poor performance.\n\n\\* Click ""install proprietary nvidia drivers"".\n\n\\* Select ""nvidia on demand"" profile from tray icon.\n\n\\* run nvidia-smi to make sure nothing is eating your nvidia GPU vram\n\n\\* install comfy and be happy. Everything is working out of the box. You can expect sub 1sec / step speed with SDXL.\n\nIf you have external monitor connected to your laptop make sure that it is run by integrated GPU (if possible).', 'Fooocus should work, it is simple and its SDXL', 'Dont']","['If you want easy mode, install [pinokio.co](http://pinokio.co) and there is optymized Forge (you will have there flux, sdxl, inpainting etc). Pinokio will install everything automatic for you. /another similar option Stability matrix and there are also Forge or InvokeAI/\n\nWant more? Search youtube for ComfyUi and very low vram.', 'You should be able to run most things atm, I have a similar setup and I can run almost everything. Forge or Reforge can run SDXL and Flux models without issues.\nComfyUI can run basically everything (video, images, voices, etc), so depends on what you want to do.\nNow getting the best results in ComfyUI will depend on the settings of the workflow, but you can get pretty good results if you are willing to take some shortcuts.\nThe main issue you might face is the time it will take to generate things and some Out of Memory situations, but those can be fixed by retrying or lowering the settings of the generation.\nSo now you just need to select and look at tutorials to learn.', 'I have same setup. Using—lowvram args in comfyui helps to avoid oom errors.', '\\* Install linux (mint is safe choice). Optional but saves you a lot of windows headache and its signature poor performance.\n\n\\* Click ""install proprietary nvidia drivers"".\n\n\\* Select ""nvidia on demand"" profile from tray icon.\n\n\\* run nvidia-smi to make sure nothing is eating your nvidia GPU vram\n\n\\* install comfy and be happy. Everything is working out of the box. You can expect sub 1sec / step speed with SDXL.\n\nIf you have external monitor connected to your laptop make sure that it is run by integrated GPU (if possible).', 'Fooocus should work, it is simple and its SDXL']",0,10,0.36,Question - Help,1751561552.0
1lqtxyg,StableDiffusion,Recommendation Upscale/remaster vhs video,I've got some old vhs movies that I had converted to DVD files some years ago. They are a tough watch these days for quality. Wondering if anyone out there has suggestions on upscalers/workflow/etc anything really on what I could do with them to (with relatively low effort) run them through to help marginally improve the visual experience? It would be nice if I could share with my family some improved versions of this files. Any advice is welcome. TIA,"['Honestly for full length video, you’d be better off running them through something like Topaz VideoAI. The open source models this community focuses on aren’t really capable of consistent enhancement for significant durations, at least not without artifacts or temporal inconsistencies.\n\nTopaz VideoAI is pricey, and isnt meant for generation, but for improving the quality of videos it does really well.']","['Honestly for full length video, you’d be better off running them through something like Topaz VideoAI. The open source models this community focuses on aren’t really capable of consistent enhancement for significant durations, at least not without artifacts or temporal inconsistencies.\n\nTopaz VideoAI is pricey, and isnt meant for generation, but for improving the quality of videos it does really well.']",0,2,0.5,Discussion,1751559985.0
1lqtizb,StableDiffusion,Flux Kontext for pose transfer??,"I found this wf somewhere on fb. I really wonder, can Flux Kontext do this task now?
I have tried many different ways of prompting so that the model in the first image posing the pose of the second image. But it's really not work at all.
Can someone share the solution for this pose transfer?","[""I've been using a trick I came up with for situations like this where I have a subject image and a pose/composition image. Only send the subject image into the Kontext conditioning. Take the pose/composition image and do VAE Encode to make that your base latent. Then it comes down to finding the right amount of denoising strength where it's strong enough to replace the contents with the likeness of your subject, but not so strong that it breaks the composition. I found that ancestral samplers can be useful for this since you can get away with more change at lower denoising levels. If I remember right I was doing about 0.80 denoising with an ancestral sampler."", 'https://preview.redd.it/vtc70w47toaf1.png?width=864&format=png&auto=webp&s=2b9cda5c2c7bb5f9ddc3aab5f6b21977b49d6e5c\n\noriginal post', 'https://preview.redd.it/jh17cdqruoaf1.png?width=1332&format=png&auto=webp&s=115f91e517c87c2ce0e7d7174f3649a0007f06bd\n\nYes, but the effect is not good. I am try my best of prompt enginnering...  \nPrompt format: The girl {description of img on the left } is the same pose of {description of the image on the right}.  \nThe desciption is interrogated by gemini 2.5 flash, too long to post here, you could see in the picture above.\n\nWell. The kontex dev version (i am using nunchaku, but fp8 is more or less similar), is not good at two picture. It rarely sucess in zero shot..', ""Share the images please, I'll give it a try later today"", 'RemindMe 7 days', 'Maybe adding depth can help?', ""https://preview.redd.it/brmk3jmyksaf1.png?width=2355&format=png&auto=webp&s=cc98f21801653fdc11255da6d93b47d91da0e7b7\n\n  \nWell, I tried. Without cfg it's hard to style transfer. \n\nUse NAG and be descriptive with the pose but it's still inconsistent."", ""I think Kontext does not have an overseeing AI, the prompt understander wouldn't know which is first or second in the workflow order. Multi frame training images may allow it to guess that left/top frame is usually first and make it work. But I think using left/top directly will be better?"", 'Reasons I see for people not sharing workflows :\n\n1. Im not on the computer (while sharing PC screenshots)\n\n2. Im still working on it\n\n3. the nodes are a mess\n\n4.I just found this workflow somewhere on the internet, on some webpage, maybe.\n\n5. My Pc just broke 5 minutes ago', ""Up! I really can't find a way either. A simple 2 images workflow don't seem to work...!""]","[""I've been using a trick I came up with for situations like this where I have a subject image and a pose/composition image. Only send the subject image into the Kontext conditioning. Take the pose/composition image and do VAE Encode to make that your base latent. Then it comes down to finding the right amount of denoising strength where it's strong enough to replace the contents with the likeness of your subject, but not so strong that it breaks the composition. I found that ancestral samplers can be useful for this since you can get away with more change at lower denoising levels. If I remember right I was doing about 0.80 denoising with an ancestral sampler."", 'https://preview.redd.it/vtc70w47toaf1.png?width=864&format=png&auto=webp&s=2b9cda5c2c7bb5f9ddc3aab5f6b21977b49d6e5c\n\noriginal post', 'https://preview.redd.it/jh17cdqruoaf1.png?width=1332&format=png&auto=webp&s=115f91e517c87c2ce0e7d7174f3649a0007f06bd\n\nYes, but the effect is not good. I am try my best of prompt enginnering...  \nPrompt format: The girl {description of img on the left } is the same pose of {description of the image on the right}.  \nThe desciption is interrogated by gemini 2.5 flash, too long to post here, you could see in the picture above.\n\nWell. The kontex dev version (i am using nunchaku, but fp8 is more or less similar), is not good at two picture. It rarely sucess in zero shot..', ""Share the images please, I'll give it a try later today"", 'RemindMe 7 days']",97,52,0.94,Question - Help,1751558981.0
1lqssg7,StableDiffusion,Flux Kontext limitations with people,"Flux Kontext can do great stuff, but when it comes to people most output is just not  usable for me.

When people get smaller, usually about the size that a full body fits to the 1024x1024 image, especially the head and hair start to show artifacts looking like a too strong JPEG compression. Ok, some img2img refinement might fix that.

But when I do ""bigger"" edits, something Kontext is really made for, it gets the overall anatomy wrong. Heads are too big, the torso is too small.

Example (and I've got much worse):

https://preview.redd.it/vxja7vr4ioaf1.png?width=1024&format=png&auto=webp&s=0254c6fe6d51cd19079da7e601057989b8dbb673

This was generated with two portrait images and the prompt ""Change the scene so that both persons are sitting on a park bench together is a lush garden"".

A quick look says it's fine. But the longer you look the creepier it gets. Just look at the sized of the head, upper body and arms.

Doing the same with other portraits (which I can't share in public) it was even worse.

And that's a distortion that's not easily fixed.


So, what are your experiences? Have you found ways around these limitations when it comes to people?","['Maintain scale and proportion helped me. Are you using fp8_scaled or bf16?', ""It's ironic because ChatGPT's image model has very similar issues.\n\nYou can get better results by taking control over the generation's resolution. Either remove/disable the FluxKontextImageScale node, which will use your original image's resolution for the generation, or replace it with an Image Resize (or equivalent) node, setting it to a resolution with a lower width-to-height ratio."", ""I'm getting good results but I am tweaking stuff to do it. I modify my input image like so:\n\nimg\\_w, img\\_h = img.size\n\n\\# ensure dimensions are multiples of 32\n\nnew\\_width = int(32 \\* round(img\\_w / 32))\n\nnew\\_height = int(32 \\* round(img\\_h / 32))\n\n\\# applying fit method\n\nif new\\_height != img\\_h or new\\_width != img\\_w:\n\nimg\\_crop = ImageOps.fit(img, (new\\_width, new\\_height), method = 1,\n\nbleed = 0.0, centering = (0.5, 0.5))\n\nelse:\n\nimg\\_crop = img.copy()\n\n\\#image dimensions\n\nwidth, height = img\\_crop.size\n\nmax\\_area = width \\* height\n\nprint(f'height = {height}, width = {width}')\n\nThen I account for max\\_area and set \\_auto\\_resize = False:\n\nimage = pipe(\n\nprompt=text\\_prompt,\n\nguidance\\_scale=guidance\\_scale,\n\nwidth = width,\n\nheight = height,\n\ngenerator=generator,\n\nimage=img\\_crop,\n\nmax\\_sequence\\_length=num\\_tokens,\n\nmax\\_area = max\\_area,\n\n\\_auto\\_resize = False,\n\nnum\\_inference\\_steps=inference\\_steps).images\\[0\\]"", ""I've been seeing that JPG compression problem a lot, too\n\nhttps://preview.redd.it/73akftxvlpaf1.png?width=1859&format=png&auto=webp&s=f0b4054143744c553cb26220a272aa757ff94736\n\nFor example here is my input image"", 'Try with NAG, and put the usual bad anatomy stuff into the nag-negative, see if that resolve your issues.', 'I do wonder how much the censorship of this model plays a role. Like SD 3 was censored so much, it was struggling a lot with female anatomy. Or maybe it is just a skill issue on my end :D', 'im having problems is with the feets and sometimes with hands, anyone know if exist a lore for fix this ?', 'Yeah kontext anatomy worse than image generators older than it is', 'maybe you need iphone flux pro max']","['Maintain scale and proportion helped me. Are you using fp8_scaled or bf16?', ""It's ironic because ChatGPT's image model has very similar issues.\n\nYou can get better results by taking control over the generation's resolution. Either remove/disable the FluxKontextImageScale node, which will use your original image's resolution for the generation, or replace it with an Image Resize (or equivalent) node, setting it to a resolution with a lower width-to-height ratio."", ""I'm getting good results but I am tweaking stuff to do it. I modify my input image like so:\n\nimg\\_w, img\\_h = img.size\n\n\\# ensure dimensions are multiples of 32\n\nnew\\_width = int(32 \\* round(img\\_w / 32))\n\nnew\\_height = int(32 \\* round(img\\_h / 32))\n\n\\# applying fit method\n\nif new\\_height != img\\_h or new\\_width != img\\_w:\n\nimg\\_crop = ImageOps.fit(img, (new\\_width, new\\_height), method = 1,\n\nbleed = 0.0, centering = (0.5, 0.5))\n\nelse:\n\nimg\\_crop = img.copy()\n\n\\#image dimensions\n\nwidth, height = img\\_crop.size\n\nmax\\_area = width \\* height\n\nprint(f'height = {height}, width = {width}')\n\nThen I account for max\\_area and set \\_auto\\_resize = False:\n\nimage = pipe(\n\nprompt=text\\_prompt,\n\nguidance\\_scale=guidance\\_scale,\n\nwidth = width,\n\nheight = height,\n\ngenerator=generator,\n\nimage=img\\_crop,\n\nmax\\_sequence\\_length=num\\_tokens,\n\nmax\\_area = max\\_area,\n\n\\_auto\\_resize = False,\n\nnum\\_inference\\_steps=inference\\_steps).images\\[0\\]"", ""I've been seeing that JPG compression problem a lot, too\n\nhttps://preview.redd.it/73akftxvlpaf1.png?width=1859&format=png&auto=webp&s=f0b4054143744c553cb26220a272aa757ff94736\n\nFor example here is my input image"", 'Try with NAG, and put the usual bad anatomy stuff into the nag-negative, see if that resolve your issues.']",24,31,0.8,Discussion,1751557229.0
1lqsjg1,StableDiffusion,Cloning voice Needing Help for birthday,"I’m for someone to help me create an voice clone of my late father using old videos and voice recordings I have saved. My daughter is about to turn 8 years old, and she has been asking for something like this since he passed away a year ago. It would mean so much to her to hear her grandpa’s voice again. My plan is to put a special message from him inside a Build-A-Bear for her birthday. I have all the audio and video files ready to share. This is a very personal and meaningful project, and I want it done with care. Thank you so much for taking the time to read this.","['Chatterbox can do pretty good work with voices, especially considering how easy it is to use. You can do voice2voice to replace existing audio with the voice you want, or you can use TTS and generate audio with the voice you want. \n\nUsing the Video Helper Suite nodes it should be pretty easy to apply your audio result to the video that you want.', ""Do you have a gaming computer you could run AI models on? With Chatterbox I think you only need a short audio sample, but I haven't used it yet."", '[https://fish.audio/](https://fish.audio/) have all the tools you need.']","['Chatterbox can do pretty good work with voices, especially considering how easy it is to use. You can do voice2voice to replace existing audio with the voice you want, or you can use TTS and generate audio with the voice you want. \n\nUsing the Video Helper Suite nodes it should be pretty easy to apply your audio result to the video that you want.', ""Do you have a gaming computer you could run AI models on? With Chatterbox I think you only need a short audio sample, but I haven't used it yet."", '[https://fish.audio/](https://fish.audio/) have all the tools you need.']",2,5,0.57,Question - Help,1751556643.0
1lqqhbm,StableDiffusion,From Happily Ever After to Rehab: The Shocking Downfall of Cartoon Icons!,Made with Flux Kontext Dev,"['Cool! What was the prompt used?', 'This is a hilarious concept.  The simpsons is awsome.']","['Cool! What was the prompt used?', 'This is a hilarious concept.  The simpsons is awsome.']",0,3,0.23,Meme,1751551594.0
1lqq3ra,StableDiffusion,Need help: DWPREPROCESSOR node error in my workflow,"**SOLVED - SWITCHED TO OLD VERSION**

Hi everyone,

I loaded a workflow and I’m getting this error related to the **DWPREPROCESSOR** node.

I opened the manager and installed the node, and I also tried updating to the latest version and even the nightly build, but it didn’t work. I’ve restarted everything, but the error is still the same.

Has anyone faced this issue before? Any one know how to fix it?

https://preview.redd.it/y1ryci46znaf1.png?width=769&format=png&auto=webp&s=5c4ab6e9e78cf6cd6c862adf1590bf3390a1982b

Thanks in advance!",['Show the full error message from console. It will tell you about what packages are failing to load and should include information about why - typically a missing dependency.'],['Show the full error message from console. It will tell you about what packages are failing to load and should include information about why - typically a missing dependency.'],0,2,0.33,Question - Help,1751550622.0
1lqpqrs,StableDiffusion,A good way to segment videos?,"I've tried SAM2 with Points Editor, but it has consistently failed at what I need it to do (mask hands in the latest case). Is there any more accurate way to segment a video now?",[],[],0,0,0.25,Question - Help,1751549664.0
1lqp6xc,StableDiffusion,"Flux kontext not working properly. for some reason, it REFUSES to change clothes or make anybody wear this jacket. I'm using the default workflow on the installation page. I tried changing the dimensions of the output photo and it still didn't work",,"['Closest output that I got:\n\nhttps://preview.redd.it/159z2jki2oaf1.png?width=320&format=png&auto=webp&s=064ca1f72617dfc527041b86a1f696f63ecc8ac2', 'https://preview.redd.it/8emd7m3yunaf1.png?width=884&format=png&auto=webp&s=d8b3e1e88a9bf61ecd17161d03569fa0992efc4a', 'https://civitai.com/models/1729159/kontextchangeclothes?modelVersionId=1956947 this lora works fine for me, besides that comfy has a promptguide that helped me alot (good/bad prompting-/words at the endsection) https://comfyui-wiki.com/en/tutorial/advanced/image/flux/flux-1-kontext', 'Change output size to be a single image. Usd image with better quality. Write normal prompt with subject action and object. Reference prompt guide at comfyexamples', ""Unfortunately I think only the trimmed version is on Youtube, but there were similar problems during the Invoke design session for Flux Kontext.  Kent went through a lot of iterations of trying to pass in a character and an outfit and get the character to wear it, and Kontext really didn't seem to want to cooperate.  Eventually he hit a combination that worked, but there were many, many error where Kontext either did nothing or substituted its own outfit rather than the picture."", '[deleted]', ""I have the same problem too, I can't seems to find the exact prompt to actually fit in, or was it due to VRAM issue that AI just couldn't process the output as requested"", 'The brown jacket has too few details on it so its hard but thats what i got\n\nhttps://preview.redd.it/jww7r6czi6bf1.png?width=1821&format=png&auto=webp&s=fe9ecbc3128141650a5bfd8286252a529c78cc81\n\nwith a more detailed one results are better', 'For kontext to work, **""You need to update your comfyui""**']","['Closest output that I got:\n\nhttps://preview.redd.it/159z2jki2oaf1.png?width=320&format=png&auto=webp&s=064ca1f72617dfc527041b86a1f696f63ecc8ac2', 'https://preview.redd.it/8emd7m3yunaf1.png?width=884&format=png&auto=webp&s=d8b3e1e88a9bf61ecd17161d03569fa0992efc4a', 'https://civitai.com/models/1729159/kontextchangeclothes?modelVersionId=1956947 this lora works fine for me, besides that comfy has a promptguide that helped me alot (good/bad prompting-/words at the endsection) https://comfyui-wiki.com/en/tutorial/advanced/image/flux/flux-1-kontext', 'Change output size to be a single image. Usd image with better quality. Write normal prompt with subject action and object. Reference prompt guide at comfyexamples', ""Unfortunately I think only the trimmed version is on Youtube, but there were similar problems during the Invoke design session for Flux Kontext.  Kent went through a lot of iterations of trying to pass in a character and an outfit and get the character to wear it, and Kontext really didn't seem to want to cooperate.  Eventually he hit a combination that worked, but there were many, many error where Kontext either did nothing or substituted its own outfit rather than the picture.""]",0,19,0.25,Question - Help,1751548188.0
1lqp2ov,StableDiffusion,Forge WebUI Colab Notebook,"Hi,

Does anyone have a good Google Colab notebook I can use for ForgeUI. I would like to attempt to use the Chroma model.

Is there anything similar to the DMD2 Lora on SDXL for Chroma or Flux to speed up generation time?

Thanks ",[],[],0,0,0.33,Question - Help,1751547866.0
1lqoyk5,StableDiffusion,What is the best Flux base model to finetune face?,Is the Flux dev best for finetuning face to get realistic output at the end?,"[""Yes it's the best imo."", 'Unless you looking for a very specific look, the best is Flux-Dev.\n\nBut if you are going for a specific type of realism, you can try to merge that realism LoRA into Flux-Dev and then train on that.  See this discussion: [https://www.reddit.com/r/StableDiffusion/comments/1ll3yat/comment/mzxa20l/](https://www.reddit.com/r/StableDiffusion/comments/1ll3yat/comment/mzxa20l/)\n\nAlmost all the Flux-dev fine-tunes out there are actually multiple LoRAs merged into Flux-Dev.']","[""Yes it's the best imo."", 'Unless you looking for a very specific look, the best is Flux-Dev.\n\nBut if you are going for a specific type of realism, you can try to merge that realism LoRA into Flux-Dev and then train on that.  See this discussion: [https://www.reddit.com/r/StableDiffusion/comments/1ll3yat/comment/mzxa20l/](https://www.reddit.com/r/StableDiffusion/comments/1ll3yat/comment/mzxa20l/)\n\nAlmost all the Flux-dev fine-tunes out there are actually multiple LoRAs merged into Flux-Dev.']",0,8,0.33,Discussion,1751547565.0
1lqopn6,StableDiffusion,Local image processing for garment image enhancement,"Looking for a locally run image processing solution to tidy up photos of garments like the attached images, any and all suggestions welcome, thank you.","['Flux kontext works well for this application. You\'ll need to do some blending because at least in my tests, the overlay has a very ""computer graphic"" look to it. Eg it\'s slapping your reference pic on the subject pic and it\'s not changing the lighting or color,', 'perhaps something like [https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT](https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT) could work.', 'Is there any reason you need a diffusion model for this? I feel like some really basic image processing could handle it', 'img2img low denoise prompt from florence.', 'When you say tidy, are you trying to go from photo 1 to 2 or 2 to 1?', 'Maybe this Kontext LoRA can help: [https://civitai.com/models/1729159/kontextchangeclothes](https://civitai.com/models/1729159/kontextchangeclothes)\n\ni.e., instead of a person, just use your ""tidy"" t-shirt as a stand-in.  May or may not work, but it is easy enough to just try it.']","['Flux kontext works well for this application. You\'ll need to do some blending because at least in my tests, the overlay has a very ""computer graphic"" look to it. Eg it\'s slapping your reference pic on the subject pic and it\'s not changing the lighting or color,', 'perhaps something like [https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT](https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT) could work.', 'Is there any reason you need a diffusion model for this? I feel like some really basic image processing could handle it', 'img2img low denoise prompt from florence.', 'When you say tidy, are you trying to go from photo 1 to 2 or 2 to 1?']",8,19,0.68,Question - Help,1751546853.0
1lqooy0,StableDiffusion,Chattable Wan & FLUX knowledge bases,"I used NotebookLM to make chattable knowledge bases for FLUX and Wan video.  

The information comes from the Banodoco Discord FLUX & Wan channels, which I scraped and added as sources.  It works incredibly well at taking unstructured chat data and turning it into organized, cited information!

**Links:**

🔗 [**FLUX Chattable KB**](https://notebooklm.google.com/notebook/8c41f069-42f9-40e6-91b1-9fa19a3adbf6)  (last updated July 1)
🔗 [**Wan 2.1 Chattable KB**](https://notebooklm.google.com/notebook/a08901b9-0511-4926-bbf8-3c86a12dc306)  (last updated June 18)

**You can ask questions like:** 

* How does FLUX compare to other image generators?
* What is FLUX Kontext?

or for Wan:

* What is VACE?
* What settings should I be using for CausVid?  What about kijai's CausVid v2?
* Can you give me an overview of the model ecosytem?
* What do people suggest to reduce VRAM usage?
* What are the main new things people discussed last week?

Thanks to the [Banodoco](https://banodoco.ai/) community for the vibrant, in-depth discussion. 🙏🏻

It would be cool to add Reddit conversations to knowledge bases like this in the future.

**Tools and info if you'd like to make your own:**

* I'm using [DiscordChatExporter](https://github.com/Tyrrrz/DiscordChatExporter) to scrape the channels.
* [discord-text-cleaner](https://nathanshipley.github.io/discord-text-cleaner/): A web tool to make the scraped text lighter by removing {Attachment} links that NotebookLM doesn't need.
* More information about my process [on Youtube here](https://youtu.be/kG81Kkal5XM), though now I just directly download to text instead of HTML as shown in the video.  Plus you can set a partition size to break the text files into chunks that will fit in NotebookLM uploads.","['Nice!! Thanks!', 'I just tried it and it seems to be actually helpful! Saving this for later :)  \nDoes this also know the official Documentations of WAN and Flux? ..and github discussions?', ""Hey this is actually really really cool!   \nI've always hated discord because of how information silo-ey and hard to search it is (And they always turn into the same question being asked over and over again👀).   \nThis is literally the perfect solution! Nice!""]","['Nice!! Thanks!', 'I just tried it and it seems to be actually helpful! Saving this for later :)  \nDoes this also know the official Documentations of WAN and Flux? ..and github discussions?', ""Hey this is actually really really cool!   \nI've always hated discord because of how information silo-ey and hard to search it is (And they always turn into the same question being asked over and over again👀).   \nThis is literally the perfect solution! Nice!""]",45,5,0.92,Resource - Update,1751546798.0
1lqo6yp,StableDiffusion,"Please, could someone extract a lore of the difference between Flux Dev and Flux Kontext? And post on hugginface or civitai",I want to test if it is a good idea to use Flux Dev + Flux Kontext as a Lora,"[""You can generate images with kontext , you don't need flux .dev\n\njust connect empty latent node""]","[""You can generate images with kontext , you don't need flux .dev\n\njust connect empty latent node""]",0,1,0.3,Question - Help,1751545347.0
1lqnhvl,StableDiffusion,Experimented with an AI face emotion tool to bring characters to life… way more realistic than I expected,"I was using an AI image tool called (CreateImg**)** and wanted to try something fun making realistic human faces that show different emotions.

I tried happy, sad, angry, surprised, and neutral… and what surprised me most is how the faces stayed the same person every time. Just different moods, but the same look. Super realistic!

I ended up spending hours playing with it because it was so fun watching the faces change like real people.

Here are a few of my favorites would love to hear what you think!
If you have any ideas for emotions or characters, I should try next, let me know ","['Why does she look like an adult head with a childs body. Creepy...', 'You did a great job with the emotions. But the face is far from realistic, it looks like the typical ""realistic AI"" face.', 'It looks very airbrushed. The emotional range and consistency is great though', 'Cute dwarf.', 'What prompt did you use ?', 'Looks very plastic-y..', 'I\'m guessing you used keyword ""realistic"". Am I right?', '> If you have any ideas for emotions or characters, I should try next, let me know \n\nNot exactly what you asked for, but a fun follow-up project would be to organize them into a logical sequence and use wan or ltx to make a looping video with the portraits as keyframes.  Maybe do a few different characters and have them interact Hollywood Squares style or like the Brady Bunch intro via montage compositing.  Or maybe use the montage as part of a larger texture for some kind of cool zoom effect where some bigger picture is being composited by all of these little, animated humans.\n\nAlternatively, maybe a routine that takes a snapshot and generates the permutations in the form of a candid photobooth filmstrip style.  You know, those things in the malls back before digital photos where you went in and made funny faces or whatever to come out and get a little series of vertically aligned photos?']","['Why does she look like an adult head with a childs body. Creepy...', 'You did a great job with the emotions. But the face is far from realistic, it looks like the typical ""realistic AI"" face.', 'It looks very airbrushed. The emotional range and consistency is great though', 'Cute dwarf.', 'What prompt did you use ?']",0,9,0.43,Discussion,1751543197.0
1lqmu19,StableDiffusion,"hedra character 3 , Alternative locally installed ?",Is there any locally installed model can do similar to hedrea with lipsync ?,"['Hunyuan Video Avatar.  But in my tests, it was nowhere near as good as Hedra 3.']","['Hunyuan Video Avatar.  But in my tests, it was nowhere near as good as Hedra 3.']",0,1,0.33,Question - Help,1751540928.0
1lqlmhn,StableDiffusion,Adviceneeded for not melting my laptop.,"I have an i7, 16gb xps13, with irisxe integrated graphics.

I want to learn about this whole Ai generated art thing so I got a copy of krita, went to github for a plugin and installed it.

...before I start playing with it, are there any beginner friendly models that I should focus on? I'm not necessarily looking for the highest quality, but I want to learn inpainting on what I have. Any advice at all? ","['> Iris XE graphics\n\nNot gonna happen.', ""That machine is very ill suited for this task. While you can probably get it to run, you'd have to spend a lot of time with finicky configuration and workarounds.\xa0\n\n\nWorse than that, even if it runs, it'll run very poorly. As in practically unusable. It'd be about as much fun as trying to play Cyberpunk in 4k with ultra settings on that laptop.\xa0\n\n\nTLDR: don't waste your time trying to fit a square peg into a round hole. Use online services instead."", ""SD1.5 has very low requirements and SDXL is also much easier on the hardware than Flux.\n\nTo get started they are great models.\n\nThe only thing to remember: every models needs it's own way to handle it. Once you've mastered prompting for SD1.5 you are starting from scratch when it comes to SDXL. And then Flux is again completely different.\n\nAnd with SD1.5 and SDXL you need fine tunes from the community to get great results. Only Flux is working well out of the box."", ""First step: Go buy some books, you'll have a lof of time for reading, with such a crap system. Sorry, Integrated Graphics don't cut it for AI."", 'More of a question to other users. I read invoke has a cloud based version. Does it have the inpainting and advanced features the local one has? If so, op can look into that']","['> Iris XE graphics\n\nNot gonna happen.', ""That machine is very ill suited for this task. While you can probably get it to run, you'd have to spend a lot of time with finicky configuration and workarounds.\xa0\n\n\nWorse than that, even if it runs, it'll run very poorly. As in practically unusable. It'd be about as much fun as trying to play Cyberpunk in 4k with ultra settings on that laptop.\xa0\n\n\nTLDR: don't waste your time trying to fit a square peg into a round hole. Use online services instead."", ""SD1.5 has very low requirements and SDXL is also much easier on the hardware than Flux.\n\nTo get started they are great models.\n\nThe only thing to remember: every models needs it's own way to handle it. Once you've mastered prompting for SD1.5 you are starting from scratch when it comes to SDXL. And then Flux is again completely different.\n\nAnd with SD1.5 and SDXL you need fine tunes from the community to get great results. Only Flux is working well out of the box."", ""First step: Go buy some books, you'll have a lof of time for reading, with such a crap system. Sorry, Integrated Graphics don't cut it for AI."", 'More of a question to other users. I read invoke has a cloud based version. Does it have the inpainting and advanced features the local one has? If so, op can look into that']",0,11,0.33,Question - Help,1751536540.0
1lqlf9f,StableDiffusion,ControlNet - Forge WebUI.  Am I using it wrong?,"Hey.
I wanted to reecreate this pose from fight club.
I've put the pose pic in  control net #1 as reference only.

I've put openpose pic, which I created in [PoseMy.art](http://PoseMy.art) as open pose in control net #2.

https://preview.redd.it/h969xr3vqmaf1.jpg?width=2285&format=pjpg&auto=webp&s=316f0ed6642134a62f16c66f3d34fff22d495272

Shouldn't this create something similar to the photo?
I'm very new to all of this.

Any advice how to proceed?

https://preview.redd.it/epfntallrmaf1.jpg?width=2194&format=pjpg&auto=webp&s=b00ebd0241f26a2ff79975dab56aa1e9ed847879

These are both ControlNet settings","[""You didn't say what models you use""]","[""You didn't say what models you use""]",1,5,0.67,Question - Help,1751535729.0
1lqldyi,StableDiffusion,Can I finetune a model to create pictures of a specific person in different settings? If after tuning I load few images of another person then the model produce good results? Any work to recommend?,,"['The answer for first question is yes. 2nd question, I am not sure what you are asking.']","['The answer for first question is yes. 2nd question, I am not sure what you are asking.']",0,3,0.33,Question - Help,1751535587.0
1lql8gd,StableDiffusion,I keep getting this error : clip missing: ['text_projection.weight'] second photo is the ./clip folder,,"[""As far as i can tell, you can ignore it since it doesn't seem to affect quality."", 'Based on the posts that I saw about this error - you need to change weight dtype from default to fp8\\_e4m3fn,', 'this is the fix\n\n[https://www.reddit.com/r/comfyui/comments/1ftizll/issue\\_fixed\\_clip\\_missing\\_text\\_projectionweight/](https://www.reddit.com/r/comfyui/comments/1ftizll/issue_fixed_clip_missing_text_projectionweight/)', ""\\['text\\_projection.weight'\\]\n\nthis is not a file, but more so the tensor module in clip l try redownload clip l since it should be 248 mb not 241 mb\n\nhttps://preview.redd.it/nxwb1mi4rmaf1.png?width=837&format=png&auto=webp&s=5d1b000832e1af0d0ef67bc939bd0f63aea1fe74\n\nsee, this is is the text projection.weight\n\n[https://huggingface.co/Comfy-Org/HiDream-I1\\_ComfyUI/blob/main/split\\_files/text\\_encoders/clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)\n\nyes i know it is hidream. but it can do it"", 'You would show the entire workflow, it is not known where the connection from DualClipLoader goes', ""Clip_L doesn't have a text projection tensor, it's something that Clip_G uses for example. If it doesn't crash the program, you can safely ignore this error, though it would make more sense if it didn't show an error message here."", ""I get that issue all the time. I don't know why it does it but I've been successfully ignoring it for 6 months now.\n\nif you're getting the error and it seems like it's taking a long time to load and you think its frozen, just keep waiting. it could take 20 minutes for the first render but the next one will be only seconds."", 'Replace your ClipTextEncode Node with a CLIPTextEncodeFlux node, this will apply both clip and t5 conditioning and should stop the errors.']","[""As far as i can tell, you can ignore it since it doesn't seem to affect quality."", 'Based on the posts that I saw about this error - you need to change weight dtype from default to fp8\\_e4m3fn,', 'this is the fix\n\n[https://www.reddit.com/r/comfyui/comments/1ftizll/issue\\_fixed\\_clip\\_missing\\_text\\_projectionweight/](https://www.reddit.com/r/comfyui/comments/1ftizll/issue_fixed_clip_missing_text_projectionweight/)', ""\\['text\\_projection.weight'\\]\n\nthis is not a file, but more so the tensor module in clip l try redownload clip l since it should be 248 mb not 241 mb\n\nhttps://preview.redd.it/nxwb1mi4rmaf1.png?width=837&format=png&auto=webp&s=5d1b000832e1af0d0ef67bc939bd0f63aea1fe74\n\nsee, this is is the text projection.weight\n\n[https://huggingface.co/Comfy-Org/HiDream-I1\\_ComfyUI/blob/main/split\\_files/text\\_encoders/clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)\n\nyes i know it is hidream. but it can do it"", 'You would show the entire workflow, it is not known where the connection from DualClipLoader goes']",3,18,0.71,Question - Help,1751534955.0
1lqk9uc,StableDiffusion,B&B,,['That’s disturbing. Did you do a style change with Kontext?'],['That’s disturbing. Did you do a style change with Kontext?'],0,1,0.43,Comparison,1751531014.0
1lqk6df,StableDiffusion,Is there an ai image fusion tool ?,I wonder there's an ai tool that allows you to fusion any kind of picture images like character fusion or object fusion. you put 2 random images or more  and the ai tool will fuse the 2 images together and create one single combination image. And the only ai tool that can do something like that was just vidnoz ai but vidnoz was full of micro transaction. And I need some ai tool like that that allows you to use for free for the first time users,"[""That's what flux kontext actually can do"", 'Ipadapters for sdxl, flux redux. But most probably you have some specific case in mind and they will disappoint you', 'WAN Phantom.\n\nI tried most of them, CN Reference, IP-Adapters, Kontext, ... nothing else comes even close.\n\nIt can do more than 2 images (preparing them like removing backgrounds helps) and is more flexible than any other method. Usually no bleeding/character merging even with mediocre prompts.\n\nThe bad news is that it is an animation model. So it is slow (even if you only do a couple of frames) and the resolution is limited.']","[""That's what flux kontext actually can do"", 'Ipadapters for sdxl, flux redux. But most probably you have some specific case in mind and they will disappoint you', 'WAN Phantom.\n\nI tried most of them, CN Reference, IP-Adapters, Kontext, ... nothing else comes even close.\n\nIt can do more than 2 images (preparing them like removing backgrounds helps) and is more flexible than any other method. Usually no bleeding/character merging even with mediocre prompts.\n\nThe bad news is that it is an animation model. So it is slow (even if you only do a couple of frames) and the resolution is limited.']",0,6,0.17,Question - Help,1751530620.0
1lqk5yb,StableDiffusion,Jib Mix Flux or Real Dream-flux1 which one gives the best realistic outcome,for people who have worked with both of these models if you had to choose one for realistic human faces which one would it be?,[],[],1,0,1.0,Question - Help,1751530573.0
1lqk4gw,StableDiffusion,How do people make high-quality AI live-action castings of Street Fighter characters with real Korean celebrities?,"Hi everyone,

I've seen some amazing AI-generated content lately where anime or game characters are reimagined as real-life actors — and I'm especially interested in doing something similar with \*\*Street Fighter characters\*\*, using \*\*Korean celebrities\*\* for a cinematic, live-action-style reinterpretation.

For example:

\- Chun-Li as a Korean actress like Kim Tae-ri or Kim Yoo-jung

\- Ryu or Ken reimagined with Korean male actors

\- Stylized like a movie poster or cinematic still



I'm trying to figure out:

\- What’s the best tool for this — Midjourney or Leonardo AI? Or something else?

\- How do people create realistic portraits that still keep the feel of the original character?

\- Do I need to use image-to-image, IP-Adapter, or reference images to get facial resemblance?

\- Any tips for crafting prompts that combine character traits + celebrity look + cinematic setting?

\- How do you achieve style consistency if making a set (e.g. a whole cast lineup)?

If you’ve done anything similar (fan-casts, reimaginings, poster-style AI art), I’d love to see examples or hear your workflow.

Thanks in advance!

[https://www.youtube.com/watch?v=Q8r369wvM\_M](https://www.youtube.com/watch?v=Q8r369wvM_M)",['일단 원본 캐릭터 이미지를 하나 생성한 다음 거기에 한국 연예인 얼굴로 페이스 스왑한 게 아닐까 싶네요. 페이스스왑 관련 워크플로우를 찾아보는 게 좋을 거 같습니다.'],['일단 원본 캐릭터 이미지를 하나 생성한 다음 거기에 한국 연예인 얼굴로 페이스 스왑한 게 아닐까 싶네요. 페이스스왑 관련 워크플로우를 찾아보는 게 좋을 거 같습니다.'],0,1,0.25,Question - Help,1751530400.0
1lqk1nk,StableDiffusion,Flux inpainting,"What do you think is the best model for inpainting?
- flux.1 dev?
- flux.1 fill
- flux.1 kontext?","[""Generally, Flux Fill is better. Although some tasks could be done better with Kontext, specifically where you need it to be very similar to what you're inpainting."", 'flux fill probably? thats why it was created.', 'I find Flux Schnell actually quite good at inpainting - for non-photorealistic content, at least. Definitely much faster than the rest', 'Flux Fill for general knowledge elements (or easy concept that can be quickly introduced with Flux Redux), Flux Dev+LoRA inpainting for specific/obscure knowledge that requires high resemblance (a specific face, object, etc.), and then Flux Kontext for other uses (combining two objects, change text, or if the task is too tricky to do with Flux Fill alone), it really just depends on your use case', ""I have a Flux Inpaint workflow that works very well even with Flux Dev Loras. My trick is to do the 1st pass with Flux Fill at 1.00 denoising to get a rough draft with great composition but bad details. Then I send it to a 2nd pass with Flux Dev at 0.50 denoising to refine the details. Enable the Lora for both passes. I'll edit in the workflow link. \n\nEdit: https://pastebin.com/QWeeSmwM"", ""In terms of details, I think the best tool for repairing currently is Photoshop's fill in AI, which cannot be achieved by any other model on the market\\~\\~""]","[""Generally, Flux Fill is better. Although some tasks could be done better with Kontext, specifically where you need it to be very similar to what you're inpainting."", 'flux fill probably? thats why it was created.', 'I find Flux Schnell actually quite good at inpainting - for non-photorealistic content, at least. Definitely much faster than the rest', 'Flux Fill for general knowledge elements (or easy concept that can be quickly introduced with Flux Redux), Flux Dev+LoRA inpainting for specific/obscure knowledge that requires high resemblance (a specific face, object, etc.), and then Flux Kontext for other uses (combining two objects, change text, or if the task is too tricky to do with Flux Fill alone), it really just depends on your use case', ""I have a Flux Inpaint workflow that works very well even with Flux Dev Loras. My trick is to do the 1st pass with Flux Fill at 1.00 denoising to get a rough draft with great composition but bad details. Then I send it to a 2nd pass with Flux Dev at 0.50 denoising to refine the details. Enable the Lora for both passes. I'll edit in the workflow link. \n\nEdit: https://pastebin.com/QWeeSmwM""]",0,13,0.5,Question - Help,1751530070.0
1lqjl3s,StableDiffusion,Is there a 14B version of Self-Forcing that is causal ?,The only one I found is bidirectional: [https://www.reddit.com/r/StableDiffusion/comments/1lcz7ij/wan\_14b\_self\_forcing\_t2v\_lora\_by\_kijai/](https://www.reddit.com/r/StableDiffusion/comments/1lcz7ij/wan_14b_self_forcing_t2v_lora_by_kijai/),[],[],1,0,0.57,Question - Help,1751528214.0
1lqij1s,StableDiffusion,"New Optimized Flux Kontext Workflow Works with 8 steps, with fine tuned step using Hyper Flux LoRA + Teacache and Upscaling step",,"['I think the workflow is missing?', 'Ad']","['I think the workflow is missing?', 'Ad']",0,2,0.33,Comparison,1751524072.0
1lqiiwb,StableDiffusion,"How do we avoid 'mila kunis' in flux kontext? When converting illustration to photo, the typical face shows up over and over",Has anyone a clever technique to have flux at least TRY to match the facial features of the prompt image?,"['This is the best solution so far: [https://civitai.com/models/766608/sameface-fix-flux-lora?modelVersionId=857446](https://civitai.com/models/766608/sameface-fix-flux-lora?modelVersionId=857446)\n\nIt uses a very interesting way of avoiding the same face effect.', 'kontext is built on flux, man. \n\nFlux face is what you are going to get.', 'Did you use ""maintain facial features"" in prompt?', 'Doesnt look like mila kunis and it is following the facial features. The picture of jinx (i believe that is her) is anime and as such unrealistic porportionally to real people. Kontext is translating it into a more realistic porportions. Thats what a real person with real features might look like if they were cast as jinx in say a movie', ""1 week and I already can't stand this face anymore..."", 'Shut up Meg.', 'You can try mixing my lora [UnFlux - Flux Dev v1.0 | Flux LoRA | Civitai](https://civitai.com/models/1732024) It works more realistically when used with Flux Dev even though it is trained with Flux Kontext, to work perfectly in Flux Kontext you need to select the correct Sampler and Scheduler.', ""Don't use distilled models""]","['This is the best solution so far: [https://civitai.com/models/766608/sameface-fix-flux-lora?modelVersionId=857446](https://civitai.com/models/766608/sameface-fix-flux-lora?modelVersionId=857446)\n\nIt uses a very interesting way of avoiding the same face effect.', 'kontext is built on flux, man. \n\nFlux face is what you are going to get.', 'Did you use ""maintain facial features"" in prompt?', 'Doesnt look like mila kunis and it is following the facial features. The picture of jinx (i believe that is her) is anime and as such unrealistic porportionally to real people. Kontext is translating it into a more realistic porportions. Thats what a real person with real features might look like if they were cast as jinx in say a movie', ""1 week and I already can't stand this face anymore...""]",8,36,0.57,Question - Help,1751524054.0
1lqhwai,StableDiffusion,How to enhance interior photos?,"Hi everyone,

I need help. I want to enhance my interior photos so they look professional, but without changing anything in the room. I've been able to mostly do it with chat gpt but its slow and not always perfect. (Pics attached at the bottom of my before and afters).


My question is, is there another or better way to get these results? I just signed up for Hugging Face after stumbling upon this forum but I have NO idea what I'm doing. I need help with someone who has experience.

here are my chat gpt results im looking to replicate:

https://preview.redd.it/hqo7dizwklaf1.png?width=1536&format=png&auto=webp&s=12b804eee176d6a84b8ea4652fef01d265f5c522

https://preview.redd.it/pixbvt3yklaf1.jpg?width=3402&format=pjpg&auto=webp&s=59d81c9630ce06892e46216542637bbb4bf7be2f

",[],[],0,0,0.33,Question - Help,1751521696.0
1lqhkl3,StableDiffusion,flux kontext + controlnet is possible ?,"flux kontext + controlnet is possible ? any workflow working ?
is it possible to copy the exact body position from the image 1 and recreate for the image 2 , or use the style in the image 1 in the image 2, I've tried both nothing worked , anyone knows how to do this kind of thing , because I've tried so far and no results...","[""Prompting doesn't help. I tried it too, same problem, question is highly relevant. Unless you can hook up a canny controlnet, it's gonna change the size and the position of the elements a little bit. I can just hope someone will find out how to do it. DM me if someone figured it out."", 'yes you would need if you want to mantain exactly the same shape and proportions as IC Light V2 does. Utherwise is always changing a bit the output', ""Yes, it's possible, but it's not perfect and it's not gonna give you what you're expecting, also multiple tries (different seeds) are required, but try this...\n\nStart with your base image that contains the desired pose, and use a prompt like 'convert the image to a depth 3D map' (or apply any other method to extract the depth map). Then, combine that depth image with the image of your subject and use a prompt such as 'the woman is in the pose of the 3D model'"", ""You can use an image (not necessarily the reference image) as the latent via VAE Encode and low denoise to 0.7-0.9, that way you have referenced img2img, or if you paint a mask, you have referenced inpainting.\n\nStill, I noted that I need a depthmap model for Kontext anyway, when the camera perspective is challenging and I want a specific scene sketched with another SDXL-based model. Flux Depth is an independent model so it doesn't work with Kontext. I hope someone will release something at some point, for casual usage it may seem that prompting does everything you want, but that's not the case if you have a concrete composition/POV  in mind."", ""you do not need controlnet for flux kontext. Just prompt's way""]","[""Prompting doesn't help. I tried it too, same problem, question is highly relevant. Unless you can hook up a canny controlnet, it's gonna change the size and the position of the elements a little bit. I can just hope someone will find out how to do it. DM me if someone figured it out."", 'yes you would need if you want to mantain exactly the same shape and proportions as IC Light V2 does. Utherwise is always changing a bit the output', ""Yes, it's possible, but it's not perfect and it's not gonna give you what you're expecting, also multiple tries (different seeds) are required, but try this...\n\nStart with your base image that contains the desired pose, and use a prompt like 'convert the image to a depth 3D map' (or apply any other method to extract the depth map). Then, combine that depth image with the image of your subject and use a prompt such as 'the woman is in the pose of the 3D model'"", ""You can use an image (not necessarily the reference image) as the latent via VAE Encode and low denoise to 0.7-0.9, that way you have referenced img2img, or if you paint a mask, you have referenced inpainting.\n\nStill, I noted that I need a depthmap model for Kontext anyway, when the camera perspective is challenging and I want a specific scene sketched with another SDXL-based model. Flux Depth is an independent model so it doesn't work with Kontext. I hope someone will release something at some point, for casual usage it may seem that prompting does everything you want, but that's not the case if you have a concrete composition/POV  in mind."", ""you do not need controlnet for flux kontext. Just prompt's way""]",2,11,0.63,Question - Help,1751520499.0
1lqha4s,StableDiffusion,Is Banodoco Dough still maintained?,"Is Banodoco /Dough still maintained? I notice there has not been an update since Dec 2024. I'm a bit confused about all these open source AI platforms .. Is there any site explainig all the differences.

Also the Pinokio browser which Dough relies on is down.

Does StableDiffusion do everything all the other AI platforms do combined?","['I assume you are talking about the following software?\n\n[https://github.com/banodoco/Dough](https://github.com/banodoco/Dough)\n\nNext time link what you are talking about so people dont have to hunt for it.\n\nAnd if you were to look at the link, right at the top next to the name of the project ""Dough"" is a label **""Public archive"".**\n\nThis means the project was stopped, abandoned or is on hiatus. As such, its no longer maintained and not currently worked on.\n\nPinokio isnt down though. You just didnt bother to look for the official link.\n\n[https://github.com/pinokiocomputer/pinokio](https://github.com/pinokiocomputer/pinokio)\n\n[https://pinokio.co/docs/#/?id=install](https://pinokio.co/docs/#/?id=install)']","['I assume you are talking about the following software?\n\n[https://github.com/banodoco/Dough](https://github.com/banodoco/Dough)\n\nNext time link what you are talking about so people dont have to hunt for it.\n\nAnd if you were to look at the link, right at the top next to the name of the project ""Dough"" is a label **""Public archive"".**\n\nThis means the project was stopped, abandoned or is on hiatus. As such, its no longer maintained and not currently worked on.\n\nPinokio isnt down though. You just didnt bother to look for the official link.\n\n[https://github.com/pinokiocomputer/pinokio](https://github.com/pinokiocomputer/pinokio)\n\n[https://pinokio.co/docs/#/?id=install](https://pinokio.co/docs/#/?id=install)']",0,2,0.44,Question - Help,1751519485.0
1lqgwl4,StableDiffusion,Homemade SD1.5 major update 1❗️,I’ve made some major improvement to my custom mobile homemade SD1.5 model. All the pictures I uploaded were created purely by the model without using any loras or addition tools. All the training and pictures I uploaded were made using my phone. I have a Mac mini m4 16gb on the way so I’m excited to push the model even further. Also I’m almost done fixing the famous hand/finger issue that sd1.5 is known for. I’m striving to make it or get as close to Midjourney as I can in term of capability. ,"[""Great results with 1.5, and I love hearing about others who are using a Mac for image generation. I've been using an M1 16GB that I got when they first released in 2020, and it's incredible to me that they're able to do things today that we couldn't even imagine 5 years ago. \n\nSince you mentioned making these using your phone, I imagine you're using Draw Things? For anyone who hasn't heard of it, I truly consider it a must-have on the Mac. There's nothing like it on Windows or Linux. It's super user friendly compared to all the web UIs. Even if Nvidia hardware is way faster, I'd still rather generate on a Mac because of Draw Things."", 'Nice work with resolution. Are you rendering it?', 'How can i use this?', 'Looking good :)', ""What phone and software are you using to train the model? Is it done on the phone locally, or in colab-like web environment where it's cloud hardware doing the work?"", 'How long does it take per image generated on your phone?', 'You deserve an upvote! This looks amazing!', 'Looks high-level!', 'How are you editing or modifying the weights, or is this all post-model?', 'Do you have this working well with sd15’s IPadapters, controlnets, and motion models?']","[""Great results with 1.5, and I love hearing about others who are using a Mac for image generation. I've been using an M1 16GB that I got when they first released in 2020, and it's incredible to me that they're able to do things today that we couldn't even imagine 5 years ago. \n\nSince you mentioned making these using your phone, I imagine you're using Draw Things? For anyone who hasn't heard of it, I truly consider it a must-have on the Mac. There's nothing like it on Windows or Linux. It's super user friendly compared to all the web UIs. Even if Nvidia hardware is way faster, I'd still rather generate on a Mac because of Draw Things."", 'How can i use this?', 'Nice work with resolution. Are you rendering it?', 'Looking good :)', ""What phone and software are you using to train the model? Is it done on the phone locally, or in colab-like web environment where it's cloud hardware doing the work?""]",100,47,0.84,News,1751518170.0
1lqfslh,StableDiffusion,Can A finetuned LCM SDLX  on a specfic domain beat DALE 3?,"Hello, I am currently working on a startup idea and we need to assess competition, any one with expertise on the topic?","['> specific domain\n\nPorn', 'Huh? DMD2 is better than LCM for XL anyway.', 'This really depends on what the domain is. I think something specific that does not require complex natural language or high accuracy, like architecture, could beat dall-e 3 (not latest gpt 4o) but it needs good dataset.', '- Adult content of any type\n- Inpainting (we got more control)', 'startup + ask on reddit = :D']","['> specific domain\n\nPorn', 'Huh? DMD2 is better than LCM for XL anyway.', 'This really depends on what the domain is. I think something specific that does not require complex natural language or high accuracy, like architecture, could beat dall-e 3 (not latest gpt 4o) but it needs good dataset.', '- Adult content of any type\n- Inpainting (we got more control)', 'startup + ask on reddit = :D']",0,14,0.14,Question - Help,1751514452.0
1lqf1y2,StableDiffusion,SimpleTuner v2.0.1 with 2x Flux training speedup on Hopper + Blackwell support now by default,"[https://github.com/bghira/SimpleTuner/releases/tag/v2.0.1](https://github.com/bghira/SimpleTuner/releases/tag/v2.0.1)

Also, now you can use Huggingface Datasets more directly, as it has its own defined databackend type, a caching layer, and fully integrated into the dataloader config pipeline such that you can cache stuff to s3 buckets or local partition, as usual.

Some small speed-ups for S3 dataset loading w/ millions of samples.


Wan 14B training speedups to come soon.","['heads up everyone: if you use this to train nsfw content he will try removing your models from every platform you publish on. doesnt matter, flux dev, flux kotext, wan, everything. do not use this trainer.', 'Do you have plans to support Chroma? I don’t see it on the GitHub.']","['heads up everyone: if you use this to train nsfw content he will try removing your models from every platform you publish on. doesnt matter, flux dev, flux kotext, wan, everything. do not use this trainer.', 'Do you have plans to support Chroma? I don’t see it on the GitHub.']",23,7,0.81,Resource - Update,1751512073.0
1lqeq7w,StableDiffusion,Installing WAN 2.1 with web interface,Sorry for the newbie question - I'm looking to test WAN 2.1 and all the tutorials for installation date back to February/March. Has the installation of WAN changed at all since then? Does installing WAN automatically install the web interface? Do I need a specific installation configuration for my specific card (3080)? Thanks!,[],[],1,0,1.0,Question - Help,1751511060.0
1lqee2e,StableDiffusion,Is there no way to reduce chest size for anime flux?,"I thought Kontext might be able to fix this but apparently it completely refuses any modification that it considers ""part of the character"". Then I searched the entirety of civitai and the 6\~7 flat chest loras are all realistic. The flux model also has no negative prompts and will mostly ignore flat chested positive prompts. ((((((((this also does not work in flux))))))). Is there just no way?",[],[],3,0,1.0,Question - Help,1751510007.0
1lqdwyr,StableDiffusion,Yes.,,"['In my opinion, this is beginning to wear out its welcome. After the last five or so, we get the idea already.', 'No.\xa0', 'No.', 'Kontext is just a glorified meme remaster generator.', 'Maybe?', 'He has now 5 fingers? Only God has it\xa0\n..', 'What prompt', 'Well, I was going to sleep. Thanks OP.', 'I have fear']","['In my opinion, this is beginning to wear out its welcome. After the last five or so, we get the idea already.', 'No.\xa0', 'No.', 'Kontext is just a glorified meme remaster generator.', 'Maybe?']",0,13,0.41,Comparison,1751508561.0
1lqditd,StableDiffusion,Universal Method for Training Kontext Loras without having to find pairs of images or edit,"So, the problem with Flux Kontext is that it needs pairs of images. For example, if you want to train an oil painting you would need a photo of a place + a corresponding painting.

It can be slow and laborious to edit or find pairs of images.

BUT - it doesn't have to be that way.

1) Get the images in the style you want. For example, Pixar Disney style.

2) Use Flux Kontext to convert these images to a style that Flux Kontext's basic model already knows. For example, cartoon.

So, you will train a Lora on a pair of Pixar images + Pixar converted to cartoon.

3) After Lora is trained. Choose any image. Photo of New York City. Use Flux Kontext to convert this photo to cartoon.

4) Lastly, apply Lora to the cartoon photo of New York City



This is a hypothetical method


","[""if kontext already knows how to do it why would you need a lora for it? the point of a lora is to provide a model with info it doesnt already know and youre just feeding it info it already knows? wouldnt it be better to use controlnet using a completely different model (SDXL, SD1.5), if we're using ai? you could even use chatgpt created pairs to train kontext."", ""Yes, I can confirm this works. I've created all my datasets this way so far. (For Kontext Lora)"", 'In theory this works, in practice Kontext output is always slightly degraded in image quality (higher contrast etc.). Using Kontext-generated images for your LoRa training may reinforce this degradation and make it even more prominent.', 'Yup but takes a lot of work cause you need to sort through the garbage flux generates on occasion. \n\nAnd this doesn’t work well for nsfw although is possibly but very tricky.', 'you dont need pairs of images what.\n\nyou literally can train a style or outfit or whatever into kontext just like you would with normal flux dev.\n\ni literally am converting all my as of now published doras to kontext right now and i am using literally the exact same training workflow and datasets and everything again, only changing the model safetensors file i train on from normal dev to kontext. and so far all these styles and outfits and stuff work very well just like dev.\n\nin kohya that is. kohya has no official kontext supoort atm but it seems to work just fine anyway.', 'Needs a document or post with a series of example images and tags and things probably.', 'Just use the image stitch node in comfyui hooked up with batch load images, if you want it to learn more portraits you would stitch down first, then the stitch node again again with down I think and it will make it skinny. You can also stitch right then stich down just use for a landscape and because you have all your data you could start with the output you want and then scale everything else around it. Like what I was saying but in reverse. You could also speed it up and take comfy out by asking deepseek for a python pillow program that did this to a set of 4/3/2 folders depending on the number of inputs you wanted to train.\xa0', 'Yeah, I figured this would be the obvious way to do it; I don’t think you even need step 3 because I don’t think the model cares that much about what it’s converting from. You could even enforce this in the training by creating pairs with different styles.', 'i trained different kontext styles, at least 10 of them with a few uploaded to civitai. All the styles are trained like regular flux without pairing. \n\nPairing is need mostly to trian particular conversion contexts.']","[""if kontext already knows how to do it why would you need a lora for it? the point of a lora is to provide a model with info it doesnt already know and youre just feeding it info it already knows? wouldnt it be better to use controlnet using a completely different model (SDXL, SD1.5), if we're using ai? you could even use chatgpt created pairs to train kontext."", ""Yes, I can confirm this works. I've created all my datasets this way so far. (For Kontext Lora)"", 'In theory this works, in practice Kontext output is always slightly degraded in image quality (higher contrast etc.). Using Kontext-generated images for your LoRa training may reinforce this degradation and make it even more prominent.', 'Yup but takes a lot of work cause you need to sort through the garbage flux generates on occasion. \n\nAnd this doesn’t work well for nsfw although is possibly but very tricky.', 'you dont need pairs of images what.\n\nyou literally can train a style or outfit or whatever into kontext just like you would with normal flux dev.\n\ni literally am converting all my as of now published doras to kontext right now and i am using literally the exact same training workflow and datasets and everything again, only changing the model safetensors file i train on from normal dev to kontext. and so far all these styles and outfits and stuff work very well just like dev.\n\nin kohya that is. kohya has no official kontext supoort atm but it seems to work just fine anyway.']",36,23,0.73,Discussion,1751507372.0
1lqd65o,StableDiffusion,train flux kontext lora without image pairs,"Is it possible to train Flux Context Loras without image pairs? If so, what tools should I use and what parameters give the best results?","['As it is an edit model you should train it how to edit - and thus you need pairs.\n\nMisusing it as an text2image model is possible, but for that task normal Flux is usually suited better.\n\n  \nAbout the image pairs: It sounds hard to get them as this is the task you want to offload to the AI.  \nBut in reality it often isn\'t that had - you just need to do the opposite. Just start with the target image and derive from it the source image.\n\nExample:\n\nColoring a b/w image is hard. But taking a color image and turn it to a b/w image is simple. And when you do that you already have a b/w image and it\'s ""colorized"" version. So a perfect pair of images for training.', 'For style and concept LoRAs, yes, because you are just teaching it a new style or concept.\n\nFor editing, probably not, because without the pair, how would the A.I. know what the edit is supposed to do?']","['As it is an edit model you should train it how to edit - and thus you need pairs.\n\nMisusing it as an text2image model is possible, but for that task normal Flux is usually suited better.\n\n  \nAbout the image pairs: It sounds hard to get them as this is the task you want to offload to the AI.  \nBut in reality it often isn\'t that had - you just need to do the opposite. Just start with the target image and derive from it the source image.\n\nExample:\n\nColoring a b/w image is hard. But taking a color image and turn it to a b/w image is simple. And when you do that you already have a b/w image and it\'s ""colorized"" version. So a perfect pair of images for training.', 'For style and concept LoRAs, yes, because you are just teaching it a new style or concept.\n\nFor editing, probably not, because without the pair, how would the A.I. know what the edit is supposed to do?']",0,2,0.25,Question - Help,1751506306.0
1lqd5gj,StableDiffusion,Best optimizer for full SDXL/Illustrious finetune?,"Hi, I am trying to fully finetune my first SDXL model, what optimizer would you prefer? For all Lora's I trained I used Prodigy, but I don't think I can fit it in the VRAM even on rented GPU. What would you suggest. What LR should I use and should I train TE as well. Thank you for replying.","['Before I got perma banned from kaggle I was using Adafactor with fused back pass, unet LR 0.000005, TE1 LR 0.000003, uses 15.9 GB vram, without mixed precision']","['Before I got perma banned from kaggle I was using Adafactor with fused back pass, unet LR 0.000005, TE1 LR 0.000003, uses 15.9 GB vram, without mixed precision']",0,2,0.5,Question - Help,1751506249.0
1lqbww1,StableDiffusion,Todays Kontext challenge...,"**Challenge: to remove the platform underneath the stones and blend it in with the grass surrounds without effecting the stones texture or color.**

***Rules: Any means necessary using prompting in Kontext workflow, but without using masking to achieve it or external tools. i.e. it must be prompt driven and Kontext model. It also must be a Kontext workflow, and you have to share how you did it.***

This began as a test using a 3D blender model screenshot in grey. I have only used photo references of Stonehenge to drive it this far, but I am not fussy about that on this final stage. I was testing the ability of Kontext to control consistency of environment background materials and looks (i.e using another image to restyle which is actually very difficult) because if we can achieve that with Kontext, time consuming modelling of 3D scenes for making video camera positions, becomes moot.

I have achieved a lot with this process, but one thing evading me still is getting rid of the damn grid and platform, and I have no idea why it is so hard to target it.

Here is how I got from 3D model to this stage with only image to image restyling. I realised the best way to approach Kontext for image to image restyling is to target just one thing at a time, then run it through again.

*(Step 1 used the chained reference latent method and two images the 3D model and a photo of stonehenge at a different angle. Step 2 wouldnt work with reference latent chain method but did work with image stitch method and same two images)*

**Step 1 -** color the stones. prompt: \`extract the image of stones in the photo and use that to swap out all the stones in the 3D model. keep the structure of the model when applying the stone texture.\`

**RESULT**: it tiled stone everywhere using the image provided, but everything including the base and what is now grass, got turned to stone.

**step 2** \- color the grass. prompt: \`extract the image of grass in the photo and use that to swap out the ground beneath the stones in the 3D model. keep the structure and texture of the stones in the model the same, only change the ground to grass.\`

**RESULT:** you are looking at it.

The problem I now have is targeting that gridded platform successfully to get rid of it. It just wont do it. Can you?

https://preview.redd.it/z88d01530kaf1.png?width=1328&format=png&auto=webp&s=4e6de40db025f1a1783a4ed855fff10b9107e931","[""https://preview.redd.it/arx3uoo5kkaf1.png?width=1466&format=png&auto=webp&s=5829e12a076fed5e74b0b94efd8fed04c7fbb404\n\ni couldn't blend the platform, this is the closest i could get with two steps, if you dont need exactly the same grass this could work"", 'https://preview.redd.it/3ssquuxnwsaf1.png?width=1280&format=png&auto=webp&s=db86781cfb426bac6a38c01b1c8aeaa72b9c03b5\n\nprompt: "" remove the (grided hexagon base) "" using flux kontext']","[""https://preview.redd.it/arx3uoo5kkaf1.png?width=1466&format=png&auto=webp&s=5829e12a076fed5e74b0b94efd8fed04c7fbb404\n\ni couldn't blend the platform, this is the closest i could get with two steps, if you dont need exactly the same grass this could work"", 'https://preview.redd.it/3ssquuxnwsaf1.png?width=1280&format=png&auto=webp&s=db86781cfb426bac6a38c01b1c8aeaa72b9c03b5\n\nprompt: "" remove the (grided hexagon base) "" using flux kontext']",0,8,0.11,Question - Help,1751502520.0
1lqbbzo,StableDiffusion,Automated illustration of a Conan story using language models + flux and other local models,[https://brianheming.substack.com/p/making-illustrated-conan-adventures-039](https://brianheming.substack.com/p/making-illustrated-conan-adventures-039),"['I tried this a few years ago with Dracula -- https://docs.google.com/document/d/1IsnynQZoxOBmZx9Jac4DfWn15YCevG63CxsIkbu8tgE/edit?tab=t.0\n\nI found it helpful to take a block of text and have an LLM summarize it, then have another LLM create the SD prompt, then make the images. \n\nhttps://github.com/pwillia7/booksplitter', ""That's pretty cool.  I'd never do it myself, because even with LORA's or other training, the characters never look entirely consistent between images.  I guess it's perfectionism (or quality control) but to me that's a bit of a dealbreaker for something like this, especially if you're going to ask money for it.""]","['I tried this a few years ago with Dracula -- https://docs.google.com/document/d/1IsnynQZoxOBmZx9Jac4DfWn15YCevG63CxsIkbu8tgE/edit?tab=t.0\n\nI found it helpful to take a block of text and have an LLM summarize it, then have another LLM create the SD prompt, then make the images. \n\nhttps://github.com/pwillia7/booksplitter', ""That's pretty cool.  I'd never do it myself, because even with LORA's or other training, the characters never look entirely consistent between images.  I guess it's perfectionism (or quality control) but to me that's a bit of a dealbreaker for something like this, especially if you're going to ask money for it.""]",24,5,0.9,Discussion,1751500839.0
1lqau9t,StableDiffusion,Discussion about training Kontext,"Anyone have any experiences yet? What are proper resolutions, and formatting , as in should it always be side by side , etc","['I still did not try because waiting for OneTrainer or Kohya-ss support, but here is the description from ""FluxKontextImageScale"" node about resolutions:\n\n>The following is a list of standard sizes used during model training.\n\n|Width|Height|Aspect Ratio|\n|:-|:-|:-|\n|672|1568|0.429|\n|688|1504|0.457|\n|720|1456|0.494|\n|752|1392|0.540|\n|800|1328|0.603|\n|832|1248|0.667|\n|880|1184|0.743|\n|944|1104|0.855|\n|1024|1024|1.000|\n|1104|944|1.170|\n|1184|880|1.345|\n|1248|832|1.500|\n|1328|800|1.660|\n|1392|752|1.851|\n|1456|720|2.022|\n|1504|688|2.186|\n|1568|672|2.333|']","['I still did not try because waiting for OneTrainer or Kohya-ss support, but here is the description from ""FluxKontextImageScale"" node about resolutions:\n\n>The following is a list of standard sizes used during model training.\n\n|Width|Height|Aspect Ratio|\n|:-|:-|:-|\n|672|1568|0.429|\n|688|1504|0.457|\n|720|1456|0.494|\n|752|1392|0.540|\n|800|1328|0.603|\n|832|1248|0.667|\n|880|1184|0.743|\n|944|1104|0.855|\n|1024|1024|1.000|\n|1104|944|1.170|\n|1184|880|1.345|\n|1248|832|1.500|\n|1328|800|1.660|\n|1392|752|1.851|\n|1456|720|2.022|\n|1504|688|2.186|\n|1568|672|2.333|']",0,1,0.25,Discussion,1751499437.0
1lqafe7,StableDiffusion,(off) the solution for the noise,"I think is kinda off, but I going to ask anyway, is there a good cooler to replace the original coolers from gpu, my is 3070, the noise when generating images is terrible, but for videos its a complete nightmare!
is there any a VERY silent cooler to replace the original from nvidia, a better and quiet, AI is great but when it comes to silence its terrible, every image and video its very annoying to generate because of the noise.","['This isn\'t the best place to be asking that question. However, as a brief answer: look up ""no shroud mod"" or ""shroudless mod"", strap on some Noctua G2\'s (for the low, low price of 45 USD each... what a bargain), and do a repaste with ptm7950 and thermal putty.', 'Noctua']","['This isn\'t the best place to be asking that question. However, as a brief answer: look up ""no shroud mod"" or ""shroudless mod"", strap on some Noctua G2\'s (for the low, low price of 45 USD each... what a bargain), and do a repaste with ptm7950 and thermal putty.', 'Noctua']",0,3,0.5,Question - Help,1751498280.0
1lqa7qx,StableDiffusion,Nice teeth bro...,,"['The first 5 ones were funny', 'https://i.redd.it/g9hnlevjmjaf1.gif', 'Welp. im not sleeping tonight.']","['The first 5 ones were funny', 'https://i.redd.it/g9hnlevjmjaf1.gif', 'Welp. im not sleeping tonight.']",0,5,0.43,Comparison,1751497702.0
1lq9t9g,StableDiffusion,"Any tips for building realistic characters, places, and animals with SDXL","Hey everyone,
I'm currently using SDXL models and trying to focus on creating realistic characters, environments, and animals. But I’m really struggling to find LoRAs that are both high quality and fully compatible with SDXL , many seem to be outdated or not giving great results. Right now I am using JuggernautXL.
Do you have any tips or tricks for achieving better realism with SDXL? Any specific LoRAs, settings, or workflows you swear by? ","['[https://github.com/roblaughter/style-reference](https://github.com/roblaughter/style-reference)\n\nCopy paste from there the prompts you like, and adjust to your need. Pure prompting, no Lora etc.\n\nThe discussion: [https://www.reddit.com/r/StableDiffusion/comments/1e0f0fd/prompt\\_only\\_photographic\\_style\\_reference\\_100/](https://www.reddit.com/r/StableDiffusion/comments/1e0f0fd/prompt_only_photographic_style_reference_100/)', 'Another great contribution from Rob Laughter, just for characters;\n\n[https://github.com/roblaughter/style-reference/blob/master/characters.md](https://github.com/roblaughter/style-reference/blob/master/characters.md)', 'Realvis 5 is the best realism model in my opinion', 'Splashed Mix DMD is a solid choice. It\'s great with photographic subjects while being a general-use model:\n\nhttps://civitai.com/models/932513?modelVersionId=1632035\n\nIt\'s fast, too, so you\'ll have an easy time testing your prompts.\n\nFor LoRAs, you\'ll want to use sdxl_enchance: https://civitai.com/models/99619?modelVersionId=947415\n\nIt\'s one of the few detailer LoRAs to actually improve your outputs without changing the style or subject like most ""detailer"" LoRAs do.']","['[https://github.com/roblaughter/style-reference](https://github.com/roblaughter/style-reference)\n\nCopy paste from there the prompts you like, and adjust to your need. Pure prompting, no Lora etc.\n\nThe discussion: [https://www.reddit.com/r/StableDiffusion/comments/1e0f0fd/prompt\\_only\\_photographic\\_style\\_reference\\_100/](https://www.reddit.com/r/StableDiffusion/comments/1e0f0fd/prompt_only_photographic_style_reference_100/)', 'Another great contribution from Rob Laughter, just for characters;\n\n[https://github.com/roblaughter/style-reference/blob/master/characters.md](https://github.com/roblaughter/style-reference/blob/master/characters.md)', 'Realvis 5 is the best realism model in my opinion', 'Splashed Mix DMD is a solid choice. It\'s great with photographic subjects while being a general-use model:\n\nhttps://civitai.com/models/932513?modelVersionId=1632035\n\nIt\'s fast, too, so you\'ll have an easy time testing your prompts.\n\nFor LoRAs, you\'ll want to use sdxl_enchance: https://civitai.com/models/99619?modelVersionId=947415\n\nIt\'s one of the few detailer LoRAs to actually improve your outputs without changing the style or subject like most ""detailer"" LoRAs do.']",0,7,0.5,Question - Help,1751496618.0
1lq93xk,StableDiffusion,Ayuda LoRa PONY,"Alguien me puede ayudar en indicarme como se puede generar un lora de pony, estoy perdido con colab y con kohya ss , no eh podido generarlos

HELP!!!",[],[],0,0,0.2,Question - Help,1751494775.0
1lq8rpj,StableDiffusion,How many credits do you needed to train Flux Kontext models on Invoke?,How many credits do you needed to train Flux Kontext models on Invoke?,"[""I don't believe they have training scripts for Kontext yet, but if that is a feature that you/your company wants, I'd recommend asking on the Discord or contacting their support email.""]","[""I don't believe they have training scripts for Kontext yet, but if that is a feature that you/your company wants, I'd recommend asking on the Discord or contacting their support email.""]",0,1,0.33,Question - Help,1751493920.0
1lq8ps6,StableDiffusion,"Help. How do you write prompts for Flux? Unfortunately the template doesn't work with short descriptions. The best way to generate a prompt is to ask an LLM to describe an image, BUT, the problem is that this method lacks creativity. I don't know how to create new images using flux","With SDXL it was quite simple. All you had to do was give a generic description like ""luxury store in neoclassical architecture style"" and the model would generate several different images.

Flux follows the prompt much better. However, the problem is that the model is not creative. It requires long descriptions.

The most obvious method is to take an existing image - the problem is the lack of creativity.

I've tried asking chatgpt to improve my prompt but unfortunately the results are not good, it doesn't work well.","[""If you don't have imagination and can't describe what you want it sounds more like a you problem than a Flux one."", 'You can use Gemma 3 to refine or enhance your prompt, is the best one for Flux and Chroma, but if you feel like is a Unet limitation, Chroma is way better for creative porpuses, also another LLM that I\'ve been testing and it can be useful is ""Veiled Rose 22B"" from what I\'ve seen it has like a roleplay focus, but because of that, the prompts are a bit more creative and less restricted than Gemma (is not super limited but sometimes the same instruction causes Gemma to generate similar prompts even on a new chat). Just in case I use LM Studio locally.', 'Short like I write my prompts for really any model. \n\nThe issue you\'re describing is also present in XL. It\'s just that the model is so much more ""capable"" or at least it THINKs it knows what a boat on a lake looks like so it\'ll give you 9 images of boats in the same lake\n\nI personally don\'t like flux that much for plenty of reasons but that is a big gripe of mine. \n\n1.5 has MASSIVE variations between renders since it\'s kinda scattershot in comparison but this leads to a lot more variation.\n\nAnyway.. the answer to your question is wildcards, use wildcards.', 'Ask for an LLM to first give a long set of instructions on what each prompt should cover and vary, followed by 5 distinct prompts about ___a luxury store in a neoclassical architecture style___. The initial instructions it will give adds enough randomization that you can redo the same initial prompt ad nauseam and keep getting very different output prompts']","[""If you don't have imagination and can't describe what you want it sounds more like a you problem than a Flux one."", 'You can use Gemma 3 to refine or enhance your prompt, is the best one for Flux and Chroma, but if you feel like is a Unet limitation, Chroma is way better for creative porpuses, also another LLM that I\'ve been testing and it can be useful is ""Veiled Rose 22B"" from what I\'ve seen it has like a roleplay focus, but because of that, the prompts are a bit more creative and less restricted than Gemma (is not super limited but sometimes the same instruction causes Gemma to generate similar prompts even on a new chat). Just in case I use LM Studio locally.', 'Short like I write my prompts for really any model. \n\nThe issue you\'re describing is also present in XL. It\'s just that the model is so much more ""capable"" or at least it THINKs it knows what a boat on a lake looks like so it\'ll give you 9 images of boats in the same lake\n\nI personally don\'t like flux that much for plenty of reasons but that is a big gripe of mine. \n\n1.5 has MASSIVE variations between renders since it\'s kinda scattershot in comparison but this leads to a lot more variation.\n\nAnyway.. the answer to your question is wildcards, use wildcards.', 'Ask for an LLM to first give a long set of instructions on what each prompt should cover and vary, followed by 5 distinct prompts about ___a luxury store in a neoclassical architecture style___. The initial instructions it will give adds enough randomization that you can redo the same initial prompt ad nauseam and keep getting very different output prompts']",0,5,0.2,Question - Help,1751493788.0
1lq8jck,StableDiffusion,Need help catching up. What’s happened since SD3?,"Hey, all. I’ve been out of the loop since the initial release of SD3 and all the drama. I was new and using 1.5 up to that point, but moved out of the country and fell out of using SD. I’m trying to pick back up, but it’s been over a year, so I don’t even know where to be begin. Can y’all provide some key developments I can look into and point me to the direction of the latest meta?","[""Since SD3? A few months after Flux was released, if you haven't heard about it, which is a popular model to use even now. There was also SD3.5, which is better than SD3 and license also got better, but it was hardly any good in comparison to Flux (especially with LoRAs).\n\nAll kinds of models were released since then. Like HiDream, which is even bigger model than Flux, or Lumina 2.0 that is more akin to SDXL in size.\n\nThe most noticeable development is video models. First LTXV, Hunyuan Video, Mochi, then Wan 2.1 (and its variations like VACE) that are current 'meta'.\n\nBecause of Flux, people begun using natural language as prompt and captions more frequently. Which made necessary to have uncensored image to text models, like JoyCaption.\n\nIn that time span, SDXL technically also got a new subset of models, Illustrious and NoobAI, in similar to Pony way.\n\nChroma is currently being trained based on de-distilled Flux Schnell and would finish its training in a bit more than a month. Flux is very censored model (despite existence of LoRAs), so that's the current uncensored version of it that is normal (regular Flux doesn't have CFG and negative prompt).\n\nNot so long ago, Flux got Flux Kontext, which is used as a different way to edit images and use consistent characters/scenes. There is also OmniGen2 (first version was also released after SD3).\n\nThere were quite a few 3D models too, like Hunyuan 3D 2.0.\n\nAnd the only audio model I remember currently is YuE.\n\nThose aren't the only things, it's hardly even a third of what happened during this period of time."", ""https://preview.redd.it/609uoidgujaf1.png?width=2248&format=png&auto=webp&s=615ae40e885893f8bc240315ea04b02e88bdcc52\n\nWell to just skip to the greatest thing so far, least for me its Flux Kontext. I've always dreamed of one day being able to edit photos from nothing more than by prompt."", ""In terms of SD3, nothing has changed. It's still where you left off. Nobody uses it."", 'I finally generated the perfect waifu and we got married. She has 6 fingers, but I kind of like it TBH.', ""I gained 5 pounds, been had hard time with my old clothes. Otherwise I'm good"", ""SD3 never really took off.\n\nI think the biggest hype right now is around Flux and their newly released model, Flux Kontext Dev. There's also Chroma, which is based on Flux.1-schnell, but it's uncensored.\n\nFor NSFW content, there's also Pony and its fine-tuned models.\n\nAs for videos, Wan 2.1 seems to be quite popular, as well as Hunyuan. But I don't know much about video creation, maybe someone else has more insight."", 'I just use SDXL still, Brixl, Crystal Clear Prime and some old ass early generation canny, depth and ip-adapter.  Flux was ok but no negative prompts so I rarely use it but Flux Kontext has me curious.', 'wow, 3...thats like not really keeping up since the Wright brothers did their airplane thing and now here we are with airliners.\n\nThe biggest in the tl/dr stuff may be this\n\nCheck out Flux and more specifically Chroma  \nXL remains pretty solid for very fast gens, but won\'t adhere to prompts as well as Chroma (flux models).\n\nFor video stuff, check out WAN things...specifically the Vace models.\n\nLearn ComfyUI...small learning curve, but man is it good once you get the nodes...just grab simple workflows and watch some tutorials. Don\'t give up, its like...legos with intent...learn the pieces and you\'ll quickly figure out how they all snap together.\n\nThere is a workflow for just about everything..but until you\'re good, avoid those ""workflow for everything and then some"" nonsense...because everyone pretends they work for nasa when making workflows for some reason...really annoying actually. Sometimes finding an actual simple workflow is impossible...so learn to make your own.', 'Every day there is something new but requires hour-hours of setup and knowing in and outs of of new comfyui programing-ish language. I gave up.', ""What hasn't happened? An overview video would be long; there's a lot of stuff in between, but I'll try hard to get the main things off the top. \n\nSince sd3, Aura Flow, Pony 7 begins development (based on Aura Flow), Pixart Sigma, Flux, Illustrious, sd3.5, Lumina, Chroma, HiDream... and there's so much in between in regards to tools and utilities.""]","[""Since SD3? A few months after Flux was released, if you haven't heard about it, which is a popular model to use even now. There was also SD3.5, which is better than SD3 and license also got better, but it was hardly any good in comparison to Flux (especially with LoRAs).\n\nAll kinds of models were released since then. Like HiDream, which is even bigger model than Flux, or Lumina 2.0 that is more akin to SDXL in size.\n\nThe most noticeable development is video models. First LTXV, Hunyuan Video, Mochi, then Wan 2.1 (and its variations like VACE) that are current 'meta'.\n\nBecause of Flux, people begun using natural language as prompt and captions more frequently. Which made necessary to have uncensored image to text models, like JoyCaption.\n\nIn that time span, SDXL technically also got a new subset of models, Illustrious and NoobAI, in similar to Pony way.\n\nChroma is currently being trained based on de-distilled Flux Schnell and would finish its training in a bit more than a month. Flux is very censored model (despite existence of LoRAs), so that's the current uncensored version of it that is normal (regular Flux doesn't have CFG and negative prompt).\n\nNot so long ago, Flux got Flux Kontext, which is used as a different way to edit images and use consistent characters/scenes. There is also OmniGen2 (first version was also released after SD3).\n\nThere were quite a few 3D models too, like Hunyuan 3D 2.0.\n\nAnd the only audio model I remember currently is YuE.\n\nThose aren't the only things, it's hardly even a third of what happened during this period of time."", ""https://preview.redd.it/609uoidgujaf1.png?width=2248&format=png&auto=webp&s=615ae40e885893f8bc240315ea04b02e88bdcc52\n\nWell to just skip to the greatest thing so far, least for me its Flux Kontext. I've always dreamed of one day being able to edit photos from nothing more than by prompt."", ""In terms of SD3, nothing has changed. It's still where you left off. Nobody uses it."", 'I finally generated the perfect waifu and we got married. She has 6 fingers, but I kind of like it TBH.', ""I gained 5 pounds, been had hard time with my old clothes. Otherwise I'm good""]",69,68,0.83,Question - Help,1751493347.0
