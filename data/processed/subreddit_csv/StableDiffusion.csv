id,subreddit,title,selftext,comments,top_comments,score,num_comments,upvote_ratio,flair,created_utc
1lso67i,StableDiffusion,These are some of my achievements in exploring virtual reality. I really like these environments and characters that do not actually exist. I hope to hear criticism and suggestions so I can continue to improve,"    Being barefoot is not because I'm a pervert, but because I want to observe the stability of flux-generated feet



    Although I can't share WF directly (but I have shared it with friends who have helped me, I just am not ready to make it public yet, so please don't blame me :-) )
    But I am willing to share every detail:
    1. Using flux-dev/chroma + amater V6 lora as the base, chroma is somewhat unstable, but occasionally it can produce amazing results
    2. Generated with 2pass, no inpaint
    3. 99% of the effect comes from RES4LYF nodes (https://github.com/ClownsharkBatwing/RES4LYF)


https://preview.redd.it/wi1xasssc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=22e89bad1a588b102f93f55b82a1969a4a69da90

https://preview.redd.it/wezfx1vtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=4c7f92051c2eb7fa77a92332dec10550a79ec09f

https://preview.redd.it/24nem6vtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=f4ab431393f7b25bf98bd686296bdc866d53d322

https://preview.redd.it/4pyy48vtc5bf1.png?width=3072&format=png&auto=webp&s=75bc5e544d54c2dd34ed3a52e34df19cbbaad632

https://preview.redd.it/w2ql4avtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=473c80a67072f2275eb2496ed64495f45d4f6910

https://preview.redd.it/sgkhxuvtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=c74ea07029d021eaa38019d91190ca4902ee3e95

https://preview.redd.it/7pd8dgvtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=ef57bfcc7c9df51e5e1fbed2adfa0a35c2136b8f

https://preview.redd.it/to6reivtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=25dc15becaaa9acc658ba6c2f84b9e0e77830138

https://preview.redd.it/3zpzcpvtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=25410238638ad68b6763993c462a145a54211959

https://preview.redd.it/ihan57wtc5bf1.jpg?width=3072&format=pjpg&auto=webp&s=90273bacc903e0d79956547243e5a1431c85736c

https://preview.redd.it/31fzv3w7d5bf1.jpg?width=3072&format=pjpg&auto=webp&s=df944d69571d6517e86f3328c4f8c4bd9fff4e20

",[],[],1,0,0.67,No Workflow,1751761284.0
1lsn181,StableDiffusion,"Weekend drop, Flux WF - will share it in the comments in a bit. These colors, dead easy to eat too much of this stuff, all the zest just evaporates like a cloud of smoke...",,['WF: [https://openart.ai/workflows/Zz7DKx49CRNnoL3yyXVf](https://openart.ai/workflows/Zz7DKx49CRNnoL3yyXVf)'],['WF: [https://openart.ai/workflows/Zz7DKx49CRNnoL3yyXVf](https://openart.ai/workflows/Zz7DKx49CRNnoL3yyXVf)'],0,1,0.5,Workflow Included,1751757827.0
1lsmwsh,StableDiffusion,I‚Äôd like to create videos with characters but use my own backgrounds. Any advice on a suitable platform? Thanks,,"['You can use of [Character forge by Stoira](https://getcharacterforge.stoira.com)', 'If you want free open source way to do it, use Wan 2.1 and VACE.  Supply the background you want as the VACE reference image and it will perfect integrate your character (assume via a LoRA) into the background.']","['You can use of [Character forge by Stoira](https://getcharacterforge.stoira.com)', 'If you want free open source way to do it, use Wan 2.1 and VACE.  Supply the background you want as the VACE reference image and it will perfect integrate your character (assume via a LoRA) into the background.']",3,2,1.0,Question - Help,1751757459.0
1lsme5s,StableDiffusion,Why is flux dev so bad with painting texture ? Any way to create a painting that looks like a painting?,Even loras trained in styles like van gogh have a strange AI feel,"['Have you tried prompts like rough painting, rough strokes, impasto, thick paint, impressionism ?\n\nBTW The guy at the table looks like Captain Flint from Black Sails. :)', ""Have you tried reducing Flux Guidance (aka distilled CFG)? The default is 3.5, but images that mimic traditional art benefit from setting it to a lower value, something like 1.4 to 2.4 (you'll need to experiment a bit)."", '""Even loras trained in styles like van gogh have a strange AI feel""\n\ndon\'t forget, not all loras are created equally. a lot of the ones you find easily, even the first results on the bigger sites like civit and tensor are lazily done, it takes some digging to find the quality ones. a good test is to run the same seed with and without the lora to see how transformative it is. also watch out for loras that came out around the same time as flux--i avoid those because those are the \'excited about the shiny new toy and didnt take their time\' loras. look for ones that have newer updates and more than 1 version', 'Try some of my artistic LoRAs: [https://civitai.com/user/NobodyButMeow/models](https://civitai.com/user/NobodyButMeow/models)\n\nDepending on the style, my LoRAs can be anywhere between 60-80% faithful to the artist\'s style.\n\nI do tend to overtrain a bit to achieve the desired style, so if you find that a LoRA is not flexible enough, you can try either a lower weight or use an earlier epoch.\n\nAlso, with some of my LoRAs, try using a lower Guidance (1.5-2.5), which may bring out more of the ""painterly texture"".  That is true for Flux-Dev in general as well.', 'Eldrich paint sketch Lora is the way', 'Feels to me like Flux (Kontext) has trouble with high resolution detail in general.\n\nWether it is paint brush strokes, skin texture or material detail, it always is either washed out or one ""plain"" color without any detail at all, resulting in this ""AI feel"".', ""You'll have much better luck using chroma"", ""Prompting is key. Btw kontext feels waaay better with styles than base flux. I did a post about styles half a year ago https://www.reddit.com/r/StableDiffusion/s/UnvE0zCHBQ\nSince then there was also some resource with list of styles it can effectively handle.\nFor getting hires textures you should upscale. Don't forget that all actual calculations are done in latent and it is really small. And then it gets to desired resolution via vae. But yeah flux was always kinda bad with styles, sd was way better at that""]","['Have you tried prompts like rough painting, rough strokes, impasto, thick paint, impressionism ?\n\nBTW The guy at the table looks like Captain Flint from Black Sails. :)', '""Even loras trained in styles like van gogh have a strange AI feel""\n\ndon\'t forget, not all loras are created equally. a lot of the ones you find easily, even the first results on the bigger sites like civit and tensor are lazily done, it takes some digging to find the quality ones. a good test is to run the same seed with and without the lora to see how transformative it is. also watch out for loras that came out around the same time as flux--i avoid those because those are the \'excited about the shiny new toy and didnt take their time\' loras. look for ones that have newer updates and more than 1 version', ""Have you tried reducing Flux Guidance (aka distilled CFG)? The default is 3.5, but images that mimic traditional art benefit from setting it to a lower value, something like 1.4 to 2.4 (you'll need to experiment a bit)."", 'Try some of my artistic LoRAs: [https://civitai.com/user/NobodyButMeow/models](https://civitai.com/user/NobodyButMeow/models)\n\nDepending on the style, my LoRAs can be anywhere between 60-80% faithful to the artist\'s style.\n\nI do tend to overtrain a bit to achieve the desired style, so if you find that a LoRA is not flexible enough, you can try either a lower weight or use an earlier epoch.\n\nAlso, with some of my LoRAs, try using a lower Guidance (1.5-2.5), which may bring out more of the ""painterly texture"".  That is true for Flux-Dev in general as well.', 'Eldrich paint sketch Lora is the way']",11,9,0.76,Discussion,1751755931.0
1lsm47h,StableDiffusion,what you guys are using to edit binary masks (black and white)?,"https://preview.redd.it/5nohenysu4bf1.png?width=1235&format=png&auto=webp&s=fe7d658924ff342814102b01c2c1de034fc98f0f

I need to edit 179 masks, just to refine some small details. Photoshop takes a long time because I need to open the image, overlay the mask, edit, change transparency, etc... Is there a specific tool that opens the image and the mask together and after editing saves the mask directly?

the print is from an application I'm making",[],[],1,0,0.67,Question - Help,1751755133.0
1lslu2d,StableDiffusion,"Kontext Dev base model. Sometimes it's interesting. I'm waiting for kohya to update to try to train lora (but apparently it's very slow because if the resolution is 512, it works with image pairs = 1024)","I don't know if the model is good with loras

I saw people saying they had bad results with flux fill

But anyway I think training takes a lot of time, several hours","['Wagner Moura!', 'Heyo, Kontext actually works pretty decent with loras if you train them well. Also you can already train the lora locally using something like AI toolkit for it. heres how to train a kontext lora locally [https://youtu.be/IPn6hkfeomo?si=i94jmekzWbQY75zY](https://youtu.be/IPn6hkfeomo?si=i94jmekzWbQY75zY)\n\nTraining does take a fair while. on a 3090 at 3000 steps its about 7 hours or so. on a 5090 its closer to something like 2.5 hours.']","['Wagner Moura!', 'Heyo, Kontext actually works pretty decent with loras if you train them well. Also you can already train the lora locally using something like AI toolkit for it. heres how to train a kontext lora locally [https://youtu.be/IPn6hkfeomo?si=i94jmekzWbQY75zY](https://youtu.be/IPn6hkfeomo?si=i94jmekzWbQY75zY)\n\nTraining does take a fair while. on a 3090 at 3000 steps its about 7 hours or so. on a 5090 its closer to something like 2.5 hours.']",4,2,0.75,No Workflow,1751754335.0
1lslihm,StableDiffusion,Can't find this problem listed anywhere online and need help! SD FORGE,"have been running into this issue since last week. I've deleted and redownloaded forge so many times and still having this issue on all clean installs.

Anyone run into this or know how to fix it?","['I have been having the same problem for some time now and I coulden\'t finda a way to fix it ether. So I did the next best thing and replaced it whit the Stable-Diffusion-Webui-Civitai-Helper. It does the exect same thing, except that it works. \n\nSo if you do get civitai heler, go in to settigns,to:\n1. Enter your CivitAi API\n2. turn ""Blocks images that are more NSFW than the chosen rating"" to XXX.  \n3. to make sure that ""Download Example Images Locally"" is off\n\nFell free to experiment whit other settings.']","['I have been having the same problem for some time now and I coulden\'t finda a way to fix it ether. So I did the next best thing and replaced it whit the Stable-Diffusion-Webui-Civitai-Helper. It does the exect same thing, except that it works. \n\nSo if you do get civitai heler, go in to settigns,to:\n1. Enter your CivitAi API\n2. turn ""Blocks images that are more NSFW than the chosen rating"" to XXX.  \n3. to make sure that ""Download Example Images Locally"" is off\n\nFell free to experiment whit other settings.']",1,3,0.67,Question - Help,1751753415.0
1lsl2cm,StableDiffusion,created with my custom prompts,I made a little website that gives free prompt that are really good started prompts [42ify.com](http://42ify.com) i got literal 1000 image prompts on there i just like to see what people get with the same prompts but in different generators,['Thanks for the ads [sarcasm ends]'],['Thanks for the ads [sarcasm ends]'],3,2,0.71,Comparison,1751752155.0
1lskq4d,StableDiffusion,How come there isn‚Äôt a popular peer-to-peer sharing community  to download models as opposed to Huggingface and Civitai?,"Is there a technical reason why the approach to hoarding and sharing models hasn‚Äôt gone the p2p route? That seems to be the best way to protect the history of these models and get around all the censorship concerns.

Or does this exist already and it‚Äôs just not popular yet?","['Torrents would work. But there needs to be a fancy front end with model previews, much like how Civitai looks.', ""I think part of the issue is a standardized naming scheme. It's basically the wild west in regards to how people name things, so it would be extremely difficult to find particular things in a p2p format. Add on top of that, without a way to look at how good the lora/models are, you're downloading them blind. Good lora's are difficult to find and a lot of them are just over-trained trash. \n\nIt would take a big change in the community and how things are done for it to be a good option. At the very least we'd need a website for people to host the images and explanations like the rule 34 website so people knew what lora's to get."", 'I think the community is just too small so no one is really doing that so far. I don‚Äôt think there is a technical issue in these. This can also be done as a private tracker so users must upload sufficiently to obtain points to download new models. All the discussion/showcaes can be kept, maybe not the inference part for test.', 'Should bring back Limewire for this.', 'Thats a great question. AI models are perfect for p2p!', ""I don't think p2p is as popular as it once was. I feel like you would just end up with a lot of dead torrents with zero seeders. Most people are leechers, after all.\n\n\nI think there's something called a seedbox but that sounds equivalent to it being hosted on a random computer. Will anyone bother?\n\n\nI guess people would rather ask for a mega link or something when they find someone on Discord who has what they want. Idk though, I'm old and the Internet has changed."", 'Build it and they will come.', ""I've seen this question asked here a few times and always felt it was a matter of time. In recent months I have been fortunate enough to get pretty good at vibe coding so I'm going to take a shot at making a website for this. Definitely going to need help/support for the bandwidth and storage hopefully some of you have high speed internet and excess storage, if we serve out of our pre-existing model directories then it should not take up much additional storage. I'll get the feature list started: would appreciate comments and expansion on it. Torrent based, model card with description + images, indicator for how many seeders, search, what else??"", 'I tried that civitaibay site earlier. First glance everything (before scrolling further down) said it had 1 seeder, tried to download one through the magnet link and it wouldnt get the metadata of the torrent so I had to get it from the actual civitai site that was linked within the torrent info on the site', 'I guess the existence of Reddit and Discord combined with all the crackdowns on private forums killed a lot of hosting options. I always prefer the ddl route for stuff, fiddling with VPNs or taking p2p risks was never my thing.\xa0\n\nModel hoarding is tough since they are so numerous and large in gigabytes. One model alone can be the size of a full game.']","['Torrents would work. But there needs to be a fancy front end with model previews, much like how Civitai looks.', ""I don't think p2p is as popular as it once was. I feel like you would just end up with a lot of dead torrents with zero seeders. Most people are leechers, after all.\n\n\nI think there's something called a seedbox but that sounds equivalent to it being hosted on a random computer. Will anyone bother?\n\n\nI guess people would rather ask for a mega link or something when they find someone on Discord who has what they want. Idk though, I'm old and the Internet has changed."", ""I think part of the issue is a standardized naming scheme. It's basically the wild west in regards to how people name things, so it would be extremely difficult to find particular things in a p2p format. Add on top of that, without a way to look at how good the lora/models are, you're downloading them blind. Good lora's are difficult to find and a lot of them are just over-trained trash. \n\nIt would take a big change in the community and how things are done for it to be a good option. At the very least we'd need a website for people to host the images and explanations like the rule 34 website so people knew what lora's to get."", 'I think the community is just too small so no one is really doing that so far. I don‚Äôt think there is a technical issue in these. This can also be done as a private tracker so users must upload sufficiently to obtain points to download new models. All the discussion/showcaes can be kept, maybe not the inference part for test.', 'Should bring back Limewire for this.']",39,38,0.85,Discussion,1751751205.0
1lsk8s6,StableDiffusion,Multiple T5 clip models. Which one should I keep?,"For some reason I have 3 T5 clip models:

* t5xxl\_fp16 (\~9.6GB)
* umt5\_xxl\_fp8\_e4m3fn\_scaled (\~6.6GB)
* t5xxl\_fp8\_e4m3fn\_scaled (\~5.0GB)

The first two are located at 'models\\clip' and the last one at 'models\\text\_encoders'.

What's the different between the two fp8 models? Is there a reason to keep them if I have the fp16 one?
I have a 3090, if that matters.","['1 TB and still low on space because of comfyui \n\nit keeps getting bigger every day', 'you may have collected different ones for running different models and workflows in the past. Sometimes you will delete one and realize you still needed it for a different workflow from like 2 months ago.', 'In my limited hardware environment, I needed all of them for professional-level work. However, I delete them when I‚Äôm not using them.', ""That's the result of people using so many variations of these. I heard about FLAN T5 as well which supposedly works better when using Chroma."", 'umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled is for WAN  \nthe other t5 models are for Flux\n\nGenerally, FP16 is higher quality, but \\~FP8 is twice as fast. Pick your poison. But for a text encoder, I would *always* choose the highest quality you can fit in system RAM.']","['1 TB and still low on space because of comfyui \n\nit keeps getting bigger every day', 'you may have collected different ones for running different models and workflows in the past. Sometimes you will delete one and realize you still needed it for a different workflow from like 2 months ago.', 'In my limited hardware environment, I needed all of them for professional-level work. However, I delete them when I‚Äôm not using them.', ""That's the result of people using so many variations of these. I heard about FLAN T5 as well which supposedly works better when using Chroma."", 'umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled is for WAN  \nthe other t5 models are for Flux\n\nGenerally, FP16 is higher quality, but \\~FP8 is twice as fast. Pick your poison. But for a text encoder, I would *always* choose the highest quality you can fit in system RAM.']",5,6,0.73,Question - Help,1751749897.0
1lsjcaa,StableDiffusion,No humans needed: AI generates and labels its own training data,"We‚Äôve been exploring how to train AI without the painful step of manual labeling‚Äîby letting the system generate its own perfectly labeled images.

The idea: start with a 3D mesh of a human body, render it photorealistically, and automatically extract all the labels (like body points, segmentation masks, depth, etc.) directly from the 3D data. No hand-labeling, no guesswork‚Äîjust pixel-perfect ground truth every time.

Here‚Äôs a short video showing how it works.

Let me know what you think‚Äîor how you might use this kind of labeled synthetic data.","['Helps to avoid privacy issues with real human images, and the limited nature of human datasets. Able to generate varying clothing, environments, poses, gender, etc.', 'AI : third leg ?! what do we have here', ""This is awesome. What is this geared up for or is it gonna be it's own repository? I am a 3d artist as well I sell my pose sets for Daz models and would love to be able to train AI sets for image generations."", ""Yeah this hasn't worked well so far"", ""I had the same idea for months now since ai am also a 3D artist. I always thought, why can't we just train models to predict data related to 3D objects?""]","['Helps to avoid privacy issues with real human images, and the limited nature of human datasets. Able to generate varying clothing, environments, poses, gender, etc.', 'AI : third leg ?! what do we have here', ""This is awesome. What is this geared up for or is it gonna be it's own repository? I am a 3d artist as well I sell my pose sets for Daz models and would love to be able to train AI sets for image generations."", ""Yeah this hasn't worked well so far"", ""I had the same idea for months now since ai am also a 3D artist. I always thought, why can't we just train models to predict data related to 3D objects?""]",27,7,0.84,Resource - Update,1751747470.0
1lsj7hq,StableDiffusion,"Can a anime/cartoon focused t2v/i2v model do ""more"" than a realistic one?","Im a noob btw I just had this random thought and wanted to ask.

But can a video model trained for only anime output do more on local machines than something like veo3 or Wan2.1? My thought was if its trained on anime/cartoons and no (or minimal for whats needed) realistic data, wouldn't it fit more due to anime styles being generally simpler than real images? Or does it still use the same number of parameters despite things being simpler in the training data?

I ask because I am REALLY hoping we get some anime video models at some point and thats what they specialize in rather than them all trying for super impressive realistic outputs (which are still cool). Like an Illustrious t2v or something.

I mean how much further could we get with 32gb VRAM if we just did cartoons or anime? Would love to see what the community content would look like for this if it let creators output more with less hardware.

Take it a step further and what if the models were broken down by style choice rather than shoving them all into 1 mega model? What sort of benefits might that have if any?

Anyway thats my random thought, am I crazy or might this be a realistic goal someday?",['pony video wen'],['pony video wen'],3,1,0.8,Discussion,1751747120.0
1lsi0rj,StableDiffusion,"Is there a ""bad"" way to prompt with natural language prompts?","Just trying to learn a little coming from more tag-based models.

Are there any notable bad ways of writing a prompt in natural language that might give bad results? or just give it a few sentences of whatever you want and thats generally correct?

like would the following be okay or might it result in problems?
""a man walking down a rainy road in a city. blue shirt, with an umbrella, he has short hair""

so its going from natural to tag a little bit but would that still work most the time?","['Many people still use Booru style tags or word tag salad and expect good responses.  The number of Flux images I see with ""1girl,,clothes,big breasts,uniform"", or ""dog,car,chasing, .noon"" astounds me.  That and the negatives... It\'s been almost three  years since I did the day one beta tests for SD and people are still using ""bad hands, missing fingers"", etc in the negatives, which are not generally applicable to Flux. \n\nBut, the NL engine tries its best to determine what you want, which is why it works ""most"" time and other times it fails miserably and then people blame the engine. For me personally, I think of natural language prompting as if you\'re describing a scene to a visually impaired person.  \n\n\n\nAs u/Dezordan says, your example is fine., but could also be:\n\n""A man who has short hair and is wearing a blue shirt is carrying an umbrella while walking down a rainy road in a city""  \n\nBTW, here is a test. Clockwise from **top left**: your prompt, my prompt, Booru style tagging, SD tagging. \n\nSD: man, short hair, blue shirt, umbrella, walking, rainy street, city, day, realistic, photo style\n\nBooru: 1boy, short hair, blue shirt, holding umbrella, walking, city street, rain, solo, outdoors, daytime, city\n\nhttps://preview.redd.it/ybmh8mlp94bf1.png?width=695&format=png&auto=webp&s=041d6eeb62ce18df5cef825ac0fa9d95c513a869', ""There is no bad prompt, but there might be non optimal prompt. When a prompt is doing what you want it's fine, and you can't break anything (only waste computation time).\n\nPersonally I also think the SDXL way of a short sentence and then some tags to round it off is a very efficient way. And writing prose for Flux is far more tedious.\n\nBut you don't have to do that (yourself). Just use an LLM to translate your style of prompt to one or few precise sentences."", 'Here\'s my learnings from using flux\n\n\\- Using normal sentences with periods is better.  When you use comma separated lists the model will still try to make sense of what you\'re saying, but it\'s as if someone started speaking to you in single words without connecting words.\n\n\\- Pronouns are good.  ""Her hair is..."",  ""His stance is..."".  It ties the request to a specific entity within the image.  But again, if you don\'t the model will still contextually figure it out... it just might get it wrong every now and then.  For example ""blue shirt"" by itself could be interpreted as ""any shirt that appears in the image should be blue"" where ""His shirt is blue."" is more precise.\n\n\\- If the model isn\'t adhering, there\'s a strong chance that either you\'re using the wrong lexical term for it or the model just don\'t know what that is.  Sometimes the concepts that a model doesn\'t know (wasn\'t trained on) about feels very random.\n\nSo the best case for your prompt would be: \n\n""A short haired man wearing a blue shirt and holding an umbrella is walking down a rainy road.""', ""It's better to avoid purple prose. So, your example is okay.  \nYou can use some simple ways to describe mood and other things, but avoid being too esoteric.""]","['Many people still use Booru style tags or word tag salad and expect good responses.  The number of Flux images I see with ""1girl,,clothes,big breasts,uniform"", or ""dog,car,chasing, .noon"" astounds me.  That and the negatives... It\'s been almost three  years since I did the day one beta tests for SD and people are still using ""bad hands, missing fingers"", etc in the negatives, which are not generally applicable to Flux. \n\nBut, the NL engine tries its best to determine what you want, which is why it works ""most"" time and other times it fails miserably and then people blame the engine. For me personally, I think of natural language prompting as if you\'re describing a scene to a visually impaired person.  \n\n\n\nAs u/Dezordan says, your example is fine., but could also be:\n\n""A man who has short hair and is wearing a blue shirt is carrying an umbrella while walking down a rainy road in a city""  \n\nBTW, here is a test. Clockwise from **top left**: your prompt, my prompt, Booru style tagging, SD tagging. \n\nSD: man, short hair, blue shirt, umbrella, walking, rainy street, city, day, realistic, photo style\n\nBooru: 1boy, short hair, blue shirt, holding umbrella, walking, city street, rain, solo, outdoors, daytime, city\n\nhttps://preview.redd.it/ybmh8mlp94bf1.png?width=695&format=png&auto=webp&s=041d6eeb62ce18df5cef825ac0fa9d95c513a869', ""There is no bad prompt, but there might be non optimal prompt. When a prompt is doing what you want it's fine, and you can't break anything (only waste computation time).\n\nPersonally I also think the SDXL way of a short sentence and then some tags to round it off is a very efficient way. And writing prose for Flux is far more tedious.\n\nBut you don't have to do that (yourself). Just use an LLM to translate your style of prompt to one or few precise sentences."", 'Here\'s my learnings from using flux\n\n\\- Using normal sentences with periods is better.  When you use comma separated lists the model will still try to make sense of what you\'re saying, but it\'s as if someone started speaking to you in single words without connecting words.\n\n\\- Pronouns are good.  ""Her hair is..."",  ""His stance is..."".  It ties the request to a specific entity within the image.  But again, if you don\'t the model will still contextually figure it out... it just might get it wrong every now and then.  For example ""blue shirt"" by itself could be interpreted as ""any shirt that appears in the image should be blue"" where ""His shirt is blue."" is more precise.\n\n\\- If the model isn\'t adhering, there\'s a strong chance that either you\'re using the wrong lexical term for it or the model just don\'t know what that is.  Sometimes the concepts that a model doesn\'t know (wasn\'t trained on) about feels very random.\n\nSo the best case for your prompt would be: \n\n""A short haired man wearing a blue shirt and holding an umbrella is walking down a rainy road.""', ""It's better to avoid purple prose. So, your example is okay.  \nYou can use some simple ways to describe mood and other things, but avoid being too esoteric.""]",2,5,0.62,Question - Help,1751743894.0
1lsgl1z,StableDiffusion,"Need help fixing my kohya SS settings, if anyone can pitch in?","I have successfully created lora's before, but today i trained a concept and a character and when i tested them it was like it didn't exist. I tried my old lora's and they work perfectly well.

I use the same folder strategy database > \[steps\]\_\[name\] \[type\]

settings I changed from defaul

model > illustreous xl > SDXL

safetensors - fp16 precision.

10 epochs - max train steps 6000

LR scheduler - cosine\_with restarts - 0.0005 learning rate, LR cycles = 3, text encoder LR = 0.00005, Unet LR = 0.0005

no half VAE, network dimension/alph 32/16, clip skip = 2, shuffle caption and flip augmentation are activated, full fp16 training, min gamma = 5, noise offset = 0.1.


and that's it for the settings i remember usually using, but for some reason today i trained a concept and a character and when tested i didn't even get a person, i got a floor a bench, random objects. It was like nothing was trained.

 ","['If you still have artifacts from your old training, find and compare the config for the differences.']","['If you still have artifacts from your old training, find and compare the config for the differences.']",2,3,1.0,Question - Help,1751740118.0
1lsgk19,StableDiffusion,Beyond the Peak: A Follow-Up on CivitAI‚Äôs Creative Decline (With Graphs!),,"[""It makes sense that CivitAI's activity is declining. Censorship issues aside, the main NSFW model remains SDXL and the site is mainly NSFW content. SDXL is almost two years old now, newer models don't have nearly as many finetunes and they take way longer to train. It's possible that people are just getting bored of it and unsubscribing just like in online games."", 'The decline in activity actually isn\'t as bad as I thought it would be. Even if they never allow celebrity content again, they need to fix their broke ass filtering ""solution."" It\'s literally one of the worst UX I\'ve ever experienced.', 'I notice loras of even anime or cartoon characters getting nuked randomly with pages leading to 404 not found. Not a fan of going on panic downloading spree but it hurts  losing models and loras. Weird how the samekosaba loras were all forced into the pg category all at once. Have a feeling vtuber loras are getting axed soon.', 'This is entirely how you interpret the data. If you look at this chart you can see that June 2025 was lower than average for 2025, but higher than every month in 2024 (included in the chart). If we looked at this data in early March we could conclude that January was the peak, and that we would see a continued decline. A single month of lower generations is not enough to conclude that the site as a whole is on a downward trend.\n\nhttps://preview.redd.it/w9gr19j444bf1.jpeg?width=800&format=pjpg&auto=webp&s=77210805bdc3f2ccdf1f4e50cb77924f2388dfae', ""It's certainly leveling off, but I think you need a few more days in July to get representative data. We're in the midst of a long weekend and holiday in the US, and one for which it's common to take extra days off.\n\nAlso purely anecdotal, but I'm noticing fewer downtimes and quicker gen times compared to a few month ago. You know, when everyone was complaining about that instead. Perhaps the reduced traffic is helping here."", 'Hey dude the article is cool but can you write it yourself rather than use chatgpt? It euphemises everything and makes it extremely cringe. I would much prefer reading an article with a few grammar errors that convey what you want it to rather than having it wrapped in euphemistic slop.']","[""It makes sense that CivitAI's activity is declining. Censorship issues aside, the main NSFW model remains SDXL and the site is mainly NSFW content. SDXL is almost two years old now, newer models don't have nearly as many finetunes and they take way longer to train. It's possible that people are just getting bored of it and unsubscribing just like in online games."", 'The decline in activity actually isn\'t as bad as I thought it would be. Even if they never allow celebrity content again, they need to fix their broke ass filtering ""solution."" It\'s literally one of the worst UX I\'ve ever experienced.', 'I notice loras of even anime or cartoon characters getting nuked randomly with pages leading to 404 not found. Not a fan of going on panic downloading spree but it hurts  losing models and loras. Weird how the samekosaba loras were all forced into the pg category all at once. Have a feeling vtuber loras are getting axed soon.', 'This is entirely how you interpret the data. If you look at this chart you can see that June 2025 was lower than average for 2025, but higher than every month in 2024 (included in the chart). If we looked at this data in early March we could conclude that January was the peak, and that we would see a continued decline. A single month of lower generations is not enough to conclude that the site as a whole is on a downward trend.\n\nhttps://preview.redd.it/w9gr19j444bf1.jpeg?width=800&format=pjpg&auto=webp&s=77210805bdc3f2ccdf1f4e50cb77924f2388dfae', ""It's certainly leveling off, but I think you need a few more days in July to get representative data. We're in the midst of a long weekend and holiday in the US, and one for which it's common to take extra days off.\n\nAlso purely anecdotal, but I'm noticing fewer downtimes and quicker gen times compared to a few month ago. You know, when everyone was complaining about that instead. Perhaps the reduced traffic is helping here.""]",23,19,0.79,News,1751740047.0
1lsg1z6,StableDiffusion,"Furlana: My AI pet portrait generator for turning your dog into bartenders, royalty, blue and white collar professionals & more - feedback welcome","My dog Lana passed away in April. She survived a big seizure in October 2024 and was never the same after that. I didn't get a chance to dress her up in all the silly, fun costumes I had planned. As I thought through how to keep her memory alive, I had this idea to build an AI-powered dog portrait generator and add all the fun themes I could think of. It is called Furlana at [https://furlana.ai](https://furlana.ai) I am extremely proud of the product given the sentimental value. I know there are lots of options in the AI pet portrait space but I believe I have built something unique, focused, vibrant and fun. You can tell me otherwise.

All you have to do is upload a photo of your dog, choose from 50+ themed outfits and your dog's stunning photo is generated. Dog services like groomers are subscribing and gifting these portraits to their customers after a service as an extra touch, and that is heartwarming for me. See before and after photos and let me know what you think. First photo is Lana

https://preview.redd.it/l9wjd3e1h3bf1.png?width=1024&format=png&auto=webp&s=37329f68a5ec454bcfaca4a048335778941a9084

https://preview.redd.it/fxo1x1l3h3bf1.jpg?width=3024&format=pjpg&auto=webp&s=d2b034df7bbcbb6d9c6017d9652ad3a6ef3b1d99

https://preview.redd.it/zktfk0g5h3bf1.jpg?width=1024&format=pjpg&auto=webp&s=c78e06507bf4648e488aefc93c6665ea488a8c94

https://preview.redd.it/djdw20x6h3bf1.jpg?width=736&format=pjpg&auto=webp&s=7bc55d664bff96f0f48839cbc12d2af8ceb55aa4

https://preview.redd.it/663p43g8h3bf1.png?width=1080&format=png&auto=webp&s=4ae5404d0a3e6b7dc1868f12825aa9e3477fab49

https://preview.redd.it/9cheori9h3bf1.png?width=683&format=png&auto=webp&s=dcb463a07000909365a710fb2e2e981ceb68c421

",['This is pretty neat! Sorry for your loss'],['This is pretty neat! Sorry for your loss'],5,2,0.69,News,1751738735.0
1lsfrfw,StableDiffusion,Wan 2.1 - Extend a video with a loop,"Heyüëãüèº

I was playing around with Wan2.1 and i got some amazing results with it. Unfortunately I am limited to a 5sec clips because of my gpu.

Is there a way to loop the 5sec video 2 times, so the video would be saved as 15sec video?

I would like to implement this in the workflow so I don‚Äôthave to do manual editing.

Has anyone found a way to do it?","[""I suppose you could use a custom node if there isn't already one.  But it's easy enough to use imagemagick on the command-line: ```convert input.webp -loop 0 output.webp``` to loop forever.  You can also do things like ```convert input.webp -coalesce -duplicate 1,-2-1 -loop 0 output.gif``` to convert your animated webp to a gif that goes forward and then backward through your images so you don't have rough cuts at the seams. \n\nImagemagick is STRONG and worth playing around with.  You can do montages and all kinds of other image editing with concise commands along with conversion between formats, etc."", ""I think the Merge Images node from the VideoHelperSuite extension can do this. You feed your 5 second video into both inputs of the node, which repeats the video twice, and then feed that output and the original 5 second video into a second Merge Images node, which will append another copy of the 5 second video.\xa0\n\n\nIt's possible that the RepeatImageBatch node with 3 repeats also does this, depending on how it handles an input with multiple images.\n\n\nNote that with the Skyreels Diffusion Forcing models (Wan-based) you can get longer videos for the same VRAM it takes you to generate 5 second videos. It gens an initial video, then uses the end portion and generates a continuation.\n\n\nHere is a workflow, but it needs the ComfyUI-WanVideoWrapper extension, I don't know if there is a native equivalent - https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_skyreels_diffusion_forcing_extension_example_01.json"", 'See my last post with the creepy clown, I put in a link to a loop workflow that works very well. Most of the last few posts I did use it too if you want examples.', ""Maybe try 'ping-pong' option from sampler (change false to true) your video will have 9sec. You will have half video forward and half backward.""]","[""I suppose you could use a custom node if there isn't already one.  But it's easy enough to use imagemagick on the command-line: ```convert input.webp -loop 0 output.webp``` to loop forever.  You can also do things like ```convert input.webp -coalesce -duplicate 1,-2-1 -loop 0 output.gif``` to convert your animated webp to a gif that goes forward and then backward through your images so you don't have rough cuts at the seams. \n\nImagemagick is STRONG and worth playing around with.  You can do montages and all kinds of other image editing with concise commands along with conversion between formats, etc."", ""I think the Merge Images node from the VideoHelperSuite extension can do this. You feed your 5 second video into both inputs of the node, which repeats the video twice, and then feed that output and the original 5 second video into a second Merge Images node, which will append another copy of the 5 second video.\xa0\n\n\nIt's possible that the RepeatImageBatch node with 3 repeats also does this, depending on how it handles an input with multiple images.\n\n\nNote that with the Skyreels Diffusion Forcing models (Wan-based) you can get longer videos for the same VRAM it takes you to generate 5 second videos. It gens an initial video, then uses the end portion and generates a continuation.\n\n\nHere is a workflow, but it needs the ComfyUI-WanVideoWrapper extension, I don't know if there is a native equivalent - https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_skyreels_diffusion_forcing_extension_example_01.json"", 'See my last post with the creepy clown, I put in a link to a loop workflow that works very well. Most of the last few posts I did use it too if you want examples.', ""Maybe try 'ping-pong' option from sampler (change false to true) your video will have 9sec. You will have half video forward and half backward.""]",1,4,0.67,Question - Help,1751737968.0
1lse6g1,StableDiffusion,How Well Are AI Model Creators Keeping Up With Aesthetic Terminology and Visual Vocabulary?,"I've been thinking about something that's been bugging me about AI image generation, and I'm curious what others think.

**The Core Issue: AI Models Need a Shared Visual Language**

Every AI model relies on what's essentially a lingua franca a shared vocabulary that connects concepts to generate what we're asking for. When we prompt these models, we're constantly trying to figure out which combination of words will unlock the specific aesthetic or visual element we want. But here's the thing: how current and comprehensive is that vocabulary?

**New Aesthetics Are Constantly Emerging**

Today I learned about ""Angelpunk"" a term describing the early 90s fascination with Judeo-Christian iconography in art and media (think Evangelion, Trigun). This got me wondering: are model creators actively updating their training data to include emerging aesthetic movements and terminology?

I stumbled across an entire aesthetics wiki that's basically a rabbit hole of visual categories I never knew existed. Did you know there's a distinction between ""techware"" and ""warware""? Both have similar vibes but completely different visual signatures. Same with synthwave, vaporwave, and outrun they all use synthetic music aesthetics but are distinctly different movements.

**The Specificity Problem**

Here's where it gets interesting: we often lack precise language for specific visual elements. Take superhero masks as an example. Most people default to ""superhero mask"" and get a domino mask, but there are actually several distinct types:
- Domino masks (classic Batman/Robin style)
- Bridge-up masks (covering from nose bridge upward)
- Hair-out variants (full coverage but hair exposed)
- Reverse masks (Winter Soldier style, covering nose down)

**The Real Question**

How well are these nuanced aesthetic categories and precise visual descriptors actually built into current AI models? Are we limited by:
1. Training data that doesn't include emerging aesthetic movements?
2. Lack of precise terminology for specific visual elements?
3. Model creators not keeping pace with evolving visual culture?

When I try to generate something specific, I often feel like I'm playing a guessing game with the model's vocabulary. Sometimes it nails exactly what I want; other times it seems like certain aesthetic concepts just don't exist in its training.

**Discussion Points:**
- Have you noticed gaps in AI models' understanding of specific aesthetics?
- Are there visual styles or elements you can't seem to get models to understand?
- How do you think model creators should approach updating aesthetic vocabulary?

I'm genuinely curious whether this is a training data issue, a terminology gap, or if I'm just not finding the right prompt combinations. What's your experience been?
","['This is why synthetic data will be huge when it becomes viable (dependable); this is how models will be able to train themselves, to fill in the immense amount of gaps that our current linguistic landscape misses-- \n\nHow many ways could I have written that sentence? And how would each way maximize its ability to convey its meaning and to whom? And yet, each image has ""one caption""? Only maximum in effect to the person who saw the image and described it and who they are to describe it! \n\nOn the other end of this; it is amazing how the process eventually inverts-- Starting with a pen or photoshop, suddenly having the computer do all the work! and now, yet again attempting to get the precision of pen and photoshop--', 'It is due to the captioned training data. If it doesn‚Äôt know a word or term you won‚Äôt be able to prompt for it, but other techniques could reproduce the content if guided with IPAdapter, Flux Redux, Flux Kontext etc. You can likely prompt an outfit type through a detailed description of it, but something specific like a type of mask requires terminology to be prompted easily. Open models tend to have a broader corpus of training data rather than very specific styles or trends like some of the closed models have (because they scraped anything and everything and sometimes optimise for these things as a feature). This is why there is a huge ecosystem around custom LoRA and models, introducing missing or niche knowledge.', ""Base model training data is captioned with whatever automatic captioning method is the best at the time.\n\nCurrently, it's Flux's embraced excessive AI essays. Instead of naming concepts or iconic visuals, it just describes them in extreme detail.\n\nTo introduce tag-style concepts, you will have to train models yourself, adding manual captions/tags where necessary, or wait until AI captioning improves/changes direction (which might take years)."", ""I use SDXL-based models like Pony and Illustrious, which use booru tags. If the boorus they were trained on have enough tagged examples of that aesthetic, the it'll work. If not, it's LoRA training time.\n\n\nAlso, for my purposes, I use artist styles rather than the aesthetic terms you are focussing on.""]","['This is why synthetic data will be huge when it becomes viable (dependable); this is how models will be able to train themselves, to fill in the immense amount of gaps that our current linguistic landscape misses-- \n\nHow many ways could I have written that sentence? And how would each way maximize its ability to convey its meaning and to whom? And yet, each image has ""one caption""? Only maximum in effect to the person who saw the image and described it and who they are to describe it! \n\nOn the other end of this; it is amazing how the process eventually inverts-- Starting with a pen or photoshop, suddenly having the computer do all the work! and now, yet again attempting to get the precision of pen and photoshop--', 'It is due to the captioned training data. If it doesn‚Äôt know a word or term you won‚Äôt be able to prompt for it, but other techniques could reproduce the content if guided with IPAdapter, Flux Redux, Flux Kontext etc. You can likely prompt an outfit type through a detailed description of it, but something specific like a type of mask requires terminology to be prompted easily. Open models tend to have a broader corpus of training data rather than very specific styles or trends like some of the closed models have (because they scraped anything and everything and sometimes optimise for these things as a feature). This is why there is a huge ecosystem around custom LoRA and models, introducing missing or niche knowledge.', ""Base model training data is captioned with whatever automatic captioning method is the best at the time.\n\nCurrently, it's Flux's embraced excessive AI essays. Instead of naming concepts or iconic visuals, it just describes them in extreme detail.\n\nTo introduce tag-style concepts, you will have to train models yourself, adding manual captions/tags where necessary, or wait until AI captioning improves/changes direction (which might take years)."", ""I use SDXL-based models like Pony and Illustrious, which use booru tags. If the boorus they were trained on have enough tagged examples of that aesthetic, the it'll work. If not, it's LoRA training time.\n\n\nAlso, for my purposes, I use artist styles rather than the aesthetic terms you are focussing on.""]",4,5,0.7,Discussion,1751733831.0
1lsdjyl,StableDiffusion,What's up with Pony 7?,"The lack of any news over the past few months can't help but give rise to unpleasant conclusions. In the official Discord channel, everyone who comes to inquire about the situation and the release date gets a stupid joke about ""two weeks"" in response. Compare this with Chroma, where the creator is always in touch, and everyone sees a clear and uninterrupted roadmap.

I think that Pony 7 was most likely a failure and AstraliteHeart simply does not want to admit it. The situation is similar to Virt-A-Mate 2.0, where after a certain time, people were also fed vague dates and the release was delayed under various formulations, and in the end, something disappointing came out, barely even pulling for alpha.

It could easily happen that when Pony comes out, it will be outdated and no one needs it.","[""I think astra is in the sunk-cost fallacy. I hate to be in their shoes, but choosing auraflow might not have been the best course. I would cut the project short and start from scratch on top of something more accessible but it's easy for me say."", ""It's incredibly hard to catch lightning in a bottle twice.\n\nSince Pony v7 started training, Illustrious, Noob, Flux, Chroma, etc. all have come out so other notable models have advanced or further pushed SDXL or new architectures.\n\nI'm sure it'll be a competent model, but I don't know that it'll have the same impact as V6 pony."", ""They may have swung big, missed big.\n\nPony being on SD 1.5 and then XL, was smaller and more agile to work on. So when XL launched they were able to pivot to that and ride it to being a success. A problem with the post XL world though is we still may not have the proper replacement. Newer models have been slower, larger, worse licensing and not training very well.\n\nChroma was able to ride the community Flux tinkering and come out on top of a schnell tune with their own tweaks. That may be the future, assuming the final tune doesn't blow apart, distills make it smaller/faster, and it trains well for loras. But it's also possible next week a new 3B param base model comes out that's good, open licensed, and easy to train and we all move to that. That would kill Chroma, though the team could possibly just pivot to the new model and start training on top of that.\n\nThe scene moves way too fast for 6 month plus projects to be viable."", ""\\> Compare this with Chroma, where the creator is always in touch, and everyone sees a clear and uninterrupted roadmap.\n\nChroma is great, that's why we are sponsoring it!\n\n\\> I think that Pony 7 was most likely a failure and AstraliteHeart simply does not want to admit it.\n\nIt's a strong model with some issues that I want to fix before release. It takes time to fix such issues. More time than I anticipated or wanted but I would rather have a decent model than release something sad. The bar is very high because of V6 and all the models that came after it.\n\nIt is also \\*\\*very expensive\\*\\* to train models of this size (in addition to knowing how to train them), hence you only see a few models coming out now (literally, Chroma is the only one really large finetune aside V7 I am aware that is not SDXL). So we have to take care of the financial part of the process too, and it takes time.\n\n  \n\\> It could easily happen that when Pony comes out, it will be outdated and no one needs it.\n\nThe best I can do is to build cool models that bring new tech to the table, this worked ok for the last \\~8 model I've released."", ""Chroma is gaining a lot of traction, and most of the tooling for it already exists. Since P7 is likely to be similar in size but using a different architecture, I think it will not become as popular as V6. LodeStones is also releasing a new checkpoint every four days on average, so people can get a taste of how the development is going. The aim is 50 epochs, and he's nearing epoch 42.\n\nIf P7 comes as a lighter model, people will use it."", 'Remember the name of this sub, models rise and fall all the time, there will always be a better model.', ""I asked a month or more ago, and then it was 'waiting for app to be approved on apple store' and then when it was approved it would recoup costs there and then release for free. That's why they rebranded to fictional.ai or whatever"", 'AstraliteHeart was in a tough position when he started on Pony 7, deciding the base model to be XL again or Flux with bad license, they compromised and now better alternatives exist. But its too late to start over', 'https://preview.redd.it/lzfoh1os13bf1.png?width=803&format=png&auto=webp&s=da06f1f2ea066753d2785c5b41fd0fa85ba7ed45\n\nThis is the latest ive seen from Pony v7 (and by latest i mean a few months ago)', 'Illustrious is pony 7']","[""I think astra is in the sunk-cost fallacy. I hate to be in their shoes, but choosing auraflow might not have been the best course. I would cut the project short and start from scratch on top of something more accessible but it's easy for me say."", ""It's incredibly hard to catch lightning in a bottle twice.\n\nSince Pony v7 started training, Illustrious, Noob, Flux, Chroma, etc. all have come out so other notable models have advanced or further pushed SDXL or new architectures.\n\nI'm sure it'll be a competent model, but I don't know that it'll have the same impact as V6 pony."", ""\\> Compare this with Chroma, where the creator is always in touch, and everyone sees a clear and uninterrupted roadmap.\n\nChroma is great, that's why we are sponsoring it!\n\n\\> I think that Pony 7 was most likely a failure and AstraliteHeart simply does not want to admit it.\n\nIt's a strong model with some issues that I want to fix before release. It takes time to fix such issues. More time than I anticipated or wanted but I would rather have a decent model than release something sad. The bar is very high because of V6 and all the models that came after it.\n\nIt is also \\*\\*very expensive\\*\\* to train models of this size (in addition to knowing how to train them), hence you only see a few models coming out now (literally, Chroma is the only one really large finetune aside V7 I am aware that is not SDXL). So we have to take care of the financial part of the process too, and it takes time.\n\n  \n\\> It could easily happen that when Pony comes out, it will be outdated and no one needs it.\n\nThe best I can do is to build cool models that bring new tech to the table, this worked ok for the last \\~8 model I've released."", ""Chroma is gaining a lot of traction, and most of the tooling for it already exists. Since P7 is likely to be similar in size but using a different architecture, I think it will not become as popular as V6. LodeStones is also releasing a new checkpoint every four days on average, so people can get a taste of how the development is going. The aim is 50 epochs, and he's nearing epoch 42.\n\nIf P7 comes as a lighter model, people will use it."", ""They may have swung big, missed big.\n\nPony being on SD 1.5 and then XL, was smaller and more agile to work on. So when XL launched they were able to pivot to that and ride it to being a success. A problem with the post XL world though is we still may not have the proper replacement. Newer models have been slower, larger, worse licensing and not training very well.\n\nChroma was able to ride the community Flux tinkering and come out on top of a schnell tune with their own tweaks. That may be the future, assuming the final tune doesn't blow apart, distills make it smaller/faster, and it trains well for loras. But it's also possible next week a new 3B param base model comes out that's good, open licensed, and easy to train and we all move to that. That would kill Chroma, though the team could possibly just pivot to the new model and start training on top of that.\n\nThe scene moves way too fast for 6 month plus projects to be viable.""]",107,92,0.87,Discussion,1751732171.0
1lsd52s,StableDiffusion,Some question about LoRA training,"Hello everyone!

I want to train a LoRA for Flux inspired by classic sword and sorcery imagery from the 1980s. Think of Larry Elmore, Keith Parkinson, Jeff Easley, Clyde Caldwell, Frank Frazetta, or Boris Vallejo, for example. I don't want the LoRA to perfectly replicate each of these styles, but rather to create a new one that works well as an amalgamation of all of them.



Well, I have three questions.

First, I would like the LoRA to recognize certain elements and learn to replicate them perfectly:

\- The character's class or stereotype: barbarian, warrior, sorcereress, wizard, thief, cleric, paladin, ranger, etc.

\- The character's race: human, elf, drow, dwarf, halfling, etc.

\- Classic creatures and monsters: orcs, goblins, skeleton warriors, vampires, dragons, beholders, mind flayers, centaurs, griffins, phoenixes, etc.

\- Specific poses: arms akimbo, arms crossed, kneeling with a weapon held, standing over corpses or ruins with a raised sword and chest expanded, etc.

\- Female breasts: I don't want to make a pornographic LoRA, but I do think it's important that it knows how to draw topless women in an anatomically correct way.

So, my first question is this: how many images of each type (dwarves, orcs, breasts, etc.) do I need to give the LoRA for it to learn how to replicate them, and from how many different angles?



Secondly, since the faces of the characters in these types of images tend to be quite neutral, to give the user more control in the future when choosing the type of faces they want, I've come up with the idea of ‚Äã‚Äãgenerating multiple images of facial expressions (anger, fear, sadness, surprise, etc.) using the Arthemy Comix Flux base model + Larry Elmore's LoRA + a LoRA of facial expressions. So, my question is: is it acceptable to use these AI-generated images as part of the LoRA training data? Will it cause me problems?



Thirdly, I want to know if I'm correctly describing the images for the LoRA training. For this image here ([https://www.this-is-cool.co.uk/wp-content/uploads/2019/07/the-art-of-clyde-caldwell.jpg](https://www.this-is-cool.co.uk/wp-content/uploads/2019/07/the-art-of-clyde-caldwell.jpg)), I wrote this description by hand. Please tell me if it's suitable and how I can improve it:

*Character left: white dragon, green eyes, standing on two legs, frontal view slightly turned to the right, looking at the sorceress; character center: female elf, elven woman, sorceress, white skin, long pointed ears, beautiful face, large breasts, short brown spiky hair, green dress, thin shoulder straps, deep v-neck showing cleavage, bare arms, bare legs, long front and back panels, large golden earrings, golden necklace, golden upper arm cuff bracelet on her left arm, golden forearm cuff bracelets on both her arms, golden jewel belt with a large embedded ruby, standing, frontal view slighty turned to the right, looking at the chest, arms raised at chest level, white magical beams projecting from hands towards a locked chest; full body shot; interior scene, treasure chamber inside a tower, glasseless large window at the background, a city with tall tower can be seen from the window, stone walls, wooden beams on the ceiling, chains hanging from the ceiling, an open treasure chest full with gold coins at the bottom left, a small round wooden table at the bottom right, an ornate small golden chest on the table; natural lighting from the window on the background, artificial lighting from the magic beams of the spell; the sorceress is casting a spell to open a locked chest; oil painting, vertical composition; sword and sorcery, medieval fantasy, old-school fantasy; Clyde Caldwell style, signature at the bottom right*


Thanks in advance for the answers!","[""You want to do a big multiconcept training. I did something similar here: [https://civitai.com/models/1434675/ancient-roman-clothing](https://civitai.com/models/1434675/ancient-roman-clothing)\n\nBut it was much smaller in number of concepts. For my training I had used about 700 images, so expect to need quite a few more.\n\nThen I guess you'll also need to train a LoKR and not a LoRA to be able to store all concepts. You'd also should aim to raise the number of images that go into one step (high batch size, gradient accumulation).\n\nFor the faces I did mask all of them. When they are required, due to the style, expect to need even more images to prevent the model learning specific faces.\n\nThe example prompt is problematic: you need to prompt in exactly the same way you'd use the model later on. I don't think anybody will write such a long prompt. And for the few thousand images you'll need I don't think you want to caption all manually. Especially as it's helping to use multicaptioning, i.e. have more than one caption per image.  \nFor this I could use Gemini to caption all my images: I created a large prompt explaining all my trigger words and then let Gemini detect them and caption them.\n\nIt'll be much work. But when it's working it's really satisfying! :)""]","[""You want to do a big multiconcept training. I did something similar here: [https://civitai.com/models/1434675/ancient-roman-clothing](https://civitai.com/models/1434675/ancient-roman-clothing)\n\nBut it was much smaller in number of concepts. For my training I had used about 700 images, so expect to need quite a few more.\n\nThen I guess you'll also need to train a LoKR and not a LoRA to be able to store all concepts. You'd also should aim to raise the number of images that go into one step (high batch size, gradient accumulation).\n\nFor the faces I did mask all of them. When they are required, due to the style, expect to need even more images to prevent the model learning specific faces.\n\nThe example prompt is problematic: you need to prompt in exactly the same way you'd use the model later on. I don't think anybody will write such a long prompt. And for the few thousand images you'll need I don't think you want to caption all manually. Especially as it's helping to use multicaptioning, i.e. have more than one caption per image.  \nFor this I could use Gemini to caption all my images: I created a large prompt explaining all my trigger words and then let Gemini detect them and caption them.\n\nIt'll be much work. But when it's working it's really satisfying! :)""]",2,1,1.0,Question - Help,1751731112.0
1lscvor,StableDiffusion,"Am I Missing Something? No One Ever Talks About F5-TTS, and it's 100% Free + Local and > Chatterbox","I see Chatterbox is the new/latest TTS tool people are enjoying, however F5-TTS has been out for awhile now and I still think it sounds better and more accurate with one-shot voice cloning, yet people rarely bring it up? You can also do faux podcast style outputs with multiple voices if you generate a script with an LLM (or type one up yourself). Chatterbox sounds like an exaggerated voice actor version of the voice you are trying to replicate yet people are all excited about it, I don't get what's so great about it","[""I'm in the same boat, I don't really like chatterbox over F5 but it does seem F5 isn't going anywhere so the community ignores it (in a similar situation with Ace Step for local music gen. It's one of the few ones we have and it can do lora training but it's dead in the water it seems). I also think the other problem is there seems to be a general consensus that for top quality local voice cloning you use xtts v2 which has been out for a while now.\n\nIt's the double edged sword of open source. Community support can work miracles but if their is no unity behind any one model then things just quickly die (nvidia being a bunch of greedy shit heels and not giving us more vram at a reasonable price isn't helping either)."", 'F5 while having a decent reading, it is unstable and hallucinates mid-sentences. Or fails to speak hyphenated words. Or suddenly changes the pace of the read.', 'Obviously most people disagree with you, which is why chatterbox is so popular. I didn‚Äôt find F5 usable, but I do chatterbox. Also, chatterbox voice cloning is better imo. I can use TYS inside silly tavern no problems. My only issue with cb is that it pronounces some words wrong, asks it doesn‚Äôt do accents well, but hopefully that will be resolved in future versions. But I think it‚Äôs solid enough to use for my use case, which is an AI personal assistant.', 'F5 has a more restrictive license I believe.', 'In case anyone is interested, I haven\'t been using them myself recently but last I heard F5 is still the best. \n\nEdit: probably time to update this with Wan and lipsync, and local music gen, \n\nAnyway:\n\nThere are so many models! https://artificialanalysis.ai/text-to-speech/arena\n\nMar2025 https://github.com/SparkAudio/Spark-TTS\n\nDec2024\n\nhttps://huggingface.co/geneing/Kokoro\n\nNewest, October 2024:\n\nF5-TTS and E2-TTS [https://www.youtube.com/watch?v=FTqAQvARMEg](https://www.youtube.com/watch?v=FTqAQvARMEg)   \nGithub Page: [https://github.com/SWivid/F5-TTS](https://github.com/SWivid/F5-TTS)   \nCode: [https://swivid.github.io/F5-TTS/](https://swivid.github.io/F5-TTS/)   \nAI Model : [https://huggingface.co/SWivid/F5-TTS](https://huggingface.co/SWivid/F5-TTS)\n\n\nu/perfect-campaign9551 says F5 tts sucks, it doesn\'t read naturally. Xttsv2 is still the king yet\n\n...\n\nYou want to hang out in r/AIVoiceMemes\n\nCoqui is fast but the voices are bad.\n\nTortoise is slow and unreliable but the voices are often great.\n\nStyleTTS2 is meant to be great and fast, but I could never figure out how to run it.\n\nThe key difference between Style and Coqui is that, I believe (things change), that you can train StyleTTS2.\n\nRVC does voice to voice, if you\'re struggling to get the \\*\\*\\*precise\\*\\*\\* pacing then you should speak into a mic and voice clone it with RVC.\n\nYou will want to seek podcasts and audiobooks on YouTube to download for audio sources.\n\nYou will want to use UVR5 to separate vocals from instrumentals if that becomes a thing.\n\nYou will eventually want to try lip syncing video, for that you will use EasyWav2Lip or possibly Face Fusion.\n\nIf you\'re having difficulty with install, there are Pinokio installs of a lot of TTS that can be easier to use, but are more limited.\n\nCheck out Jarod\'s Journey for all of the advice, especially about Tortoise: [https://www.youtube.com/@Jarods\\_Journey](https://www.youtube.com/@Jarods_Journey)\n\nCheck out P3tro for the only good installation tutorial about RVC: [https://www.youtube.com/watch?v=qZ12-Vm2ryc&t=58s&ab\\_channel=p3tro](https://www.youtube.com/watch?v=qZ12-Vm2ryc&t=58s&ab_channel=p3tro)\n\nEdit: Jarod made a gui for StyleTTS2. Also, try alltalk?\n\nEdit: u/a_beautifil_rhind \n\nstyletts has a better model called vokan.\nhttps://huggingface.co/ShoukanLabs/Vokan/tree/main/Model\n\nThere\'s also fish-audio now in addition to xtts. Also voicecraft.\n\nEdit: u/tavirabon\n\nCoqui (XTTS) can be finetuned https://github.com/daswer123/xtts-finetune-webui\n\nAlso https://github.com/RVC-Boss/GPT-SoVITS which is a step up from other zero-shot TTS and most few-shot TTS (>1 minute of clear natural speech) finetuning\n\nEdit: u/battlerepulsiveO\n\nYou can use the huggingface model of XTTS V2 because there are people who have finetuned XTTS V2 before. It\'s really simple to train with different methods like one that has automated for you where you just drop in the audio files. Or you can personally create a dataset and a csv file with the name of the audio file and the transcription, and all the wav files should be stored inside a wav folder. It all depends on the notebook you\'re using.\n\nEdit: u/dumpimel\n\nhave you tried alltalk? it\'s based on coqui\n\nhttps://github.com/erew123/alltalk_tts\n\nyou drop a 20s .wav in the ""voices"" folder and it\'s pretty decent at reproducing the voice\n\nthey also say you can finetune it further', ""It works pretty good.  But, it's not maintained, as I understand.\n\nI've used it and created a batcher for it."", ""I made the mistake of using ElevenLabs *first*. If you've never used TTS before, F5 is impressive. If you've used state of the art TTS (ElevenLabs), then F5, Chatterbox, SoVITS, and all the other local models sound like ass.""]","[""I'm in the same boat, I don't really like chatterbox over F5 but it does seem F5 isn't going anywhere so the community ignores it (in a similar situation with Ace Step for local music gen. It's one of the few ones we have and it can do lora training but it's dead in the water it seems). I also think the other problem is there seems to be a general consensus that for top quality local voice cloning you use xtts v2 which has been out for a while now.\n\nIt's the double edged sword of open source. Community support can work miracles but if their is no unity behind any one model then things just quickly die (nvidia being a bunch of greedy shit heels and not giving us more vram at a reasonable price isn't helping either)."", 'F5 while having a decent reading, it is unstable and hallucinates mid-sentences. Or fails to speak hyphenated words. Or suddenly changes the pace of the read.', 'Obviously most people disagree with you, which is why chatterbox is so popular. I didn‚Äôt find F5 usable, but I do chatterbox. Also, chatterbox voice cloning is better imo. I can use TYS inside silly tavern no problems. My only issue with cb is that it pronounces some words wrong, asks it doesn‚Äôt do accents well, but hopefully that will be resolved in future versions. But I think it‚Äôs solid enough to use for my use case, which is an AI personal assistant.', 'F5 has a more restrictive license I believe.', 'In case anyone is interested, I haven\'t been using them myself recently but last I heard F5 is still the best. \n\nEdit: probably time to update this with Wan and lipsync, and local music gen, \n\nAnyway:\n\nThere are so many models! https://artificialanalysis.ai/text-to-speech/arena\n\nMar2025 https://github.com/SparkAudio/Spark-TTS\n\nDec2024\n\nhttps://huggingface.co/geneing/Kokoro\n\nNewest, October 2024:\n\nF5-TTS and E2-TTS [https://www.youtube.com/watch?v=FTqAQvARMEg](https://www.youtube.com/watch?v=FTqAQvARMEg)   \nGithub Page: [https://github.com/SWivid/F5-TTS](https://github.com/SWivid/F5-TTS)   \nCode: [https://swivid.github.io/F5-TTS/](https://swivid.github.io/F5-TTS/)   \nAI Model : [https://huggingface.co/SWivid/F5-TTS](https://huggingface.co/SWivid/F5-TTS)\n\n\nu/perfect-campaign9551 says F5 tts sucks, it doesn\'t read naturally. Xttsv2 is still the king yet\n\n...\n\nYou want to hang out in r/AIVoiceMemes\n\nCoqui is fast but the voices are bad.\n\nTortoise is slow and unreliable but the voices are often great.\n\nStyleTTS2 is meant to be great and fast, but I could never figure out how to run it.\n\nThe key difference between Style and Coqui is that, I believe (things change), that you can train StyleTTS2.\n\nRVC does voice to voice, if you\'re struggling to get the \\*\\*\\*precise\\*\\*\\* pacing then you should speak into a mic and voice clone it with RVC.\n\nYou will want to seek podcasts and audiobooks on YouTube to download for audio sources.\n\nYou will want to use UVR5 to separate vocals from instrumentals if that becomes a thing.\n\nYou will eventually want to try lip syncing video, for that you will use EasyWav2Lip or possibly Face Fusion.\n\nIf you\'re having difficulty with install, there are Pinokio installs of a lot of TTS that can be easier to use, but are more limited.\n\nCheck out Jarod\'s Journey for all of the advice, especially about Tortoise: [https://www.youtube.com/@Jarods\\_Journey](https://www.youtube.com/@Jarods_Journey)\n\nCheck out P3tro for the only good installation tutorial about RVC: [https://www.youtube.com/watch?v=qZ12-Vm2ryc&t=58s&ab\\_channel=p3tro](https://www.youtube.com/watch?v=qZ12-Vm2ryc&t=58s&ab_channel=p3tro)\n\nEdit: Jarod made a gui for StyleTTS2. Also, try alltalk?\n\nEdit: u/a_beautifil_rhind \n\nstyletts has a better model called vokan.\nhttps://huggingface.co/ShoukanLabs/Vokan/tree/main/Model\n\nThere\'s also fish-audio now in addition to xtts. Also voicecraft.\n\nEdit: u/tavirabon\n\nCoqui (XTTS) can be finetuned https://github.com/daswer123/xtts-finetune-webui\n\nAlso https://github.com/RVC-Boss/GPT-SoVITS which is a step up from other zero-shot TTS and most few-shot TTS (>1 minute of clear natural speech) finetuning\n\nEdit: u/battlerepulsiveO\n\nYou can use the huggingface model of XTTS V2 because there are people who have finetuned XTTS V2 before. It\'s really simple to train with different methods like one that has automated for you where you just drop in the audio files. Or you can personally create a dataset and a csv file with the name of the audio file and the transcription, and all the wav files should be stored inside a wav folder. It all depends on the notebook you\'re using.\n\nEdit: u/dumpimel\n\nhave you tried alltalk? it\'s based on coqui\n\nhttps://github.com/erew123/alltalk_tts\n\nyou drop a 20s .wav in the ""voices"" folder and it\'s pretty decent at reproducing the voice\n\nthey also say you can finetune it further']",20,24,0.85,Discussion,1751730410.0
1lscnvj,StableDiffusion,Whats the best AI short story generator currently?,"Hey. I am looking for the best AI short story generator out there, mostly for understanding them from a technical perspective. I am seeing lots of AI short story content on social media and wonder if they all do it with their own workflow or if there is a SOTA public platform for this. As a reference, I am talking about content like this, for example - https://x.com/TrungTPhan/status/1940905807908110711 (I am not affiliated with that post)","['Try asking /r/localllama', 'chatgpt', ""I don't think there is an AI to do that. It's simply making a lot of short videos and putting them together to make a story."", ""It's probably all about creative prompting and possibly invoking styles of good authors."", 'I mean AI short story generators with Video capabilities. Not text only btw']","['Try asking /r/localllama', 'chatgpt', ""I don't think there is an AI to do that. It's simply making a lot of short videos and putting them together to make a story."", ""It's probably all about creative prompting and possibly invoking styles of good authors."", 'I mean AI short story generators with Video capabilities. Not text only btw']",0,5,0.29,Question - Help,1751729835.0
1lsce15,StableDiffusion,Wan2.1/vace plus upscale in topaz,Image made in chatgpt then elements changed with flux inpainting. Wan2.1/vace then upscaled twice separately! Then lipsync comped onto mouths. ,"[""Good job.  The level of consistency from one frame to the next is astonishing relative to what we expected so recently.\n\nI know that some models have or could be trained to have some skill working with 360 degree panoramas.  It would be really interesting to experiment with rending the entire 360 viewpoint at all times, either for use on VR headsets or for having richer footage to use in studio cuts and orchestration of smooth panning for conventional fov videos.  How will film making change when we're only using virtual cameras?"", 'Nice! Punch into a closeup here and there, even on Morty just blinking, and u have a sit com', 'What is the lipsync? Pretty good result for open source', 'strawberry smiggles', 'What‚Äôs the lipsync being used?', 'Lipsync please! All of the open source lipsync posts suck, so you must be doing something differently or you‚Äôre using that one (I forget its name) that works exclusively with avatars.', 'table looks different', ""that's pretty good, i like how well it matched up with rick's voice toward the end. i wonder how your workflow looks with some Robot Chicken sketches haha""]","[""Good job.  The level of consistency from one frame to the next is astonishing relative to what we expected so recently.\n\nI know that some models have or could be trained to have some skill working with 360 degree panoramas.  It would be really interesting to experiment with rending the entire 360 viewpoint at all times, either for use on VR headsets or for having richer footage to use in studio cuts and orchestration of smooth panning for conventional fov videos.  How will film making change when we're only using virtual cameras?"", 'What is the lipsync? Pretty good result for open source', 'Nice! Punch into a closeup here and there, even on Morty just blinking, and u have a sit com', 'strawberry smiggles', 'What‚Äôs the lipsync being used?']",34,11,0.79,Animation - Video,1751729084.0
1lscbye,StableDiffusion,Character consistency for cartoon styles,"Anybody know where I can read more about generating better prompts for consistent characters? I know about using Loras etc. but I just want to get started with simple stickfigure characters.

For example, I tried using these prompts:


>In a minimalist cartoon illustration featuring a flat pastel-yellow background (#F7E36B), the scene is set in a 16:9 landscape aspect ratio, maintaining a vivid, playful aesthetic. The foreground showcases a stick figure with an oversized round white head, a thick black outline (\~8px), dot eyes, expressive eyebrows, and a comically surprised mouth. The character has thin, exaggerated stick arms and legs, with mitten-like hands and flat feet, wearing only plain white shorts. The expression suggests a realization or shock, contributing to the theme of self-sabotage.

>In the middle ground, another stick figure, similarly designed, strikes a pondering pose, featuring one hand on their head and the other hand outstretched as if questioning. Both figures maintain a playful interaction while adhering to the cartoon's bold, expressive visual style.

>In the background, a simplistic setting of a single, thick-outlined black table with a minimal white vase on it adds context without clutter. The environment features essential elements only, each with super-thick black outlines and solid white interiors, ensuring a clean, humorous visual narrative.

>A speech bubble (with a thick black outline) emerges from the first character's mouth, capturing the sentence, ""Did you know you're probably sabotaging your own happiness right now and don‚Äôt even realize it?"" in a playful, hand-drawn font. The text inside the bubble is pure black, enhancing readability and fitting seamlessly into the cartoon world.

>The composition leaves a clean, 5% margin around the edges, maintaining focus and clarity, ensuring that all visual elements and messages are easily recognizable at a glance. This image instantly conveys the theme of subconscious actions affecting one's well-being in a humorous, relatable manner.

https://preview.redd.it/6s13t17go2bf1.png?width=1024&format=png&auto=webp&s=b682b3619b281e2175df68bb3104f0fd42747735

>In the foreground, a cartoon stick figure with two arms and two legs sits at the edge of a white bed, its oversized bald white head hung low, reflecting a sense of melancholy. The figure's expressive brows are furrowed and mouth slightly downturned, amplifying the feeling of dissatisfaction. Beside it, a simple white alarm clock rings loudly, indicating the start of another monotonous day.

>In the middle ground, an ascending staircase fades into an indistinct hallway, symbolizing the path of routine and the unknown longing for something more. The stick figure's eyes glance longingly toward this direction, hinting at a deep, unfulfilled desire for change.

>In the background, behind the staircase, a minimalist cityscape silhouette can be seen through a large window with thick black outlines. The sky is an indistinct blank white, devoid of vibrant color, reinforcing the sense of a life lacking excitement.

>The color palette is uniform and monochrome with solid white fills and black outlines consistent across all elements, ensuring clarity in the depiction of an autopilot existence. The tone of the image is somber yet gently comedic due to the exaggerated proportions and expressive features, maintaining the playful integrity of the cartoon style.

>Overall, the image portrays a simplistic yet profound moment of introspection, making the viewer contemplate the feeling of living on autopilot while expressing it through a coherent, stylized cartoon world.

https://preview.redd.it/j1iy3v4oo2bf1.png?width=1024&format=png&auto=webp&s=ca21ab90c63901f6f01925e6662d719f3f8212fc

So it was unable to keep the same style. Should I pass in less information as the prompt? I am using FLUX-Schnell to generate the images. I have been playing around with hundreds of prompts, reading tutorials to get the best prompts etc. but nothing seems to allow me to keep a consistent style across the images... I am hoping to create a comic book style workflow.","[""I think the purple prose like that is less effective than you'd think. You most likely wouldn't be able to maintain style through prompt alone. Something like Flux Kontext would have a better chance of doing it, at least it can maintain the character.""]","[""I think the purple prose like that is less effective than you'd think. You most likely wouldn't be able to maintain style through prompt alone. Something like Flux Kontext would have a better chance of doing it, at least it can maintain the character.""]",0,2,0.33,Question - Help,1751728932.0
1lsb708,StableDiffusion,Best model to use as base for SDXL Lora?,"Hey guys, what's the model to use as base? I know at some point SDXL itself was the way to go but doing it on some finetunes can give better results, so wondering if anyone got any idea. I haven't trained a realistic lora in a while so out of date.","['If it is for personal use, train it on the model you will use for inference. If you plan to share it, training it on base SDXL will allow it to be used on the broadest range of finetunes at the cost of some fidelity.']","['If it is for personal use, train it on the model you will use for inference. If you plan to share it, training it on base SDXL will allow it to be used on the broadest range of finetunes at the cost of some fidelity.']",0,1,0.33,Question - Help,1751725846.0
1lsap7b,StableDiffusion,7800xt to 5070ti,"I am looking to upgrade my video card as I am done messing with amd when it comes to generative ai. I am hoping to hear from anyone with 50xx series cards and how they are performing for them, particularly with wan 2.1. a little input would be great to help me make my decision. Right now I'm looking at the 5070ti cards.

Thanks!","['wait for the 5070ti 24gb', 'A change to NVIDIA makes sense especially for AI regardless which GPU. More memory is more important than (slightly) more performance. I upgraded from a 2080ti to a 5090 and it was definitely the right choice.', ""The RTX 5070 Ti is a good choice for AI tasks but also look at RTX 5060 Ti 16GB - this offers a more budget-friendly option with same amount of memory, albeit with slower performance but I am sure the price difference is more compared to the performance difference.   \n\n\nIf you're looking for a significant upgrade then wait and keep an eye for the 5070 Ti Super 24GB, which could be the best upgrade even for 4k games."", 'Just built a PC with these two components for a generative only machine.\n\n5070ti runs quite well IF you are on a good driver. Every other driver causes issues.', ""What's your use case? What GenAI tech stack are you using?""]","['wait for the 5070ti 24gb', 'A change to NVIDIA makes sense especially for AI regardless which GPU. More memory is more important than (slightly) more performance. I upgraded from a 2080ti to a 5090 and it was definitely the right choice.', ""The RTX 5070 Ti is a good choice for AI tasks but also look at RTX 5060 Ti 16GB - this offers a more budget-friendly option with same amount of memory, albeit with slower performance but I am sure the price difference is more compared to the performance difference.   \n\n\nIf you're looking for a significant upgrade then wait and keep an eye for the 5070 Ti Super 24GB, which could be the best upgrade even for 4k games."", 'Just built a PC with these two components for a generative only machine.\n\n5070ti runs quite well IF you are on a good driver. Every other driver causes issues.', ""What's your use case? What GenAI tech stack are you using?""]",1,11,0.67,Question - Help,1751724466.0
1lsals9,StableDiffusion,Opensource Image2image generator,"Hi all,

Alpha-VLLM‚Äôs Lumina-mGPT-2.0 was going to release an Image to Image generator to take on chatGPT‚Äôs one but nothing seems to have been released. Anyone know of another both diffusion or autoregression image 2 image generator that is open source?",['https://github.com/VectorSpaceLab/OmniGen2'],['https://github.com/VectorSpaceLab/OmniGen2'],0,1,0.5,Question - Help,1751724198.0
1lsa9so,StableDiffusion,Alternative to RVC for real time?,RVC is pretty dated at this point. Many new ones have released but they're TTS instead of voice conversion. I'm pretty left behind in the voice section. What's a good newer alternative?,"[""I don't think there is any. Most current papers only use TTS unfortunately, as you mentioned.\n\nI actually theorycrafted a newer realtime V2V architecture based on newer papers, but it's a hassle and training time is huge with a good dataset on a consumer gpu as mine. Also having to train submodules like the vocoder from scratch adds to that time even more"", 'RVC does real time though...', 'I‚Äôm also on the lookout for one, I‚Äôve looked but unable to find a newer alternative .', 'For what purpose? Voice cloning? There are a few good TTS alternatives. If you want voice to voice, you can use STT and then TTS with a reference voice. Some newer alternatives have really good zero shot voice cloning.', ""I used it throughout this [narrated noir video](https://www.youtube.com/watch?v=mIDSYRsuSFM) and a lot of people picked up on it. If people are noticing things then thats a disctraction. So yea, I am looking for improved alternatives too.  \n  \nRVC is good and it allows us to add our own dramatic inflection to the voice, but the process is laborious and also has a crackle and drop outs that can take a bit of work to fix sometimes.  In other ways it is too good, my best trained actors I can't use because its too obvious who they are.\n\nI personally dont like many TTS they sound like AI or just inflict in the wrong places. So following this thread in hopes of seeing new stuff but I doubt many are working on it.""]","[""I don't think there is any. Most current papers only use TTS unfortunately, as you mentioned.\n\nI actually theorycrafted a newer realtime V2V architecture based on newer papers, but it's a hassle and training time is huge with a good dataset on a consumer gpu as mine. Also having to train submodules like the vocoder from scratch adds to that time even more"", 'RVC does real time though...', 'I‚Äôm also on the lookout for one, I‚Äôve looked but unable to find a newer alternative .', 'For what purpose? Voice cloning? There are a few good TTS alternatives. If you want voice to voice, you can use STT and then TTS with a reference voice. Some newer alternatives have really good zero shot voice cloning.', ""I used it throughout this [narrated noir video](https://www.youtube.com/watch?v=mIDSYRsuSFM) and a lot of people picked up on it. If people are noticing things then thats a disctraction. So yea, I am looking for improved alternatives too.  \n  \nRVC is good and it allows us to add our own dramatic inflection to the voice, but the process is laborious and also has a crackle and drop outs that can take a bit of work to fix sometimes.  In other ways it is too good, my best trained actors I can't use because its too obvious who they are.\n\nI personally dont like many TTS they sound like AI or just inflict in the wrong places. So following this thread in hopes of seeing new stuff but I doubt many are working on it.""]",16,7,0.86,Question - Help,1751723218.0
1lrphxy,StableDiffusion,Face fusion 3.2.0 10 second content filter?,"Hi!. Does anybody know how to stop this from happening? No matter how long the video is, it‚Äôll cut it as a 10 second video and then save it as ‚Äútemp‚Äù which doesn‚Äôt do with the other (sfw) videos Im pretty sure. I‚Äôm also using Pinokio version 3.9.0. (It‚Äôs my body with an AI generated face). Any help would be really appreciated ",[],[],1,0,1.0,Question - Help,1751652797.0
1ls0mnl,StableDiffusion,How to train a lora for pony?,Is there any equivalent of flux gym but for the pony model? I try use kohya but is so confusing and only allows loras based on SDXL. I don't know if it's compatible. I'm a newbie in this. I have only used fluxgym in the past. Preferible I prefer using a cloud service like Mimic Pc since my own PC is not that good. Thanks ,"[""Pony is literally SDXL model. If you don't want to use Kohya, use OneTrainer or something.""]","[""Pony is literally SDXL model. If you don't want to use Kohya, use OneTrainer or something.""]",1,1,1.0,Question - Help,1751686641.0
1ls2q7u,StableDiffusion,i can download 100K+ LoRA and organize from civitai,"desktop app - [https://github.com/rajeevbarde/civit-lora-download](https://github.com/rajeevbarde/civit-lora-download)


it does lot of things .... all details in README.

this was vibe coded in 14 days Cursor trial plan.... bugs expected",[],[],6,0,0.75,Tutorial - Guide,1751694536.0
1ls8ftk,StableDiffusion,Looking for the BEST faceswap tool without a gpu or cpu only,"The title says it all.  I'm looking for a face-swap tool that can reproduce the original expression of the person on original image or video, while looking realistic. I tried Roop(?) or something like that but the person never looked like the source material or the skin texture was weird even though I used high quality images and videos.

If there's a paid tool (Paid as in an online service where I can buy credits or rent a gpu and do the swap there) it would be good too.


Are there any tools that I can create a model (like a Lora) for a person's face? I feel like source images aren't really a good way to be consistent if there's makeup etc. involved.


Thanks for the help!","['roop, facefusion or anything with inswapper 128. isn\'t really cut it for perfect one to one swap. but that also the only ""zero train"" face swapper.  \n  \nIf you want fullface simmilarity you have to go deepfacelab, and it can do CPU, but it needs to be trained first', 'New facefusion 3.3 is dope it has new hyperswap model. Free if you ok with basic command prompt (ask ChatGPT he will guide you) to install or pay 20 bucks for quick installer and support devs\xa0']","['roop, facefusion or anything with inswapper 128. isn\'t really cut it for perfect one to one swap. but that also the only ""zero train"" face swapper.  \n  \nIf you want fullface simmilarity you have to go deepfacelab, and it can do CPU, but it needs to be trained first', 'New facefusion 3.3 is dope it has new hyperswap model. Free if you ok with basic command prompt (ask ChatGPT he will guide you) to install or pay 20 bucks for quick installer and support devs\xa0']",0,4,0.14,Question - Help,1751717429.0
1ls9phn,StableDiffusion,Can you animate characters on a green screen?,"I've tried with img2vid, with a picture of a character on a green screen already, but it always ended up changing background color or shade at some point in the animation. ","[""A good first step might be applying a reference only control net and tweaking its strength until you get a nice balance between consistency and flexibility.\n\nBut, honestly, that you're having these problems suggests you're using older models.  Newer ones are much better at applying your prompts.  There are models and workflows that can stamp out all your keyframes on a single image and others than can take your single starting image and generate entire videos from it.""]","[""A good first step might be applying a reference only control net and tweaking its strength until you get a nice balance between consistency and flexibility.\n\nBut, honestly, that you're having these problems suggests you're using older models.  Newer ones are much better at applying your prompts.  There are models and workflows that can stamp out all your keyframes on a single image and others than can take your single starting image and generate entire videos from it.""]",0,9,0.33,Question - Help,1751721515.0
1ls96gn,StableDiffusion,What model is this?,"Looks extremely real and they‚Äôve nailed the Lora

The videos are unbelievable realistic

What do you think is being used?

https://www.tiktok.com/@ai.kalai?_t=ZN-8xm4Y38srUE&_r=1","['Veo3?', 'What do you think the compute/cost necessary for this kind of generate this on rental servers?', 'Veo 3 you can tell by how the audio distorts, then likely Topaz upscaling', 'Does Google Veo 3 allow image-to-video?  If not, then how do they get consistent-looking characters from video to video?']","['Veo3?', 'What do you think the compute/cost necessary for this kind of generate this on rental servers?', 'Veo 3 you can tell by how the audio distorts, then likely Topaz upscaling', 'Does Google Veo 3 allow image-to-video?  If not, then how do they get consistent-looking characters from video to video?']",0,10,0.11,Discussion,1751719906.0
1ls8z68,StableDiffusion,"Is it worth training loras for FluxFill? I don't see anyone doing that. Is it better to use other methods like ACE++, Redux, Alimama controlnet ?","I'm not sure, but I've read some people saying that loras trained with flux fill are not good","['You can try to train flux kontext loras, it the same if not better', ""I don't know but you can try one alternative that was found during flux1 kontext release\n\nsomeone was loading flux dev loras via flux dev and then merging it with flux kontext and then removing the difference (or something like that) and they said it worked nice\n\nit is worth trying the same approach but with flux fill instead of kontext :)""]","['You can try to train flux kontext loras, it the same if not better', ""I don't know but you can try one alternative that was found during flux1 kontext release\n\nsomeone was loading flux dev loras via flux dev and then merging it with flux kontext and then removing the difference (or something like that) and they said it worked nice\n\nit is worth trying the same approach but with flux fill instead of kontext :)""]",2,2,0.63,Discussion,1751719235.0
1ls8a4q,StableDiffusion,Audio dubbing locally,I used eleven labs but now I am looking for a local solution to upload an audio and then be able to translate it or change accent to another language without changing the voice. any suggestion with the model and workflow ? ,[],[],1,0,1.0,Question - Help,1751716897.0
1ls6rh0,StableDiffusion,"Why can't I find any 5090 on Vast.ai? Even resetting filters (or setting them to maximum) won't help. I rented 5090s before, so it definitely worked before. Did they banned them? Is the website buggy? Am I just stupid and disabled something? What's going on?","Also, a while ago someone posted a website where we could see the number of different GPUs on vast.ai. Could someone check if there are any 5090s there?","[""You have an older template that's excluding it because it's not compatible. Use the CUDA 12.8 one."", ""I checked and I see plenty of 5090's. Don't know how to help you but just letting you know the problem is something on your end. Maybe ask in the chat feature of the Vast.ai website?""]","[""You have an older template that's excluding it because it's not compatible. Use the CUDA 12.8 one."", ""I checked and I see plenty of 5090's. Don't know how to help you but just letting you know the problem is something on your end. Maybe ask in the chat feature of the Vast.ai website?""]",1,4,0.56,Question - Help,1751711105.0
1ls6qj7,StableDiffusion,Igorr's ADHD - How did they do it?,"Not sure this is the right sub, but anyway, hoping it is: I'm trying to wrap my head around at how Meatdept could achive such outstanding results with this video using ""proprietary and open-source"" tools.

From the video caption, they state: ""we explored the possibilities of AI for this new Igorrr music video: ""ADHD"". We embraced almost all existing tools, both proprietary and open source, diverting and mixing them with our 3D tools"".

I tried the combination Flux + Wan2.1, but the results were nowhere close to this. Veo 3 is way too fresh IMO for a work that probably took a month or two at the very least. And a major detail: the consistency is unbelievable, the characters, the style and the photography stay pretty much the same throughout all the countless scenes/shots. Any ideas what they could've used?","['So there\'s a lot going on here, but most of it is ""a lot of work"". Notice that this is built out of very short segments, 3 to 5 seconds . . . in this almost 5 minute film, there\'s maybe 60-80 segments, an astonishing number (some are repeats though). The very short shot length does suggest AI generation \n\nWhat I think I\'m seeing is a staggering amount of planning, followed by a heckuva lot of work in generating, and then a ton of work editing it all together. You could do a \\_lot\\_ of this in easy tools, not hard to get a five second clip with this kind of theme, but what\'s really hard is to get dozens of these clips which add up to a coherent (and funny !) narrative.\n\nPeople often miss how important editing is to film. Understanding where the beats are in a scene, how to cut from A to B to C . . . that\'s a art.  When I look at any one segment of this, I think to myself ""yeah, I know how I\'d get that clip\'. Where my jaw drops is how they\'ve pulled this all together in this coordinated, coherent way. Doing that is a matter not just of a lot of work, but also of an artistic vision. So yeah, most folks can learn to do a 3 to 5 second clip from some still source material that looks like this, but the magic is in the coherence of dozens of clips that make sense and tell a great story. \n\nIf you\'re looking for image quality that\'s at this level -- its going to be image based video for sure. That is you\'re going to want to render a bunch of reference frames, of the highest quality, and then use them for image to video. Not sure which application it would be . . . quality wise, a lot of things can produce stuff that looks this snappy. If I were guessing, I might have guessed Kling video, given the time that this was created (back in April) that limits some of what it could be (eg could be Midjourney for some of the still sources, but there was no MJ video at that pointP   . . . but I bet with the right tweaks it could have been done in a lot of other things. The only thing I\'m pretty certain of is that this isn\'t text to video, there have to be image references here . . . that\'s \\[part of\\] what creates the consistency. Indeed, as they describe what they put together, this looks like it was storyboarded and planned with a lot of tools including 3D', ""I'm amazed that not even AI can reach the level of absurdity and mindfuckery that Chris Cunningham works do. It really reminds me how much of a genius that man is.   \nIf someone don't know him yet, check [Rubber Johnny](https://www.youtube.com/watch?v=9-gyf23k26I&list=RD9-gyf23k26I&start_radio=1) to get an idea."", 'Just read what you quoted. They used many other techniques than AI generative models. They could use for example Blender 3D for stuff like tracking, masking, and adding objects.', 'thank you for reminding me to listen more to Igorrr. Sorry for not providing any useful info about how to do it, how they made it :3', 'Looks like Flux with a Ben Stein and maybe like a Clark Gable LoRA, mixed with 1950s mid-century modern or atomic-age/retro-futurism LoRA, then fed into whatever I2V tools they used. Maybe WAN or proprietary ones (they said they used some closed-source tools in the video description). They also mentioned they used 3D tools, maybe to generate the more abstract 3D rendered clips.\n\nAnd then a lot of editing.', 'This is what people who say things like ‚Äòin a few years all movies and tv shows will be done with AI, just type in what you want and it‚Äôll be generated for you‚Äô miss. Yes you can do it with AI, but for it to be any good takes a ton of hard work, planning and talent. Same as with anything else.\xa0', '> How did they do it ?\n\nDrugs. Essentially.', 'awesome video. The early part was Veo I think - the beginning pace was slower and more realistic. I suspect they worked on it for a while and Veo came out and they had to use it. I think the way it warms up into the surrealism is very effective.\n\nA small thing I noticed how they must have made 5 second clips of the main character pulling facial expressions and then sped that up x5 so his movements synced with the beat. So much work must have gone into this.  Music and video creation is blending more and more.']","['So there\'s a lot going on here, but most of it is ""a lot of work"". Notice that this is built out of very short segments, 3 to 5 seconds . . . in this almost 5 minute film, there\'s maybe 60-80 segments, an astonishing number (some are repeats though). The very short shot length does suggest AI generation \n\nWhat I think I\'m seeing is a staggering amount of planning, followed by a heckuva lot of work in generating, and then a ton of work editing it all together. You could do a \\_lot\\_ of this in easy tools, not hard to get a five second clip with this kind of theme, but what\'s really hard is to get dozens of these clips which add up to a coherent (and funny !) narrative.\n\nPeople often miss how important editing is to film. Understanding where the beats are in a scene, how to cut from A to B to C . . . that\'s a art.  When I look at any one segment of this, I think to myself ""yeah, I know how I\'d get that clip\'. Where my jaw drops is how they\'ve pulled this all together in this coordinated, coherent way. Doing that is a matter not just of a lot of work, but also of an artistic vision. So yeah, most folks can learn to do a 3 to 5 second clip from some still source material that looks like this, but the magic is in the coherence of dozens of clips that make sense and tell a great story. \n\nIf you\'re looking for image quality that\'s at this level -- its going to be image based video for sure. That is you\'re going to want to render a bunch of reference frames, of the highest quality, and then use them for image to video. Not sure which application it would be . . . quality wise, a lot of things can produce stuff that looks this snappy. If I were guessing, I might have guessed Kling video, given the time that this was created (back in April) that limits some of what it could be (eg could be Midjourney for some of the still sources, but there was no MJ video at that pointP   . . . but I bet with the right tweaks it could have been done in a lot of other things. The only thing I\'m pretty certain of is that this isn\'t text to video, there have to be image references here . . . that\'s \\[part of\\] what creates the consistency. Indeed, as they describe what they put together, this looks like it was storyboarded and planned with a lot of tools including 3D', ""I'm amazed that not even AI can reach the level of absurdity and mindfuckery that Chris Cunningham works do. It really reminds me how much of a genius that man is.   \nIf someone don't know him yet, check [Rubber Johnny](https://www.youtube.com/watch?v=9-gyf23k26I&list=RD9-gyf23k26I&start_radio=1) to get an idea."", 'Just read what you quoted. They used many other techniques than AI generative models. They could use for example Blender 3D for stuff like tracking, masking, and adding objects.', 'thank you for reminding me to listen more to Igorrr. Sorry for not providing any useful info about how to do it, how they made it :3', 'Looks like Flux with a Ben Stein and maybe like a Clark Gable LoRA, mixed with 1950s mid-century modern or atomic-age/retro-futurism LoRA, then fed into whatever I2V tools they used. Maybe WAN or proprietary ones (they said they used some closed-source tools in the video description). They also mentioned they used 3D tools, maybe to generate the more abstract 3D rendered clips.\n\nAnd then a lot of editing.']",12,10,0.62,Question - Help,1751710996.0
1ls65i0,StableDiffusion,"New to Stable Diffusion, Need Help Getting Started","Hey everyone, I‚Äôm new to Stable Diffusion and image generation in general. I‚Äôm really interested in using it to generate consistent images for my projects.

Can anyone guide me on how to set it up and use it properly? Do I need to pay for anything to get started?

Also, if there‚Äôs a YouTube video or tutorial that explains everything for beginners, I‚Äôd really appreciate it if you could drop the link!

Lastly, is there any AI tool similar to Stable Diffusion but for video generation? I‚Äôd love to explore that too.

Thanks in advance!","['SwarmUI is simple to use, and uses ComfyUI.  Swarm has a built-in model downloader that will maintain metadata from civitai.  \n\n[https://github.com/mcmonkeyprojects/SwarmUI](https://github.com/mcmonkeyprojects/SwarmUI)', 'If you prefer simple then try simple interface like ""Fooocus"" for stable diffusion XL modes:\xa0[YouTube - Fooocus installation](https://youtu.be/3tAaL57rhoU?si=ArrTPhPQJkbfW7ZN)\n\nThis\xa0[playlist - YouTube](https://youtube.com/playlist?list=PLPFN04WspxqsslRSpiLmwGR8QTpDYNv7z&si=-QOo5_980RbiniKn)\xa0is for beginners which covers topic like prompt, models, lora, weights, in-paint, out-paint, image to image, canny, refiners, open pose, consistent character, training a LoRA.\n\nAfter this if you wish to go further then look at Comfy UI', ""If you don't have the GPU, you can use online services for free or at very low cost: [Free Flux/SDXL Online Generators](https://new.reddit.com/r/StableDiffusion/comments/18h7r2h/free_online_sdxl_generators/)\n\nHere are some ELi5 guides that I wrote a while back which may or may not help:\n\n[ELi5: Absolute beginner's guide to getting started in A.I. Image generation](https://new.reddit.com/r/StableDiffusion/comments/1b2mhjv/eli5_absolute_beginners_guide_to_getting_started/)\n\n[ELi5: What are SD models, and where to find them](https://new.reddit.com/r/StableDiffusion/comments/11s6485/eli5_what_are_sd_models_and_where_to_find_them/)"", 'What GPU do you have?\n\nDownload [Stability Matrix](https://github.com/LykosAI/StabilityMatrix). That program is an installer/manager for the various UIs. This is all free by the way.\n\nOn Matrix download Forge or reForge as they will be the easiest to start with.\n\nGo to [civit.ai](http://civit.ai) and make an account. Download checkpoints that you like.\n\nFor video look at Wan 2.1 and you can run this on ComfyUI, one of the UIs you can download through Stability Matrix. You might struggle to run this depending on how much VRAM you have.']","['SwarmUI is simple to use, and uses ComfyUI.  Swarm has a built-in model downloader that will maintain metadata from civitai.  \n\n[https://github.com/mcmonkeyprojects/SwarmUI](https://github.com/mcmonkeyprojects/SwarmUI)', 'If you prefer simple then try simple interface like ""Fooocus"" for stable diffusion XL modes:\xa0[YouTube - Fooocus installation](https://youtu.be/3tAaL57rhoU?si=ArrTPhPQJkbfW7ZN)\n\nThis\xa0[playlist - YouTube](https://youtube.com/playlist?list=PLPFN04WspxqsslRSpiLmwGR8QTpDYNv7z&si=-QOo5_980RbiniKn)\xa0is for beginners which covers topic like prompt, models, lora, weights, in-paint, out-paint, image to image, canny, refiners, open pose, consistent character, training a LoRA.\n\nAfter this if you wish to go further then look at Comfy UI', ""If you don't have the GPU, you can use online services for free or at very low cost: [Free Flux/SDXL Online Generators](https://new.reddit.com/r/StableDiffusion/comments/18h7r2h/free_online_sdxl_generators/)\n\nHere are some ELi5 guides that I wrote a while back which may or may not help:\n\n[ELi5: Absolute beginner's guide to getting started in A.I. Image generation](https://new.reddit.com/r/StableDiffusion/comments/1b2mhjv/eli5_absolute_beginners_guide_to_getting_started/)\n\n[ELi5: What are SD models, and where to find them](https://new.reddit.com/r/StableDiffusion/comments/11s6485/eli5_what_are_sd_models_and_where_to_find_them/)"", 'What GPU do you have?\n\nDownload [Stability Matrix](https://github.com/LykosAI/StabilityMatrix). That program is an installer/manager for the various UIs. This is all free by the way.\n\nOn Matrix download Forge or reForge as they will be the easiest to start with.\n\nGo to [civit.ai](http://civit.ai) and make an account. Download checkpoints that you like.\n\nFor video look at Wan 2.1 and you can run this on ComfyUI, one of the UIs you can download through Stability Matrix. You might struggle to run this depending on how much VRAM you have.']",0,8,0.38,Question - Help,1751708555.0
1ls639w,StableDiffusion,What the hell is even happening.... I'm using the default flux kontext workflow from the documentations docs.comfy.org/tutorials/flux/flux-1-kontext-dev (don't ask about the naked guy i'm using i randomly found him on facebook),,"['oh I like the high heels', ""\\>while preserving the girl's facial features\n\nYeah, tell me about it"", ' we dont see a lot of naked guys on this sub OP\n\nyou found a naked guy on facebook in heels , lol\n\nuse latenet refrance , you can search it , you seem good at searching ( wink)', 'what are you cooking bro', ""Flux Kontext Dev support for multiple images is very janky. It's really only trained on single reference edits."", 'Try latent stitch, not image', 'Someone still got the bottom part of the cloth remover prompt active', 'Prompt guide ‚Ä¶ read it']","[""\\>while preserving the girl's facial features\n\nYeah, tell me about it"", 'oh I like the high heels', ' we dont see a lot of naked guys on this sub OP\n\nyou found a naked guy on facebook in heels , lol\n\nuse latenet refrance , you can search it , you seem good at searching ( wink)', 'what are you cooking bro', ""Flux Kontext Dev support for multiple images is very janky. It's really only trained on single reference edits.""]",0,14,0.35,Question - Help,1751708284.0
1ls5jqq,StableDiffusion,BeltOut: An open source pitch-perfect (SINGING!@#$) voice-to-voice timbre transfer model based on ChatterboxVC,"Hello! My name is Shiko Kudo, I'm currently an undergraduate at National Taiwan University. I've been around the sub for a long while, but... today is a bit special. I've been working all this morning and then afternoon with bated breath, finalizing everything with a project I've been doing so that I can finally get it into a place ready for making public. It's been a couple of days of this, and so I've decided to push through and get it out today on a beautiful weekend. AHH, can't wait anymore, here it is!!:

They say timbre is the only thing you can't change about your voice... well, not anymore.

**BeltOut** ([HF](https://huggingface.co/Bill13579/beltout), [GH](https://github.com/bill13579/beltout)) is **the world's first *pitch-perfect*, zero-shot, voice-to-voice timbre transfer model with *a generalized understanding of timbre and how it affects delivery of performances***. It is based on ChatterboxVC. As far as I know it is the first of its kind, being able to deliver eye-watering results for timbres it has never *ever* seen before (all included examples are of this sort) on many singing and other extreme vocal recordings.

It is explicitly different from existing voice-to-voice Voice Cloning models, in the way that it is not just entirely unconcerned with modifying anything other than timbre, but is even more importantly *entirely unconcerned with the specific timbre to map into*. The goal of the model is to learn *how* differences in vocal cords and head shape and all of those factors that contribute to the immutable timbre of a voice affects delivery of vocal intent in general, so that it can guess how the same performance will sound out of such a different base physical timbre.

This model represents timbre as just a list of 192 numbers, the **x-vector**. Taking this in along with your audio recording, the model creates a new recording, guessing how the same vocal sounds and intended effect would have sounded coming out of a different vocal cord.

In essence, instead of the usual `Performance -> Timbre Stripper -> Timbre ""Painter"" for a Specific Cloned Voice`, the model is a timbre shifter. It does `Performance -> Universal Timbre Shifter -> Performance with Desired Timbre`.

This allows for unprecedented control in singing, because as they say, timbre is the only thing you truly cannot hope to change without literally changing how your head is shaped; everything else can be controlled by you with practice, and this model gives you the freedom to do so while also giving you a way to change that last, immutable part.

# Some Points

* Small, running comfortably on my 6gb laptop 3060
* *Extremely* expressive emotional preservation, translating feel across timbres
* Preserves singing details like precise fine-grained vibrato, shouting notes, intonation with ease
* Adapts the original audio signal's timbre-reliant performance details, such as the ability to hit higher notes, very well to otherwise difficult timbres where such things are harder
* Incredibly powerful, doing all of this with just a single x-vector and the source audio file. No need for any reference audio files; in fact you can just generate a random 192 dimensional vector and it will generate a result that sounds like a completely new timbre
* Architecturally, only 335 out of all training samples in the 84,924 audio files large dataset was actually ""singing with words"", with an additional 3500 or so being scale runs from the VocalSet dataset. Singing with words is emergent and entirely learned by the model itself, learning singing despite mostly seeing SER data
* Make sure to read the [technical report](https://github.com/Bill13579/beltout/blob/main/TECHNICAL_REPORT.md)!! Trust me, it's a fun ride with twists and turns, ups and downs, and so much more.

Join the Discord https://discord.gg/MJzxacYQ!!!!! It's less about anything and more about I wanna hear what amazing things you do with it.

# Examples and Tips

The x-vectors, and the source audio recordings are both available on the repositories under the `examples` folder for reproduction.

`sd-01*.wav` on the repo, [https://youtu.be/5EwvLR8XOts](https://youtu.be/5EwvLR8XOts) (output) / [https://youtu.be/wNTfxwtg3pU](https://youtu.be/wNTfxwtg3pU) (input, yours truly)

`sd-02*.wav` on the repo, [https://youtu.be/KodmJ2HkWeg](https://youtu.be/KodmJ2HkWeg) (output) / [https://youtu.be/H9xkWPKtVN0](https://youtu.be/H9xkWPKtVN0) (input)

**[NEW]** [https://youtu.be/LccqjwzLh0Y](https://youtu.be/LccqjwzLh0Y) (output) / [https://youtu.be/k8PwwSJtayE](https://youtu.be/k8PwwSJtayE) (input)

**[NEW]** [https://youtu.be/E4r2vdrCXME](https://youtu.be/E4r2vdrCXME) (output) / [https://youtu.be/9mmmFv7H8AU](https://youtu.be/9mmmFv7H8AU) (input) (Note that although the input *sounds* like it was recorded willy-nilly, this input is actually after **more than a dozen takes**. The input is not random, if you listen closely you'll realize that if you do not look at the timbre, the rhythm, the pitch contour, and the intonations are all carefully controlled. The laid back nature of the source recording is intentional as well. Thus, only because *everything other than timbre is managed carefully*, when the model applies the timbre on top, it can sound realistic.)

Note that a very important thing to know about this model is that it is a *vocal timbre* transfer model. The details on how this is the case is inside the technical reports, but the result is that, unlike voice-to-voice models that try to help you out by fixing performance details that might be hard to do in the target timbre, and thus simultaneously either destroy certain parts of the original performance or make it ""better"", so to say, but removing control from you, this model will not do any of the heavy-lifting of making the performance match that timbre for you!!

You'll need to do that.

Thus, when recording with the purpose of converting with the model later, you'll need to be mindful and perform accordingly. For example, listen to this clip of a recording I did of Falco Lombardi from `0:00` to `0:30`: [https://youtu.be/o5pu7fjr9Rs](https://youtu.be/o5pu7fjr9Rs)

Pause at `0:30`. This performance would be adequate for many characters, but for this specific timbre, the result is unsatisfying. Listen from `0:30` to `1:00` to hear the result.

To fix this, the performance has to change accordingly. Listen from `1:00` to `1:30` for the new performance, also from yours truly ('s completely dead throat after around 50 takes).

Then, listen to the result from `1:30` to `2:00`. It is a marked improvement.

Sometimes however, with certain timbres like Falco here, the model still doesn't get it exactly right. I've decided to include such an example instead of sweeping it under the rug. In this case, I've found that a trick can be utilized to help the model sort of ""exaggerate"" its application of the x-vector in order to have it more confidently apply the new timbre and its learned nuances. It is very simple: we simply make the magnitude of the x-vector bigger. In this case by 2 times. You can imagine that doubling it will cause the network to essentially double whatever processing it used to do, thereby making deeper changes. There is a small drop in fidelity, but the increase in the final performance is well worth it. Listen from `2:00` to `2:30`.

You can do this trick in the Gradio interface.

Another tip is that in the Gradio interface, you can calculate a statistical average of the x-vectors of massive sample audio files; make sure to utilize it, and play around with the Chunk Size as well. I've found that the larger the chunk you can fit into VRAM, the better the resulting vectors, so a chunk size of 40s sounds better than 10s for me; however, this is subjective and your mileage may vary. Trust your ears.

# Supported Lanugage

The model was trained on a variety of languages, and not just speech. Shouts, belting, rasping, head voice, ...

As a baseline, I have tested Japanese, and it worked pretty well.

In general, the aim with this model was to get it to learn how different sounds created by human voices would've sounded produced out of a different physical vocal cord. This was done using various techniques while training, detailed in the technical sections. Thus, the supported types of vocalizations is vastly higher than TTS models or even other voice-to-voice models.

However, since the model's job is *only* to make sure your voice has a new timbre, the result will only sound natural if you give a performance matching (or compatible in some way) with that timbre. For example, asking the model to apply a low, deep timbre to a soprano opera voice recording will probably result in something bad.

Try it out, let me know how it handles what you throw at it!

# Socials

There's a [Discord](https://discord.gg/MJzxacYQ) where people gather; hop on, share your singing or voice acting or machine learning or anything! It might not be exactly what you expect, although I have a feeling you'll like it. ;)

My personal socials: [Github](https://github.com/Bill13579), [Huggingface](https://huggingface.co/Bill13579), [LinkedIn](https://www.linkedin.com/in/shiko-kudo-a44b86339/), [BlueSky](https://bsky.app/profile/kudoshiko.bsky.social), [X/Twitter](https://x.com/kudoshiko),

# Closing

This ain't the closing, you kidding!?? I'm so incredibly excited to finally get this out I'm going to be around for days weeks months hearing people experience the joy of getting to suddenly play around with a infinite amount of new timbres from the one they had up, and hearing their performances. I know I felt that way...

I'm sure that a new model will come soon to displace all this, but, speaking of which...

# Call to train

If you read through the technical report, you might be surprised to learn among other things just how incredibly quickly this model was trained.

It wasn't without difficulties; each problem solved in that report was days spent gruelling over a solution. However, I was surprised myself even that in the end, with the right considerations, optimizations, and head-strong persistence, many many problems ended up with extremely elegant solutions that would have frankly never come up without the restrictions.

And this just proves more that people doing training locally isn't just feasible, isn't just interesting and fun (although that's what I'd argue is the most important part to never lose sight of), but incredibly important.

So please, train a model, share it with all of us. Share it on as many places as you possibly can so that it will be there always. This is how local AI goes round, right? I'll be waiting, always, and hungry for more.

\- Shiko","[""That's interesting, I wonder if it can do voiceovers for films?"", ""I don't understand what this is"", 'Is it possible to improve the sound quality in this way?\n\nFor example, I have several acapelas from Udio, I like the tone of the intonation, but the blurred transitions and the compressed spectrum are completely depressing. Is it possible to improve with this tool, bring the sound closer to the real sound of the voice? Even if the timbre changes a little?', 'Oh we gotta get this in front of T-pain! \n\nAwesome work.', 'Great work and share', 'How can I help (contribute) your project? Train or something?', ""Hi! It's very cool! I have it installed and it process the audio, and the output file is being created,  however in gradio I have an error and there is no output to listen.   \n  \nAlso, I have a question about training data quality. I've tried with random voice since I don't have some nice recordings of voices right now. It changes the voice, but the output is rather very low quality despite of input which is clear and loud. Was your model trained on hi quality recordings or various types, like crappy mic, webcam, voip recordings?   \n  \nWhat quality or parameters input of voice recording should have?"", ""Sounds interesting. But i'm not tech savvy so I'll wait for someone to make a installer with an interface."", 'The app GUI and guide are really confusing. It needs some video instructions how to use the app.\n\nEDIT: Also got an error: s3tokenizer\\\\model\\_v2.py"", line 70, in apply\\_rotary\\_emb\n\nreturn xq \\* cos + xq\\_r \\* sin, xk \\* cos + xk\\_r \\* sin\n\n\\~\\~\\~\\^\\~\\~\\~\\~\n\nRuntimeError: The size of tensor a (4933) must match the size of tensor b (2048) at non-singleton dimension 1', ""You've exceeded the download quota for your checkpoints, people can't download them. Do you have them elsewhere, like on HF?""]","[""I don't understand what this is"", ""That's interesting, I wonder if it can do voiceovers for films?"", ""Sounds interesting. But i'm not tech savvy so I'll wait for someone to make a installer with an interface."", 'Is it possible to improve the sound quality in this way?\n\nFor example, I have several acapelas from Udio, I like the tone of the intonation, but the blurred transitions and the compressed spectrum are completely depressing. Is it possible to improve with this tool, bring the sound closer to the real sound of the voice? Even if the timbre changes a little?', 'Oh we gotta get this in front of T-pain! \n\nAwesome work.']",215,68,0.98,Resource - Update,1751706007.0
1ls5dod,StableDiffusion,Flux Kontext Ultimate Workflow include Fine Tune & Upscaling at 8 Steps Using 6 GB of Vram,"Hey folks,

 Ultimate image editing workflow in Flux Kontext, is finally ready for testing and feedback! Everything is laid out to be fast, flexible, and intuitive for both artists and power users.

# üîß How It Works:

* Select your components: Choose your preferred models GGUF or DEV version.
* Add single or multiple images: Drop in as many images as you want to edit.
* Enter your prompt: The final and most crucial step ‚Äî your prompt drives how the edits are applied across all images i added my used prompt on the workflow.

# ‚ö° What's New in the Optimized Version:

* üöÄ Faster generation speeds (significantly optimized backend using LORA and TEACACHE)
* ‚öôÔ∏è Better results using fine tuning step with flux model
* üîÅ Higher resolution with SDXL Lightning Upscaling
* ‚ö° Better generation time 4 min to get 2K results VS 5 min to get kontext results at low res

**WORKFLOW LINK (FREEEE)**

[https://www.patreon.com/posts/flux-kontext-at-133429402?utm\_medium=clipboard\_copy&utm\_source=copyLink&utm\_campaign=postshare\_creator&utm\_content=join\_link](https://www.patreon.com/posts/flux-kontext-at-133429402?utm_medium=clipboard_copy&utm_source=copyLink&utm_campaign=postshare_creator&utm_content=join_link)","['`invalid prompt: {\'type\': \'invalid_prompt\', \'message\': \'Cannot execute because a node is missing the class_type property.\', \'details\': ""Node ID \'#315\'"", \'extra_info\': {}}`\n\nNothing clear here. Only [this](https://i.imgur.com/0mNAGGZ.png) which doesn\'t show up in Manager', 'No nunchaku is a no go for me. Nice idea though.']","['No nunchaku is a no go for me. Nice idea though.', '`invalid prompt: {\'type\': \'invalid_prompt\', \'message\': \'Cannot execute because a node is missing the class_type property.\', \'details\': ""Node ID \'#315\'"", \'extra_info\': {}}`\n\nNothing clear here. Only [this](https://i.imgur.com/0mNAGGZ.png) which doesn\'t show up in Manager']",51,13,0.79,Tutorial - Guide,1751705306.0
1ls544m,StableDiffusion,What's the highest quality i2v possible on 5090?,"So I've been in to AI from the start, I have printed canvases going back to SD 1.5 in my living room, I love everything about it and an reasonably comfortable with comfy.

Regarding ai video experience, I've pretty much tested each be model and it came out for fun and to see where we are in terms of real usability for real world cases.

Well a real world cases has come up to get a friend out of a hole. I work as a photographer and was shooting a wedding with a videographer, he's been at it 20 years and is very experienced but has somehow lost the shot of the bride and groom walking back down the aisle, this could be disastrous for him as one tricky bride can ruin a reputation and I know he's distraught.

At this point she is unaware, and I happen to have a good few high quality/resolution stills which I attempted to animate. They look pretty amazing from a distance of you squint, but these are real people and their faces just aren't detailed enough and loose consistency. I spent a few hours at it last night trying flf and single image i2v.

Speed is not a concern as it usually has been in my messing around with my own video generations, so I'm not sure really what the absolute optimumum quality can be pulled out to help with this situation from my new 5090, assuming that most speed ups degrade quality.

Is there a way to use a reference image to swap faces throughout such as reactor after the initial generation considering there are two faces? I only tried reactor once a good while ago. I'm considering splitting the generated video in half from 1024*720 to 512*720 as they couple are centrally framed so could potentially have only one of them in each video and then rejoin but that feels convoluted.

Maybe there is a great technique/workflow/solution I'm missing. I've increased steps which helped a bit, gone higher than 720p which also helped but hasn't got it to the stage that I think it would pass for a few seconds in a 5 minute video, I mean it might at first watch but after a rewatch they'll probably notice.

I'm using wan 720p i2v, I know 2.2 is coming so maybe that might be a further improvement, but he needs to get there fun delivered in the coming week. Just to note he doesn't need much of them walking down the aisle, it's just a few seconds as long as it's there, my tests have been on 81 frames, interpolated and I'll upscale properly once I've got a decent result.

Any other suggestions or workflows for this would be greatly appreciated!","['My advice is to use paid services with first - last frame options. Existing open source aren‚Äôt there yet.\xa0\n\nAn even better option would be animate one of your photos of her walking down isle without last frame and forget about last frame. You only need 2 seconds before switching to another shot.\xa0\n\nKling 2.1, VEO3 (with first frame options), Midjourney video (With upscaler later), and new Seedance Pro can all do the job pretty well.\xa0', 'This will get him, and you, into legal problems.']","['My advice is to use paid services with first - last frame options. Existing open source aren‚Äôt there yet.\xa0\n\nAn even better option would be animate one of your photos of her walking down isle without last frame and forget about last frame. You only need 2 seconds before switching to another shot.\xa0\n\nKling 2.1, VEO3 (with first frame options), Midjourney video (With upscaler later), and new Seedance Pro can all do the job pretty well.\xa0', 'This will get him, and you, into legal problems.']",0,15,0.42,Question - Help,1751704166.0
1ls3r31,StableDiffusion,Create Concept (v1.0_Flux1D) [WORK IN PROGRESS],"https://preview.redd.it/7qhx0kse60bf1.png?width=864&format=png&auto=webp&s=ebd011161d210204025196ebd1ddb237fb40a0c5

https://preview.redd.it/ns8g51ii60bf1.png?width=896&format=png&auto=webp&s=313dfbf00c4c1bbc4f86699b10fca17317e6d845

https://preview.redd.it/gcz9g0ii60bf1.png?width=896&format=png&auto=webp&s=ff613f8948221f8803feb45c55d2a442942987a8

https://preview.redd.it/bx7kzyhi60bf1.png?width=896&format=png&auto=webp&s=d08e9fc40d57ff62ebf8087cc2d40360372fb565

https://preview.redd.it/lrkqyzji60bf1.png?width=896&format=png&auto=webp&s=ad6e8330ba9d89873751e501588c7bc2439711fa

https://preview.redd.it/c64b80ii60bf1.png?width=896&format=png&auto=webp&s=b76872df66a517075cebfc1e8351549ab6a794b9

https://preview.redd.it/25hdh2ii60bf1.png?width=896&format=png&auto=webp&s=9702e54c21ff1a682592a3e1a43d4225a5dbb6c0

https://preview.redd.it/el8cezhi60bf1.png?width=896&format=png&auto=webp&s=3404c207a4ca82cda69fd30bfab461f24b304dee

https://preview.redd.it/jihbzzhi60bf1.png?width=896&format=png&auto=webp&s=d2e7f577c7e761904072462dedfea2302fb42a81

https://preview.redd.it/rux6ayhi60bf1.png?width=864&format=png&auto=webp&s=1a141267be77594eedfeb78bd2e68d7d273ef8a5

https://preview.redd.it/4c7ssuhi60bf1.png?width=864&format=png&auto=webp&s=116be8f45058e5b66467ce3a7ac2cf337d11c835

[https://civitai.com/models/1324671](https://civitai.com/models/1324671)
",[],[],2,0,0.67,Resource - Update,1751698550.0
1ls3r1g,StableDiffusion,Need help,"I have installed wan 2.1 but there is no option for text to image to image to video, it is missing. what to do. please guide me.

https://preview.redd.it/uzwh1a0e70bf1.png?width=680&format=png&auto=webp&s=0f9107875899c2c30a93b660a49769fbf6c9a35d




thanks","['You are inside Pinokio interface, start the program what uses Wan2.1 models: [https://www.youtube.com/watch?v=Ls8QOgkSm4w&t=4m20s](https://www.youtube.com/watch?v=Ls8QOgkSm4w&t=4m20s)']","['You are inside Pinokio interface, start the program what uses Wan2.1 models: [https://www.youtube.com/watch?v=Ls8QOgkSm4w&t=4m20s](https://www.youtube.com/watch?v=Ls8QOgkSm4w&t=4m20s)']",0,1,0.4,Question - Help,1751698545.0
1ls26js,StableDiffusion,How to find the original links of downloaded LORAs?,"I downloaded dozens of LORAs and I have no idea what most of them do, I forgot to save their CivitAI link and now I can't see a preview of what their generations look like.

Is it possible to find the civitai link I originally downloaded them from?","[""I made a couple of related scripts some time ago:\n\nhttps://github.com/RupertAvery/civitai-scripts\n\n`download_your_model_metadata.py` will download the LORAs metadata as a .JSON file based on it's checksum.\n\nIt basically:\n\n* computes the LORAs checksum\n* uses the Civitai API to search for the model based on the checksum\n* downloads the metadata\n\nThe metadata should contain the url of the model and the trigger words and the author notes.\n\nFrom the README\n\n## Prerequisites\n\n* Python 3.10+\n* requests package\n\n```\npip install requests\n```\n\n## Download Metadata for your models\n\n`download_your_model_metadata.py` downloads metadata based on your models. \n\nJust point the script at the folder or network folder where your models are.\n\nThis computes the SHA-256 of your model and uses it to lookup the model in Civitai. Useful when you don't know the actual name of the model.\n\nThe data downloaded is the raw api response in json format. It will be downloaded next to your model with the same filename and .json extension.\n\nWorks with LORAs and Checkpoints. .pt should work too if you change the extension in the script.\n\nIf you are already using Stability Matrix or something similar to download metadata from Civitai, then this isn't really useful.\n\nThe purpose of this is to quickly grab the metadata for all your LORAs, which includes any trigger words.\n\nUsage:\n\n```\npython download_your_model_metadata.py <path to safetensors>\n```"", ""Which of the following is the easiest way to find out what trigger words a given, already downloaded LORA uses? The best would be a very short workflow, or an easy-to-use table, because I don't want to add nodes one by one to my countless - for me complicated - existing workflows. What do you recommend?"", 'If you downloaded from civitai while logged in you can click on your profile > download history.', 'Seems like all you want is the trigger words. Just make it good practice to grab a screenshot of the CivitAi page, along with saving an encapsulated .MHTML of the same page. Give both the same name as the LoRA. Store both alongside the downloaded LoRA file.', 'I use this:\n\n[https://github.com/Xypher7/lora-metadata-viewer](https://github.com/Xypher7/lora-metadata-viewer)\n\n* it can sometimes correctly guess the trigger words ...\n* can list what words were used in the training images, thus make prompt suggestions ...\n* can sometimes find again where on Civitai the LoRA was originally posted ...\n\nOnline demo, fully functional:\n\n[https://xypher7.github.io/lora-metadata-viewer](https://xypher7.github.io/lora-metadata-viewer)', 'You can import them into Stability Matrix, it will find the Civitai link via file hash and import the metadata into local storage.', 'They are extensions to save that kind of info for auto1111 or forge.\n\nI use this one : [https://github.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper](https://github.com/butaixianran/Stable-Diffusion-Webui-Civitai-Helper)', 'I open sourced it few hours ago.... it can fetch metadata from civitai, download\n\n[https://github.com/rajeevbarde/civit-lora-download](https://github.com/rajeevbarde/civit-lora-download)\n\nhttps://preview.redd.it/nl63gckwtzaf1.png?width=2460&format=png&auto=webp&s=e4954e810bd36b6707f0505bc8ecaaa4f9bbfff3', 'Surely it is pretty obvious from the file name most of the time?', ""You only need this node\n\n[https://github.com/willmiao/ComfyUI-Lora-Manager](https://github.com/willmiao/ComfyUI-Lora-Manager)\n\nyou will have a new button in the top that will launch a lora manager interface + explorer\n\nyou will have to add the lora loader ( lora manager ) node and connect it to your workflow  and you can send any  Lora to that node from that interface. You don't need to copy trigger words anymore,\n\nit will sync with civitai and show the example images\n\nit is amazing node""]","[""I made a couple of related scripts some time ago:\n\nhttps://github.com/RupertAvery/civitai-scripts\n\n`download_your_model_metadata.py` will download the LORAs metadata as a .JSON file based on it's checksum.\n\nIt basically:\n\n* computes the LORAs checksum\n* uses the Civitai API to search for the model based on the checksum\n* downloads the metadata\n\nThe metadata should contain the url of the model and the trigger words and the author notes.\n\nFrom the README\n\n## Prerequisites\n\n* Python 3.10+\n* requests package\n\n```\npip install requests\n```\n\n## Download Metadata for your models\n\n`download_your_model_metadata.py` downloads metadata based on your models. \n\nJust point the script at the folder or network folder where your models are.\n\nThis computes the SHA-256 of your model and uses it to lookup the model in Civitai. Useful when you don't know the actual name of the model.\n\nThe data downloaded is the raw api response in json format. It will be downloaded next to your model with the same filename and .json extension.\n\nWorks with LORAs and Checkpoints. .pt should work too if you change the extension in the script.\n\nIf you are already using Stability Matrix or something similar to download metadata from Civitai, then this isn't really useful.\n\nThe purpose of this is to quickly grab the metadata for all your LORAs, which includes any trigger words.\n\nUsage:\n\n```\npython download_your_model_metadata.py <path to safetensors>\n```"", ""Which of the following is the easiest way to find out what trigger words a given, already downloaded LORA uses? The best would be a very short workflow, or an easy-to-use table, because I don't want to add nodes one by one to my countless - for me complicated - existing workflows. What do you recommend?"", 'If you downloaded from civitai while logged in you can click on your profile > download history.', 'Seems like all you want is the trigger words. Just make it good practice to grab a screenshot of the CivitAi page, along with saving an encapsulated .MHTML of the same page. Give both the same name as the LoRA. Store both alongside the downloaded LoRA file.', 'I use this:\n\n[https://github.com/Xypher7/lora-metadata-viewer](https://github.com/Xypher7/lora-metadata-viewer)\n\n* it can sometimes correctly guess the trigger words ...\n* can list what words were used in the training images, thus make prompt suggestions ...\n* can sometimes find again where on Civitai the LoRA was originally posted ...\n\nOnline demo, fully functional:\n\n[https://xypher7.github.io/lora-metadata-viewer](https://xypher7.github.io/lora-metadata-viewer)']",4,12,0.67,Question - Help,1751692418.0
1lrzgf5,StableDiffusion,Does how powerful my GPU affect the quality of the final image output or does it only affect how fast I get my output?,,"[""The only way in which it affects quality is if it limits the model you can run. So, for example, with Flux Dev, the full fp 16 model is 20+ Gigabytes. You may not be able to run that on your GPU (basically it only runs comfortably on a GPU with 24 or 32 Gigabytes of VRAM, eg a 3090, 4090, or 50900\n\nBut on cards with less memory many folks will run the fp 8 model, which is usually 11 Gigabytes or so.\n\nThere's some quality loss there vs FP 16. Not a lot, you might not even notice, but some and more noticeable in some scenarios than other. It show up more in image 2 image, for example.\n\nSimilarly, if you look at the text encoder, you might choose the T5XXL FP8 vs the T5XXL FP 16 to save on VRAM, personally I find this effect is greater than with the checkpoint.\n\nSo no - a better GPU doesn't make better images, IFF you can run the same models. But sometimes the difference between a 4090 -- where you can easily run the FP 16 model and encoder in the 24 GB of VRAM, vs a 4060 where the 8 GB of VRAM mean you're going to have to choose less capable models . . . that will mean lower quality."", 'It should only affect the speed as long as everything else software-wise is the same.', 'We have a few variations based on internal architectures but your GPU will one affect speed.', ""Not in theory, but definitely in practice.  \n\n/u/amp1212 robustly answered the most common way: VRAM optimizations sacrificing quality.  But you can also lose out by simply not having access to the same hardware features.  A good but trivial example is the way some tools once used gpu rng when available even though it didn't have any particular advantage over using standardized algorithms on the CPU.  You'd get differences because of your hardware even though it was not a capability issue and they would usually look worse because most images put on display are selected for quality."", 'You can do it in your head with pen and paper and quality will be the same']","[""The only way in which it affects quality is if it limits the model you can run. So, for example, with Flux Dev, the full fp 16 model is 20+ Gigabytes. You may not be able to run that on your GPU (basically it only runs comfortably on a GPU with 24 or 32 Gigabytes of VRAM, eg a 3090, 4090, or 50900\n\nBut on cards with less memory many folks will run the fp 8 model, which is usually 11 Gigabytes or so.\n\nThere's some quality loss there vs FP 16. Not a lot, you might not even notice, but some and more noticeable in some scenarios than other. It show up more in image 2 image, for example.\n\nSimilarly, if you look at the text encoder, you might choose the T5XXL FP8 vs the T5XXL FP 16 to save on VRAM, personally I find this effect is greater than with the checkpoint.\n\nSo no - a better GPU doesn't make better images, IFF you can run the same models. But sometimes the difference between a 4090 -- where you can easily run the FP 16 model and encoder in the 24 GB of VRAM, vs a 4060 where the 8 GB of VRAM mean you're going to have to choose less capable models . . . that will mean lower quality."", 'It should only affect the speed as long as everything else software-wise is the same.', 'We have a few variations based on internal architectures but your GPU will one affect speed.', ""Not in theory, but definitely in practice.  \n\n/u/amp1212 robustly answered the most common way: VRAM optimizations sacrificing quality.  But you can also lose out by simply not having access to the same hardware features.  A good but trivial example is the way some tools once used gpu rng when available even though it didn't have any particular advantage over using standardized algorithms on the CPU.  You'd get differences because of your hardware even though it was not a capability issue and they would usually look worse because most images put on display are selected for quality."", 'You can do it in your head with pen and paper and quality will be the same']",0,7,0.27,Question - Help,1751682374.0
1lryd72,StableDiffusion,Morbid Art Styles,What's your thoughts and opinions on this? ,"[""It's ok."", 'Love these!!', 'Glad it‚Äôs not all boobs', 'Love it, I definitely prefer the ones that less fully red, the darker ones with just a hint of color are dope', 'Feels like Mignola', 'kind of reminds me of the cursed lora i found, https://imgur.com/a/DNdskl1 (i call it cursed because when i used the trigger word,it spat out the images in the gallery, vs the girl it was supposed to generate.)', 'Just wanted to say I love this style. Carry on.', 'You call that morbid???', '12 is üî•', 'Reminds me of the old Samurai Jack on cartoon network']","[""It's ok."", 'Glad it‚Äôs not all boobs', 'Love these!!', 'Love it, I definitely prefer the ones that less fully red, the darker ones with just a hint of color are dope', 'You call that morbid???']",130,13,0.83,Discussion,1751678480.0
1lry8jn,StableDiffusion,CREEPY JOINTS,"JOINTS.

Using a workflow that was orginally made to loop videos, by simply adding a second different video input, you can join two similar clips seemlessly. The method creates and blends in a new 5 second join between clips that follows the motion and context almost perfectly. Here are 4 joined clips.


Wan 2.1, MMAudio, Stable Audio, Kontext



The original worflow for looping is here: [https://www.reddit.com/r/StableDiffusion/comments/1ktljys/loop\_anything\_with\_wan21\_vace/](https://www.reddit.com/r/StableDiffusion/comments/1ktljys/loop_anything_with_wan21_vace/)","['Interesting. How did you manage to reduce the color issues between clips? Post? If not, may you elaborate a little bit more your steps, please? Thanks, and nice video!', '![gif](giphy|7krK2aL5IEUTK)']","['Interesting. How did you manage to reduce the color issues between clips? Post? If not, may you elaborate a little bit more your steps, please? Thanks, and nice video!', '![gif](giphy|7krK2aL5IEUTK)']",13,7,0.76,Animation - Video,1751678032.0
1lrxzzv,StableDiffusion,YuzuUI: A New Frontend for SD WebUI,"I was frustrated with the UX of SD WebUI, so I built a separate frontend UI app: [https://github.com/crstp/sd-yuzu-ui](https://github.com/crstp/sd-yuzu-ui)

https://preview.redd.it/nzje645qcyaf1.jpg?width=1640&format=pjpg&auto=webp&s=13124ed74bbd16305b4d32485a3d570430785fa2



Features:

* Saves tab states and restores generated images after restarting the app
* Applies batch setting changes across multiple tabs (e.g., replace prompts across tabs)
* Significantly reduces memory usage, even with many open tabs
* Offers more advanced autocompletion

It's focused on txt2img. The UI looks pretty much like the original WebUI, but the extra features make it way easier to work with lots of prompts.

If you often generate lots of txt2img images across multiple tabs in WebUI, this might be useful for you.","['No inpaint?', 'What is on the backend?']","['No inpaint?', 'What is on the backend?']",37,7,0.85,News,1751677176.0
1lrxzc8,StableDiffusion,Lora x controlnet,"Does anyone know the best way to use a lora in a controlnet, I mean copying the photo and just changing the face. Sorry for the translation errors, I'm üáßüá∑",[],[],0,0,0.25,Question - Help,1751677110.0
1lrww1r,StableDiffusion,Is there anything out there to make the skin look more realistic?,,"['I recommend posting what are using right now so people can recommend you more of a ""tweaks approach"" rather than total redo. \n\nIt\'s Flux, right? What CFG? Sampler? Scheduler? Any LORAs? Prompt? Seed?', 'https://imgur.com/a/RasoAep\n\nThis good enough skin for you?\n\nCreated using my FLUX LoRa + my recommended workflow (linked in the model description), using a ChatGPT prompt of your image.\n\nLink: https://civitai.com/models/970862/amateur-snapshot-photo-style-lora-flux', 'SDXL or Chroma with low denoise', 'A trick that I use is to plug KSampler Advanced with the exact same seed and run it on the output from a regular KSampler, repeating the last few steps. Works on pretty much everything and amplifies details quite a lot', 'Some asians have skin like this', 'https://preview.redd.it/lcizbrwo8yaf1.png?width=2048&format=png&auto=webp&s=a8127f431ee951e56430d753537b9a12e38a866d\n\nany difference ?', 'I don\'t use Flux but, for trying to reproduce the feeling of real pictures, I prefer low resolution images to HD. Usually, something not higher than 1440p. Higher than that, skin starts looking ""fake"" and characters less ""realistic"".\n\nFor upscaling, I only use two things: HiResFix for txt2img, and ADetailer for img2img. (ADetailer can be use for upscaling if you set it right. Not sure if this is common knowledge or not, it\'s something I discovered by accident.)\n\nI usually prefer images that are taken with low environmental lighting, or with strong flash light over a dark background.', 'Use ‚Äúmultiply sigmas‚Äù node set to 0.99 rather than 1. Or use detail daemon. Or both.', 'Porcelain like texture....smooth, prompt matters', 'I have an upscale that solves this']","['I recommend posting what are using right now so people can recommend you more of a ""tweaks approach"" rather than total redo. \n\nIt\'s Flux, right? What CFG? Sampler? Scheduler? Any LORAs? Prompt? Seed?', 'https://imgur.com/a/RasoAep\n\nThis good enough skin for you?\n\nCreated using my FLUX LoRa + my recommended workflow (linked in the model description), using a ChatGPT prompt of your image.\n\nLink: https://civitai.com/models/970862/amateur-snapshot-photo-style-lora-flux', 'SDXL or Chroma with low denoise', 'Some asians have skin like this', 'https://preview.redd.it/lcizbrwo8yaf1.png?width=2048&format=png&auto=webp&s=a8127f431ee951e56430d753537b9a12e38a866d\n\nany difference ?']",82,46,0.82,Question - Help,1751673498.0
1lrw55p,StableDiffusion,Flux Kontext Image to Tattoo,"Flux Kontext keeps on amazing me. Using the below Tattoo Flux dev Lora you can transfer any image to a tattoo style on human skin as shown in the examples.

https://civitai.com/models/867846/tattoo-flux-lora",[],[],15,0,0.8,Tutorial - Guide,1751671166.0
1lrv41n,StableDiffusion,Flux kontext dev tested prompts,"For some time now there is a Flux kontext dev model among the users, can you share here the best prompts that do exactly the image editing (inpaint) they should ? There are many pages and videos with basic examples, but nowhere are all the capabilities of this model described. Which working prompts have surprised you with their ability ? Write their wording and share with others the possibilities of this great editing model. Thank you. I'm happy to learn new possibilities. I apologize for my English :)","['https://docs.bfl.ai/guides/prompting_guide_kontext_i2i\n\nThis is the official prompting guide', 'remove watermark\n\nmaintain scale, proportions\n\nupscale this image, make it crisp, add details. Maintain style, background an lighting']","['https://docs.bfl.ai/guides/prompting_guide_kontext_i2i\n\nThis is the official prompting guide', 'remove watermark\n\nmaintain scale, proportions\n\nupscale this image, make it crisp, add details. Maintain style, background an lighting']",1,4,0.57,Discussion,1751668070.0
1lrudoy,StableDiffusion,Remastered FF.,,"['looks like a bad photoshop for a comedy podcast thumbnail. But I like anything retro.', 'So lazy.they look nothing like in game\nDo more effort on the face.its like lazy img2img']","['looks like a bad photoshop for a comedy podcast thumbnail. But I like anything retro.', 'So lazy.they look nothing like in game\nDo more effort on the face.its like lazy img2img']",0,2,0.33,Comparison,1751665956.0
1lru62m,StableDiffusion,"""Forgotten Models"" Series: PixArt Sigma + SD 3.5 L Turbo as Refiner.",,"['This is a cool series, thanks.', 'Prompts: [https://civitai.com/posts/19150556](https://civitai.com/posts/19150556)', 'These look nice and are very interesting, but ultimately more of a tool for inspiration than actual targeted use.\n\nBecause should you want to recreate most of the images purposefully, you wouldnt be able to. At least not without the exact settings and prompt used.\n\nThese are like results from a slot machine. At some point its going to be right and spit out something pretty, but if you have a concrete goal and vision in mind its very hard to create that.\n\nImage one:\n\nThere are essentially just two tags with direct information on what should be created.\n\nI will maybe give it ""biomechanical alien"", though those lips and the chin are too human like to be alien. But ""eye contact"" is completely missing.\n\nSo you either confused the model with the flood of ""quality"" tags or it has shit prompt adherence that it didnt do the two things it was told to do.\n\nImage two:\n\nIt is a blocky spaceship, but it has neither round nor red thrusters engines. I would also debate the support spacecraft flying around it with red streaks but maybe its some absurdly large spaceship and the support crafts are just tiny and thus just streaks.\n\nImage three:\n\n""long claws and six arms"" No claws and neither does it have six appendages in total even. ""stripe texture of body"" no stripes at all. ""on the background is a wall full of bodies and corpses and carcass of dead aliens"" the walls are filled with pipes not corpses. Did the ""illuminating space marine"" get eaten?\n\n\\-------------\n\nThis might be a useful tool to create hundreds of inspiration images where you can get ideas from, but if the model doesnt actually show what you want it to show, its not that useful. No matter how pretty it is.', ""I see why they're forgotten."", 'ngl, those look amazing. Would it be possible to share a comfy workflow? I would appreciate it!', ""Those are the ai-est looking ai pics I've ever seen""]","['This is a cool series, thanks.', 'Prompts: [https://civitai.com/posts/19150556](https://civitai.com/posts/19150556)', 'These look nice and are very interesting, but ultimately more of a tool for inspiration than actual targeted use.\n\nBecause should you want to recreate most of the images purposefully, you wouldnt be able to. At least not without the exact settings and prompt used.\n\nThese are like results from a slot machine. At some point its going to be right and spit out something pretty, but if you have a concrete goal and vision in mind its very hard to create that.\n\nImage one:\n\nThere are essentially just two tags with direct information on what should be created.\n\nI will maybe give it ""biomechanical alien"", though those lips and the chin are too human like to be alien. But ""eye contact"" is completely missing.\n\nSo you either confused the model with the flood of ""quality"" tags or it has shit prompt adherence that it didnt do the two things it was told to do.\n\nImage two:\n\nIt is a blocky spaceship, but it has neither round nor red thrusters engines. I would also debate the support spacecraft flying around it with red streaks but maybe its some absurdly large spaceship and the support crafts are just tiny and thus just streaks.\n\nImage three:\n\n""long claws and six arms"" No claws and neither does it have six appendages in total even. ""stripe texture of body"" no stripes at all. ""on the background is a wall full of bodies and corpses and carcass of dead aliens"" the walls are filled with pipes not corpses. Did the ""illuminating space marine"" get eaten?\n\n\\-------------\n\nThis might be a useful tool to create hundreds of inspiration images where you can get ideas from, but if the model doesnt actually show what you want it to show, its not that useful. No matter how pretty it is.', ""I see why they're forgotten."", 'ngl, those look amazing. Would it be possible to share a comfy workflow? I would appreciate it!']",48,12,0.87,Workflow Included,1751665358.0
1lru0mu,StableDiffusion,Use Kontext on Apple Silicon (MFLUX updated),,[],[],5,0,0.78,Resource - Update,1751664921.0
1lrtnxk,StableDiffusion,VLM multi-turn captioning app,,"['Github: https://github.com/victorchall/vlm-caption\n\nTLDR app: use multi-turn prompts with a vision model to create captions, with some extra bells and whistles.  \n\nTLDR install: Install LM Studio to self host the VLM model of your choice, edit a caption.yaml file to setup various things, then double click the exe to run.\n\nLatest release download:\n\nhttps://github.com/victorchall/vlm-caption/actions/runs/16079650854/artifacts/3467782671', ""So how's this differ from\xa0https://github.com/jhc13/taggui\n\n\nThe VLMs produce better caption outputs?"", 'Does this support joycaption gguf?']","['Github: https://github.com/victorchall/vlm-caption\n\nTLDR app: use multi-turn prompts with a vision model to create captions, with some extra bells and whistles.  \n\nTLDR install: Install LM Studio to self host the VLM model of your choice, edit a caption.yaml file to setup various things, then double click the exe to run.\n\nLatest release download:\n\nhttps://github.com/victorchall/vlm-caption/actions/runs/16079650854/artifacts/3467782671', ""So how's this differ from\xa0https://github.com/jhc13/taggui\n\n\nThe VLMs produce better caption outputs?"", 'Does this support joycaption gguf?']",2,8,0.63,Resource - Update,1751663921.0
1lrt2si,StableDiffusion,Can flux Kontext fix Body horror?,"I haven't gotten around to make space for flux Kontext to try it, but maybe someone already did this: when using too many or incompatible Loras for flux, bodies start to get mixed with the environment in truly horrible ways. Sometimes the arm of one person seamlessly extends into the leg of another person and stuff like that. Can you use Kontext to untangle that mess to still safe the image by saying something like ""fix the body autonomy"" or similar?","['I doubt it.  If you cannot fix it via img2img with some high denoise with Flux-Dev, then Flux-Kontext probably won\'t do it either.\n\nFlux-Kontext is mainly an image editor, used to replace say the background, style or clothing.  I somehow doubt that any kind of ""fix the body anatomy"" training went into it.  I suppose somebody can make a LoRA for it, but base Kontext most likely won\'t have it.\n\nBut you can try, I could be wrong.  Try to modify the image by say changing to a different background (from indoor to outdoor, for example) and see what happens.\n\nBTW, it does not do NSFW üòÅ.\n\nIf you can post a SFW example here, I can try it for you, or you can try it online for free yourself at tensor. art', 'Generally not. Its not impossible, but it generally helps to start with a hand edit in a photoeditor to simplify your Thing. Basically, twisted nightmare fuel abominations pose challenges for genAI tools in figuring out ""what is this supposed to be"". Its much easier to fill in a missing limb than it is to sort out just which of six limbs are supposed to be the right ones.\n\nAs a practical matter, if you find you\'re generating abominations, and you\'d like something a little less Island of Dr Moreau, you\'d be better off regenerating with a better checkpoint, lora and prompt', 'Not really as of right now. HOWEVER! Kontext LoRA training allows for input-image to output-image training, meaning theoretically you can gather a dataset of your malformed generations that need fixing, use this a your Start images and manually fix your dataset with inpainting. Use the fixed images as end images, use a prompt like ‚Äûfix the subjects hands‚Äú and train. The lora should then be able to achieve this.\n\nEdit: I genuinely think a LoRA like this could be a really useful tool for the community, so if anyone wants to make it happen hmu']","['I doubt it.  If you cannot fix it via img2img with some high denoise with Flux-Dev, then Flux-Kontext probably won\'t do it either.\n\nFlux-Kontext is mainly an image editor, used to replace say the background, style or clothing.  I somehow doubt that any kind of ""fix the body anatomy"" training went into it.  I suppose somebody can make a LoRA for it, but base Kontext most likely won\'t have it.\n\nBut you can try, I could be wrong.  Try to modify the image by say changing to a different background (from indoor to outdoor, for example) and see what happens.\n\nBTW, it does not do NSFW üòÅ.\n\nIf you can post a SFW example here, I can try it for you, or you can try it online for free yourself at tensor. art', 'Generally not. Its not impossible, but it generally helps to start with a hand edit in a photoeditor to simplify your Thing. Basically, twisted nightmare fuel abominations pose challenges for genAI tools in figuring out ""what is this supposed to be"". Its much easier to fill in a missing limb than it is to sort out just which of six limbs are supposed to be the right ones.\n\nAs a practical matter, if you find you\'re generating abominations, and you\'d like something a little less Island of Dr Moreau, you\'d be better off regenerating with a better checkpoint, lora and prompt', 'Not really as of right now. HOWEVER! Kontext LoRA training allows for input-image to output-image training, meaning theoretically you can gather a dataset of your malformed generations that need fixing, use this a your Start images and manually fix your dataset with inpainting. Use the fixed images as end images, use a prompt like ‚Äûfix the subjects hands‚Äú and train. The lora should then be able to achieve this.\n\nEdit: I genuinely think a LoRA like this could be a really useful tool for the community, so if anyone wants to make it happen hmu']",2,6,0.62,Question - Help,1751662298.0
1lrrrdp,StableDiffusion,Ovis-U1-3B small yet capable all to all free model,"1 input
Prompt :Make the same place Abandent deserted ruined old destroyed , realistic photo.
2 result

3 input
Prompt:Use white marble pillars to hold pergulla
4 result","['Try it here\n\nhttps://huggingface.co/spaces/AIDC-AI/Ovis-U1-3B', 'Model download\n\n\nhttps://huggingface.co/AIDC-AI/Ovis-U1-3B\n\n\nThe project is released under Apache License 2.0\n\n\nI am not the owner of it', '>Building on the foundation of the Ovis series, Ovis-U1 is a 3-billion-parameter unified model that seamlessly integrates\xa0**multimodal understanding**,\xa0**text-to-image generation**, and\xa0**image editing**\xa0within a single powerful framework.\n\nhttps://preview.redd.it/uww4kg5g6zaf1.png?width=2782&format=png&auto=webp&s=bc7735f0b4500b9e958de038c8ab66357fe6feb3\n\nThis could be interesting. I am definitely looking forward playing with it when it gets integrated into ComfyUI.', 'Hay Mr Kijai  save us', ""I wish it didn't use the SDXL VAE..."", '[deleted]']","['Model download\n\n\nhttps://huggingface.co/AIDC-AI/Ovis-U1-3B\n\n\nThe project is released under Apache License 2.0\n\n\nI am not the owner of it', 'Try it here\n\nhttps://huggingface.co/spaces/AIDC-AI/Ovis-U1-3B', '>Building on the foundation of the Ovis series, Ovis-U1 is a 3-billion-parameter unified model that seamlessly integrates\xa0**multimodal understanding**,\xa0**text-to-image generation**, and\xa0**image editing**\xa0within a single powerful framework.\n\nhttps://preview.redd.it/uww4kg5g6zaf1.png?width=2782&format=png&auto=webp&s=bc7735f0b4500b9e958de038c8ab66357fe6feb3\n\nThis could be interesting. I am definitely looking forward playing with it when it gets integrated into ComfyUI.', 'Hay Mr Kijai  save us', ""I wish it didn't use the SDXL VAE...""]",81,12,0.92,Resource - Update,1751658696.0
1lrrdyi,StableDiffusion,Can we take a moment to appreciate how insane Flux Kontext dev is?,"Just wanted to drop some thoughts because I‚Äôve been seeing some people throwing shade at Flux Kontext dev and honestly‚Ä¶ I don‚Äôt get it.

I‚Äôve been messing with AI models and image gen since late 2022. Back then, everything already felt like magic, but it was kinda hard to actually gen/edit images the way I wanted. You‚Äôd spend a lot of time inpainting, doing weird workarounds, or just Photoshopping it by hand.

And now‚Ä¶ we can literally prompt edits. Like, ‚Äúoh, I want to change this part‚Äù and boom, the model can just do it (most of the time lol). Sure, sometimes you still need to do some manual touch-ups, upscaling, or extra passes, but man, the fact we can even *do this* locally on our PCs is just surreal to me.

I get that nothing‚Äôs perfect, but some posts I see like ‚Äúehh, Kontext dev kinda sucks‚Äù really make me stop and go: bro‚Ä¶ this is crazy tech. We‚Äôre living in a timeline where this stuff is just available to us.

Anyway, I‚Äôm super grateful for the devs behind Flux Kontext. It‚Äôs an incredible tool and it‚Äôs made image gen and editing even more fun!","[""Kontext can do impressive things - but fails impressively on other tasks where you'd think it should be easy doable.\n\nE.g. watermark removal is surprising good (except that the whole image got soft).\n\nAnd face swap isn't working.\n\nI'm pretty sure that LoRAs will appear to fix these things."", 'To me this all feels like the 90s and the computer and internet world.  Everything was moving fast and was surprisingly hard to get things working a lot of the time and there was all sorts of space and energy for hobbyist creators to make sites and tools.  So much overlap of competing effort and so many things that didn\'t quite play well together and so it often felt like a maze.\n\nI remember scouring through magazines, computer stores, and the wholly inadequate online sites to find things like new tools I could plug into Visual Basic for building end user interfaces.  Things like calendar tools or text display controls with extra functionality than the base ones.\n\nWith AI images and video it has a lot of the same feel but now we have all sorts of social media/collaborative tools to get info, like here on Reddit or Huggingface.  However, that just means that the info and option availiblity is 100x the size and the learning curve for newer people like myself can be pretty steep.  When I open some workflows in Comfy my jaw drops, and the vaunted ""don\'t worry it will download the nodes for you!"" almost never works because things get out-of-date or moved so fast.\n\nFun times.  Enjoy this kind of exciting, creative energy because in time it will become much better...but also much more commercial and corporate and with locked down access and censorship.', 'I tried it for a bit and it was neat.  Mostly it seemed to basically just replace the background and keep the subject the same but lower resolution - OR - it would change the subject to not the subject. I‚Äôm sure I‚Äôm using it wrong but I don‚Äôt get the hype.', ""Sorry mate, it can't even change a face with another random face. Not really impressed with the current state of things, inpaint and control nets are still king. Sure Kontext is a convenient package but if it works 20% of the time, what's the point?"", ""It works fine for low res images but when you try a higher res image it just  lowers it so for me it's pointless,flux dev fill works better"", 'So far im dissapointed. I just get lots of blur with higher quality images. The stitching and combining feels like a gambling game and and sometimes it just does nothing I guess when you trigger its filter', 'How can i appreciate anything censored???', 'Sometimes, it is a matter of expectations.\n\nSome people read the original announcements and posts (which are based on Kontext Max/Pro) and got all excited about it.  When Kontext-Dev does not live up to their expectations, they quickly soured on it.\n\nKontext-Dev is finicky and does not always work, I admit that.  But I also see a lot of potential, specially the ability to train custom editing LoRAs for it.  My main problem is my own lack of imagination/creativity to come up with such idea.  Some of the most obvious and useful ideas such as watermark removal are built in.  NSFW ideas such as cloth removal is obvious and is in fact quite trivial to do (i.e., generating the training set is very easy).\n\nBut at least I will be training some artistic LoRAs for it, once tensor support Kontext training.', ""Don't make a comment after using it for one or two times. You will know if you test it more. This thing often doesn't work. It works at the beginning, but it doesn't work again. I looked at the preview when it was generated and found that it just synthesized the steps into the model. Some pictures can be done, and some pictures have the same steps and workflow, and even the prompt words have not been changed, but it just doesn't work\\~\\~ I don't understand why this thing is so unstable? ? In addition, almost all pictures are lossy after editing, and the colors don't match, there are seams"", 'https://preview.redd.it/7pt5adkvxwaf1.jpeg?width=1200&format=pjpg&auto=webp&s=7ddc57d6aa319aba066fdf939efced0a4a038956\n\nLooks like a million years ago.']","[""Kontext can do impressive things - but fails impressively on other tasks where you'd think it should be easy doable.\n\nE.g. watermark removal is surprising good (except that the whole image got soft).\n\nAnd face swap isn't working.\n\nI'm pretty sure that LoRAs will appear to fix these things."", ""It works fine for low res images but when you try a higher res image it just  lowers it so for me it's pointless,flux dev fill works better"", ""Sorry mate, it can't even change a face with another random face. Not really impressed with the current state of things, inpaint and control nets are still king. Sure Kontext is a convenient package but if it works 20% of the time, what's the point?"", 'To me this all feels like the 90s and the computer and internet world.  Everything was moving fast and was surprisingly hard to get things working a lot of the time and there was all sorts of space and energy for hobbyist creators to make sites and tools.  So much overlap of competing effort and so many things that didn\'t quite play well together and so it often felt like a maze.\n\nI remember scouring through magazines, computer stores, and the wholly inadequate online sites to find things like new tools I could plug into Visual Basic for building end user interfaces.  Things like calendar tools or text display controls with extra functionality than the base ones.\n\nWith AI images and video it has a lot of the same feel but now we have all sorts of social media/collaborative tools to get info, like here on Reddit or Huggingface.  However, that just means that the info and option availiblity is 100x the size and the learning curve for newer people like myself can be pretty steep.  When I open some workflows in Comfy my jaw drops, and the vaunted ""don\'t worry it will download the nodes for you!"" almost never works because things get out-of-date or moved so fast.\n\nFun times.  Enjoy this kind of exciting, creative energy because in time it will become much better...but also much more commercial and corporate and with locked down access and censorship.', 'How can i appreciate anything censored???']",210,90,0.85,Discussion,1751657696.0
1lrqa6g,StableDiffusion,How come openpose always generates black images for me?,,"[""I'm trying to recreate this pose from https://www.pixiv.net/en/artworks/122806305 but I can't get controlnet to generate the pose"", 'Have you tried with AIO Aux preprocessor node instead? You have multiple controlnet models there, including openpose', ""Sometimes that's just how it is. It doesn't detect it. Perhaps it considers it all a background or something.""]","[""I'm trying to recreate this pose from https://www.pixiv.net/en/artworks/122806305 but I can't get controlnet to generate the pose"", 'Have you tried with AIO Aux preprocessor node instead? You have multiple controlnet models there, including openpose', ""Sometimes that's just how it is. It doesn't detect it. Perhaps it considers it all a background or something.""]",1,10,0.54,Question - Help,1751654810.0
1lrplle,StableDiffusion,How Can I Swap My Face Into an AI Image Accurately?,"I generated an AI image and I‚Äôd like to replace the face in it with my own, while keeping my facial features accurate and realistic. Is there an easy way to do this? I‚Äôve tried using GPT‚Äôs image generation, but it struggles to get my face right and keep everything consistent","[""The best way is to create or pay someone to create a LORA of you from 20 or so photos of you. You can then inpaint with that LORA to replace the character's head in the AI image with your own and it will produce accurate and realistic results from all angles.\n\nIf you don't have a powerful PC to do it yourself you can use online services like CivitAI to have the LORA made and to do the face swaps with it on there.\n\nA less accurate option is to use faceswappers (either online or locally like FaceFusion) but this won't get the headshape and hairline etc right for you, which is important in getting a likeness accurate."", 'Train a LoRA with 15 - 20 images: [https://youtu.be/-L9tP7\\_9ejI?si=4A9EabhLvHVnPX9T](https://youtu.be/-L9tP7_9ejI?si=4A9EabhLvHVnPX9T)', 'https://github.com/Gourieff/ComfyUI-ReActor', 'Dm me I can help you']","[""The best way is to create or pay someone to create a LORA of you from 20 or so photos of you. You can then inpaint with that LORA to replace the character's head in the AI image with your own and it will produce accurate and realistic results from all angles.\n\nIf you don't have a powerful PC to do it yourself you can use online services like CivitAI to have the LORA made and to do the face swaps with it on there.\n\nA less accurate option is to use faceswappers (either online or locally like FaceFusion) but this won't get the headshape and hairline etc right for you, which is important in getting a likeness accurate."", 'Train a LoRA with 15 - 20 images: [https://youtu.be/-L9tP7\\_9ejI?si=4A9EabhLvHVnPX9T](https://youtu.be/-L9tP7_9ejI?si=4A9EabhLvHVnPX9T)', 'https://github.com/Gourieff/ComfyUI-ReActor', 'Dm me I can help you']",0,4,0.42,Question - Help,1751653045.0
1lrowce,StableDiffusion,My first try at making an autoregressive colorizer model,"Hi everyone,
This is my first try ever on making an autoregressive (sort of) AI model that can colorize any 2D lineart image.

For now, it has only trained for a small amount of time and only works on \~4 specific images I have. Maybe when I have time and money, I'll try to expand it with a larger dataset (and see if it'll work).","['First test with 500 image pairs, validate your results without using training images. And then you can think of scaling it.', 'Never test your model on dataset used to train them, it means literally nothing.', 'Man, if that thing can colorize all the manga it will be a beast', 'Update: On new tested images, it gives me a full flat purple image.', '90s internet vibes.', 'This is really cool, expand your data set and keep going!', 'Can you explain more? How do you do that, and what kind of model you are using?', ""i don't understand what you wrote, but i like what i see"", ""New Update as of 05/07/25: Now images not in dataset starts colorizing but it show too much artifacts like blurriness.\n\nI think I'm missing a lot here because the training doesn't improve that much despite feeding the model with 100 new pairs."", 'I am making a new t2i model and would love to talk to you about stuff']","['Never test your model on dataset used to train them, it means literally nothing.', 'First test with 500 image pairs, validate your results without using training images. And then you can think of scaling it.', 'Update: On new tested images, it gives me a full flat purple image.', 'Man, if that thing can colorize all the manga it will be a beast', '90s internet vibes.']",431,31,0.96,Discussion,1751651330.0
1lros7q,StableDiffusion,Simpletuner creator is reporting N S F W loras on huggingface and they are being removed. The community needs to look elsewhere to post controversial loras,"There is a Flux Fill link to remove clothes that was on the site several months ago. And today it disappeared.

Until recently it was not common for hugginface to remove anything","['He has a long history of being that type of concern troll: https://github.com/comfyanonymous/ComfyUI/issues/949', ""This guy has blocked access to his discord it seems.\n\nBut not only that, He edited my message on github.\n\nhttps://preview.redd.it/a13f3qhkcxaf1.png?width=606&format=png&auto=webp&s=ae60913142718ccc6182de3874672cfe5b61742c\n\nI didn't say thank you."", ""This is why we can't have nice things"", 'Open source fine tuning tool creator against NSFW how does that even make sense?', 'bro must be fun at parties', 'We need torrents for this kind of stuff, in other cases it will be removed sooner or later.', 'bro has no life', ""There's a few mirror sites here: https://datahoarding.org/archives.html#CivitAI\n\nIf you know of others let me know and we can add them."", 'https://preview.redd.it/uagi60owxxaf1.png?width=1672&format=png&auto=webp&s=5742485a338d8c060e7f72d468a195a2743aeac7\n\nand he keeps doing it.\n\nalso his repo on hugginface, he probably deleted or hid it because there were people commenting about it.  \n[https://huggingface.co/bghira/simpletuner](https://huggingface.co/bghira/simpletuner)', 'Explain yourself u/terminusresearchorg\n\nEdit: they blocked me']","['bro must be fun at parties', 'Open source fine tuning tool creator against NSFW how does that even make sense?', 'He has a long history of being that type of concern troll: https://github.com/comfyanonymous/ComfyUI/issues/949', ""This guy has blocked access to his discord it seems.\n\nBut not only that, He edited my message on github.\n\nhttps://preview.redd.it/a13f3qhkcxaf1.png?width=606&format=png&auto=webp&s=ae60913142718ccc6182de3874672cfe5b61742c\n\nI didn't say thank you."", ""I really don't understand why he is wasting his time on this. Does he work for hugginface or black forest labs?""]",541,273,0.94,Discussion,1751651041.0
1lrop5e,StableDiffusion,How to use ai to add to a video?,"Hello all,

I am wondering if there is a way to use stable diffusion to add items to a video that i recorded? Specifically, i am trying to add shades rolling down in front of my house window in a couple different colours to see a general idea for what they would look like. If this is possible, where would i start? I do have a beefy gpu and have knowledge of coding if that makes a difference.

Thanks in advance for your replies!","[""Try searching for video to video or v2v workflows, I guess.  Though for your example, you can probably get away with a still image or a start/stop image and an i2v workflow.\n\n> I do have a beefy gpu\n\nYou're on the bleeding edge to do 5 secs of 720p on a 5090.""]","[""Try searching for video to video or v2v workflows, I guess.  Though for your example, you can probably get away with a still image or a start/stop image and an i2v workflow.\n\n> I do have a beefy gpu\n\nYou're on the bleeding edge to do 5 secs of 720p on a 5090.""]",0,1,0.33,Question - Help,1751650833.0
1lrobrd,StableDiffusion,What is her name?,"https://preview.redd.it/qx664pgo6waf1.png?width=768&format=png&auto=webp&s=9fea32925ba0c1e30de19563163a47986e77cde0

","['1girl, blonde, floral dress, no legs.', 'AND HER NAME IS JOOOOOHN CEEEEENAAAA !!!!!!', 'darude sandstorm\n\nseriously though, posts like these should get you culled from society', 'never goon', 'Hi, my name is, what? My name is, who?\n\nMy name is, chka-chka, Slim Shady\n\nHi, my name is, huh? My name is, what?\n\nMy name is, chka-chka, Slim Shady']","['1girl, blonde, floral dress, no legs.', 'AND HER NAME IS JOOOOOHN CEEEEENAAAA !!!!!!', 'darude sandstorm\n\nseriously though, posts like these should get you culled from society', 'never goon', 'Hi, my name is, what? My name is, who?\n\nMy name is, chka-chka, Slim Shady\n\nHi, my name is, huh? My name is, what?\n\nMy name is, chka-chka, Slim Shady']",0,8,0.05,Discussion,1751649895.0
1lrnt9h,StableDiffusion,Flux Kontext is not working for this image,"https://preview.redd.it/hrkmn5hk2waf1.png?width=1637&format=png&auto=webp&s=30beb56448c54664c9341085e538477f987d0b4b

How can i solve it? ","['https://preview.redd.it/ne1wb9q29waf1.jpeg?width=1542&format=pjpg&auto=webp&s=c9b71da9c154ed1786ee11068eb51834c667c079\n\nKontext is absolutely unpredictable. This prompt works: ""remove the person from the left.""', ""Kontext is not something predictable, you can't know what would work with a certain image without testing it first, probably inpainting is the best solution, it is also not possible to understand the changes that were made in this workflow, images outside the standard size never help."", 'Try: Remove the person on the left from the photo while maintaining the original composition']","['https://preview.redd.it/ne1wb9q29waf1.jpeg?width=1542&format=pjpg&auto=webp&s=c9b71da9c154ed1786ee11068eb51834c667c079\n\nKontext is absolutely unpredictable. This prompt works: ""remove the person from the left.""', ""Kontext is not something predictable, you can't know what would work with a certain image without testing it first, probably inpainting is the best solution, it is also not possible to understand the changes that were made in this workflow, images outside the standard size never help."", 'Try: Remove the person on the left from the photo while maintaining the original composition']",0,5,0.13,Question - Help,1751648607.0
1lrno3b,StableDiffusion,Krita AI results looks too similar,"i liked the idea of local AI as it would mean I could set it to generate 1000 results then go to bed and see the results in the morning

However I was dissapointed that every generation is so similar. for example if I prompt ""medieval fantasy city at night"" I basically get the exact same hue of green/blue, and the same type of buildings and identical vibe

is there a way to tell it to randomize the results more?","['Write longer prompts. Flowmatching models like flux converge to pretty much the same result if you use short ones or use something like chroma with a low CFG.', ""You could use wildcards and similar to randomize some details about prompts. Otherwise models would generate similar images. In case of Krita AI you'd need to create a custom workflow for this."", 'https://preview.redd.it/j1r78q8wq1bf1.png?width=2188&format=png&auto=webp&s=af955e90f40f874679002154d7d07acc183c090e\n\nPlug a loop into the denoise. This will vary the image... By a lot.']","['Write longer prompts. Flowmatching models like flux converge to pretty much the same result if you use short ones or use something like chroma with a low CFG.', ""You could use wildcards and similar to randomize some details about prompts. Otherwise models would generate similar images. In case of Krita AI you'd need to create a custom workflow for this."", 'https://preview.redd.it/j1r78q8wq1bf1.png?width=2188&format=png&auto=webp&s=af955e90f40f874679002154d7d07acc183c090e\n\nPlug a loop into the denoise. This will vary the image... By a lot.']",0,8,0.2,Question - Help,1751648266.0
1lrngyp,StableDiffusion,Trying to use an upscaling workflow using a nunchaku based FLUX model (Works great on low vram and it outputs 4K images + Workflow included),"The workflow : https://drive.google.com/file/d/1Vc00eMbB3xZXO6PLnc6a6_yDM4ebszgp/view?usp=sharing

The model : https://civitai.com/models/1545303?modelVersionId=1861654

The upscaler : https://huggingface.co/uwg/upscaler/blob/main/ESRGAN/4x_NMKD-Siax_200k.pth","['the chin ass seems to be intensified as well, thanks for the workflow by the way.', 'oof she got hit in the chin by flux HARD \n\nhttps://preview.redd.it/r97ttj970xaf1.png?width=200&format=png&auto=webp&s=5022d000c5f94629e88bcb91c90ead0e85409431', '""Johny Silverarm""', 'Does the ultimate upscale work on nunchaku yet?', 'I want a normal upscale to like 20k px and above, what do you recommend?', 'isnt SVD a video model?', 'What do you mean by LOW RAM?  \nCause I have only 4 GB VRAM and it might interest me.']","['the chin ass seems to be intensified as well, thanks for the workflow by the way.', 'oof she got hit in the chin by flux HARD \n\nhttps://preview.redd.it/r97ttj970xaf1.png?width=200&format=png&auto=webp&s=5022d000c5f94629e88bcb91c90ead0e85409431', '""Johny Silverarm""', 'Does the ultimate upscale work on nunchaku yet?', 'I want a normal upscale to like 20k px and above, what do you recommend?']",40,13,0.83,Tutorial - Guide,1751647770.0
1lrn7ta,StableDiffusion,Any one having trouble wit Kontext using SWARMUI,"I have SWARMui installed with the comfi backend. As far as i know, it all works fine, i can generate images etc. When it comes to Kontext, though, neither the FP8 or DEV16 version pay attention to my prompts. Load initial image, etc, type prompt just not getting the great results i see on the sub What settings are everyone using inside swarm? ","['Did you set the metadata of the model to Flux Kontext? It has to be set manually in swarmUI, in the small drop-down menu that appears next to the model (and edit metadata)']","['Did you set the metadata of the model to Flux Kontext? It has to be set manually in swarmUI, in the small drop-down menu that appears next to the model (and edit metadata)']",0,2,0.4,Question - Help,1751647131.0
1lrmnfj,StableDiffusion,"I built a GUI tool for FLUX LoRA manipulation - advanced layer merging, face and style pre-sets, subtraction, layer zeroing, metadata editing and more.  Tried to build what I wanted, something easy.","Hey everyone,

I've been working on a tool called LoRA the Explorer - it's a GUI for advanced FLUX LoRA manipulation. Got tired of CLI-only options and wanted something more accessible.

**What it does:**

* Layer-based merging (take face from one LoRA, style from another)
* LoRA subtraction (remove unwanted influences)
* Layer targeting (mute specific layers)
* Works with LoRAs from any training tool

**Real use cases:**

* Take facial features from a character LoRA and merge with an art style LoRA
* Remove face changes from style LoRAs to make them character-neutral
* Extract costumes/clothing without the associated face (Gandalf robes, no Ian McKellen)
* Fix overtrained LoRAs by replacing problematic layers with clean ones
* Create hybrid concepts by mixing layers from differnt sources

The demo image shows what's possible with layer merging - taking specific layers from different LoRAs to create someting new.

It's free and open source. Built on top of kohya-ss's sd-scripts.

GitHub: [github.com/shootthesound/lora-the-explorer](https://github.com/shootthesound/lora-the-explorer)

Happy to answer questions or take feedback. Already got some ideas for v1.5 but wanted to get this out there first.

Notes: I've put a lot of work into edge cases! Some early flux trainers were not great on metadata accuracy, I've implemented loads of behind the scenes fixes when this occurs (most often in the Merge tab).  If a merge fails, I suggest trying concat mode (tickbox on the gui).

The merge failures are FAR less likely on the Layer merging tab, as this technique extracts layers and inserts into a new lora in a different way, making it all the more robust.  I may for version 1.5, harness an adaption of this technique for the regular merge tool.  But for now I need sleep and wanted to get this out!","[""Thank you SO much for explaining a bit of block and layer architecture in your GitHub post. I've always known what block/layer targeting could do for LoRAs and generations, but never understood which layers needed to be targeted to achieve a given goal. \n\nI don't know why, but reading through your post made it click for me. Thank you greatly for your contributions. I'm looking forward to trying this out today!"", 'That sounds great! Is it possible to do the same for Illustrious or Pony? If possible, could you make a video explaining and showing how things will work? That would help us understand it better.', ""Quick demo clip, - the Loras in the video are linked in app and on the GitHub.  I'll do a better video when I'm not in look after the kids mode!\n\n[https://www.dropbox.com/scl/fi/h3cclw6houcdwrp48knvw/LoraTheExplorer.webm?rlkey=llevswi4osrspln80pykzf270&st=mb4jhh42&dl=0](https://www.dropbox.com/scl/fi/h3cclw6houcdwrp48knvw/LoraTheExplorer.webm?rlkey=llevswi4osrspln80pykzf270&st=mb4jhh42&dl=0)"", ""Ran into an issue using the bat for installation, and I can't find the particular function in the code either:\n> ‚ùå Installation failed: 'charmap' codec can't encode character '\\U0001f9ed' in position 16: character maps to <undefined>\n\nThe odd part is that it occurred after getting a success message that lora-the-explorer was successfully installed. Running the bat fails for security reasons on W11 (with no way to override in the popup dialogue) but running the python launch command in terminal notifies me that sd-scripts was not installed. \n\nI should have a version of that on my computer from a kohya install, so what's my options here? Is there a fix to be made, a manual install, or can I link my existing sd-scripts location?"", 'Gj! Does it work with loras trained with AI-Toolkit?', ""does this mean you can combine character lora and nsfw lora and make img without distortion like few nsfw loras that don't affect face?"", ""So help me understand the Layer Targeting tool. With the presets guidance, I was working under the assumption that Facial Layers preset would preserve (not remove) facial details while discarding the rest of the information. The sidebar seems to have a conflicting opinion on this, citing that the facial layers *removes* the face while keeping style/costume and yet is ideal for character loras. \n\nTrying it, it seems to have been a description snafu because the facial layers preset does indeed retain training detail on the face. Or maybe I've encountered a placebo effect somehow in testing the output. \n\nWhen selecting the layers on this page and generating a lora result, what am I looking at? A lora with those selected layers removed or a lora with *only* those layers?"", 'Did you do more tests with the person / face specific layers? I remember a thread that demonstrated 7/12/16/20 but I am quite certain that those were not ALL layers if you want to keep the identity absolutely intact.', 'A lot of bug fixes overnight, including adding an update.bat file etc.  Either do a manual git pull or reinstall in a fresh folder to make updating easy in future.', 'Can you imagine how a bird man would be actually dumb and pecking at everything shiny lol']","[""Thank you SO much for explaining a bit of block and layer architecture in your GitHub post. I've always known what block/layer targeting could do for LoRAs and generations, but never understood which layers needed to be targeted to achieve a given goal. \n\nI don't know why, but reading through your post made it click for me. Thank you greatly for your contributions. I'm looking forward to trying this out today!"", 'That sounds great! Is it possible to do the same for Illustrious or Pony? If possible, could you make a video explaining and showing how things will work? That would help us understand it better.', ""Quick demo clip, - the Loras in the video are linked in app and on the GitHub.  I'll do a better video when I'm not in look after the kids mode!\n\n[https://www.dropbox.com/scl/fi/h3cclw6houcdwrp48knvw/LoraTheExplorer.webm?rlkey=llevswi4osrspln80pykzf270&st=mb4jhh42&dl=0](https://www.dropbox.com/scl/fi/h3cclw6houcdwrp48knvw/LoraTheExplorer.webm?rlkey=llevswi4osrspln80pykzf270&st=mb4jhh42&dl=0)"", ""Ran into an issue using the bat for installation, and I can't find the particular function in the code either:\n> ‚ùå Installation failed: 'charmap' codec can't encode character '\\U0001f9ed' in position 16: character maps to <undefined>\n\nThe odd part is that it occurred after getting a success message that lora-the-explorer was successfully installed. Running the bat fails for security reasons on W11 (with no way to override in the popup dialogue) but running the python launch command in terminal notifies me that sd-scripts was not installed. \n\nI should have a version of that on my computer from a kohya install, so what's my options here? Is there a fix to be made, a manual install, or can I link my existing sd-scripts location?"", 'Gj! Does it work with loras trained with AI-Toolkit?']",56,33,0.91,Resource - Update,1751645678.0
1lrl7ht,StableDiffusion,Ignorant question: how Flix Kontext Lora are trained ?,I never trained a Lora but AFAIK you need to collect a dataset of image-prompt pairs. How is the story is different in kontext ? What I mean is the dataset contains a tuple of three component (input image + prompt + result) instead of a pair?,"['Control image and target image with a prompt', 'from the maker of ai toolkit himself\nhttps://www.youtube.com/watch?v=WSWubJ4eFqI', '[First test using Kontext Dev Lora trainer : r/FluxAI](https://www.reddit.com/r/FluxAI/comments/1lmgcov/first_test_using_kontext_dev_lora_trainer/)', 'you dont need to do before/after training for kontext. you can also just train normal like you would any normal style or outfit.']","['Control image and target image with a prompt', 'from the maker of ai toolkit himself\nhttps://www.youtube.com/watch?v=WSWubJ4eFqI', '[First test using Kontext Dev Lora trainer : r/FluxAI](https://www.reddit.com/r/FluxAI/comments/1lmgcov/first_test_using_kontext_dev_lora_trainer/)', 'you dont need to do before/after training for kontext. you can also just train normal like you would any normal style or outfit.']",0,6,0.25,Discussion,1751642097.0
1lrjjnh,StableDiffusion,REQUESTED TUTORIAL FOR AI STORY SHORTS FOR FREE,"Recently i have uploaded a video on these community [https://www.reddit.com/r/aivideo/comments/1lpua34/ai\_story\_for\_kids\_suggestions/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/aivideo/comments/1lpua34/ai_story_for_kids_suggestions/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)

Where people asked me what is the procedure of it so these is the best tutorial for it [https://youtu.be/Od-Y4Mt2Ung](https://youtu.be/Od-Y4Mt2Ung)   ",[],[],0,0,0.22,Discussion,1751637858.0
1lrixrr,StableDiffusion,How to make that graphics?,,"['Chatgpt image generator\xa0', 'just use Flux Kontext in comfy, give it the start image and tell it what you want the next image to be.']","['Chatgpt image generator\xa0', 'just use Flux Kontext in comfy, give it the start image and tell it what you want the next image to be.']",0,9,0.11,Question - Help,1751636220.0
1lria24,StableDiffusion,Anything better than Lustify for naughties?,Lustify is decent wondered if anyone has other recommendations for adult stuff?,"[""Wait for the new version of Lustify, I'm working on it currently. A lot of reworks are being done"", ""Chroma is quickly becoming my Lustify replacement.\n\nIt can do anything Lustify does but output higher visual aesthetic with creative variety and excellent prompt adherence.\n\nDownside is the model is still in training so you get weird seeds (every 5 generations one horror result) and it's quite slow because it uses Flux with negative prompt (2x slower than normal flux)\n\nYou can however speed it up with sage attention, torch compile"", 'Big Love and it has tons of custom loras', 'My  [Jib Mix Illustrious ](https://civitai.com/models/1255024/jib-mix-illustrious-realistic)model defaults to NSWF. but my [Jib Mix Realistic SDXL](https://civitai.com/models/194768/jib-mix-realistic-xl) has better details but can also do NSFW but is not as good at complex NSFW poses as Illustrious.\n\nhttps://preview.redd.it/n337gcfh8vaf1.png?width=2249&format=png&auto=webp&s=b3815a5a6e0dea037fe62b610340ef16291de57c', 'Started using Chroma and GGUF Q8 in forge UI - it is good: [https://youtu.be/-H5WXCQiVjk?si=tpPyT-aOJP1TnVXm](https://youtu.be/-H5WXCQiVjk?si=tpPyT-aOJP1TnVXm)', ""Lustify is great but frankly I wouldn''t rely on just one model, they all have different flavours.\n\n  \nMust have's in terms of realism including Lustify are BigASP and P\\*\\*nmaster Pro/Amateur models by iamddtla on civitai. BigLove is good at prompt adherence and very specific things but lacks on Realism.\n\nAll are superb but will give you different flavours."", 'BigLust is my number followed by my custom merge Pyros + BigAsp2', ""Wow thanks for the replies, just tried Chroma only got an 8GB card followed that Youtube Nosleep posted very usefull! Its very slow but really nice quality, one day I'll bite the bullet and get a 16gb card but my trusty 2070 super keeps chugging away lol."", 'Pony with flux inpainting. Thank me later', 'You could try [https://civitai.com/models/277058/epicrealism-xl](https://civitai.com/models/277058/epicrealism-xl)\n\nI use it for most of my stuff.']","[""Wait for the new version of Lustify, I'm working on it currently. A lot of reworks are being done"", ""Chroma is quickly becoming my Lustify replacement.\n\nIt can do anything Lustify does but output higher visual aesthetic with creative variety and excellent prompt adherence.\n\nDownside is the model is still in training so you get weird seeds (every 5 generations one horror result) and it's quite slow because it uses Flux with negative prompt (2x slower than normal flux)\n\nYou can however speed it up with sage attention, torch compile"", 'My  [Jib Mix Illustrious ](https://civitai.com/models/1255024/jib-mix-illustrious-realistic)model defaults to NSWF. but my [Jib Mix Realistic SDXL](https://civitai.com/models/194768/jib-mix-realistic-xl) has better details but can also do NSFW but is not as good at complex NSFW poses as Illustrious.\n\nhttps://preview.redd.it/n337gcfh8vaf1.png?width=2249&format=png&auto=webp&s=b3815a5a6e0dea037fe62b610340ef16291de57c', 'Big Love and it has tons of custom loras', 'Started using Chroma and GGUF Q8 in forge UI - it is good: [https://youtu.be/-H5WXCQiVjk?si=tpPyT-aOJP1TnVXm](https://youtu.be/-H5WXCQiVjk?si=tpPyT-aOJP1TnVXm)']",84,106,0.87,Question - Help,1751634335.0
1lrgs0m,StableDiffusion,"Omni Avatar looking pretty good - However, this took 26 minutes on an H100","This looks very good imo for open source, this is using the Wan 14B model with 30 steps and 720P resolution.
","['What a fantastic freeze‚Äëframe backdrop we‚Äôve got here!', 'lol stop complaining guys.. every new thing kinda sucks at first. And this one at least seems to add sound!', 'I urgently need skilled sound engineers; all the voices currently sound overly polished, as if they were created in a studio‚Äîno background sounds, no echo, nothing natural.', 'Jesus people are so god damned picky all of a sudden. I thought it was quite realistic for open source.', 'It looks great. Sure, the background is static, but nobody would notice that if they saw a 4s clip once.\n\nWhat kills it for me is the voice. It sounds reasonably human. But like a human voice actress, not like a human speaking to me or like a human making influencer content', '26 minutes on an H100‚Ä¶', 'People complaining about the background, just rotoscope her out and put the overlay woman over a background video from real life or generated separately\n\nThe bigger issue is the shit speed', 'Yeah, this is very good quality, but the price is too high. What chances this will get more efficient?', 'Did you use any accelerators like in the github mentioned? [FusioniX](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/blob/main/FusionX_LoRa/Wan2.1_I2V_14B_FusionX_LoRA.safetensors) and [lightx2v](https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_T2V_14B_lightx2v_cfg_step_distill_lora_rank32.safetensors) LoRA acceleration? Tea Cache?', 'Reminds me of Hunyuan Video Avatar at the start, where she overly exaggerates the ""Hi!"" with her face. I don\'t know why they tend to do that, but they end up looking like chickens pecking at feed. Other than that, the facial animation isn\'t bad really.']","['What a fantastic freeze‚Äëframe backdrop we‚Äôve got here!', 'lol stop complaining guys.. every new thing kinda sucks at first. And this one at least seems to add sound!', 'Jesus people are so god damned picky all of a sudden. I thought it was quite realistic for open source.', 'It looks great. Sure, the background is static, but nobody would notice that if they saw a 4s clip once.\n\nWhat kills it for me is the voice. It sounds reasonably human. But like a human voice actress, not like a human speaking to me or like a human making influencer content', 'I urgently need skilled sound engineers; all the voices currently sound overly polished, as if they were created in a studio‚Äîno background sounds, no echo, nothing natural.']",207,85,0.81,Discussion,1751629695.0
1lrgre3,StableDiffusion,"In 4k video restoration and upscale , open-source tech has now outperformed proprietary, paid alternatives.","In my free time, I usually try to replicate closed-source AI technology. Due to work requirements, I am currently researching video super-resolution and restoration. On the most difficult old TV series ""Journey to the West"" to super-resolution and restore, I tried 7 or 8 different methods, and finally found that the open source effect after fine-tuning is really good, and it is much better than the strongest topaz in character consistency, noise reduction, and image restoration.","['And which open source model did you use?', 'https://preview.redd.it/xlxe9h87evaf1.png?width=566&format=png&auto=webp&s=61e8414260dca17c94319e9a1c2e8abe3c805f21', 'Not revealing how you did that in an open source community is akin to marketing closed source alternatives. Hence, this will be downvoted.', 'Is ""Open Source"" the brand name? Because I don\'t see anything open source in the post.', 'the open source one looks horrdenus, like early days of AI', ""Idk there's a blurry line (hehe) between restoration and reconstruction.\n\nI know it's for personal use, the source is obvisouly an old dvd not scanned reels but still, in cinema not only sharpening isn't a necessity for 99% of the shots but altering the picture that much is a big no no if the director and probably a director of photography doesn't give the green light.\n\nCleaning up some shots is fine but I personnally think it's overdone akin to vfx at that point.\n\nThe unknown open source project is for whoever while Topaz is aiming at the pro market (and fails but that's besides the point, they don't like AI or anything that alters the filmmaker's intent or hinder any sort of creative liberty like autofocus for exemple) that's the reason it has this look. First time I read about Topaz starlight, it looks like shit tho,"", 'Looks good!\nwe need more open source upscalers for video,I been using waifu2x, which is decent', 'Open source models on most tech can achieve great results because.. its open source and people pool their knowledge.  But it usually requires a lot of work. And when it comes down to it, no one wants to do the work.', ""And all three look bad, I'd take an old blurry video over some AI scaled uncanny valley crap"", ""I'd dispute it beating topaz, I know a number of eggheads who have been trying and yet to see it happen for real and they have tweaked every thing that could be tweaked. not saying never, but I am chin scratching to believe you have achieved it and this example is...well... it is an example of something happening.\n\nfeel free to share the workflow.\n\nEDIT: I see you did and it is the workflow from the example directory. I rest my case.\n\nIf it was me, once I realised people here actually know what is going on, I would quietly delete this post and pretend I never made it.""]","['And which open source model did you use?', 'https://preview.redd.it/xlxe9h87evaf1.png?width=566&format=png&auto=webp&s=61e8414260dca17c94319e9a1c2e8abe3c805f21', 'Not revealing how you did that in an open source community is akin to marketing closed source alternatives. Hence, this will be downvoted.', 'Is ""Open Source"" the brand name? Because I don\'t see anything open source in the post.', ""Idk there's a blurry line (hehe) between restoration and reconstruction.\n\nI know it's for personal use, the source is obvisouly an old dvd not scanned reels but still, in cinema not only sharpening isn't a necessity for 99% of the shots but altering the picture that much is a big no no if the director and probably a director of photography doesn't give the green light.\n\nCleaning up some shots is fine but I personnally think it's overdone akin to vfx at that point.\n\nThe unknown open source project is for whoever while Topaz is aiming at the pro market (and fails but that's besides the point, they don't like AI or anything that alters the filmmaker's intent or hinder any sort of creative liberty like autofocus for exemple) that's the reason it has this look. First time I read about Topaz starlight, it looks like shit tho,""]",216,70,0.76,Discussion,1751629636.0
